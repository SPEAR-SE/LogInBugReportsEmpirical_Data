{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13036412","self":"https://issues.apache.org/jira/rest/api/2/issue/13036412","key":"HIVE-15671","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340269","id":"12340269","name":"2.3.0","archived":false,"released":true,"releaseDate":"2017-07-18"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-01-20T03:30:05.809+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Feb 13 19:08:33 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_191097_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_2132127447","customfield_12312321":null,"resolutiondate":"2017-02-13T19:08:33.921+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15671/watchers","watchCount":7,"isWatching":false},"created":"2017-01-20T02:49:55.438+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329363","id":"12329363","name":"1.1.0","archived":false,"released":true,"releaseDate":"2015-03-07"}],"issuelinks":[{"id":"12495892","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12495892","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13047177","key":"HIVE-16071","self":"https://issues.apache.org/jira/rest/api/2/issue/13047177","fields":{"summary":"HoS RPCServer misuses the timeout in its RPC handshake","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12494145","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12494145","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13042651","key":"HIVE-15893","self":"https://issues.apache.org/jira/rest/api/2/issue/13042651","fields":{"summary":"Followup on HIVE-15671","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-21T18:36:02.974+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12323200","id":"12323200","name":"Spark","description":"Hive on Spark"}],"timeoriginalestimate":null,"description":"{code}\n  /**\n   * Tells the RPC server to expect a connection from a new client.\n   * ...\n   */\n  public Future<Rpc> registerClient(final String clientId, String secret,\n      RpcDispatcher serverDispatcher) {\n    return registerClient(clientId, secret, serverDispatcher, config.getServerConnectTimeoutMs());\n  }\n{code}\n\n{{config.getServerConnectTimeoutMs()}} returns value for *hive.spark.client.server.connect.timeout*, which is meant for timeout for handshake between Hive client and remote Spark driver. Instead, the timeout should be *hive.spark.client.connect.timeout*, which is for timeout for remote Spark driver in connecting back to Hive client.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12848640","id":"12848640","filename":"HIVE-15671.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T22:38:22.328+0000","size":716,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12848640/HIVE-15671.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12848453","id":"12848453","filename":"HIVE-15671.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T02:53:00.888+0000","size":785,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12848453/HIVE-15671.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"RPCServer.registerClient() erroneously uses server/client handshake timeout for connection timeout","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15831072","id":"15831072","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~vanzin]/[~lirui], could you please review?\ncc: [~csun]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T02:55:12.241+0000","updated":"2017-01-20T02:55:12.241+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15831115","id":"15831115","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"Hmm, the options are poorly named (my fault, and they always confuse me when I look at them now), but the current use looks correct.\n\n\"client.connect.timeout\" is for the connection that the Spark driver opens to HS2.\n\"server.connect.timeout\" is actually used in two places, but is basically the time allowed between HS2 starting the Spark driver, and the SASL handshake to finish.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T03:30:05.809+0000","updated":"2017-01-20T03:30:29.490+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15831131","id":"15831131","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks, [~vanzin]. To confirm, when you say \"the current use looks correct\", do you mean with or without the patch? {{registerClient()}} is called before the remote driver connecting back to HS2. For that, I think it should use client.connect.timeout. However, {{config.getServerConnectTimeoutMs()}} returns the value for *server.connect.timeout*. That's why I think the patch here is needed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T03:45:51.973+0000","updated":"2017-01-20T03:45:51.973+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15831138","id":"15831138","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"I mean the current code without the patch.\n\n{{registerClient()}} is called on the server side; so it basically starts the countdown for the \"SASL handshake\" timeout (which is what \"getServerConnectTimeoutMs()\" and is probably a better name for that method). The client should connect back and authenticate within that timeout.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T03:50:20.977+0000","updated":"2017-01-20T03:50:20.977+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15831178","id":"15831178","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Actually my understanding is a little different. Checking the code, I see:\n1. On server side (RpcServer constructor), saslHandler is set a timeout using {{getServerConnectTimeoutMs()}}.\n2. On client side, in {{Rpc.createClient()}}, saslHandler is also set a timeout using  {{getServerConnectTimeoutMs()}}.\nThese two are consistent, which I don't see any issue.\n\nOn the other hand, \n3. On server side, in {{Repc.registerClient()}}, ClientInfo stores {{getServerConnectTimeoutMs()}}. And, the timeout happens, the exception is TimeoutException(\"Timed out waiting for client connection.\").\n4. On client side, in {{Rpc.createClient()}}, the channel is initialized with {{getConnectTimeoutMs()}}.\n\nTo me, it seems there is mismatch between 3 and 4. In 3, the timeout message implies \"connection timeout\", while the value is what is supposed to be that for saslHandler handshake. This is why I think 3 should use {{getConnectTimeoutMs()}} instead.\n\nCould you take another look?\n\nI actually ran into issues with this. Our cluster is constantly busy, and it takes minutes for the Hive to get a YARN container to launch the remote driver. In that case, the query fails with a failure of creating a spark session. For such a scenario, I supposed we should increase *client.connect.timeout*. However, that's not effective. On the other hand, if I increase *server.connect.timeout*, Hive waits longer  for the driver to come up, which is good. However, doing that has a bad consequence that Hive will wait as long to declare a failure if for any reason the remote driver becomes dead.\n\nWith the patch in place, the problem is solved in both cases. I only need to increase *client.connect.timeout* and keep *server.connect.timeout* unchanged.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T04:31:00.559+0000","updated":"2017-01-20T04:33:13.557+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15831452","id":"15831452","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~xuefuz], I tried your patch locally. {{hive.spark.client.connect.timeout}} defaults to 1000ms. But starting the RemoteDriver can easily take longer than that. Therefore my job just failed with \"Timed out waiting for client connection\".\nI'm quite ignorant about the Rpc code. What I see is we have two timeout configs with different default value. And the specific code here needs the one with the bigger default value. I'd really appreciate it if [~vanzin] could give more detailed explanations about the purposes of the two configs, and whether they're used inconsistently as Xuefu pointed out.\n\nBesides, the naming is really confusing to me, like SparkClient is the RpcServer, and we pass ClientProtocol to serverDispatcher etc. I understand the client/server concepts are probably reversed for HS2/RemoteDriver and Rpc. Wondering if it's better to make it somehow consistent.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-01-20T09:36:02.624+0000","updated":"2017-01-20T09:36:02.624+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15832159","id":"15832159","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"[~xuefuz] I see what you mean, but I think your analysis is slightly off.\n\n1 and 2 are actually where the problem, if any, is; 2 should use {{getConnectTimeoutMs()}} instead of the server version. As Rui said, the \"server timeout\" here, which is actually the \"authentication timeout\", needs to be much longer than the client timeout since it involves the time to start the driver.\n\nSo basically: all calls made on the client side (= Spark driver) should use {{getConnectTimeoutMs()}}, all calls made on the server side (= HS2) should use {{getServerConnectTimeoutMs()}} (although, if I remember the code correct, the one timeout set up in {{registerClient()}} ends up taking precedence over all others on the server path).\n\n> doing that has a bad consequence that Hive will wait as long to declare a failure if for any reason the remote driver becomes dead\n\nThat's kinda hard to solve, because the server doesn't know which client connected until two things happen: first the driver has started, second the driver completed the SASL handshake to identify itself. A lot of things can go wrong in that time. There's already some code, IIRC, that fails the session if the spark-submit job dies with an error, but aside from that, it's kinda hard to do more.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T17:41:32.390+0000","updated":"2017-01-20T17:41:32.390+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15832245","id":"15832245","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~vanzin], thanks for your insight. I think we are approaching to something. I'm going to change #2 to use {{getConnectTimeoutMs()}} and try it out. Naming is one thing, but yes, the server-side timeout should be bigger. When I tested with my patch, I actually made *client.connect.timeout* much bigger than *server.connect.timeout* and that's why I didn't have the problem that [~lirui] got. \n\n{quote}That's kinda hard to solve, because the server doesn't know which client connected until...{quote}\nMy original problem (with no patch so ever) was about a busy cluster where it took longer time (up to 10m) to get a container to run the driver. To overcome that, I increased *server.connect.timeout* to 10m which worked. With that, however, I got a different problem when the driver suddenly dies (due to OOM, for instance), at which point the driver had already connected back to Hive and the job was running. In such a case, Hive wouldn't detect the driver was gone until 10m later. My patch here was to solve this problem.\n\nWith the new understanding, I'd like to make sure that both the problems are solved: 1. user should be able to increase *server.connect.timeout* to handler longer startup of the driver. 2. Hive should be able to immediately detect the death of the driver (after connection has been made).\n\nAny additional thoughts?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T18:48:21.772+0000","updated":"2017-01-20T18:48:21.772+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15832257","id":"15832257","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. I got a different problem when the driver suddenly dies (due to OOM, for instance) ... Hive wouldn't detect the driver was gone until 10m later.\n\nIf you mean it dies before the SASL handshake is complete, then in that case maybe my understanding that the server timeout applies to the whole connection + handshake is wrong and that should be fixed. i.e. the timeout set up in {{registerClient}} should apply to the whole handshake and not only until there's a connection.\n\nBut if it dies after the SASL handshake, then it seems like the problem is somewhere else and shouldn't really be related to either of these timeouts.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T18:58:22.570+0000","updated":"2017-01-20T18:58:22.570+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15832582","id":"15832582","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Patch #1 followed what [~vanzin] suggested. With it, I observed the following behavior:\n\n1. Increasing *server.connect.timeout* will make hive wait longer for the driver to connect back, which solves the busy cluster problem.\n2. Killing driver while the job is running immediately fails the query on Hive side with the following error:\n{code}\n2017-01-20 22:01:08,235\tStage-2_0: 7(+3)/685\tStage-3_0: 0/1\t\n2017-01-20 22:01:09,237\tStage-2_0: 16(+6)/685\tStage-3_0: 0/1\t\nFailed to monitor Job[ 1] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n{code}\n\nThis meets my expectation.\n\nHowever, I didn't test the case of driver death before connecting back to Hive. (It's also hard to construct such a test case.) In that case, I assume that Hive will wait for *server.connect.timeout* before declaring a failure. I guess there isn't much we can do for this case. I don't think the change here has any implication on this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T23:11:13.650+0000","updated":"2017-01-20T23:12:50.385+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15832749","id":"15832749","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~vanzin], could you please review the patch? Thanks.\n[~lirui], Could you also try and review the patch? Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-21T02:35:23.811+0000","updated":"2017-01-21T02:35:23.811+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15832821","id":"15832821","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12848640/HIVE-15671.1.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10974 tests executed\n*Failed tests:*\n{noformat}\nTestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)\norg.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)\norg.apache.hadoop.hive.cli.TestHBaseNegativeCliDriver.testCliDriver[cascade_dbdrop] (batchId=226)\norg.apache.hadoop.hive.cli.TestHBaseNegativeCliDriver.testCliDriver[generatehfiles_require_family_path] (batchId=226)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=135)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=136)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[limit_pushdown3] (batchId=144)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part] (batchId=149)\norg.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testTaskStatus (batchId=213)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/3085/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/3085/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3085/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 9 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12848640 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-01-21T05:31:25.626+0000","updated":"2017-01-21T05:31:25.626+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15834115","id":"15834115","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~xuefuz], I tried your case but didn't reproduce your issue. Here's my findings (w/o patch):\n# I set {{hive.spark.client.server.connect.timeout}} to 10min and kill the driver during execution of the job. (Hive CLI + yarn-cluster mode)\n# Hive can detect the job failure instantly. But whether the CLI can return instantly (blocking on {{RemoteSparkJobMonitor.startMonitor}}) depends on whether we're in the middle of retrieving job progress from the driver. If we're, CLI needs to wait for {{hive.spark.client.future.timeout}}, default to 1min. If not, CLI returns instantly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-01-23T09:22:27.630+0000","updated":"2017-01-23T09:22:27.630+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15834977","id":"15834977","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"The patch looks ok but I don't know how it relates to the issue you described. This is shortening the timeout on the client side, and you seemed to be concerned about some long timeout on the server side. This patch will just make sessions fail more quickly when the server is in a weird state and is taking long to reply to client messages.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-23T18:13:37.171+0000","updated":"2017-01-23T18:13:37.171+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15835626","id":"15835626","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks, [~lirui] and [~vanzin].\n\nI retried the case that Rui described. Yes, Hive detects the disconnection right after the driver process is killed. I think killing the process will close the socket, so the other end detects the problem right way.\n\nI guess my original problem is not about killing the driver process. Rather, if the connection between Hive and the driver is congested but not broken before the driver exists abnormally, Hive will not detect a broken connection, so it will not time out until *server.connect.timeout* has elapsed. I agree the patch doesn't help this case. I also agree with Marcelo that the patch only makes the driver be more willing to exit if Hive happens to be busy.\n\nLet me step back and recap what I really need. 1. I want Hive to wait longer for the driver to connect back in case of a busy cluster. Increasing *server.connect.timeout* solves the problem. 2. I also want Hive to detect a nonreachable driver early in case of network congestion or abnormal exit. In reality, #2 is also decided by *server.connect.timeout*.\n\nNow, I'm wondering if it's possible to define a timeout for the initial connection (driver connecting back to Hive), and another timeout for subsequent communications between Hive and the driver. Is this possible at all?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-24T04:27:14.258+0000","updated":"2017-01-24T04:27:14.258+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15835669","id":"15835669","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"As a side note, the patch here might be still needed. If Hive is busy or the connection between Hive and the driver is congested, we want the driver to go away quicker too. When this happen in one case, I saw that the driver, not knowing Hive was gone, was still allocating executors and killing them after 60s because they idled out. (This probably doesn't happen that much in normal conditions. I got this because apparently we are having networking issues at the moment.)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-24T05:21:28.243+0000","updated":"2017-01-24T05:21:28.243+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15857738","id":"15857738","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"body":"I may encounter this situation you mentioned. I run a query, Hive on Spark, failed with error:\n2017-02-08 09:50:59,331 Stage-2_0: 1039(+2)/1041        Stage-3_0: 796(+456)/1520       Stage-4_0: 0/2021       Stage-5_0: 0/1009       Stage-6_0: 0/1\n2017-02-08 09:51:00,335 Stage-2_0: 1040(+1)/1041        Stage-3_0: 914(+398)/1520       Stage-4_0: 0/2021       Stage-5_0: 0/1009       Stage-6_0: 0/1\n2017-02-08 09:51:01,338 Stage-2_0: 1041/1041 Finished   Stage-3_0: 961(+383)/1520       Stage-4_0: 0/2021       Stage-5_0: 0/1009       Stage-6_0: 0/1\nFailed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n\nthe driver was indeed failed with some unknown reason:\n\n17/02/08 09:51:04 INFO exec.Utilities: PLAN PATH = hdfs://hsx-node1:8020/tmp/hive/root/b723c85d-2a7b-469e-bab1-9c165b25e656/hive_2017-02-08_09-49-37_890_6267025825539539056-1/-mr-10006/71a9dacb-a463-40ef-9e86-78d3b8e3738d/map.xml\n17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1169.0 in stage 3.0 (TID 2519)\n17/02/08 09:51:04 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown\n17/02/08 09:51:04 INFO storage.MemoryStore: MemoryStore cleared\n17/02/08 09:51:04 INFO storage.BlockManager: BlockManager stopped\n17/02/08 09:51:04 INFO exec.Utilities: PLAN PATH = hdfs://hsx-node1:8020/tmp/hive/root/b723c85d-2a7b-469e-bab1-9c165b25e656/hive_2017-02-08_09-49-37_890_6267025825539539056-1/-mr-10006/71a9dacb-a463-40ef-9e86-78d3b8e3738d/map.xml\n17/02/08 09:51:04 WARN executor.CoarseGrainedExecutorBackend: An unknown (hsx-node1:42777) driver disconnected.\n17/02/08 09:51:04 ERROR executor.CoarseGrainedExecutorBackend: Driver 192.168.1.1:42777 disassociated! Shutting down.\n17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1105.0 in stage 3.0 (TID 2511)\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called\n17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk6/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-71da1dfc-99bd-4687-bc2f-33452db8de3d\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk2/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-7f134d81-e77e-4b92-bd99-0a51d0962c14\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk5/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-77a90d63-fb05-4bc6-8d5e-1562cc502e6c\n17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk4/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-91f8b91a-114d-4340-8560-d3cd085c1cd4\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk1/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-a3c24f9e-8609-48f0-9d37-0de7ae06682a\n17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk7/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-f6120a43-2158-4780-927c-c5786b78f53e\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk3/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-e17931ad-9e8a-45da-86f8-9a0fdca0fad1\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk8/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-4de34175-f871-4c28-8ec0-d2fc0020c5c3\n17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1137.0 in stage 3.0 (TID 2515)\n17/02/08 09:51:04 INFO executor.Executor: Executor killed task 897.0 in stage 3.0 (TID 2417)\n17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1225.0 in stage 3.0 (TID 2526)\n17/02/08 09:51:04 INFO executor.Executor: Executor killed task 905.0 in stage 3.0 (TID 2423)\n\nin hive's log, \n\n2017-02-08T09:51:04,327  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO scheduler.TaskSetManager: Finished task 971.0 in stage 3.0 (TID 2218) in 5948 ms on hsx-node8 (1338/1520)\n2017-02-08T09:51:04,346  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO rpc.RpcDispatcher: [DriverProtocol] Closing channel due to exception in pipeline (org.apache.hive.spark.client.RemoteDriver$DriverProtocol.handle(io.netty.channel.ChannelHandlerContext, org.apache.hive.spark.client.rpc.Rpc$MessageHeader)).\n2017-02-08T09:51:04,346  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 WARN rpc.RpcDispatcher: [DriverProtocol] Expected RPC header, got org.apache.hive.spark.client.rpc.Rpc$NullMessage instead.\n2017-02-08T09:51:04,347  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO rpc.RpcDispatcher: [DriverProtocol] Closing channel due to exception in pipeline (null).\n2017-02-08T09:51:04,347  INFO [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Closing channel due to exception in pipeline (Connection reset by peer).\n2017-02-08T09:51:04,347  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception\n2017-02-08T09:51:04,347  INFO [stderr-redir-1] client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.\n2017-02-08T09:51:04,347  INFO [stderr-redir-1] client.SparkClientImpl:  at com.google.common.base.Preconditions.checkState(Preconditions.java:149)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 WARN rpc.Rpc: Failed to send RPC, closing connection.\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl: java.nio.channels.ClosedChannelException\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 WARN client.RemoteDriver: Shutting down driver because RPC channel was closed.\n2017-02-08T09:51:04,348  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO client.RemoteDriver: Shutting down remote driver.\n\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl:  at com.google.common.base.Preconditions.checkState(Preconditions.java:149)\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)\n2017-02-08T09:51:04,349  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO scheduler.DAGScheduler: Asked to cancel job 2\n2017-02-08T09:51:04,350  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception\n\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl: java.lang.InterruptedException\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at java.lang.Object.wait(Native Method)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at java.lang.Object.wait(Object.java:502)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.JobWaiter.awaitResult(JobWaiter.scala:73)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.SimpleFutureAction.org$apache$spark$SimpleFutureAction$$awaitResult(FutureAction.scala:165)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:120)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:108)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at scala.concurrent.Await$.ready(package.scala:86)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:303)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:316)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:362)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:323)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at java.lang.Thread.run(Thread.java:745)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at com.google.common.base.Preconditions.checkState(Preconditions.java:149)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)\n2017-02-08T09:51:04,351  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n2017-02-08T09:51:04,352  INFO [stderr-redir-1] client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)\n\n2017-02-08T09:51:04,654  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-65f40590-d87f-4701-b374-6b3b2a11538c\n2017-02-08T09:52:04,346  WARN [b723c85d-2a7b-469e-bab1-9c165b25e656 main] impl.RemoteSparkJobStatus: Error getting stage info\njava.util.concurrent.TimeoutException\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~[netty-all-4.0.23.Final.jar:4.0.23.Final]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:161) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:101) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1997) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1688) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1419) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:430) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:446) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:749) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_60]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_60]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221) ~[spark-assembly-1.6.2-hadoop2.6.0.jar:1.6.2]\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136) ~[spark-assembly-1.6.2-hadoop2.6.0.jar:1.6.2]\n2017-02-08T09:52:04,346 ERROR [b723c85d-2a7b-469e-bab1-9c165b25e656 main] status.SparkJobMonitor: Failed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'\njava.lang.IllegalStateException: RPC channel is closed.\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:149) ~[guava-14.0.1.jar:?]\n        at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n\nalso in container's log, I find Driver still request for executors:\n\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 77 executor(s).\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 2 executor containers\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 76 executor(s).\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 75 executor(s).\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 74 executor(s).\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 73 executor(s).\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 71 executor(s).\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 2 executor containers\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 70 executor(s).\n17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers\n17/02/08 09:51:04 INFO yarn.YarnAllocator: Driver requested a total number of 50 executor(s).\n17/02/08 09:51:04 INFO yarn.YarnAllocator: Canceling requests for 0 executor containers\n17/02/08 09:51:04 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.\n17/02/08 09:51:04 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).\n17/02/08 09:51:04 INFO yarn.YarnAllocator: Canceling requests for 0 executor containers\n17/02/08 09:51:04 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.\n17/02/08 09:51:04 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. 192.168.1.1:42777\n17/02/08 09:51:04 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hsx-node1:42777\n17/02/08 09:51:04 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0\n17/02/08 09:51:04 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED\n17/02/08 09:51:04 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.\n17/02/08 09:51:04 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1486453422616_0150\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called\n\n\nSo, is this situation the client to server timeout? I only set hive.spark.job.monitor.timeout=3600s;","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-02-08T09:47:41.087+0000","updated":"2017-02-08T09:47:41.087+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15857739","id":"15857739","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"body":"I am also very confused about these timeouts.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-02-08T09:49:51.910+0000","updated":"2017-02-08T09:49:51.910+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15857950","id":"15857950","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~KaiXu], your case might be different. The connection is explicitly closed due to \n{code}\n2017-02-08T09:51:04,346 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 WARN rpc.RpcDispatcher: [DriverProtocol] Expected RPC header, got org.apache.hive.spark.client.rpc.Rpc$NullMessage instead.\n2017-02-08T09:51:04,347 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO rpc.RpcDispatcher: [DriverProtocol] Closing channel due to exception in pipeline (null).\n2017-02-08T09:51:04,347 INFO [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Closing channel due to exception in pipeline (Connection reset by peer).\n{code}\nI haven't seen this kind of exception, so am not sure how it happens. If this can be reproduced, complete logs (driver, hive, yarn) would be helpful.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-08T12:55:31.711+0000","updated":"2017-02-08T12:57:12.081+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15857975","id":"15857975","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"body":"this error occurs when several queries run at the same time with large data scale, in fact it would not occur when running the query separately, but it can frequently occur when running together again.\n\nthe connection is closed suddenly, seems to be killed manually.  \n2017-02-08 09:51:01,338 Stage-2_0: 1041/1041 Finished   Stage-3_0: 961(+383)/1520       Stage-4_0: 0/2021       Stage-5_0: 0/1009       Stage-6_0: 0/1\nFailed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n\nfound only one ERROR in yarn application log, it seems the driver was closed but not know what caused it close, above comment is hive's log, any suggestions shall be appreciated!\n\n17/02/08 09:51:00 INFO executor.Executor: Finished task 1492.0 in stage 3.0 (TID 2168). 3294 bytes result sent to driver\n17/02/08 09:51:00 INFO executor.Executor: Finished task 556.0 in stage 3.0 (TID 1587). 3312 bytes result sent to driver\n17/02/08 09:51:00 INFO executor.Executor: Finished task 1412.0 in stage 3.0 (TID 2136). 3294 bytes result sent to driver\n17/02/08 09:51:00 INFO executor.Executor: Finished task 1236.0 in stage 3.0 (TID 2007). 3294 bytes result sent to driver\n17/02/08 09:51:04 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown\n17/02/08 09:51:04 INFO storage.MemoryStore: MemoryStore cleared\n17/02/08 09:51:04 INFO storage.BlockManager: BlockManager stopped\n17/02/08 09:51:04 WARN executor.CoarseGrainedExecutorBackend: An unknown (hsx-node1:42777) driver disconnected.\n17/02/08 09:51:04 ERROR executor.CoarseGrainedExecutorBackend: Driver 192.168.1.1:42777 disassociated! Shutting down.\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk8/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-a8167f0b-f3c3-458f-ad51-8a0f4bcda4f3\n17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk1/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-26cba445-66d2-4b78-a428-17881c92f0f6\n17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-02-08T13:22:58.028+0000","updated":"2017-02-08T13:22:58.028+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15859267","id":"15859267","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~xuefuz], can you get a thread dump of HS2/CLI when you hit the 2nd problem? Then we can find out how the JVM is hanging.\n\n[~KaiXu], I also saw some similar issue before. Please feel free to open a JIRA for it and continue the investigation there. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-02-09T09:46:27.345+0000","updated":"2017-02-09T09:46:27.345+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15861113","id":"15861113","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks, [~lirui], I will try to reproduce, though it might be hard to do so.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-10T11:10:25.203+0000","updated":"2017-02-10T11:10:25.203+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15861114","id":"15861114","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~vanzin], to backtrack a little bit, I have a followup question about your comment.\n{quote}\nThat's kinda hard to solve, because the server doesn't know which client connected until two things happen: first the driver has started, second the driver completed the SASL handshake to identify itself. A lot of things can go wrong in that time. There's already some code, IIRC, that fails the session if the spark-submit job dies with an error, but aside from that, it's kinda hard to do more.\n{quote}\nI was talking about server detecting a driver problem after it has connected back to the server. I'm wondering which timeout applies in case of a problem on the driver side, such as long GC, stall connection between the server and the driver, etc. It's kind of long if this timeout is also server.connect.timeout, which is increased to 10m in our case to accommodate for the busy cluster. To me it doesn't seem that such a timeout exist, in absence of a heartbeat mechanism.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-10T11:10:36.191+0000","updated":"2017-02-10T11:10:58.054+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15861241","id":"15861241","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"body":"I created HIVE-15859 for the issue, comments or suggestions are welcomed. Thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-02-10T13:18:15.421+0000","updated":"2017-02-10T13:18:15.421+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15861611","id":"15861611","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. I was talking about server detecting a driver problem after it has connected back to the server.\n\nHmm. That is definitely not any of the \"connect\" timeouts, which probably means it isn't configured and is just using netty's default (which is probably no timeout?). Would probably need something using {{io.netty.handler.timeout.IdleStateHandler}}, and also some periodic \"ping\" so that the connection isn't torn down without reason.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-10T18:06:37.175+0000","updated":"2017-02-10T18:06:37.175+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15863989","id":"15863989","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the input, [~vanzin]! For what you suggested, I think it is deemed for more investigation and development. Since Hive will monitor job after the driver connected back, hopefully, the monitoring thread will detect any network/driver issue. With HIVE-15860, the issue might have been resolved.\n\nI will create a separate JIRA for your proposal. In the mean time, I think Patch #1 is still needed as we also like the driver to detect any issue with Hive sooner. What do you think? Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-13T17:06:23.591+0000","updated":"2017-02-13T17:06:23.591+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15864066","id":"15864066","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"bq.  In the mean time, I think Patch #1 is still needed\n\nSounds fine to me.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-13T17:46:38.455+0000","updated":"2017-02-13T17:46:38.455+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15864186","id":"15864186","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"body":"+1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-13T18:45:39.040+0000","updated":"2017-02-13T18:45:39.040+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13036412/comment/15864231","id":"15864231","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Committed to master. Thanks to Marchelo, Jimmy, and Rui for the review.\nCreated HIVE-15893 as a followup.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-13T19:08:33.968+0000","updated":"2017-02-13T19:22:07.906+0000"}],"maxResults":29,"total":29,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15671/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i38ytj:"}}