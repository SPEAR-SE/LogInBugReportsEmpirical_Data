{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13093663","self":"https://issues.apache.org/jira/rest/api/2/issue/13093663","key":"HIVE-17287","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-08-10T03:42:25.000+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Aug 15 09:07:33 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17287/watchers","watchCount":6,"isWatching":false},"created":"2017-08-10T03:32:38.215+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-08-15T09:07:33.164+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"In [tpcds/query67.sql|https://github.com/kellyzly/hive-testbench/blob/hive14/sample-queries-tpcds/query67.sql], fact table {{store_sales}} joins with small tables {{date_dim}}, {{item}},{{store}}. After join, groupby the intermediate data.\nHere the data of {{store_sales}} on 3TB tpcds is skewed:  there are 1824 partitions. The biggest partition is 25.7G and others are 715M.\n{code}\nhadoop fs -du -h /user/hive/warehouse/tpcds_bin_partitioned_parquet_3000.db/store_sales\n....\n715.0 M  /user/hive/warehouse/tpcds_bin_partitioned_parquet_3000.db/store_sales/ss_sold_date_sk=2452639\n713.9 M  /user/hive/warehouse/tpcds_bin_partitioned_parquet_3000.db/store_sales/ss_sold_date_sk=2452640\n714.1 M  /user/hive/warehouse/tpcds_bin_partitioned_parquet_3000.db/store_sales/ss_sold_date_sk=2452641\n712.9 M  /user/hive/warehouse/tpcds_bin_partitioned_parquet_3000.db/store_sales/ss_sold_date_sk=2452642\n25.7 G   /user/hive/warehouse/tpcds_bin_partitioned_parquet_3000.db/store_sales/ss_sold_date_sk=__HIVE_DEFAULT_PARTITION__\n{code}\nThe skewed table {{store_sales}} caused the failed job. Is there any way to solve the groupby problem of skewed table?  I tried to enable {{hive.groupby.skewindata}} to first divide the data more evenly then start do group by. But the job still hangs. ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12881892","id":"12881892","filename":"compare_groupby_groupby_rollup.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-15T08:54:21.321+0000","size":218712,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12881892/compare_groupby_groupby_rollup.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12881692","id":"12881692","filename":"not_stages_completed_but_job_completed.PNG","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-14T07:18:20.663+0000","size":79477,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12881692/not_stages_completed_but_job_completed.PNG"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12881413","id":"12881413","filename":"query67-fail-at-groupby.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T07:32:17.796+0000","size":199865,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12881413/query67-fail-at-groupby.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12881418","id":"12881418","filename":"query67-groupby_shuffle_metric.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T08:18:39.156+0000","size":55561,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12881418/query67-groupby_shuffle_metric.png"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"HoS can not deal with skewed data group by","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16121004","id":"16121004","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"[~liyunzhang_intel]: I suspect HoS is loading each partition as an independent RDD, which removes the effect of SemanticAnalyzer::genNotNullFilterForJoinSourcePlan()?\n\nAs little as I know about Hive-on-Spark, Query67 does not read any row from the default partition in MR or Tez.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-08-10T03:42:25.000+0000","updated":"2017-08-10T03:42:49.232+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16121021","id":"16121021","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. these part of data will not be load because there is no match data in join?\n\nYes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-08-10T03:58:54.297+0000","updated":"2017-08-10T03:58:54.297+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16121023","id":"16121023","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~gopalv]:  thanks for your comments\n{quote}\nAs little as I know about Hive-on-Spark, Query67 does not read any row from the default partition in MR or Tez.\n\n{quote}\nthe default_partition stores the data(25.7G) which ss_sold_date_sk is null. In MR/Tez, these part of data will not be load because there is no match data in join? \n\n{quote}\nI suspect HoS is loading each partition as an independent RDD, which removes the effect of SemanticAnalyzer::genNotNullFilterForJoinSourcePlan()?\n{quote}\nI also see the filter to filter null data in the explain, although HOS load partition as an independent RDD, i think the filter should work after loading data in theory.\npart of explain of query67\n{code}\n Map 1 \n            Map Operator Tree:\n                TableScan\n                  alias: store_sales\n                  filterExpr: (ss_store_sk is not null and ss_item_sk is not null) (type: boolean)\n                  Statistics: Num rows: 8251124389 Data size: 181524736558 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: (ss_store_sk is not null and ss_item_sk is not null) (type: boolean)\n                    Statistics: Num rows: 8251124389 Data size: 181524736558 Basic stats: COMPLETE Column stats: NONE\n                    Select Operator\n                      expressions: ss_item_sk (type: bigint), ss_store_sk (type: bigint), ss_quantity (type: int), ss_sales_price (type: double), ss_sold_date_sk (type: bigint) \n                      outputColumnNames: _col0, _col1, _col2, _col3, _col4\n                      Statistics: Num rows: 8251124389 Data size: 181524736558 Basic stats: COMPLETE Column stats: NONE\n                      Map Join Operator\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-10T04:01:33.303+0000","updated":"2017-08-10T04:02:20.519+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16121026","id":"16121026","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"{code}\nstore_sales.ss_sold_date_sk=date_dim.d_date_sk\n{code}\n\nImplies ss_sold_date_sk is not null.\n\nThere's another optimizer in Hive called PCR, which removes any Filter which is no-op.\n\nSo there's some possibility that \"explain extended <query>\" won't list the default partition at all.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-08-10T04:04:59.321+0000","updated":"2017-08-10T04:04:59.321+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16122840","id":"16122840","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~gopalv] or [~lirui]:  after enable \"hive.optimize.ppd\", the default partition /user/hive/warehouse/tpcds_bin_partitioned_parquet_3000.db/store_sales/ss_sold_date_sk=__HIVE_DEFAULT_PARTITION__ has been filtered, so will not load this part of data. But the group by is still skewed.  Modify tpcds/query67.sql to output the result of join to view the result of join(before group by) is skewed or not\n{code}\nset hive.optimize.ppd=true;\nset spark.app.name=\"query677.ppd.true\";\ndrop table if exists result_677;\ncreate table result_677 stored as TEXTFILE as\nselect i_category\n                  ,i_class\n                  ,i_brand\n                  ,i_product_name\n                  ,d_year\n                  ,d_qoy\n                  ,d_moy\n                  ,s_store_id\n                  ,store_sales.ss_sold_date_sk\n                  ,store_sales.ss_item_sk\n                  ,store_sales.ss_store_sk\n            from store_sales\n                ,date_dim\n                ,store\n                ,item\n       where  store_sales.ss_sold_date_sk=date_dim.d_date_sk\n          and store_sales.ss_item_sk=item.i_item_sk\n          and store_sales.ss_store_sk = store.s_store_sk\n          and d_month_seq between 1193 and 1193+11;\n{code}\n\nthe result is\n{code}\nhadoop fs -du -h  hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/\n\n105.5 M  hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000000_0\n46.8 M   hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000001_0\n4.0 M    hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000002_0\n47.4 M   hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000003_0\n215.1 M  hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000004_0\n77.7 M   hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000005_0\n0        hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000006_0\n0        hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000007_0\n0        hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000008_0\n0        hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000009_0\n0        hdfs://bdpe38:9000/user/hive/warehouse/tpcds_bin_partitioned_parquet_10.db/result_677/000010_0\n{code}\n\nThe result of join is skewed. The biggest is 215M while the smallest is 0M.  Is there way to make the output of join is even so that the following groupby will not skewed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T04:53:57.658+0000","updated":"2017-08-11T04:53:57.658+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16122897","id":"16122897","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~gopalv],[~lirui]ï¼š the result why the output of join is skewed is because I convert all join to map join. In following query, fact table is store_sales and  dimension tables are date_dim,store and item. The total size of date_dim, store and item is smaller than the {{hive.auto.convert.join.noconditionaltask.size}}.  Hive starts 11 map works to read store_sales and do map join. There is possibility that there is no records in one map work because no match data in other dimension tables with store_sales.\n{code}\n select i_category\n                  ,i_class\n                  ,i_brand\n                  ,i_product_name\n                  ,d_year\n                  ,d_qoy\n                  ,d_moy\n                  ,s_store_id\n                  ,store_sales.ss_sold_date_sk\n                  ,store_sales.ss_item_sk\n                  ,store_sales.ss_store_sk\n            from store_sales\n                ,date_dim\n                ,store\n                ,item\n       where  store_sales.ss_sold_date_sk=date_dim.d_date_sk\n          and store_sales.ss_item_sk=item.i_item_sk\n          and store_sales.ss_store_sk = store.s_store_sk\n          and d_month_seq between 1193 and 1193+11;\n\n{code}\nIt is reasonable that the result of map join is not even but is there any way to make it even? because it will cause the data assigned to the group by tasks is not even if group by operation follows the map join. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T05:51:44.436+0000","updated":"2017-08-11T05:51:44.436+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16122929","id":"16122929","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~kellyzly], I'm trying to understand how the group by is skewed. If you do group by after the map join, then the 11 map-join tasks will output data to be shuffled again. Only 6 tasks have data to output, and the other 5 tasks don't output because no records are generated by the map join. However, this doesn't mean the following shuffle is necessarily skewed. E.g. if you have 100 downstream tasks, they all can fetch from the 6 upstream tasks, as long as the grouping key is evenly distributed. So have you verified whether the group key is skewed?\nIt'll be strange if the key is not skewed but the shuffle is. One possible reason is spark can somehow give reduce tasks location preference which may affect the case you described, you can trying setting {{spark.shuffle.reduceLocality.enabled=false}} to disable it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T06:37:57.805+0000","updated":"2017-08-11T06:37:57.805+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16122955","id":"16122955","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: thanks for comments\n{{spark.shuffle.reduceLocality.enabled}} is [disabled|https://issues.apache.org/jira/browse/SPARK-10087] in spark1.5 so I guess this value is false in my cluster because i use spark2.0. \n{quote}\n11 map-join tasks will output data to be shuffled again. Only 6 tasks have data to output, and the other 5 tasks don't output because no records are generated by the map join.However, this doesn't mean the following shuffle is necessarily skewed.\n{quote}\nYes, this does not mean the group by key is skewed. 1 thing i need to confirm with you is that the 5 tasks without any records will also be sent to next stage(groupby stage) even there is no record.  That's why in spark history server some tasks spends nearly 0 seconds to finish while others spends several minutes to finish in groupby stage.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T07:16:52.047+0000","updated":"2017-08-11T07:16:52.047+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16122970","id":"16122970","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"the attached query67-fail-at-groupby.png shows that  in group by stage, only 1 task failed while others passed. So i suspect the group by key is skewed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T07:33:18.465+0000","updated":"2017-08-11T07:33:18.465+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16122984","id":"16122984","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"That config is enabled by default in 2.0:\nhttps://github.com/apache/spark/blob/v2.0.0/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L274","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T07:48:41.587+0000","updated":"2017-08-11T07:48:41.587+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16122994","id":"16122994","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"To determine whether a shuffle is skewed, you need to look at the shuffle input metrics of tasks in that stage. In your case, that should be the tasks following the map-join.\nbq. the 5 tasks without any records will also be sent to next stage\n\nIf the tasks have no output, then the downstream tasks won't fetch anything from these tasks. I'm not sure what you mean by \"sent to next stage\"?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T07:56:32.480+0000","updated":"2017-08-11T07:56:32.480+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16123021","id":"16123021","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: attached is the [query67-groupby_shuffle_metric.png|https://issues.apache.org/jira/secure/attachment/12881418/query67-groupby_shuffle_metric.png] of group by stage. It seems that the shuffle read metrics is not even.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T08:21:51.847+0000","updated":"2017-08-11T08:21:51.847+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16123036","id":"16123036","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"OK that seems a skewed shuffle to me. You can run some statistics on the group key to confirm, in case there's some issue like HIVE-17114.\nBesides, what will the metrics look like if you enable {{hive.groupby.skewindata}}? That optimization will shuffle twice for the group by. The 1st shuffle is partitioned randomly. You can verify it in the explain:\n{noformat}\n                      Reduce Output Operator\n                        key expressions: _col0 (type: string)\n                        sort order: +\n                        Map-reduce partition columns: rand() (type: double)\n                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T08:35:21.462+0000","updated":"2017-08-11T08:35:21.462+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16123123","id":"16123123","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui] :\nbq.You can run some statistics on the group key to confirm\n\nnot very understand, you mean \"add select count(i_category), i_category,.... to see the number of every key\"?\nbq.  what will the metrics look like if you enable hive.groupby.skewindata?\nbefore i enabled {{hive.groupby.skewindata}}, still hangs on the group by stage after  sending the data randomly.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T10:03:50.229+0000","updated":"2017-08-11T10:03:50.229+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16123204","id":"16123204","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~kellyzly], I mean you can check each of the group keys to see how they are skewed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-11T11:19:22.176+0000","updated":"2017-08-11T11:19:22.176+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16124263","id":"16124263","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kellyzly], just curious, what error did you get for the failed tasks? Memory related?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-08-11T23:28:37.711+0000","updated":"2017-08-11T23:28:37.711+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16125125","id":"16125125","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]: the memory related error is\n{noformat}\nContainer killed by YARN for exceeding memory limits. 36.1 GB of 33 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n{noformat}\n It showed it exceeded the memory assigned to the task. I can increase the value of spark.yarn.executor.memoryOverhead. But i guess even i increase the value, the error will appear again as the problem is the key is not even for some task in group by operation.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-14T01:47:45.454+0000","updated":"2017-08-14T01:47:45.454+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16125130","id":"16125130","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Have you tried {{hive.spark.use.groupby.shuffle}}? I think it can avoid unbounded mem usage.\nFor the error you mentioned, I usually disable {{yarn.nodemanager.pmem-check-enabled}} as a workaround.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-14T02:01:28.627+0000","updated":"2017-08-14T02:01:28.627+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16125155","id":"16125155","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]:\nbq.Have you tried hive.spark.use.groupby.shuffle? I think it can avoid unbounded mem usage.\n  I have not enabled {{hive.spark.use.groupby.shuffle}} in my cluster. Will try this configuration later. But why in HiveConf it says \"Spark groupByKey transformation has better performance but uses unbounded memory\". Will this use unbounded memory?\nbq.For the error you mentioned, I usually disable yarn.nodemanager.pmem-check-enabled as a workaround.\nhave disabled this configuration in my cluster but error still occurred.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-14T02:29:18.478+0000","updated":"2017-08-14T02:29:18.478+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16125156","id":"16125156","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"The groupByKey shuffle uses unbounded memory. You can set {{hive.spark.use.groupby.shuffle=false}} to use MR shuffle instead. By default the config is true.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-14T02:36:03.508+0000","updated":"2017-08-14T02:36:03.508+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16125326","id":"16125326","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]:  in current case, i have not set {{hive.spark.use.groupby.shuffle}}, but i think the value is true because the default value is true. After disabling {{spark.shuffle.reduceLocality.enabled}}, i reran the query67.  It showed passed. But one strange thing i found is [not all stages finished but the result of spark job is completed|https://issues.apache.org/jira/secure/attachment/12881692/not_stages_completed_but_job_completed.PNG].I don't know whether this is bug of spark. Meanwhile in spark history server, the shuffle read metrics are still skewed. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-14T07:20:52.915+0000","updated":"2017-08-14T07:20:52.915+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16125371","id":"16125371","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~kellyzly], disabling {{spark.shuffle.reduceLocality.enabled}} can make reduce tasks more evenly distributed among executors. But as you said, it can't solve the skew in data. As to the screenshot, I don't know the cause either. What if you look at the metrics of all the tasks? Are they all in complete state?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-14T08:35:32.733+0000","updated":"2017-08-14T08:35:32.733+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16125415","id":"16125415","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: When i viewed all tasks, saw that 1 task was  still running. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-14T08:56:20.006+0000","updated":"2017-08-14T08:56:20.006+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16126964","id":"16126964","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui] ,[~gopalv]: some update about skewed data group by.\nthe problem happens when using group by with rollup. in [attached pic|https://issues.apache.org/jira/secure/attachment/12881892/compare_groupby_groupby_rollup.png], you can see the difference, if using group by with roll up, the shuffle read data is 96.8g while using group by, the shuffle read data is 68.1g.  when i only using group by, the shuffle read metrics is very even.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-15T08:57:00.534+0000","updated":"2017-08-15T08:57:00.534+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093663/comment/16126973","id":"16126973","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~kellyzly], group by w/ rollup and group by w/o rollup are different queries and it's normal that the shuffle read is different for different queries. I guess some of the group keys are skewed. And it'll also be good to verify whether HoS can properly handle group by w/ rollup.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-15T09:07:33.164+0000","updated":"2017-08-15T09:07:33.164+0000"}],"maxResults":25,"total":25,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17287/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3imif:"}}