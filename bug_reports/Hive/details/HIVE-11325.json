{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12846437","self":"https://issues.apache.org/jira/rest/api/2/issue/12846437","key":"HIVE-11325","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-10-16T10:40:40.753+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Nov 15 09:29:27 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-11325/watchers","watchCount":4,"isWatching":false},"created":"2015-07-21T05:53:40.623+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329278","id":"12329278","description":"Branch 1.0 release","name":"1.0.0","archived":false,"released":true,"releaseDate":"2015-02-04"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-11-15T09:29:27.010+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12313461","id":"12313461","name":"HBase Handler"}],"timeoriginalestimate":null,"description":"No idea why {{hbase_handler_bulk.q}} does not catch this if its being run regularly in Hive builds, but here's the gist of the issue:\n\nThe condition at https://github.com/apache/hive/blob/master/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java#L152-L164 indicates that we will infinitely loop until we find a file whose last path component (the name) is equal to the column family name.\n\nIn execution, however, the iteration enters an actual infinite loop cause the file we end up considering as the srcDir name, is actually the region file, whose name will never match the family name.\n\nThis is an example of the IPC the listing loop of a 100% progress task gets stuck in:\n\n{code}\n2015-07-21 10:32:20,662 TRACE [main] org.apache.hadoop.ipc.ProtobufRpcEngine: 1: Call -> cdh54.vm/172.16.29.132:8020: getListing {src: \"/user/hive/warehouse/hbase_test/_temporary/1/_temporary/attempt_1436935612068_0011_m_000000_0/family/97112ac1c09548ae87bd85af072d2e8c\" startAfter: \"\" needLocation: false}\n2015-07-21 10:32:20,662 DEBUG [IPC Parameter Sending Thread #1] org.apache.hadoop.ipc.Client: IPC Client (1551465414) connection to cdh54.vm/172.16.29.132:8020 from hive sending #510346\n2015-07-21 10:32:20,662 DEBUG [IPC Client (1551465414) connection to cdh54.vm/172.16.29.132:8020 from hive] org.apache.hadoop.ipc.Client: IPC Client (1551465414) connection to cdh54.vm/172.16.29.132:8020 from hive got value #510346\n2015-07-21 10:32:20,662 DEBUG [main] org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getListing took 0ms\n2015-07-21 10:32:20,662 TRACE [main] org.apache.hadoop.ipc.ProtobufRpcEngine: 1: Response <- cdh54.vm/172.16.29.132:8020: getListing {dirList { partialListing { fileType: IS_FILE path: \"\" length: 863 permission { perm: 4600 } owner: \"hive\" group: \"hive\" modification_time: 1437454718130 access_time: 1437454717973 block_replication: 1 blocksize: 134217728 fileId: 33960 childrenNum: 0 storagePolicy: 0 } remainingEntries: 0 }}\n{code}\n\nThe path we are getting out of the listing results is {{/user/hive/warehouse/hbase_test/_temporary/1/_temporary/attempt_1436935612068_0011_m_000000_0/family/97112ac1c09548ae87bd85af072d2e8c}}, but instead of checking the path's parent {{family}} we're instead looping infinitely over its hashed filename {{97112ac1c09548ae87bd85af072d2e8c}} cause it does not match {{family}}.\n\nIt stays in the infinite loop therefore, until the MR framework kills it away due to an idle task timeout (and then since the subsequent task attempts fail outright, the job fails).\n\nWhile doing a {{getPath().getParent()}} will resolve that, is that infinite loop even necessary? Especially given the fact that we throw exceptions if there are no entries or there is more than one entry.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12746327","id":"12746327","filename":"HIVE-11325.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-07-21T11:03:40.609+0000","size":1601,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12746327/HIVE-11325.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Infinite loop in HiveHFileOutputFormat","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12846437/comment/14634774","id":"14634774","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"I missed the srcDir declare, which'd explain the loop (we're walking). I'm checking why it doesn't abort at the family directory.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-07-21T08:38:49.437+0000","updated":"2015-07-21T08:38:49.437+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12846437/comment/14634795","id":"14634795","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"So this happens if the family name is not provided correctly via the {{set hfile.family.path=/path/…/familyname;}} statement. We should avoid looping over if we've reached a file instead of a directory, to prevent the hang. We could throw an error instead.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-07-21T08:55:03.128+0000","updated":"2015-07-21T08:55:03.128+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12846437/comment/14958483","id":"14958483","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"Not quite sure I have the test case complete, but submitting for review so I can get help over that if its not.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-15T07:54:34.016+0000","updated":"2015-10-15T07:54:34.016+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12846437/comment/14960498","id":"14960498","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12746327/HIVE-11325.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5674/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5674/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5674/\n\nMessages:\n{noformat}\n**** This message was trimmed, see log for full details ****\n[WARNING] /data/hive-ptest/working/apache-github-source-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-cli ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/cli/src/test/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-cli ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/cli/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/cli/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/cli/target/tmp/conf\n     [copy] Copying 14 files to /data/hive-ptest/working/apache-github-source-source/cli/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-cli ---\n[INFO] Compiling 4 source files to /data/hive-ptest/working/apache-github-source-source/cli/target/test-classes\n[WARNING] /data/hive-ptest/working/apache-github-source-source/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java: /data/hive-ptest/working/apache-github-source-source/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java uses unchecked or unsafe operations.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-cli ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-cli ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/cli/target/hive-cli-2.0.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-cli ---\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-cli ---\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/cli/target/hive-cli-2.0.0-SNAPSHOT.jar to /home/hiveptest/.m2/repository/org/apache/hive/hive-cli/2.0.0-SNAPSHOT/hive-cli-2.0.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/cli/pom.xml to /home/hiveptest/.m2/repository/org/apache/hive/hive-cli/2.0.0-SNAPSHOT/hive-cli-2.0.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Contrib 2.0.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-contrib ---\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/contrib/target\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/contrib (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-contrib ---\n[INFO] \n[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-contrib ---\n[WARNING] Invalid project model for artifact [pentaho-aggdesigner-algorithm:org.pentaho:5.1.5-jhyde]. It will be ignored by the remote resources Mojo.\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-contrib ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/contrib/src/main/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-contrib ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-contrib ---\n[INFO] Compiling 40 source files to /data/hive-ptest/working/apache-github-source-source/contrib/target/classes\n[WARNING] /data/hive-ptest/working/apache-github-source-source/contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMax.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/contrib/src/java/org/apache/hadoop/hive/contrib/udaf/example/UDAFExampleMax.java: Recompile with -Xlint:deprecation for details.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleStructPrint.java: /data/hive-ptest/working/apache-github-source-source/contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleStructPrint.java uses unchecked or unsafe operations.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleStructPrint.java: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-contrib ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/contrib/src/test/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-contrib ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/contrib/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/contrib/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/contrib/target/tmp/conf\n     [copy] Copying 14 files to /data/hive-ptest/working/apache-github-source-source/contrib/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-contrib ---\n[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-github-source-source/contrib/target/test-classes\n[WARNING] /data/hive-ptest/working/apache-github-source-source/contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java: /data/hive-ptest/working/apache-github-source-source/contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java uses or overrides a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/contrib/src/test/org/apache/hadoop/hive/contrib/serde2/TestRegexSerDe.java: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-contrib ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-contrib ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/contrib/target/hive-contrib-2.0.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-contrib ---\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-contrib ---\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/contrib/target/hive-contrib-2.0.0-SNAPSHOT.jar to /home/hiveptest/.m2/repository/org/apache/hive/hive-contrib/2.0.0-SNAPSHOT/hive-contrib-2.0.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/contrib/pom.xml to /home/hiveptest/.m2/repository/org/apache/hive/hive-contrib/2.0.0-SNAPSHOT/hive-contrib-2.0.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive HBase Handler 2.0.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hbase-handler ---\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/hbase-handler/target\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/hbase-handler (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-hbase-handler ---\n[INFO] \n[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hbase-handler ---\n[WARNING] Invalid project model for artifact [pentaho-aggdesigner-algorithm:org.pentaho:5.1.5-jhyde]. It will be ignored by the remote resources Mojo.\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hbase-handler ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/hbase-handler/src/main/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hbase-handler ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hbase-handler ---\n[INFO] Compiling 33 source files to /data/hive-ptest/working/apache-github-source-source/hbase-handler/target/classes\n[INFO] -------------------------------------------------------------\n[WARNING] COMPILATION WARNING : \n[INFO] -------------------------------------------------------------\n[WARNING] /data/hive-ptest/working/apache-github-source-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/AbstractHBaseKeyFactory.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/AbstractHBaseKeyFactory.java: Recompile with -Xlint:deprecation for details.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDeParameters.java: Some input files use unchecked or unsafe operations.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDeParameters.java: Recompile with -Xlint:unchecked for details.\n[INFO] 4 warnings \n[INFO] -------------------------------------------------------------\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR : \n[INFO] -------------------------------------------------------------\n[ERROR] /data/hive-ptest/working/apache-github-source-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java:[164,24] cannot find symbol\n  symbol:   method isDir()\n  location: variable srcDir of type org.apache.hadoop.fs.Path\n[INFO] 1 error\n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Hive .............................................. SUCCESS [13.189s]\n[INFO] Hive Shims Common ................................. SUCCESS [15.029s]\n[INFO] Hive Shims 0.20S .................................. SUCCESS [3.959s]\n[INFO] Hive Shims 0.23 ................................... SUCCESS [11.769s]\n[INFO] Hive Shims Scheduler .............................. SUCCESS [2.347s]\n[INFO] Hive Shims ........................................ SUCCESS [3.572s]\n[INFO] Hive Storage API .................................. SUCCESS [3.830s]\n[INFO] Hive Common ....................................... SUCCESS [22.237s]\n[INFO] Hive Serde ........................................ SUCCESS [20.788s]\n[INFO] Hive Metastore .................................... SUCCESS [1:20.849s]\n[INFO] Hive Ant Utilities ................................ SUCCESS [2.109s]\n[INFO] Hive Llap Client .................................. SUCCESS [8.562s]\n[INFO] Spark Remote Client ............................... SUCCESS [15.956s]\n[INFO] Hive Query Language ............................... SUCCESS [3:23.412s]\n[INFO] Hive Service ...................................... SUCCESS [15.057s]\n[INFO] Hive Accumulo Handler ............................. SUCCESS [7.041s]\n[INFO] Hive JDBC ......................................... SUCCESS [23.719s]\n[INFO] Hive Beeline ...................................... SUCCESS [4.057s]\n[INFO] Hive CLI .......................................... SUCCESS [5.716s]\n[INFO] Hive Contrib ...................................... SUCCESS [2.653s]\n[INFO] Hive HBase Handler ................................ FAILURE [3.810s]\n[INFO] Hive HCatalog ..................................... SKIPPED\n[INFO] Hive HCatalog Core ................................ SKIPPED\n[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED\n[INFO] Hive HCatalog Server Extensions ................... SKIPPED\n[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED\n[INFO] Hive HCatalog Webhcat ............................. SKIPPED\n[INFO] Hive HCatalog Streaming ........................... SKIPPED\n[INFO] Hive HPL/SQL ...................................... SKIPPED\n[INFO] Hive HWI .......................................... SKIPPED\n[INFO] Hive ODBC ......................................... SKIPPED\n[INFO] Hive Shims Aggregator ............................. SKIPPED\n[INFO] Hive TestUtils .................................... SKIPPED\n[INFO] Hive Packaging .................................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 7:52.943s\n[INFO] Finished at: Fri Oct 16 06:40:38 EDT 2015\n[INFO] Final Memory: 215M/1065M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-hbase-handler: Compilation failure\n[ERROR] /data/hive-ptest/working/apache-github-source-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java:[164,24] cannot find symbol\n[ERROR] symbol:   method isDir()\n[ERROR] location: variable srcDir of type org.apache.hadoop.fs.Path\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-hbase-handler\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12746327 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2015-10-16T10:40:40.753+0000","updated":"2015-10-16T10:40:40.753+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12846437/comment/15666644","id":"15666644","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12746327/HIVE-11325.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/2125/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/2125/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2125/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'\n2016-11-15 09:27:42.966\n+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]\n+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'MAVEN_OPTS=-Xmx1g '\n+ MAVEN_OPTS='-Xmx1g '\n+ cd /data/hiveptest/working/\n+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-2125/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ date '+%Y-%m-%d %T.%3N'\n2016-11-15 09:27:42.969\n+ cd apache-github-source-source\n+ git fetch origin\n+ git reset --hard HEAD\nHEAD is now at 4427eab HIVE-13931: Add support for HikariCP and replace BoneCP usage with HikariCP (Prasanth Jayachandran reviewed by Sushanth Sowmyan)\n+ git clean -f -d\nRemoving data/files/parquet_non_dictionary_types.txt\nRemoving ql/src/java/org/apache/hadoop/hive/ql/io/parquet/ParquetRecordReaderBase.java\nRemoving ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/\nRemoving ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java\nRemoving ql/src/test/queries/clientpositive/parquet_types_non_dictionary_encoding_vectorization.q\nRemoving ql/src/test/queries/clientpositive/parquet_types_vectorization.q\nRemoving ql/src/test/results/clientpositive/parquet_types_non_dictionary_encoding_vectorization.q.out\nRemoving ql/src/test/results/clientpositive/parquet_types_vectorization.q.out\n+ git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n+ git reset --hard origin/master\nHEAD is now at 4427eab HIVE-13931: Add support for HikariCP and replace BoneCP usage with HikariCP (Prasanth Jayachandran reviewed by Sushanth Sowmyan)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ date '+%Y-%m-%d %T.%3N'\n2016-11-15 09:27:43.905\n+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hiveptest/working/scratch/build.patch\n+ [[ -f /data/hiveptest/working/scratch/build.patch ]]\n+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh\n+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch\nGoing to apply patch with: patch -p0\npatching file hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java\npatching file hbase-handler/src/test/queries/negative/generatehfiles_bad_family_path.q\n+ [[ maven == \\m\\a\\v\\e\\n ]]\n+ rm -rf /data/hiveptest/working/maven/org/apache/hive\n+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven\nANTLR Parser Generator  Version 3.5.2\nOutput file /data/hiveptest/working/apache-github-source-source/metastore/target/generated-sources/antlr3/org/apache/hadoop/hive/metastore/parser/FilterParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/parser/Filter.g\norg/apache/hadoop/hive/metastore/parser/Filter.g\nDataNucleus Enhancer (version 4.1.6) for API \"JDO\"\nDataNucleus Enhancer : Classpath\n>>  /usr/share/maven/boot/plexus-classworlds-2.x.jar\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDatabase\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFieldSchema\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MType\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTable\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MConstraint\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MOrder\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStringList\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartition\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MIndex\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRole\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRoleMap\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMasterKey\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDelegationToken\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MVersionTable\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MResourceUri\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFunction\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationLog\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationNextId\nDataNucleus Enhancer completed with success for 30 classes. Timings : input=196 ms, enhance=210 ms, total=406 ms. Consult the log for full details\nANTLR Parser Generator  Version 3.5.2\nOutput file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveLexer.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g\norg/apache/hadoop/hive/ql/parse/HiveLexer.g\nOutput file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g\norg/apache/hadoop/hive/ql/parse/HiveParser.g\nGenerating vector expression code\nGenerating vector expression test code\n[ERROR] COMPILATION ERROR : \n[ERROR] /data/hiveptest/working/apache-github-source-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java:[164,24] cannot find symbol\n  symbol:   method isDir()\n  location: variable srcDir of type org.apache.hadoop.fs.Path\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-hbase-handler: Compilation failure\n[ERROR] /data/hiveptest/working/apache-github-source-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java:[164,24] cannot find symbol\n[ERROR] symbol:   method isDir()\n[ERROR] location: variable srcDir of type org.apache.hadoop.fs.Path\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-hbase-handler\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12746327 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-11-15T09:29:27.010+0000","updated":"2016-11-15T09:29:27.010+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-11325/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2hh8n:"}}