{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13093220","self":"https://issues.apache.org/jira/rest/api/2/issue/13093220","key":"HIVE-17270","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340268","id":"12340268","name":"3.0.0","archived":false,"released":true,"releaseDate":"2018-05-21"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/11","id":"11","description":"","name":"Done"},"customfield_12312322":null,"customfield_12310220":"2017-08-08T17:21:24.425+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue May 22 23:59:21 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_7411646025_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2017-11-02T11:01:18.034+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17270/watchers","watchCount":5,"isWatching":false},"created":"2017-08-08T16:13:52.070+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340268","id":"12340268","name":"3.0.0","archived":false,"released":true,"releaseDate":"2018-05-21"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-05-22T23:59:21.770+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12323200","id":"12323200","name":"Spark","description":"Hive on Spark"}],"timeoriginalestimate":null,"description":"The hive-site.xml shows, that the TestMiniSparkOnYarnCliDriver uses 2 cores, and 2 executor instances to run the queries. See: https://github.com/apache/hive/blob/master/data/conf/spark/yarn-client/hive-site.xml#L233\n\nWhen reading the log files for the query tests, I see the following:\n{code}\n2017-08-08T07:41:03,315  INFO [0381325d-2c8c-46fb-ab51-423defaddd84 main] session.SparkSession: Spark cluster current has executors: 1, total cores: 2, memory per executor: 512M, memoryFraction: 0.4\n{code}\nSee: http://104.198.109.242/logs/PreCommit-HIVE-Build-6299/succeeded/171-TestMiniSparkOnYarnCliDriver-insert_overwrite_directory2.q-scriptfile1.q-vector_outer_join0.q-and-17-more/logs/hive.log\n\nWhen running the tests against a real cluster, I found that running an explain query for the first time I see 1 executor, but running it for the second time I see 2 executors.\n\nAlso setting some spark configuration on the cluster resets this behavior. For the first time I will see 1 executor, and for the second time I will see 2 executors again.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Qtest results show wrong number of executors","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"subtasks":[{"id":"13093853","key":"HIVE-17291","self":"https://issues.apache.org/jira/rest/api/2/issue/13093853","fields":{"summary":"Set the number of executors based on config if client does not provide information","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}},{"id":"13093854","key":"HIVE-17292","self":"https://issues.apache.org/jira/rest/api/2/issue/13093854","fields":{"summary":"Change TestMiniSparkOnYarnCliDriver test configuration to use the configured cores","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16118561","id":"16118561","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"body":"Also the number of reducers shown in the golden files are 2, and based on the configuration I think it should be 4. See for example:\nhttps://github.com/apache/hive/blob/master/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_2.q.out#L197\n\nI think this has to has some connection with the creation and - in case of the config change - the recreation of the RpcServer, which for the first time.\n\nI will dig further into it, but any pointers would be nice [~stakiar], or [~xuefuz].\n\nThanks,\nPeter","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"created":"2017-08-08T16:19:27.215+0000","updated":"2017-08-08T16:19:27.215+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16118672","id":"16118672","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"I'm not sure why the MiniSparkOnYarn cluster shows only 1 executor. My best guess is that the tests get are started as soon as 1 executor has started (see {{QTestUtil#createSessionState}}). Its possible the test just finishes before the second executor even gets created.\n\nWhen running against a real cluster, it depends if {{spark.dynamicAllocation.enabled}} is set to true or not. If it is true, then the number of executors will be scaled up and down depending on the resource load.\n\n{{spark.dynamicAllocation.enabled}} is {{false]} by default, so I don't think its enabled fro the MiniSparkOnYarn tests (although maybe we should change that).\n\nI'm not sure I understand your comment about the reducers. Why should it be 4 instead of 2?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2017-08-08T17:21:24.425+0000","updated":"2017-08-08T17:21:24.425+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16118723","id":"16118723","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"body":"??I'm not sure why the MiniSparkOnYarn cluster shows only 1 executor. My best guess is that the tests get are started as soon as 1 executor has started (see QTestUtil#createSessionState). Its possible the test just finishes before the second executor even gets created.??\n\nThe strange thing is, that not only the first query is showing only 1 executor in the test files, but all of them. But when I was running the tests on the cluster the first run shows 1 executor, the following ones 2 (until I change some spark configuration and the next one is only 1 executor again)\n\nOn the cluster I have unset {{spark.dynamicAllocation.enabled}}, to match the config of the {{MiniSparkOnYarn}} tests\n\nThe number of reducers are printed by this:\n{code:title=ExplainTask}\n  private JSONObject outputMap(Map<?, ?> mp, boolean hasHeader, PrintStream out,\n      boolean extended, boolean jsonOutput, int indent) throws Exception {\n[..]\n            boolean isFirst = true;\n            for (SparkWork.Dependency dep: (List<SparkWork.Dependency>) ent.getValue()) {\n              if (!isFirst) {\n                out.print(\", \");\n              } else {\n                out.print(\"<- \");\n                isFirst = false;\n              }\n              out.print(dep.getName());\n              out.print(\" (\");\n              out.print(dep.getShuffleType());\n              out.print(\", \");\n              out.print(dep.getNumPartitions());\n              out.print(\")\");\n            }\n[..]\n    return jsonOutput ? json : null;\n  }\n{code}\n\nThe GenSparkUtils.getEdgeProperty sets this:\n{code:title=GenSparkUtils}\n  public static SparkEdgeProperty getEdgeProperty(ReduceSinkOperator reduceSink,\n      ReduceWork reduceWork) throws SemanticException {\n    SparkEdgeProperty edgeProperty = new SparkEdgeProperty(SparkEdgeProperty.SHUFFLE_NONE);\n    edgeProperty.setNumPartitions(reduceWork.getNumReduceTasks());\n[..]\n    return edgeProperty;\n  }\n{code}\n\nWhich is set by SetSparkReducerParallelism:\n{code:title=SetSparkReducerParallelism}\n  @Override\n  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n[..]\n        LOG.info(\"Set parallelism for reduce sink \" + sink + \" to: \" + numReducers +\n            \" (calculated)\");        <-- I see this in the logs which are matching the values in the explain plans\n        desc.setNumReducers(numReducers);\n[..]\n    return false;\n  }\n{code}\n\nThis is the depth where I had to go home today :)\nIf no new pointers, then I will dig deeper tomorrow :)\n\nThanks,\nPeter","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"created":"2017-08-08T17:42:06.279+0000","updated":"2017-08-08T17:42:06.279+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16118797","id":"16118797","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"1. The number of executors could be a matter of MiniSparkOnYarn configuration. By default, it has only one node manager. However, I'm not sure how many containers are allowed by that node manager. cc: [~lirui].\n2. I didn't get why the reducer should be 4. Though the number of available cores/memory plays a role, data size is also a factor. {{SetSparkReducerParallelism}} is the right place to look.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-08-08T18:21:47.605+0000","updated":"2017-08-08T18:21:47.605+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16118850","id":"16118850","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"body":"{code:title=SparkSessionImpl}\n  @Override\n  public ObjectPair<Long, Integer> getMemoryAndCores() throws Exception {\n    SparkConf sparkConf = hiveSparkClient.getSparkConf();\n    int numExecutors = hiveSparkClient.getExecutorCount();\n[..]\n    int totalCores;\n    String masterURL = sparkConf.get(\"spark.master\");\n    if (masterURL.startsWith(\"spark\")) {\n[..]\n    } else {\n      int coresPerExecutor = sparkConf.getInt(\"spark.executor.cores\", 1);\n      totalCores = numExecutors * coresPerExecutor;\n    }\n    totalCores = totalCores / sparkConf.getInt(\"spark.task.cpus\", 1);\n\n    long memoryPerTaskInBytes = totalMemory / totalCores;\n    LOG.info(\"Spark cluster current has executors: \" + numExecutors\n        + \", total cores: \" + totalCores + \", memory per executor: \"\n        + executorMemoryInMB + \"M, memoryFraction: \" + memoryFraction);\n    return new ObjectPair<Long, Integer>(Long.valueOf(memoryPerTaskInBytes),\n        Integer.valueOf(totalCores));\n  }\n{code}\n\nSo my guess is the problem with {{hiveSparkClient.getExecutorCount()}}\n\nThis seems right, but... Who knows :)\n{code:title=SparkClientImpl.GetExecutorCountJob}\n  private static class GetExecutorCountJob implements Job<Integer> {\n      private static final long serialVersionUID = 1L;\n\n      @Override\n      public Integer call(JobContext jc) throws Exception {\n        // minus 1 here otherwise driver is also counted as an executor\n        int count = jc.sc().sc().getExecutorMemoryStatus().size() - 1;\n        return Integer.valueOf(count);\n      }\n\n  }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"created":"2017-08-08T18:49:23.113+0000","updated":"2017-08-08T18:49:23.113+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16119244","id":"16119244","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"When automatically deciding numReducers, it should be no less than numCores. On the other side, {{spark.executor.instances}} is numContainers spark will request from YARN. How many containers can be really allocated is up to YARN.\nSo I can think of two possible reasons why we have 2 instead of 4 here.\n# Only 1 container is allocated, in which case 2 is the right way to go.\n# 2 containers are allocated but only 1 has started running when we get the executor count. This is a common case in real cluster and can make our test result unstable. We should find a way to fix it.\n\nI guess we can monitor the log of the mini-yarn test to see how many cores we really have during execution.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-09T00:35:58.260+0000","updated":"2017-08-09T00:35:58.260+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16120039","id":"16120039","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"body":"Ok, I think I understand now what is happening.\n\nWhen running {{TestMiniSparkOnYarnCliDriver}} tests, we set the number of execution instances to 2 in the {{data/conf/spark/yarn-client/hive-site.xml}}:\n{code}\n<property>\n  <name>spark.executor.instances</name>\n  <value>2</value>\n</property>\n{code}\n\nWhen running this code against a {{Hadoop23Shims.MiniSparkShim}} we create the cluster with this {{new MiniSparkOnYARNCluster(\"sparkOnYarn\")}}. This means that the cluster is created with 1 nodeManager. So no matter what we set in the configuration we will have only 1 executor (SparkClientImpl.GetExecutorCountJob will return 1). Thus the resulting explain output will show 2*1 reducers:\n{code}\n   Stage: Stage-1\n     Spark\n       Edges:\n         Reducer 2 <- Map 1 (GROUP, 2)      <-- number of reducers are instances * cores\n         Reducer 3 <- Reducer 2 (SORT, 1)\n{code}\n\nThe good news is, that the resulting output is consistent - always will show {{2}}.\nNo change is absolutely needed, but I would prefer to use consistent configuration - so I propose one of the followings:\n# Change the {{spark.executor.instances}} to 1 in the hive-site.xml, or\n# Change the cluster creation to have more nodes {{new MiniSparkOnYARNCluster(\"sparkOnYarn\", 1, 2)}}\n\nI expect these changes will not affect the test outputs at all. I personally prefer the 1st, since it will require less resource to run the tests, and I do not see the point of having more cores for every test.\n\n\nAnother change I propose is that when we are not able to get the executors we should fall back to the one provided by the configuration, so we can provide better experience in the first run too. So instead of this:\n{code:title=SparkClientImpl}\n  public ObjectPair<Long, Integer> getMemoryAndCores() throws Exception {\n    SparkConf sparkConf = hiveSparkClient.getSparkConf();\n    int numExecutors = hiveSparkClient.getExecutorCount();\n    // at start-up, we may be unable to get number of executors\n    if (numExecutors <= 0) {\n      return new ObjectPair<Long, Integer>(-1L, -1);\n    }\n[..]\n  }\n{code}\n\nWe should read the configuration:\n{code}\n  @Override\n  public ObjectPair<Long, Integer> getMemoryAndCores() throws Exception {\n    SparkConf sparkConf = hiveSparkClient.getSparkConf();\n    int numExecutors = hiveSparkClient.getExecutorCount();\n    // at start-up, we may be unable to get number of executors, use the configuration values in this case\n    if (numExecutors <= 0) {\n      numExecutors = sparkConf.getInt(\"spark.executor.instances\", 1);\n    }\n[..]\n  }\n{code}\n\nWhat do you think about this [~stakiar], [~xuefuz], [~lirui]? If we agree, I can create the required patches / jiras etc.\nAnd many thanks for your pointers! Those are helped me tremendously!\n\nPeter","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"created":"2017-08-09T14:56:44.969+0000","updated":"2017-08-09T14:56:44.969+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16120310","id":"16120310","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the investigation, [~pvary].\n\nAuto determining the number of reducers seems rather complicated. Currently, we don't try to get memory/core when dynamic allocation is enabled because of the dynamic nature of the executors. While it seems reasonable to use {{spark.executor.instances}} as an input, the help is limited in my opinion as static allocation is expected to be rare in real production.\n\nI also had some doubts on the original idea which was to automatically and dynamically deciding the parallelism based on available memory/cores. Maybe we should back to the basis, where the number of reducers is solely determined (statically) by the total shuffled data size (stats) divided by the configuration \"bytes per reducer\". I'm open to all proposals, including doing this for dynamic allocation and using {{spark.executor.instances}} for static allocation.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-08-09T17:28:12.375+0000","updated":"2017-08-09T17:28:12.375+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16121020","id":"16121020","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~pvary], I don't understand why 1 NM means we can only have 1 executor. IIUC, NM can allocate executor/container if there's enough memory. Each executor requires 512MB mem, and the AM take another 512MB. So we use 1536MB in total. We start the NM with [2048MB total mem|https://github.com/apache/hive/blob/master/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java#L493], which should be able to satisfy all the requests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-08-10T03:58:01.390+0000","updated":"2017-08-10T03:58:01.390+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16121768","id":"16121768","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"body":"Thanks [~lirui] for the pointers. Based on that i was able to finally identify the source of the problem {{yarn.scheduler.minimum-allocation-mb}}. By default it is 1024 (MB), so the FairScheduler will use it as an increment. If we set it to 512, the we will have 4 reducers as expected.\n\n[~xuefuz]: If I understand the code correctly, when {{spark.dynamicAllocation.enabled}} is set to {{true}} then {{SetSparkReducerParallelism.getSparkMemoryAndCores()}} will set {{SetSparkReducerParallelism.sparkMemoryAndCores}} to {{null}}. And if this is null, then the number of reducers should be based only on the size of the data. So everything should be great.\nIn code:\n{code:title=SetSparkReducerParallelism.process()}\n[..]\n          // Divide it by 2 so that we can have more reducers\n          long bytesPerReducer = context.getConf().getLongVar(HiveConf.ConfVars.BYTESPERREDUCER) / 2;\n          int numReducers = Utilities.estimateReducers(numberOfBytes, bytesPerReducer,\n              maxReducers, false);\n\n          getSparkMemoryAndCores(context);        <-- this will return null, if dynamicAllocation is enabled\n          if (sparkMemoryAndCores != null &&\n              sparkMemoryAndCores.getFirst() > 0 && sparkMemoryAndCores.getSecond() > 0) {\n            // warn the user if bytes per reducer is much larger than memory per task\n            if ((double) sparkMemoryAndCores.getFirst() / bytesPerReducer < 0.5) {\n              LOG.warn(\"Average load of a reducer is much larger than its available memory. \" +\n                  \"Consider decreasing hive.exec.reducers.bytes.per.reducer\");\n            }\n\n            // If there are more cores, use the number of cores\n            numReducers = Math.max(numReducers, sparkMemoryAndCores.getSecond());\n          }\n          numReducers = Math.min(numReducers, maxReducers);\n          LOG.info(\"Set parallelism for reduce sink \" + sink + \" to: \" + numReducers +\n              \" (calculated)\");\n          desc.setNumReducers(numReducers);\n[..]\n{code}\n\nSo I will provide a patch to use the configuration values if dynamicAllocation is turned off, and the {{SparkSessionImpl.getExecutorCount()}} returns negative number.\n\nOne more question remains - or after [~lirui] comment - we have a better  option:\nHow to handle the misconfiguration in the tests:\n# Change the spark.executor.instances to 1 in the hive-site.xml, or\n# Change the yarn.scheduler.minimum-allocation-mb to 512, so we really will have 2 executors\n\nThanks for your help [~lirui], [~xuefuz]!\n\nPeter\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"created":"2017-08-10T15:04:01.700+0000","updated":"2017-08-10T15:04:01.700+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16235546","id":"16235546","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"body":"All subtasks are closed, so closing this too","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"created":"2017-11-02T11:01:18.078+0000","updated":"2017-11-02T11:01:18.078+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13093220/comment/16486154","id":"16486154","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgarg","name":"vgarg","key":"vgarg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=vgarg&avatarId=30430","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=vgarg&avatarId=30430","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=vgarg&avatarId=30430","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=vgarg&avatarId=30430"},"displayName":"Vineet Garg","active":true,"timeZone":"America/Los_Angeles"},"body":"Hive 3.0.0 has been released so closing this jira.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgarg","name":"vgarg","key":"vgarg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=vgarg&avatarId=30430","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=vgarg&avatarId=30430","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=vgarg&avatarId=30430","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=vgarg&avatarId=30430"},"displayName":"Vineet Garg","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-22T23:59:21.768+0000","updated":"2018-05-22T23:59:21.768+0000"}],"maxResults":12,"total":12,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17270/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3ijsn:"}}