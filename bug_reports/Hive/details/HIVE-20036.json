{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13169159","self":"https://issues.apache.org/jira/rest/api/2/issue/13169159","key":"HIVE-20036","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/10004","id":"10004","description":"Not A Bug","name":"Not A Bug"},"customfield_12312322":null,"customfield_12310220":null,"customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Aug 16 05:02:16 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_4116431011_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2018-08-16T05:02:16.294+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-20036/watchers","watchCount":1,"isWatching":false},"created":"2018-06-29T13:35:05.320+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12342162","id":"12342162","name":"2.3.3","archived":false,"released":false,"releaseDate":"2018-04-03"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Matrix0xCC","name":"Matrix0xCC","key":"matrix0xcc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"Matrix0xCC","active":true,"timeZone":"Etc/UTC"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-08-16T05:02:16.325+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312584","id":"12312584","name":"Metastore","description":"Tracks issue dealing with metastore."}],"timeoriginalestimate":null,"description":"I'm using Hive 2.3.3 with Hadoop 3.0.0 and Spark 2.2.1.\r\n\r\nI've created a partitioned orc table and enabled compaction. \r\n\r\nBut the compaction task keeps failing and complains that a URI cannot be resolved.\r\n\r\nhere is the yarn application diagnostics log:\r\n{code:java}\r\nApplication application_1529550480937_0033 failed 2 times due to AM Container for appattempt_1529550480937_0033_000002 exited with exitCode: -1000\r\nFailing this attempt.Diagnostics: [2018-06-29 17:25:25.656]Port 8020 specified in URI hdfs://hadoopcluster:8020/tmp/hadoop-yarn/staging/smsuser/.staging/job_1529550480937_0033/job.splitmetainfo but host 'hadoopcluster' is a logical (HA) namenode and does not use port information.\r\njava.io.IOException: Port 8020 specified in URI hdfs://hadoopcluster:8020/tmp/hadoop-yarn/staging/smsuser/.staging/job_1529550480937_0033/job.splitmetainfo but host 'hadoopcluster' is a logical (HA) namenode and does not use port information.\r\nat org.apache.hadoop.hdfs.NameNodeProxiesClient.createFailoverProxyProvider(NameNodeProxiesClient.java:266)\r\nat org.apache.hadoop.hdfs.NameNodeProxiesClient.createFailoverProxyProvider(NameNodeProxiesClient.java:217)\r\nat org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:127)\r\nat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:355)\r\nat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:289)\r\nat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:163)\r\nat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3288)\r\nat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)\r\nat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3337)\r\nat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3305)\r\nat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:476)\r\nat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\nat org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:251)\r\nat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63)\r\nat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:366)\r\nat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:364)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)\r\nat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:364)\r\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:241)\r\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:234)\r\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:222)\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748)\r\nFor more detailed output, check the application tracking page: http://cluster-master:8088/cluster/app/application_1529550480937_0033 Then click on links to logs of each attempt.\r\n. Failing the application.\r\n{code}\r\nThis is my core-site.xml and hdfs-site.xml\r\n{code:xml}\r\n<configuration>\r\n<property>\r\n<name>hadoop.tmp.dir</name>\r\n<value>file:/opt/hdfs/tmp/</value>\r\n<description>A base for other temporary directories.</description>\r\n</property>\r\n\r\n<property>\r\n<name>io.file.buffer.size</name>\r\n<!-- 128k -->\r\n<value>131072</value>\r\n</property>\r\n\r\n<property>\r\n<name>fs.defaultFS</name>\r\n<value>hdfs://hadoopcluster</value>\r\n</property>\r\n\r\n<property>\r\n<name>hadoop.proxyuser.smsuser.hosts</name>\r\n<value>*</value>\r\n</property>\r\n\r\n<property>\r\n<name>hadoop.proxyuser.smsuser.groups</name>\r\n<value>*</value>\r\n</property>\r\n\r\n</configuration>\r\n{code}\r\n{code:xml}\r\n<configuration>\r\n<property>\r\n<name>dfs.nameservices</name>\r\n<value>hadoopcluster</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.ha.namenodes.hadoopcluster</name>\r\n<value>cluster-master,cluster-backup</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.namenode.rpc-address.hadoopcluster.cluster-master</name>\r\n<value>cluster-master:9820</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.namenode.rpc-address.hadoopcluster.cluster-backup</name>\r\n<value>cluster-backup:9820</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.namenode.http-address.hadoopcluster.cluster-master</name>\r\n<value>cluster-master:9870</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.namenode.http-address.hadoopcluster.cluster-backup</name>\r\n<value>cluster-backup:9870</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.namenode.shared.edits.dir</name>\r\n<value>qjournal://cluster-node1:8485;cluster-node2:8485;cluster-node3:8485/hadoopcluster</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.client.failover.proxy.provider.hadoopcluster</name>\r\n<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.ha.fencing.methods</name>\r\n<value>sshfence</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.ha.fencing.ssh.private-key-files</name>\r\n<value>/home/smsuser/.ssh/id_rsa</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.journalnode.edits.dir</name>\r\n<value>/opt/hdfs/journal</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.replication</name>\r\n<value>3</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.namenode.name.dir</name>\r\n<value>/opt/hdfs/name</value>\r\n</property>\r\n\r\n<property>\r\n<name>dfs.datanode.name.dir</name>\r\n<value>/opt/hdfs/data</value>\r\n</property>\r\n\r\n{code}\r\n I guess there may be a configuration mistake but I failed to dig out after searching a lot and reading the src code.\r\n\r\n \r\n\r\nPlease help me. Thanks a lot.\r\n\r\n \r\n\r\n ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive Compactor MapReduce  task keeps failing due to wrong hadoop URI.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Matrix0xCC","name":"Matrix0xCC","key":"matrix0xcc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"Matrix0xCC","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Matrix0xCC","name":"Matrix0xCC","key":"matrix0xcc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"Matrix0xCC","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":"2019-06-07","customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13169159/comment/16581956","id":"16581956","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Matrix0xCC","name":"Matrix0xCC","key":"matrix0xcc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"Matrix0xCC","active":true,"timeZone":"Etc/UTC"},"body":"Fixed by myself. \r\n\r\nHadoop 3.0.0 use 9820 as the default RPC port while prior and later versions all used 8020, which leads to a compatibility problem.\r\n\r\nI upgrade Hadoop to 3.0.3 and the compaction works perfectly now.\r\n\r\nSee this issue:\r\n[Change default NameNode RPC port back to 8020|https://issues.apache.org/jira/browse/HDFS-12990]\r\n\r\n \r\n\r\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Matrix0xCC","name":"Matrix0xCC","key":"matrix0xcc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"Matrix0xCC","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-16T04:55:13.807+0000","updated":"2018-08-16T04:56:19.960+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13169159/comment/16581960","id":"16581960","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Matrix0xCC","name":"Matrix0xCC","key":"matrix0xcc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"Matrix0xCC","active":true,"timeZone":"Etc/UTC"},"body":"Not a bug of Hive, but a compatibility problem of HDFS.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Matrix0xCC","name":"Matrix0xCC","key":"matrix0xcc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"Matrix0xCC","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-16T05:02:16.320+0000","updated":"2018-08-16T05:02:16.320+0000"}],"maxResults":2,"total":2,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-20036/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3vdwv:"}}