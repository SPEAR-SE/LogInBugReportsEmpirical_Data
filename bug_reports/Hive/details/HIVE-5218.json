{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12667054","self":"https://issues.apache.org/jira/rest/api/2/issue/12667054","key":"HIVE-5218","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12324986","id":"12324986","description":"released","name":"0.13.0","archived":false,"released":true,"releaseDate":"2014-04-21"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2013-09-06T19:44:01.993+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Mar 06 15:04:46 UTC 2014","customfield_12310420":"346991","customfield_12312320":null,"customfield_12310222":"10002_*:*_2_*:*_11452362648_*|*_1_*:*_2_*:*_4302010100_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-03-06T15:04:46.461+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-5218/watchers","watchCount":9,"isWatching":false},"created":"2013-09-05T06:51:53.749+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"6.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12324312","id":"12324312","description":"released","name":"0.12.0","archived":false,"released":true,"releaseDate":"2013-10-15"}],"issuelinks":[{"id":"12384311","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12384311","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12663780","key":"HIVE-5099","self":"https://issues.apache.org/jira/rest/api/2/issue/12663780","fields":{"summary":"Some partition publish operation cause OOM in metastore backed by SQL Server","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12383508","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12383508","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12663780","key":"HIVE-5099","self":"https://issues.apache.org/jira/rest/api/2/issue/12663780","fields":{"summary":"Some partition publish operation cause OOM in metastore backed by SQL Server","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12374774","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12374774","type":{"id":"12310050","name":"Regression","inward":"is broken by","outward":"breaks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310050"},"inwardIssue":{"id":"12613843","key":"HIVE-3632","self":"https://issues.apache.org/jira/rest/api/2/issue/12613843","fields":{"summary":"Upgrade datanucleus to support JDK7","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-03-06T15:04:46.494+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312584","id":"12312584","name":"Metastore","description":"Tracks issue dealing with metastore."}],"timeoriginalestimate":null,"description":"HIVE-3632 upgraded datanucleus version to 3.2.x, however, this version of datanucleus doesn't work with SQLServer as the metastore. The problem is that datanucleus tries to use fully qualified object name to find a table in the database but couldn't find it.\n\nIf I downgrade the version to HIVE-2084, SQLServer works fine.\n\nIt could be a bug in datanucleus.\n\nThis is the detailed exception I'm getting when using datanucleus 3.2.x with SQL Server:\n\n{noformat}\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTa\nsk. MetaException(message:javax.jdo.JDOException: Exception thrown calling table\n.exists() for a2ee36af45e9f46c19e995bfd2d9b5fd1hivemetastore..SEQUENCE_TABLE\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusExc\neption(NucleusJDOHelper.java:596)\n        at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPe\nrsistenceManager.java:732)\nâ€¦\n        at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawS\ntore.java:111)\n        at $Proxy0.createTable(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_tabl\ne_core(HiveMetaStore.java:1071)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_tabl\ne_with_environment_context(HiveMetaStore.java:1104)\nâ€¦\n        at $Proxy11.create_table_with_environment_context(Unknown Source)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$cr\neate_table_with_environment_context.getResult(ThriftHiveMetastore.java:6417)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$cr\neate_table_with_environment_context.getResult(ThriftHiveMetastore.java:6401)\n\nNestedThrowablesStackTrace:\ncom.microsoft.sqlserver.jdbc.SQLServerException: There is already an object name\nd 'SEQUENCE_TABLE' in the database.\n        at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError\n(SQLServerException.java:197)\n        at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServ\nerStatement.java:1493)\n        at com.microsoft.sqlserver.jdbc.SQLServerStatement.doExecuteStatement(SQ\nLServerStatement.java:775)\n        at com.microsoft.sqlserver.jdbc.SQLServerStatement$StmtExecCmd.doExecute\n(SQLServerStatement.java:676)\n        at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4615)\n        at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLSe\nrverConnection.java:1400)\n        at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLSer\nverStatement.java:179)\n        at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLS\nerverStatement.java:154)\n        at com.microsoft.sqlserver.jdbc.SQLServerStatement.execute(SQLServerStat\nement.java:649)\n        at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:300)\n        at org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatement(A\nbstractTable.java:760)\n        at org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatementLi\nst(AbstractTable.java:711)\n        at org.datanucleus.store.rdbms.table.AbstractTable.create(AbstractTable.\njava:425)\n        at org.datanucleus.store.rdbms.table.AbstractTable.exists(AbstractTable.\njava:488)\n        at org.datanucleus.store.rdbms.valuegenerator.TableGenerator.repositoryE\nxists(TableGenerator.java:242)\n        at org.datanucleus.store.rdbms.valuegenerator.AbstractRDBMSGenerator.obt\nainGenerationBlock(AbstractRDBMSGenerator.java:86)\n        at org.datanucleus.store.valuegenerator.AbstractGenerator.obtainGenerati\nonBlock(AbstractGenerator.java:197)\n        at org.datanucleus.store.valuegenerator.AbstractGenerator.next(AbstractG\nenerator.java:105)\n        at org.datanucleus.store.rdbms.RDBMSStoreManager.getStrategyValueForGene\nrator(RDBMSStoreManager.java:2019)\n        at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractS\ntoreManager.java:1385)\n        at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl\n.java:3727)\n        at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.jav\na:2574)\n        at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOS\ntateManager.java:526)\n        at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(O\nbjectProviderFactoryImpl.java:202)\n        at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNe\nw(ExecutionContextImpl.java:1326)\n        at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionC\nontextImpl.java:2123)\n        at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionConte\nxtImpl.java:1972)\n        at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextIm\npl.java:1820)\n        at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionC\nontextThreadedImpl.java:217)\n        at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPe\nrsistenceManager.java:727)\n        at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersi\nstenceManager.java:752)\n        at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.\njava:646)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.\njava:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawS\ntore.java:111)\n        at $Proxy0.createTable(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_tabl\ne_core(HiveMetaStore.java:1071)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_tabl\ne_with_environment_context(HiveMetaStore.java:1104)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.\njava:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHM\nSHandler.java:103)\n        at $Proxy11.create_table_with_environment_context(Unknown Source)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$cr\neate_table_with_environment_context.getResult(ThriftHiveMetastore.java:6417)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$cr\neate_table_with_environment_context.getResult(ThriftHiveMetastore.java:6401)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.process(TSetI\npAddressProcessor.java:48)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadP\noolServer.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.\njava:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor\n.java:603)\n        at java.lang.Thread.run(Thread.java:722)\n{noformat}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12602173","id":"12602173","filename":"0001-HIVE-5218-datanucleus-does-not-work-with-SQLServer-i.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sergey.soldatov","name":"sergey.soldatov","key":"sergey.soldatov","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Soldatov","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-09T17:52:00.252+0000","size":1687,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12602173/0001-HIVE-5218-datanucleus-does-not-work-with-SQLServer-i.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12610570","id":"12610570","filename":"HIVE-5218.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-28T14:55:05.914+0000","size":559,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12610570/HIVE-5218.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12603115","id":"12603115","filename":"HIVE-5218.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-13T21:42:15.438+0000","size":2763,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12603115/HIVE-5218.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12613423","id":"12613423","filename":"HIVE-5218-trunk.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-12T21:05:43.177+0000","size":846,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12613423/HIVE-5218-trunk.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12612507","id":"12612507","filename":"HIVE-5218-trunk.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-07T02:06:51.555+0000","size":846,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12612507/HIVE-5218-trunk.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12610218","id":"12610218","filename":"HIVE-5218-v2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-25T01:47:45.545+0000","size":559,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12610218/HIVE-5218-v2.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"347291","customfield_12312823":null,"summary":"datanucleus does not work with MS SQLServer in Hive metastore","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13760550","id":"13760550","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"body":"With downgrade - as in HIVE-2084 - JDK7 stops working, actually. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-06T19:44:01.993+0000","updated":"2013-09-06T19:44:01.993+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13760553","id":"13760553","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"Seems like a DN bug to me?  Have you tried the latest version of DN?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-06T19:49:56.133+0000","updated":"2013-09-06T19:49:56.133+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13760579","id":"13760579","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"body":"Actually, it looks like 4900 isn't complete.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-06T20:17:46.604+0000","updated":"2013-09-06T20:17:46.604+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13760580","id":"13760580","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"body":"I meant HIVE-4900","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-06T20:17:55.840+0000","updated":"2013-09-06T20:17:55.840+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13760588","id":"13760588","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~cos] It's certainly possible, but could you please be more specific what is missing in HIVE-4900? Please feel free to provide a fix and attach your patch here so that we know what you're referring to.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-06T20:27:46.614+0000","updated":"2013-09-06T20:27:46.614+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13762094","id":"13762094","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sergey.soldatov","name":"sergey.soldatov","key":"sergey.soldatov","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Soldatov","active":true,"timeZone":"Etc/UTC"},"body":" Could you please try the attachmed fix. If it's still fail, could you please attach the logs.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sergey.soldatov","name":"sergey.soldatov","key":"sergey.soldatov","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Soldatov","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-09T17:52:52.202+0000","updated":"2013-09-09T17:52:52.202+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13762671","id":"13762671","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"[~sergey.soldatov] I tried your patch but it doesn't fix the problem.\n\nThe problem is not with user defined table, it's actually with the SEQUENCE_TABLE table which datanucleus uses internally to track the sequence number. I did a little bit research and found that it's a datanucleus bug, with my fix in datanucleus this problem went away.\n\nI submitted a JIRA there:\nhttp://www.datanucleus.org/servlet/jira/browse/NUCRDBMS-692\n\nHowever, for hive should we revert the datanucleus upgrade until the patch in NUCRDBMS-692 get checked in (3.2.7 the soonest)?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-10T02:58:57.284+0000","updated":"2013-09-10T02:58:57.284+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13762707","id":"13762707","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"body":"Hmm, this is interesting... I have tried this patch on top of HIVE-4900 and seems to solve the problem with the version in datanucleus used by Hive atm. Running patched version of Hive with Postgres 9.2 as metastore shows no problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-10T04:04:15.074+0000","updated":"2013-09-10T04:04:15.074+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13762728","id":"13762728","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"{quote}\nHowever, for hive should we revert the datanucleus upgrade until the patch in NUCRDBMS-692 get checked in (3.2.7 the soonest)?\n{quote}\n\nBefore we make a decision, I think we need to evaluate the impact and possible workarounds. Maybe there are some sort of configs that can be set. Also, I'm wondering if DN guys have confirmed the bug.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-10T04:54:30.282+0000","updated":"2013-09-10T04:54:30.282+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13767001","id":"13767001","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"[~cos]This bug only happens with SQL Server as metastore.\n\nI attached a patch that downgrades datanucleus to 3.0.x. \n\nI understand that this version of datanucleus doesn't work with JDK7. But I think it's more important that we don't release a version of hive that doesn't work with SQL Server as metastore.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-13T21:42:15.441+0000","updated":"2013-09-13T21:42:15.441+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13767045","id":"13767045","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. I understand that this version of datanucleus doesn't work with JDK7. But I think it's more important that we don't release a version of hive that doesn't work with SQL Server as metastore.\n\nI don't agree. We don't even provide scripts for SQL Server:\n\nhttps://github.com/apache/hive/tree/trunk/metastore/scripts/upgrade\n\nwhereas Java 6 is [end of life|http://www.oracle.com/technetwork/java/eol-135779.html].","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-13T22:10:47.082+0000","updated":"2013-09-13T22:10:47.082+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13767057","id":"13767057","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"[~brocknoland]Thanks for your response. I understand that SQL Server specific scripts are not provided for database upgrade. But are you saying that SQL Server is NOT a supported database for metastore?\n\nBtw, SQL Server works fine with previous Hive releases, e.g. 0.11.0. So this will be a regression issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-13T22:21:27.858+0000","updated":"2013-09-13T22:21:27.858+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13767205","id":"13767205","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi,\n\nI don't want to block SQL Server support. However, SQL Server is just as supported as DB2, Informix, Sybase, SQLite, Teradata, Netezza, etc. It might work or it might not. There is no offical support for any database, all \"support\" is [de facto|https://issues.apache.org/jira/browse/HIVE-1391?focusedCommentId=12975068&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12975068]. Given there are no sql scripts for SQL Server shipped with Hive we cannot even claim de facto support for SQL Server.\n\nI'd be more than happy to review SQL Server scripts and review any changes required to upgrade to a version of DN with SQL Server support fixed, but reverting a change that has [widespread|https://issues.apache.org/jira/browse/HIVE-3632?focusedCommentId=13708673&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13708673] community support is the not the way towards SQL Server support.\n\nUntil DN is able to provide a fix or workaround that we can incorporate in Hive, I suggest SQL Server users apply your patch on top of the Hive 0.12 release. Minor customization of Apache software is extremely common as all Apache releases are [source code|http://www.apache.org/dev/release.html] with the binaries provided only as a convenience to users.\n\nBrock","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-14T00:01:02.079+0000","updated":"2013-09-14T00:01:02.079+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13767249","id":"13767249","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"body":"Let's make this ticket clear - this is MS SQLServer, not _any_ SQL server.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-14T00:40:56.579+0000","updated":"2013-09-14T00:40:56.579+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13799658","id":"13799658","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Datanucleus patch was committed:\nhttp://www.datanucleus.org/servlet/jira/browse/NUCRDBMS-692\n\nSo we can move to 3.2.7 when it is released. (according to their website, it's October 2013).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-18T23:43:17.442+0000","updated":"2013-10-18T23:43:17.442+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13803019","id":"13803019","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andy","name":"andy","key":"andy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Andy Jefferson","active":true,"timeZone":"Etc/UTC"},"body":"FYI 3.2.7 of datanucleus-rdbms is released","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andy","name":"andy","key":"andy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Andy Jefferson","active":true,"timeZone":"Etc/UTC"},"created":"2013-10-23T17:01:16.708+0000","updated":"2013-10-23T17:01:16.708+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13803036","id":"13803036","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"Great! @shanyu, I'd be happy to review a patch upgrading to 3.2.7.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-23T17:25:58.188+0000","updated":"2013-10-23T17:25:58.188+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13804922","id":"13804922","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Attaching a patch to upgrade datanucleus version that has the change to support MS SQLServer 2005 and later.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-25T01:47:45.555+0000","updated":"2013-10-25T01:47:45.555+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13806827","id":"13806827","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"Reuploading the patch with a correct name for testing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-28T14:55:05.918+0000","updated":"2013-10-28T14:55:05.918+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13806942","id":"13806942","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:green}Overall{color}: +1 all checks pass\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12610570/HIVE-5218.2.patch\n\n{color:green}SUCCESS:{color} +1 4502 tests passed\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1270/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1270/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\n{noformat}\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-10-28T17:09:53.405+0000","updated":"2013-10-28T17:09:53.405+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13815555","id":"13815555","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Create a new patch that is applicable on trunk (because it moved to use maven)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanyu","name":"shanyu","key":"shanyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"shanyu zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-07T02:06:51.559+0000","updated":"2013-11-07T02:06:51.559+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13816108","id":"13816108","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you Shanyu! +1 pending tests","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-07T16:40:11.496+0000","updated":"2013-11-07T16:40:11.496+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13816133","id":"13816133","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 no tests executed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12612507/HIVE-5218-trunk.patch\n\nTest results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/173/testReport\nConsole output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/173/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-173/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ svn = \\s\\v\\n ]]\n+ [[ -n '' ]]\n+ [[ -d apache-svn-trunk-source ]]\n+ [[ ! -d apache-svn-trunk-source/.svn ]]\n+ [[ ! -d apache-svn-trunk-source ]]\n+ cd apache-svn-trunk-source\n+ svn revert -R .\nReverted 'hbase-handler/src/test/results/positive/hbase_stats2.q.out'\nReverted 'hbase-handler/src/test/results/positive/hbase_stats3.q.out'\nReverted 'hbase-handler/src/test/results/positive/hbase_stats.q.out'\nReverted 'hbase-handler/src/test/results/positive/hbase_stats_empty_partition.q.out'\nReverted 'metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java'\nReverted 'common/src/java/org/apache/hadoop/hive/common/StatsSetupConst.java'\nReverted 'ql/src/test/results/clientnegative/unset_table_property.q.out'\nReverted 'ql/src/test/results/clientnegative/stats_partialscan_autogether.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort.q.out'\nReverted 'ql/src/test/results/clientpositive/groupby_ppr.q.out'\nReverted 'ql/src/test/results/clientpositive/stats8.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_num_buckets.q.out'\nReverted 'ql/src/test/results/clientpositive/input_part7.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin5.q.out'\nReverted 'ql/src/test/results/clientpositive/pcr.q.out'\nReverted 'ql/src/test/results/clientpositive/show_tblproperties.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_map_operators.q.out'\nReverted 'ql/src/test/results/clientpositive/stats3.q.out'\nReverted 'ql/src/test/results/clientpositive/join33.q.out'\nReverted 'ql/src/test/results/clientpositive/input_part2.q.out'\nReverted 'ql/src/test/results/clientpositive/alter_partition_coltype.q.out'\nReverted 'ql/src/test/results/clientpositive/stats_noscan_1.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_4.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_dyn_part.q.out'\nReverted 'ql/src/test/results/clientpositive/load_dyn_part8.q.out'\nReverted 'ql/src/test/results/clientpositive/sample9.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_merge.q.out'\nReverted 'ql/src/test/results/clientpositive/describe_table.q.out'\nReverted 'ql/src/test/results/clientpositive/groupby_map_ppr.q.out'\nReverted 'ql/src/test/results/clientpositive/groupby_sort_6.q.out'\nReverted 'ql/src/test/results/clientpositive/sample4.q.out'\nReverted 'ql/src/test/results/clientpositive/stats18.q.out'\nReverted 'ql/src/test/results/clientpositive/push_or.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketcontext_7.q.out'\nReverted 'ql/src/test/results/clientpositive/groupby_sort_1.q.out'\nReverted 'ql/src/test/results/clientpositive/stats13.q.out'\nReverted 'ql/src/test/results/clientpositive/udf_reflect2.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_11.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_convert_join.q.out'\nReverted 'ql/src/test/results/clientpositive/rand_partitionpruner1.q.out'\nReverted 'ql/src/test/results/clientpositive/combine2_hadoop20.q.out'\nReverted 'ql/src/test/results/clientpositive/show_create_table_alter.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketcontext_2.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_join_reordering_values.q.out'\nReverted 'ql/src/test/results/clientpositive/stats_only_null.q.out'\nReverted 'ql/src/test/results/clientpositive/bucket2.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_multi_insert.q.out'\nReverted 'ql/src/test/results/clientpositive/groupby_map_ppr_multi_distinct.q.out'\nReverted 'ql/src/test/results/clientpositive/parallel_orderby.q.out'\nReverted 'ql/src/test/results/clientpositive/filter_join_breaktask.q.out'\nReverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out'\nReverted 'ql/src/test/results/clientpositive/join17.q.out'\nReverted 'ql/src/test/results/clientpositive/input_part9.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin7.q.out'\nReverted 'ql/src/test/results/clientpositive/alter_table_serde2.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin11.q.out'\nReverted 'ql/src/test/results/clientpositive/join26.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out'\nReverted 'ql/src/test/results/clientpositive/rcfile_default_format.q.out'\nReverted 'ql/src/test/results/clientpositive/stats5.q.out'\nReverted 'ql/src/test/results/clientpositive/ppd_join_filter.q.out'\nReverted 'ql/src/test/results/clientpositive/join35.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin2.q.out'\nReverted 'ql/src/test/results/clientpositive/alter_partition_clusterby_sortby.q.out'\nReverted 'ql/src/test/results/clientpositive/join_map_ppr.q.out'\nReverted 'ql/src/test/results/clientpositive/stats0.q.out'\nReverted 'ql/src/test/results/clientpositive/join9.q.out'\nReverted 'ql/src/test/results/clientpositive/smb_mapjoin_11.q.out'\nReverted 'ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_1.q.out'\nReverted 'ql/src/test/results/clientpositive/sample6.q.out'\nReverted 'ql/src/test/results/clientpositive/join_filters_overlap.q.out'\nReverted 'ql/src/test/results/clientpositive/stats_empty_partition.q.out'\nReverted 'ql/src/test/results/clientpositive/create_alter_list_bucketing_table1.q.out'\nReverted 'ql/src/test/results/clientpositive/bucket_map_join_1.q.out'\nReverted 'ql/src/test/results/clientpositive/sample1.q.out'\nReverted 'ql/src/test/results/clientpositive/stats15.q.out'\nReverted 'ql/src/test/results/clientpositive/stats_partscan_1.q.out'\nReverted 'ql/src/test/results/clientpositive/reduce_deduplicate.q.out'\nReverted 'ql/src/test/results/clientpositive/rand_partitionpruner3.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketcontext_4.q.out'\nReverted 'ql/src/test/results/clientpositive/stats10.q.out'\nReverted 'ql/src/test/results/clientpositive/bucket4.q.out'\nReverted 'ql/src/test/results/clientpositive/udtf_explode.q.out'\nReverted 'ql/src/test/results/clientpositive/merge3.q.out'\nReverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out'\nReverted 'ql/src/test/results/clientpositive/binary_output_format.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin9.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin13.q.out'\nReverted 'ql/src/test/results/clientpositive/stats7.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketizedhiveinputformat.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin4.q.out'\nReverted 'ql/src/test/results/clientpositive/union22.q.out'\nReverted 'ql/src/test/results/clientpositive/smb_mapjoin_13.q.out'\nReverted 'ql/src/test/results/clientpositive/unset_table_view_property.q.out'\nReverted 'ql/src/test/results/clientpositive/join32.q.out'\nReverted 'ql/src/test/results/clientpositive/ctas_uses_database_location.q.out'\nReverted 'ql/src/test/results/clientpositive/input_part1.q.out'\nReverted 'ql/src/test/results/clientpositive/groupby_sort_skew_1.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_8.q.out'\nReverted 'ql/src/test/results/clientpositive/columnstats_partlvl.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_3.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin_negative3.q.out'\nReverted 'ql/src/test/results/clientpositive/sample8.q.out'\nReverted 'ql/src/test/results/clientpositive/transform_ppr2.q.out'\nReverted 'ql/src/test/results/clientpositive/union_ppr.q.out'\nReverted 'ql/src/test/results/clientpositive/serde_user_properties.q.out'\nReverted 'ql/src/test/results/clientpositive/alter_table_not_sorted.q.out'\nReverted 'ql/src/test/results/clientpositive/alter_numbuckets_partitioned_table.q.out'\nReverted 'ql/src/test/results/clientpositive/ctas_hadoop20.q.out'\nReverted 'ql/src/test/results/clientpositive/ppd_vc.q.out'\nReverted 'ql/src/test/results/clientpositive/dynamic_partition_skip_default.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketcontext_6.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_bucketed_table.q.out'\nReverted 'ql/src/test/results/clientpositive/stats12.q.out'\nReverted 'ql/src/test/results/clientpositive/router_join_ppr.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketcontext_1.q.out'\nReverted 'ql/src/test/results/clientpositive/bucket1.q.out'\nReverted 'ql/src/test/results/clientpositive/input42.q.out'\nReverted 'ql/src/test/results/clientpositive/stats9.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_grouping_operators.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin10.q.out'\nReverted 'ql/src/test/results/clientpositive/union24.q.out'\nReverted 'ql/src/test/results/clientpositive/metadata_only_queries.q.out'\nReverted 'ql/src/test/results/clientpositive/stats4.q.out'\nReverted 'ql/src/test/results/clientpositive/columnstats_tbllvl.q.out'\nReverted 'ql/src/test/results/clientpositive/smb_mapjoin_15.q.out'\nReverted 'ql/src/test/results/clientpositive/join34.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin1.q.out'\nReverted 'ql/src/test/results/clientpositive/sample10.q.out'\nReverted 'ql/src/test/results/clientpositive/stats_noscan_2.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_5.q.out'\nReverted 'ql/src/test/results/clientpositive/louter_join_ppr.q.out'\nReverted 'ql/src/test/results/clientpositive/sample5.q.out'\nReverted 'ql/src/test/results/clientpositive/infer_bucket_sort_reducers_power_two.q.out'\nReverted 'ql/src/test/results/clientpositive/stats19.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketcontext_8.q.out'\nReverted 'ql/src/test/results/clientpositive/udf_explode.q.out'\nReverted 'ql/src/test/results/clientpositive/alter_numbuckets_partitioned_table2.q.out'\nReverted 'ql/src/test/results/clientpositive/stats14.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_12.q.out'\nReverted 'ql/src/test/results/clientpositive/rand_partitionpruner2.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketcontext_3.q.out'\nReverted 'ql/src/test/results/clientpositive/bucket3.q.out'\nReverted 'ql/src/test/results/clientpositive/groupby_ppr_multi_distinct.q.out'\nReverted 'ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin8.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin12.q.out'\nReverted 'ql/src/test/results/clientpositive/stats6.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin3.q.out'\nReverted 'ql/src/test/results/clientpositive/alter_skewed_table.q.out'\nReverted 'ql/src/test/results/clientpositive/stats1.q.out'\nReverted 'ql/src/test/results/clientpositive/smb_mapjoin_12.q.out'\nReverted 'ql/src/test/results/clientpositive/join32_lessSize.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_7.q.out'\nReverted 'ql/src/test/results/clientpositive/outer_join_ppr.q.out'\nReverted 'ql/src/test/results/clientpositive/list_bucket_dml_10.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_sortmerge_join_2.q.out'\nReverted 'ql/src/test/results/clientpositive/sample7.q.out'\nReverted 'ql/src/test/results/clientpositive/transform_ppr1.q.out'\nReverted 'ql/src/test/results/clientpositive/regexp_extract.q.out'\nReverted 'ql/src/test/results/clientpositive/bucket_map_join_2.q.out'\nReverted 'ql/src/test/results/clientpositive/sample2.q.out'\nReverted 'ql/src/test/results/clientpositive/stats16.q.out'\nReverted 'ql/src/test/results/clientpositive/disable_merge_for_bucketing.q.out'\nReverted 'ql/src/test/results/clientpositive/ppd_union_view.q.out'\nReverted 'ql/src/test/results/clientpositive/ctas_colname.q.out'\nReverted 'ql/src/test/results/clientpositive/truncate_column.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketcontext_5.q.out'\nReverted 'ql/src/test/results/clientpositive/describe_comment_nonascii.q.out'\nReverted 'ql/src/test/results/clientpositive/stats11.q.out'\nReverted 'ql/src/test/results/clientpositive/bucket5.q.out'\nReverted 'ql/src/test/results/clientpositive/input23.q.out'\nReverted 'ql/src/test/results/compiler/plan/join2.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input2.q.xml'\nReverted 'ql/src/test/results/compiler/plan/join3.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input3.q.xml'\nReverted 'ql/src/test/results/compiler/plan/join4.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input4.q.xml'\nReverted 'ql/src/test/results/compiler/plan/join5.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input5.q.xml'\nReverted 'ql/src/test/results/compiler/plan/join6.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input_testxpath2.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input6.q.xml'\nReverted 'ql/src/test/results/compiler/plan/join7.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input7.q.xml'\nReverted 'ql/src/test/results/compiler/plan/join8.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input_testsequencefile.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input8.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input9.q.xml'\nReverted 'ql/src/test/results/compiler/plan/union.q.xml'\nReverted 'ql/src/test/results/compiler/plan/udf1.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input_testxpath.q.xml'\nReverted 'ql/src/test/results/compiler/plan/udf6.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input_part1.q.xml'\nReverted 'ql/src/test/results/compiler/plan/groupby1.q.xml'\nReverted 'ql/src/test/results/compiler/plan/groupby2.q.xml'\nReverted 'ql/src/test/results/compiler/plan/udf_case.q.xml'\nReverted 'ql/src/test/results/compiler/plan/groupby3.q.xml'\nReverted 'ql/src/test/results/compiler/plan/subq.q.xml'\nReverted 'ql/src/test/results/compiler/plan/groupby4.q.xml'\nReverted 'ql/src/test/results/compiler/plan/groupby5.q.xml'\nReverted 'ql/src/test/results/compiler/plan/groupby6.q.xml'\nReverted 'ql/src/test/results/compiler/plan/case_sensitivity.q.xml'\nReverted 'ql/src/test/results/compiler/plan/udf_when.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input20.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample1.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample2.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample3.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample4.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample5.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample6.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample7.q.xml'\nReverted 'ql/src/test/results/compiler/plan/cast1.q.xml'\nReverted 'ql/src/test/results/compiler/plan/join1.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input1.q.xml'\nReverted 'ql/src/test/queries/clientpositive/stats_only_null.q'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java'\n++ awk '{print $2}'\n++ egrep -v '^X|^Performing status on external'\n++ svn status --no-ignore\n+ rm -rf target datanucleus.log ant/target shims/0.20/target shims/assembly/target shims/0.20S/target shims/0.23/target shims/common/target shims/common-secure/target metastore/target common/target common/src/gen serde/target ql/src/test/results/clientpositive/stats_invalidation.q.out ql/src/test/queries/clientpositive/stats_invalidation.q\n+ svn update\n\nFetching external item into 'hcatalog/src/test/e2e/harness'\nExternal at revision 1539718.\n\nAt revision 1539718.\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nGoing to apply patch with: patch -p0\n(Stripping trailing CRs from patch.)\npatching file pom.xml\nHunk #1 succeeded at 81 (offset 19 lines).\n+ [[ maven == \\m\\a\\v\\e\\n ]]\n+ rm -rf /data/hive-ptest/working/maven/org/apache/hive\n+ mvn -B clean install -DskipTests -Dmaven.repo.local=/data/hive-ptest/working/maven\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO] \n[INFO] Hive\n[INFO] Hive Ant Utilities\n[INFO] Hive Shims Common\n[INFO] Hive Shims 0.20\n[INFO] Hive Shims Secure Common\n[INFO] Hive Shims 0.20S\n[INFO] Hive Shims 0.23\n[INFO] Hive Shims\n[INFO] Hive Common\n[INFO] Hive Serde\n[INFO] Hive Metastore\n[INFO] Hive Query Language\n[INFO] Hive Service\n[INFO] Hive JDBC\n[INFO] Hive Beeline\n[INFO] Hive CLI\n[INFO] Hive Contrib\n[INFO] Hive HBase Handler\n[INFO] Hive HCatalog\n[INFO] Hive HCatalog Core\n[INFO] Hive HCatalog Pig Adapter\n[INFO] Hive HCatalog Server Extensions\n[INFO] Hive HCatalog Webhcat Java Client\n[INFO] Hive HCatalog Webhcat\n[INFO] Hive HCatalog HBase Storage Handler\n[INFO] Hive HWI\n[INFO] Hive ODBC\n[INFO] Hive Shims Aggregator\n[INFO] Hive TestUtils\n[INFO] Hive Packaging\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive/0.13.0-SNAPSHOT/hive-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Ant Utilities 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-ant ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/ant (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-ant ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/ant/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-ant ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-ant ---\n[INFO] Compiling 5 source files to /data/hive-ptest/working/apache-svn-trunk-source/ant/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/org/apache/hadoop/hive/ant/DistinctElementsClassPath.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-ant ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/ant/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-ant ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-ant ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-ant ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-ant ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/hive-ant-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-ant ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/ant/target/hive-ant-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-ant/0.13.0-SNAPSHOT/hive-ant-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/ant/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-ant/0.13.0-SNAPSHOT/hive-ant-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-common ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/common (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-common ---\n[INFO] Compiling 15 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-common ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-common ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-common ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-common ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-common ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common/0.13.0-SNAPSHOT/hive-shims-common-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common/0.13.0-SNAPSHOT/hive-shims-common-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.20 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.20 ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20 (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.20 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.20 ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.20 ---\n[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.20 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.20 ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.20 ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.20 ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.20 ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.20 ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20/0.13.0-SNAPSHOT/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20/0.13.0-SNAPSHOT/hive-shims-0.20-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims Secure Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-common-secure ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-common-secure ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-common-secure ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-common-secure ---\n[INFO] Compiling 12 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-common-secure ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-common-secure ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-common-secure ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-common-secure ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-common-secure ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-common-secure ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common-secure/0.13.0-SNAPSHOT/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common-secure/0.13.0-SNAPSHOT/hive-shims-common-secure-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.20S 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.20S ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.20S ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.20S ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.20S ---\n[INFO] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.20S ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.20S ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.20S ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.20S ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.20S ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.20S ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20S/0.13.0-SNAPSHOT/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20S/0.13.0-SNAPSHOT/hive-shims-0.20S-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.23 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.23 ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23 (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.23 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.23 ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.23 ---\n[INFO] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.23 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.23 ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.23 ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.23 ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.23 ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.23 ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.23/0.13.0-SNAPSHOT/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.23/0.13.0-SNAPSHOT/hive-shims-0.23-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims ---\n[WARNING] JAR will be empty - no content was marked for inclusion!\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-assembly-plugin:2.3:single (uberjar) @ hive-shims ---\n[INFO] Reading assembly descriptor: src/assemble/uberjar.xml\n[WARNING] Artifact: org.apache.hive:hive-shims:jar:0.13.0-SNAPSHOT references the same file as the assembly destination file. Moving it to a temporary location for inclusion.\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[WARNING] Configuration options: 'appendAssemblyId' is set to false, and 'classifier' is missing.\nInstead of attaching the assembly file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar, it will become the file for main project artifact.\nNOTE: If multiple descriptors or descriptor-formats are provided for this project, the value of this file will be non-deterministic!\n[WARNING] Replacing pre-existing project main-artifact file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/archive-tmp/hive-shims-0.13.0-SNAPSHOT.jar\nwith assembly file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.13.0-SNAPSHOT/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.13.0-SNAPSHOT/hive-shims-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-common ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/common (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (generate-version-annotation) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-common ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/common/src/gen added.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-common ---\n[INFO] Compiling 31 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 4 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-common ---\n[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/test-classes\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-common ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-common ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-common ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.13.0-SNAPSHOT/hive-common-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.13.0-SNAPSHOT/hive-common-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Serde 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-serde ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/serde (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-serde ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/protobuf/gen-java added.\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/thrift/gen-javabean added.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-serde ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-serde ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-serde ---\n[INFO] Compiling 351 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-serde ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-serde ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-serde ---\n[INFO] Compiling 41 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/test-classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-serde ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-serde ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-serde ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.13.0-SNAPSHOT/hive-serde-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.13.0-SNAPSHOT/hive-serde-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Metastore 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\nDownloading: http://www.datanucleus.org/downloads/maven2/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.pom\nDownloading: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.pom\nDownloaded: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.pom (10 KB at 242.5 KB/sec)\nDownloading: http://www.datanucleus.org/downloads/maven2/org/datanucleus/datanucleus-core/3.2.8/datanucleus-core-3.2.8.pom\nDownloading: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-core/3.2.8/datanucleus-core-3.2.8.pom\nDownloaded: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-core/3.2.8/datanucleus-core-3.2.8.pom (14 KB at 569.1 KB/sec)\nDownloading: http://www.datanucleus.org/downloads/maven2/org/datanucleus/datanucleus-rdbms/3.2.7/datanucleus-rdbms-3.2.7.pom\nDownloading: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-rdbms/3.2.7/datanucleus-rdbms-3.2.7.pom\nDownloaded: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-rdbms/3.2.7/datanucleus-rdbms-3.2.7.pom (13 KB at 429.8 KB/sec)\nDownloading: http://www.datanucleus.org/downloads/maven2/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.jar\nDownloading: http://www.datanucleus.org/downloads/maven2/org/datanucleus/datanucleus-core/3.2.8/datanucleus-core-3.2.8.jar\nDownloading: http://www.datanucleus.org/downloads/maven2/org/datanucleus/datanucleus-rdbms/3.2.7/datanucleus-rdbms-3.2.7.jar\nDownloading: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-core/3.2.8/datanucleus-core-3.2.8.jar\nDownloading: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.jar\nDownloading: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-rdbms/3.2.7/datanucleus-rdbms-3.2.7.jar\nDownloaded: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.jar (332 KB at 1303.3 KB/sec)\nDownloaded: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-rdbms/3.2.7/datanucleus-rdbms-3.2.7.jar (1731 KB at 5001.6 KB/sec)\nDownloaded: http://repo.maven.apache.org/maven2/org/datanucleus/datanucleus-core/3.2.8/datanucleus-core-3.2.8.jar (1773 KB at 5036.9 KB/sec)\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-metastore ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/metastore (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-metastore ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/model added.\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/gen/thrift/gen-javabean added.\n[INFO] \n[INFO] --- antlr3-maven-plugin:3.4:antlr (default) @ hive-metastore ---\n[INFO] ANTLR: Processing source directory /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java\nANTLR Parser Generator  Version 3.4\norg/apache/hadoop/hive/metastore/parser/Filter.g\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-metastore ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-metastore ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-metastore ---\n[INFO] Compiling 132 source files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- datanucleus-maven-plugin:3.3.0-release:enhance (default) @ hive-metastore ---\n[INFO] DataNucleus Enhancer (version 3.2.8) for API \"JDO\" using JRE \"1.6\"\nDataNucleus Enhancer : Classpath\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-maven-plugin/3.3.0-release/datanucleus-maven-plugin-3.3.0-release.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-core/3.2.8/datanucleus-core-3.2.8.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/plexus/plexus-utils/3.0.8/plexus-utils-3.0.8.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/plexus/plexus-component-annotations/1.5.5/plexus-component-annotations-1.5.5.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-inject-bean/2.3.0/sisu-inject-bean-2.3.0.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-guice/3.1.0/sisu-guice-3.1.0-no_aop.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-guava/0.9.9/sisu-guava-0.9.9.jar\n>>  /data/hive-ptest/working/maven/org/apache/xbean/xbean-reflect/3.4/xbean-reflect-3.4.jar\n>>  /data/hive-ptest/working/maven/log4j/log4j/1.2.12/log4j-1.2.12.jar\n>>  /data/hive-ptest/working/maven/commons-logging/commons-logging-api/1.1/commons-logging-api-1.1.jar\n>>  /data/hive-ptest/working/maven/com/google/collections/google-collections/1.0/google-collections-1.0.jar\n>>  /data/hive-ptest/working/maven/junit/junit/3.8.2/junit-3.8.2.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/classes\n>>  /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar\n>>  /data/hive-ptest/working/maven/org/tukaani/xz/1.0/xz-1.0.jar\n>>  /data/hive-ptest/working/maven/commons-codec/commons-codec/1.4/commons-codec-1.4.jar\n>>  /data/hive-ptest/working/maven/org/apache/avro/avro/1.7.1/avro-1.7.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar\n>>  /data/hive-ptest/working/maven/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar\n>>  /data/hive-ptest/working/maven/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.3/zookeeper-3.4.3.jar\n>>  /data/hive-ptest/working/maven/jline/jline/0.9.94/jline-0.9.94.jar\n>>  /data/hive-ptest/working/maven/org/jboss/netty/netty/3.2.2.Final/netty-3.2.2.Final.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/com/google/guava/guava/11.0.2/guava-11.0.2.jar\n>>  /data/hive-ptest/working/maven/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar\n>>  /data/hive-ptest/working/maven/commons-cli/commons-cli/1.2/commons-cli-1.2.jar\n>>  /data/hive-ptest/working/maven/commons-lang/commons-lang/2.4/commons-lang-2.4.jar\n>>  /data/hive-ptest/working/maven/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/derby/derby/10.4.2.0/derby-10.4.2.0.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-rdbms/3.2.7/datanucleus-rdbms-3.2.7.jar\n>>  /data/hive-ptest/working/maven/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar\n>>  /data/hive-ptest/working/maven/javax/transaction/jta/1.1/jta-1.1.jar\n>>  /data/hive-ptest/working/maven/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar\n>>  /data/hive-ptest/working/maven/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar\n>>  /data/hive-ptest/working/maven/antlr/antlr/2.7.7/antlr-2.7.7.jar\n>>  /data/hive-ptest/working/maven/org/apache/thrift/libfb303/0.9.0/libfb303-0.9.0.jar\n>>  /data/hive-ptest/working/maven/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar\n>>  /data/hive-ptest/working/maven/org/apache/httpcomponents/httpclient/4.1.3/httpclient-4.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/httpcomponents/httpcore/4.1.3/httpcore-4.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/hadoop/hadoop-core/1.2.1/hadoop-core-1.2.1.jar\n>>  /data/hive-ptest/working/maven/xmlenc/xmlenc/0.52/xmlenc-0.52.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar\n>>  /data/hive-ptest/working/maven/stax/stax-api/1.0.1/stax-api-1.0.1.jar\n>>  /data/hive-ptest/working/maven/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar\n>>  /data/hive-ptest/working/maven/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar\n>>  /data/hive-ptest/working/maven/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar\n>>  /data/hive-ptest/working/maven/javax/activation/activation/1.1/activation-1.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-jaxrs/1.7.1/jackson-jaxrs-1.7.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar\n>>  /data/hive-ptest/working/maven/asm/asm/3.1/asm-3.1.jar\n>>  /data/hive-ptest/working/maven/commons-io/commons-io/2.1/commons-io-2.1.jar\n>>  /data/hive-ptest/working/maven/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.jar\n>>  /data/hive-ptest/working/maven/org/apache/commons/commons-math/2.1/commons-math-2.1.jar\n>>  /data/hive-ptest/working/maven/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar\n>>  /data/hive-ptest/working/maven/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar\n>>  /data/hive-ptest/working/maven/commons-digester/commons-digester/1.8/commons-digester-1.8.jar\n>>  /data/hive-ptest/working/maven/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar\n>>  /data/hive-ptest/working/maven/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar\n>>  /data/hive-ptest/working/maven/commons-net/commons-net/1.4.1/commons-net-1.4.1.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar\n>>  /data/hive-ptest/working/maven/tomcat/jasper-runtime/5.5.12/jasper-runtime-5.5.12.jar\n>>  /data/hive-ptest/working/maven/tomcat/jasper-compiler/5.5.12/jasper-compiler-5.5.12.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar\n>>  /data/hive-ptest/working/maven/ant/ant/1.6.5/ant-1.6.5.jar\n>>  /data/hive-ptest/working/maven/commons-el/commons-el/1.0/commons-el-1.0.jar\n>>  /data/hive-ptest/working/maven/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar\n>>  /data/hive-ptest/working/maven/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar\n>>  /data/hive-ptest/working/maven/oro/oro/2.0.8/oro-2.0.8.jar\n>>  /data/hive-ptest/working/maven/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar\n>>  /data/hive-ptest/working/maven/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar\n>>  /data/hive-ptest/working/maven/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar\n>>  /data/hive-ptest/working/maven/log4j/log4j/1.2.16/log4j-1.2.16.jar\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDatabase\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MFieldSchema\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MType\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTable\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MOrder\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MStringList\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartition\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MIndex\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MRole\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MRoleMap\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MMasterKey\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDelegationToken\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MVersionTable\nDataNucleus Enhancer completed with success for 25 classes. Timings : input=592 ms, enhance=926 ms, total=1518 ms. Consult the log for full details\n\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-metastore ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-metastore ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-metastore ---\n[INFO] Compiling 10 source files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/test-classes\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-metastore ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-metastore ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:test-jar (default) @ hive-metastore ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT-tests.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-metastore ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT.pom\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT-tests.jar\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Query Language 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Hive .............................................. SUCCESS [2.514s]\n[INFO] Hive Ant Utilities ................................ SUCCESS [6.814s]\n[INFO] Hive Shims Common ................................. SUCCESS [2.807s]\n[INFO] Hive Shims 0.20 ................................... SUCCESS [1.785s]\n[INFO] Hive Shims Secure Common .......................... SUCCESS [3.234s]\n[INFO] Hive Shims 0.20S .................................. SUCCESS [1.388s]\n[INFO] Hive Shims 0.23 ................................... SUCCESS [3.196s]\n[INFO] Hive Shims ........................................ SUCCESS [3.323s]\n[INFO] Hive Common ....................................... SUCCESS [4.364s]\n[INFO] Hive Serde ........................................ SUCCESS [11.472s]\n[INFO] Hive Metastore .................................... SUCCESS [26.922s]\n[INFO] Hive Query Language ............................... FAILURE [0.612s]\n[INFO] Hive Service ...................................... SKIPPED\n[INFO] Hive JDBC ......................................... SKIPPED\n[INFO] Hive Beeline ...................................... SKIPPED\n[INFO] Hive CLI .......................................... SKIPPED\n[INFO] Hive Contrib ...................................... SKIPPED\n[INFO] Hive HBase Handler ................................ SKIPPED\n[INFO] Hive HCatalog ..................................... SKIPPED\n[INFO] Hive HCatalog Core ................................ SKIPPED\n[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED\n[INFO] Hive HCatalog Server Extensions ................... SKIPPED\n[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED\n[INFO] Hive HCatalog Webhcat ............................. SKIPPED\n[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED\n[INFO] Hive HWI .......................................... SKIPPED\n[INFO] Hive ODBC ......................................... SKIPPED\n[INFO] Hive Shims Aggregator ............................. SKIPPED\n[INFO] Hive TestUtils .................................... SKIPPED\n[INFO] Hive Packaging .................................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 1:10.700s\n[INFO] Finished at: Thu Nov 07 12:12:01 EST 2013\n[INFO] Final Memory: 42M/192M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project hive-exec: Could not resolve dependencies for project org.apache.hive:hive-exec:jar:0.13.0-SNAPSHOT: Could not find artifact org.apache.hive:hive-shims:jar:uberjar:0.13.0-SNAPSHOT -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-exec\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12612507","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-11-07T17:12:13.029+0000","updated":"2013-11-07T17:12:13.029+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13820462","id":"13820462","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"reattaching patch to run tests","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-12T21:05:43.183+0000","updated":"2013-11-12T21:05:43.183+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13821630","id":"13821630","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:green}Overall{color}: +1 all checks pass\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12613423/HIVE-5218-trunk.patch\n\n{color:green}SUCCESS:{color} +1 4604 tests passed\n\nTest results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/261/testReport\nConsole output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/261/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12613423","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-11-13T18:19:39.064+0000","updated":"2013-11-13T18:19:39.064+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13821664","id":"13821664","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I'd just like to point out that passing unit tests might not be enough, as these tests are against embedded derby. There is a way to hack into the build such that tests are running against other DBs such as MySQL. Also, I'd think manual testing on other aspects might be necessary, such as existing metastore from previous release(s), upgrade path, etc.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-13T18:50:38.004+0000","updated":"2013-11-13T18:50:38.004+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13821752","id":"13821752","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"[~xuefuz] This is just a minor version upgrade of datanucleus (from one 3.2.x to newer 3.2.x version), and I would expect datanucleus to have tests running against various databases (specially with mysql). \nHave you run the unit tests with mysql before ? Is it just matter to changing  data/conf/hive-site.xml  ?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-13T19:48:35.927+0000","updated":"2013-11-13T19:48:35.927+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13821767","id":"13821767","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~thejas] I think you're right that this is a minor upgrade and I expect that DN has its test with MySQL. My concern is more about whether the new DN works for hive + other DB. In HIVE-3632, we actually found problems during manual testing and some flags were set on hive side in order to work with meta created in previous releases. It's unlikely that we will have similar issue, but it might be good to be on the safe side.\n\nYes. it's matter of changing the config.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-13T19:59:31.200+0000","updated":"2013-11-13T19:59:31.200+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12667054/comment/13922613","id":"13922613","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"Marking as duplicate. HIVE-5099 has patch that upgrades to a newer datanucleus version.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-06T15:04:46.489+0000","updated":"2014-03-06T15:04:46.489+0000"}],"maxResults":29,"total":29,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-5218/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1nu1z:"}}