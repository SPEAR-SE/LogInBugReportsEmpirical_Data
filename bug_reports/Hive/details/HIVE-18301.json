{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13125797","self":"https://issues.apache.org/jira/rest/api/2/issue/13125797","key":"HIVE-18301","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-12-20T05:19:06.890+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Feb 02 09:43:49 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-18301/watchers","watchCount":7,"isWatching":false},"created":"2017-12-19T05:26:23.040+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12522536","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12522536","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"outwardIssue":{"id":"13100713","key":"HIVE-17486","self":"https://issues.apache.org/jira/rest/api/2/issue/13100713","fields":{"summary":"Enable SharedWorkOptimizer in tez on HOS","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-02-02T09:59:32.539+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[],"timeoriginalestimate":null,"description":"Before IOContext problem is found in MapTran when spark rdd cache is enabled in HIVE-8920.\r\nso we disabled rdd cache in MapTran at [SparkPlanGenerator|https://github.com/kellyzly/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java#L202].  The problem is IOContext seems not initialized correctly in the spark yarn client/cluster mode and caused the exception like \r\n{code}\r\nJob aborted due to stage failure: Task 93 in stage 0.0 failed 4 times, most recent failure: Lost task 93.3 in stage 0.0 (TID 616, bdpe48): java.lang.RuntimeException: Error processing row: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:165)\r\n\tat org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\r\n\tat org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\r\n\tat org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\r\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hive.ql.exec.AbstractMapOperator.getNominalPath(AbstractMapOperator.java:101)\r\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:516)\r\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1187)\r\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:546)\r\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:152)\r\n\t... 12 more\r\n\r\nDriver stacktrace:\r\n{code}\r\nin yarn client/cluster mode, sometimes [ExecMapperContext#currentInputPath|https://github.com/kellyzly/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java#L109] is null when rdd cach is enabled.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12907839","id":"12907839","filename":"HIVE-18301.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-26T08:24:21.726+0000","size":64252,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12907839/HIVE-18301.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12908669","id":"12908669","filename":"HIVE-18301.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-31T22:27:58.269+0000","size":56004,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12908669/HIVE-18301.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12908960","id":"12908960","filename":"HIVE-18301.3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-02-02T09:55:07.686+0000","size":68717,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12908960/HIVE-18301.3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12907432","id":"12907432","filename":"HIVE-18301.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-24T05:37:38.363+0000","size":52210,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12907432/HIVE-18301.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Investigate to enable MapInput cache in Hive on Spark","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16297894","id":"16297894","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"If we can cache MapInput, will it be simpler to dynamically identify same MapInputs and cache them, in order to achieve the purpose of HIVE-17486?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-20T05:19:06.890+0000","updated":"2017-12-20T05:19:06.890+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16297897","id":"16297897","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"yes.  I can achieve to merge the same tables into 1 mapInput and run successfully such case(DS/query28) in HIVE-17486 in spark local mode.  This exception only happens in yarn mode. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-20T05:22:53.157+0000","updated":"2017-12-20T05:23:17.664+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16300999","id":"16300999","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"The reason why have this NPE is because [org.apache.hadoop.hive.ql.io.IOContextMap#sparkThreadLocal|https://github.com/kellyzly/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java#L54 ] is ThreadLocal variable.    The value of the same ThreadLocal variable maybe different in different threads.\r\nwe set the InputPath by\r\n{code}\r\n CombineHiveRecordReader#init\r\n->HiveContextAwareRecordReader.initIOContext\r\n->IOContext.setInputPath\r\n{code}\r\n\r\nwe get the InputPath by\r\n{code}\r\nSparkMapRecordHandler.processRow\r\n->MapOperator.process\r\n->MapOperator.cleanUpInputFileChangedOp\r\n->ExecMapperContext.getCurrentInputPath\r\n->IOContext#getInputPath\r\n{code}\r\n\r\nwhen cache is enabled, there is no setInputPath , so return null when call IOContext#getInputPath","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-22T06:16:08.980+0000","updated":"2017-12-22T06:16:08.980+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16301027","id":"16301027","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz],[~lirui]: Is there any way to store IOcontext in some place and then reinitialize it in current code when rdd cache is enabled?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-22T06:40:08.815+0000","updated":"2017-12-22T06:40:08.815+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16301713","id":"16301713","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz],[~csun]: I read jiras about MapInput IOContext problem and enable MapInput rdd cache. And found the problem only happens OContext problem with multiple MapWorks cloned for multi-insert \\[Spark Branch\\] like HIVE-8920 mentioned.\r\nIn HIVE-8920, I found the failure case is like\r\n{code}\r\nfrom (select * from dec union all select * from dec2) s\r\ninsert overwrite table dec3 select s.name, sum(s.value) group by s.name\r\ninsert overwrite table dec4 select s.name, s.value order by s.value;\r\n{code}\r\nI indeed saw the exception in my hive.log like\r\n{code}\r\nCaused by: java.lang.IllegalStateException: Invalid input path hdfs://localhost:8020/user/hive/warehouse/dec2/dec.txt\r\n        at org.apache.hadoop.hive.ql.exec.MapOperator.getNominalPath(MapOperator.java:406)\r\n        at org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:442)\r\n{code}\r\n\r\nhere the problem  happens on the MapInput is the union result of dec and dec2 case. But when I modify case\r\n{code}\r\nfrom (select * from dec ) s\r\ninsert overwrite table dec3 select s.name, sum(s.value) group by s.name\r\ninsert overwrite table dec4 select s.name, s.value order by s.value;\r\n{code}\r\nNo such exception whether in local or yarn mode.\r\n\r\nWhether the problem only happens  in such complicated case( the rdd cache is the  union result of two tables)?  If only happen in such complicated case, why not only disable MapInput rdd cache in such case? Is there any other reason to disable MapInput#rdd cache? Please spend some time to view it as both of you have experience on it, thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-22T17:34:36.606+0000","updated":"2017-12-22T17:34:36.606+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16303680","id":"16303680","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I think we need to investigate how the input path is used, and what the operators need to do when input file changes, etc. My understanding is these information will be lost if the HadoopRDD is cached.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-26T09:31:18.361+0000","updated":"2017-12-26T09:31:18.361+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16304359","id":"16304359","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]:\r\n{quote}\r\nMy understanding is these information will be lost if the HadoopRDD is cached.\r\n{quote}\r\n\r\nYou mean that HadoopRDD will not store the spark plan? If yes, actually in hive, it stores the spark plan on a file on  hdfs and deserialize and serialize from the file. See more code in \r\nhive/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#getBaseWork.  If not, please spend more time to explain detail.\r\n\r\nHere my question is that is there any other reason to disable MapInput#cache besides  avoiding  multi-insert cases which there is union operator after {{from}}\r\n{code}\r\n\r\nfrom (select * from dec union all select * from dec2) s\r\ninsert overwrite table dec3 select s.name, sum(s.value) group by s.name\r\ninsert overwrite table dec4 select s.name, s.value order by s.value;\r\n\r\n{code}\r\n\r\nIf there is no other reason to disable MapInput# cache, I guess  for HIVE-17486, we can enable MapInput cache because HIVE-17486 is merge same single table.  There is few case like above ( from (select A union B) ....).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-27T08:37:09.535+0000","updated":"2017-12-27T08:37:09.535+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16304383","id":"16304383","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"My understanding is if the HadoopRDD is cached, the records are not produced by record reader and IOContext is not populated. Therefore the information in IOContext will be unavailable, e.g. the input path. This may cause problem because some operators need to take certain actions when input file changes -- {{Operator::cleanUpInputFileChanged}}.\r\nSo basically my point is we have to figure out the scenarios where IOContext is necessary. Then decide whether we should disable caching in such cases.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-27T09:21:19.699+0000","updated":"2017-12-27T09:21:19.699+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16306218","id":"16306218","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I think SMB map join is one case where the \"input file change\" event is necessary -- whenever the big table input file changes, SMBMapJoinOperator needs to find corresponding input files for small tables in order to performa bucketed join. Maybe we can identify all such cases and make sure MapInput cache is disabled. For other cases, we can cache MapInput and just fix the NPE.\r\n[~xuefuz], could you share your thoughts on this? Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-29T11:46:45.229+0000","updated":"2017-12-29T11:46:45.229+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16307619","id":"16307619","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"thanks for Rui's comment, will spend some time to investigate the NPE problem .Here I want to say in [HIVE-8920|https://issues.apache.org/jira/browse/HIVE-8920], the exception is \r\n{code}\r\nCaused by: java.lang.IllegalStateException: Invalid input path hdfs://localhost:8020/user/hive/warehouse/dec2/dec.txt\r\n        at org.apache.hadoop.hive.ql.exec.MapOperator.getNominalPath(MapOperator.java:406)\r\n        at org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:442)\r\n\r\n{code}\r\n\r\nNot NPE, so there are two different problems. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-02T02:47:13.441+0000","updated":"2018-01-02T02:47:13.441+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16309361","id":"16309361","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"Here some update about NPE:\r\n the normal case when enable rdd cache\r\nthe stacktrace of initIOContext which will intialize ExecMapperContext#setCurrentInputPath is\r\n{code}\r\norg.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.initIOContext(HiveContextAwareRecordReader.java:175)\r\n org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.initIOContext(HiveContextAwareRecordReader.java:211)\r\n org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:101)\r\n sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:257)\r\n org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.<init>(HadoopShimsSecure.java:217)\r\n org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:346)\r\n org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:712)\r\n org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:246)\r\n org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:209)\r\n org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:102)\r\n org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\r\n org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\r\n org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)\r\n org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\r\n org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\r\n org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\r\n org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\r\n org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\r\n org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\r\n org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\r\n org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\r\n org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)\r\n org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\r\n org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\r\n org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\r\n org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\r\n org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\r\n org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\r\n org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n java.lang.Thread.run(Thread.java:745)\r\n{code}\r\n\r\nthe stacktrace of ExecMapperContext#getCurrentInputPath\r\n{code}\r\norg.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getCurrentInputPath(ExecMapperContext.java:113)\r\n org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:512)\r\n org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1187)\r\n org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:543)\r\n org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\r\n org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\r\n org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\r\n org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\r\n scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\r\n org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:213)\r\n org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)\r\n org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\r\n org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\r\n org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\r\n org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\r\n org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\r\n org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\r\n org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n java.lang.Thread.run(Thread.java:745)\r\n currentInputPath is hdfs://bdpe42\r\n java.lang.Thread.getStackTrace(Thread.java:1552)\r\n org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext.getCurrentInputPath(ExecMapperContext.java:113)\r\n org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:512)\r\n org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1187)\r\n org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:543)\r\n org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\r\n org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\r\n org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\r\n org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\r\n scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\r\n org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:213)\r\n org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)\r\n org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\r\n org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\r\n org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\r\n org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\r\n org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\r\n org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\r\n org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n java.lang.Thread.run(Thread.java:745)\r\n{code}\r\n\r\nactually they are in the same thread, so NPE is not thrown out even {{org.apache.hadoop.hive.ql.io.IOContextMap#sparkThreadLocal}} is ThreadLocal variable.\r\n\r\nIn the NPE case\r\nthe stacktrace to ExecMapperContext#getCurrentInputPath\r\n{code}\r\n742 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.hadoop.hive.ql.exec.MapOperator.cleanUpInputFileChangedOp(MapOperator.java:512)\r\n743 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1187)\r\n744 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:543)\r\n745 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\r\n746 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\r\n747 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\r\n748 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\r\n749 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\r\n750 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\r\n751 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n752 18/01/03 01:01:32 INFO Executor task launch worker-1 ExecMapperContext: org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n{code}\r\n\r\nExecMapperContext#getCurrentInputPath and ExecMapperContext#setCurrentInputPath is in different thread, so NPE is thrown out. I don't know why in normal case {code}\r\nMemoryStore.putIteratorAsValues->HiveMapFunctionResultList.processNextRecord\r\n{code}\r\nin NPE case\r\n{code}\r\nBypassMergeSortShuffleWriter.write->HiveMapFunctionResultList.processNextRecord\r\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-03T09:37:08.839+0000","updated":"2018-01-03T09:37:08.839+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16330190","id":"16330190","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]:\r\n{quote}My understanding is if the HadoopRDD is cached, the records are not produced by record reader and IOContext is not populated. Therefore the information in IOContext will be unavailable, e.g. the input path. This may cause problem because some operators need to take certain actions when input file changes – {{Operator::cleanUpInputFileChanged}}.\r\n So basically my point is we have to figure out the scenarios where IOContext is necessary. Then decide whether we should disable caching in such cases.\r\n{quote}\r\nYes, if HadoopRDD is cached, it will not call\r\n{code:java}\r\nCombineHiveRecordReader#init\r\n->HiveContextAwareRecordReader.initIOContext\r\n->IOContext.setInputPath\r\n{code}\r\n. \r\n It will use the cached result to call MapOperator#process(Writable value), so NPE is thrown because at that time IOContext.getInputPath return null. Now I just modify the code of MapOperator#process(Writable value) like [link|https://github.com/kellyzly/hive/commit/e81b7df572e2c543095f55dd160b428c355da2fb]\r\n\r\nHere my question is \r\n 1. when {{context.getIoCxt().getInputPath() == null}}, I think in this situation, this record is from cache not from CombineHiveRecordReader. We need not to call MapOperator#cleanUpInputFileChanged because MapOperator#cleanUpInputFileChanged is only designed for one Mapper scanning multiple files(like CombineFileInputFormat) and multiple partitions and inputPath will change in these situations and need to call {{cleanUpInputFileChanged}} to reinitialize [some variables|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java#L532] but we need not consider reinitialization for a cached record. Is my understanding right? If right, is there any other way to judge this is cached record or not except by {{context.getIoCxt().getInputPath() == null}}\r\n 2. how to initiliaze IOContext#getInputPath in cache situation? we need this variable to reinitialize MapOperator::currentCtxs in MapOperator#initializeContexts\r\n{code:java}\r\n  public void initializeContexts() {\r\n    Path fpath = getExecContext().getCurrentInputPath();\r\n    String nominalPath = getNominalPath(fpath);\r\n    Map<Operator<?>, MapOpCtx> contexts = opCtxMap.get(nominalPath);\r\n    currentCtxs = contexts.values().toArray(new MapOpCtx[contexts.size()]);\r\n  }\r\n{code}\r\nin the code, we store MapOpCtx for every MapOperator in opCtxMap(Map<String, Map<Operator<?>, MapOpCtx>>). In table with partitions, there will be multiple elements in opCtxMap( opCtxMap.keySet() is a set containing partition names). Currently I test on a table without partitions and can directly use opCtxMap.values().iterator().next() to initialize [context|https://github.com/kellyzly/hive/blob/HIVE-17486.4/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java#L713] and runs successfully in yarn mode. But I guess this is not right with partitioned table.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-18T07:34:58.174+0000","updated":"2018-01-18T07:34:58.174+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16336898","id":"16336898","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"{quote}We need not to call MapOperator#cleanUpInputFileChanged because MapOperator#cleanUpInputFileChanged is only designed for one Mapper scanning multiple files\r\n{quote}\r\nWhen RDD is cached, mapper reads records from the cache. But I think those records may come from multiple underlying files right? And we won't be able to tell the file boundaries because they're cached.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-24T04:58:13.264+0000","updated":"2018-01-24T04:58:13.264+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16336941","id":"16336941","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: {quote}\r\nAnd we won't be able to tell the file boundaries because they're cached.\r\n{quote}\r\nyes.  when IOContext.getInputPath() = null  only means the record is cached.  Can not know file boundaries.\r\nSo IOContext.getInputPath is necessary.  ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-24T05:31:27.425+0000","updated":"2018-01-24T05:31:27.425+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16336946","id":"16336946","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"In HIVE-18301.patch, it provides one solution to transfer the {{IOContext::inputPath}}\r\n{code:java}\r\n  inputRDD1                inputRDD2\r\n        |CopyFunction            | CopyFunction\r\n    CopyRDD1                CopyRDD2\r\n        |                       |\r\n       MT_11                       MT_12\r\n        |                       |\r\n       RT_1                         RT_2\r\n         \\                      /\r\n                     Union  \r\n{code}\r\nMT_11 will call following stack to initialize IOContext::inputPath\r\n{code:java}\r\n CombineHiveRecordReader#init\r\n->HiveContextAwareRecordReader.initIOContext\r\n->IOContext.setInputPath\r\n{code}\r\ninputRDD1 and inputRDD2 are same table's rdd, so CopyRDD1 and CopyRDD2 are same rdd if rdd cache is enabled. When MT_12 will not call CombineHiveRecordReader#init to initialize {{IOContext::inputPath}} but {{MapOperator#process(Writable value)}} still need this value. IOContext is bound to single thread, so the value is different in different thread. {{inputRDD1-CopyRDD1-MT_11-RT_1}} and {{inputRDD2-CopyRDD2-MT_12-RT_2}} is called in different thread. So IOContext can not be shared between these two threads.\r\n\r\nFor this issue, I gave following solution:\r\n We save the inputPath in CopyRDD1 when {{inputRDD1-CopyRDD1-MT_11-RT_1}} is executed. CopyRDD2 get the cached value and inputPath from CopyRDD1 which is stored in spark cache manager. We reinitialized the {{IOContext::inputPath}} in {{MapOperator#process(Writable value)}} in MT_12.\r\n *where to setInputPath?*\r\n MapInput#CopyFunction#call, save inputPath in the first element of returned tuple\r\n{code:java}\r\n public MapInput(SparkPlan sparkPlan, JavaPairRDD<WritableComparable, Writable> hadoopRDD) {\r\n     this(sparkPlan, hadoopRDD, false);\r\n@@ -79,10 +83,19 @@ public void setToCache(boolean toCache) {\r\n     call(Tuple2<WritableComparable, Writable> tuple) throws Exception {\r\n       if (conf == null) {\r\n         conf = new Configuration();\r\n+        conf.set(\"hive.execution.engine\",\"spark\");\r\n       }\r\n-\r\n-      return new Tuple2<WritableComparable, Writable>(tuple._1(),\r\n-          WritableUtils.clone(tuple._2(), conf));\r\n+      //                CopyFunction       MapFunction\r\n+      //  HADOOPRDD-----------------> RDD1-------------> RDD2.....\r\n+      // these transformation are in one stage and will be executed by 1 spark task(thread),\r\n+      // IOContext.get(conf).getInputPath will not be null.\r\n+      String inputPath = IOContextMap.get(conf).getInputPath().toString();\r\n+      Text inputPathText = new Text(inputPath);\r\n+      // save inputPath in the first element of returned tuple\r\n+      // before we need not use tuple._1() in SparkMapRecordHandler#processRow\r\n+      // so replace inputPathText with tuple._1().\r\n+      return new Tuple2<WritableComparable, Writable>(inputPathText,\r\n+        WritableUtils.clone(tuple._2(), conf));\r\n     }\r\n\r\n   }\r\n{code}\r\n*where to getInputPath?*\r\n{code:java}\r\nSparkMapRecordHandler#getInputPath\r\npublic void processRow(Object key, Object value) throws IOException {\r\n....\r\n+    if (HiveConf.getBoolVar(jc, HiveConf.ConfVars.HIVE_SPARK_SHARED_WORK_OPTIMIZATION)) {\r\n+      Path inputPath = IOContextMap.get(jc).getInputPath();\r\n+      // when inputPath is null, it means the record is cached \r\n+      if (inputPath == null) {\r\n+        Text pathText = (Text) key;\r\n+        IOContextMap.get(jc).setInputPath(new Path(pathText.toString()));\r\n+      }\r\n+    }\r\n....\r\n{code}\r\n[~lirui], [~xuefuz], [~stakiar],[~csun], please give me your suggesions, thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-24T05:37:17.087+0000","updated":"2018-01-24T05:47:33.125+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16337632","id":"16337632","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~kellyzly], does the proposed solution mean we need to cache the input path for each record of the table? I wonder whether we can reuse the Text for same input paths. Besides, it's not efficient to check the job conf each time we process a row. You can just check it once and remember the value. Anyway, it's better to get some measurement of the overhead.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-24T14:13:07.941+0000","updated":"2018-01-24T14:13:07.941+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16338586","id":"16338586","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]:\r\n{quote}does the proposed solution mean we need to cache the input path for each record of the table?\r\n{quote}\r\n\r\n yes.\r\n\r\n \r\n{quote}I wonder whether we can reuse the Text for same input paths. Besides, it's not efficient to check the job conf each time we process a row. You can just check it once and remember the value.\r\n{quote}\r\nOK, I will reuse to reduce some computation.\r\n [~lirui], [~xuefuz],[~csun],[~Ferd]:\r\n\r\nThe concern to this solution is \r\n 1. it may make the RDD bigger( I don't compare the original tuple._1() and inputPath, will do it later)\r\n 2. I modify the {{HadoopRDD-CopyRDD-MT-11}}, here CopyRDD does not store the original data anymore, CopyRDD store the inputPath and orignal data. It maybe confusing.\r\n\r\nDo you think idea is ok to commit, if not ok, maybe I need to try other solution or can you provide other better solution? Thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-25T01:51:18.149+0000","updated":"2018-01-25T04:05:42.068+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16340697","id":"16340697","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"HIVE-18301.1.patch to trigger Hive QA","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-26T08:18:38.602+0000","updated":"2018-01-26T08:18:38.602+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16340717","id":"16340717","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"for the problem it may make the RDD bigger:\r\n\r\noriginal tuple._1() is  [CombineKey|https://github.com/apache/hive/blob/master/shims/common/src/main/java/org/apache/hadoop/hive/shims/CombineHiveKey.java] which stores offset(LongWriteObject), and in currentSolution , tuple._1() is inputPath ([org.apache.hadoop.io|https://github.com/kellyzly/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java] [.Text|https://github.com/kellyzly/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java]) whose size is decided by actual input file path length.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-26T08:31:51.573+0000","updated":"2018-01-26T08:31:51.573+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16342164","id":"16342164","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  1s{color} | {color:blue} Findbugs executables are not available. {color} |\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n|| || || || {color:brown} master Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 15s{color} | {color:blue} Maven dependency ordering for branch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m  6s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  7s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 47s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} master passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 20s{color} | {color:blue} Maven dependency ordering for patch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 26s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 12s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 12s{color} | {color:green} the patch passed {color} |\r\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 35s{color} | {color:red} ql: The patch generated 49 new + 76 unchanged - 2 fixed = 125 total (was 78) {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 11s{color} | {color:red} The patch generated 7 ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black} 14m 38s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |\r\n| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |\r\n| git revision | master / cebe012 |\r\n| Default Java | 1.8.0_111 |\r\n| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8871/yetus/diff-checkstyle-ql.txt |\r\n| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-8871/yetus/patch-asflicense-problems.txt |\r\n| modules | C: common ql itests U: . |\r\n| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8871/yetus.txt |\r\n| Powered by | Apache Yetus    http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2018-01-27T14:49:58.248+0000","updated":"2018-01-27T14:49:58.248+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16342195","id":"16342195","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12907839/HIVE-18301.1.patch\n\n{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 21 failed/errored test(s), 11839 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=240)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_hook] (batchId=13)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=36)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[row__id] (batchId=78)\norg.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_move_tbl] (batchId=175)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=152)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=172)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=167)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=171)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=161)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[resourceplan] (batchId=164)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=161)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorization_input_format_excludes] (batchId=163)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=122)\norg.apache.hadoop.hive.ql.TestCreateUdfEntities.testUdfWithDfsResource (batchId=228)\norg.apache.hadoop.hive.ql.exec.TestOperators.testNoConditionalTaskSizeForLlap (batchId=282)\norg.apache.hadoop.hive.ql.io.TestDruidRecordWriter.testWrite (batchId=256)\norg.apache.hive.beeline.cli.TestHiveCli.testNoErrorDB (batchId=188)\norg.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=234)\norg.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=234)\norg.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=234)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/8871/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/8871/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8871/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.YetusPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 21 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12907839 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2018-01-27T15:46:01.155+0000","updated":"2018-01-27T15:46:01.155+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16342203","id":"16342203","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n|| || || || {color:brown} master Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 15s{color} | {color:blue} Maven dependency ordering for branch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 15s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  7s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 50s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  3s{color} | {color:green} master passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 22s{color} | {color:blue} Maven dependency ordering for patch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 29s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |\r\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 34s{color} | {color:red} ql: The patch generated 49 new + 76 unchanged - 2 fixed = 125 total (was 78) {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 11s{color} | {color:red} The patch generated 7 ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black} 14m 52s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |\r\n| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |\r\n| git revision | master / cebe012 |\r\n| Default Java | 1.8.0_111 |\r\n| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8872/yetus/diff-checkstyle-ql.txt |\r\n| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-8872/yetus/patch-asflicense-problems.txt |\r\n| modules | C: common ql itests U: . |\r\n| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8872/yetus.txt |\r\n| Powered by | Apache Yetus    http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2018-01-27T16:07:33.316+0000","updated":"2018-01-27T16:07:33.316+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16342225","id":"16342225","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12907839/HIVE-18301.1.patch\n\n{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 21 failed/errored test(s), 11839 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=240)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_hook] (batchId=13)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=36)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[row__id] (batchId=78)\norg.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_move_tbl] (batchId=175)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=172)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=167)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=171)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=161)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] (batchId=166)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[resourceplan] (batchId=164)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=161)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorization_input_format_excludes] (batchId=163)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=122)\norg.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query39] (batchId=250)\norg.apache.hadoop.hive.ql.exec.TestOperators.testNoConditionalTaskSizeForLlap (batchId=282)\norg.apache.hadoop.hive.ql.io.TestDruidRecordWriter.testWrite (batchId=256)\norg.apache.hive.beeline.cli.TestHiveCli.testNoErrorDB (batchId=188)\norg.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=234)\norg.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=234)\norg.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=234)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/8872/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/8872/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8872/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.YetusPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 21 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12907839 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2018-01-27T17:02:28.221+0000","updated":"2018-01-27T17:02:28.221+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16346196","id":"16346196","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~kellyzly], is the input path the only thing we need to store with cached RDD? The IOContext has quite a few other fields. I wonder whether they are available if the RDD is cached.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-31T03:18:42.687+0000","updated":"2018-01-31T03:18:42.687+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16346418","id":"16346418","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: thanks for question.  \r\n{quote}\r\n\r\nThe IOContext has quite a few other fields. I wonder whether they are available if the RDD is cached.q\r\n\r\n{quote}\r\nYes, IOContext#other fields are called in HiveContextAwareRecordReader and other places. We can skip fields which is only used in HiveContextAwareRecordReader, like IOContext#nextBlockStart, IOContext#isBlockPointer.  Others like IOContext#isBinarySearching is also used in FilterOperator#process.  Can not skip to cache it. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-31T08:25:04.427+0000","updated":"2018-01-31T08:25:04.427+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16347253","id":"16347253","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"It seems that IOContext is used in many places and the logics complicated. Instead of putting the input patch in each row, like what the patch is proposing, could we send a serialized IOContext object as a special row whenever the content of the object changes? I'm not sure how feasible it's, so it's just a rough idea to be explored.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-01-31T17:41:20.058+0000","updated":"2018-01-31T17:41:20.058+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16348349","id":"16348349","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  1s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n|| || || || {color:brown} master Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 23s{color} | {color:blue} Maven dependency ordering for branch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 46s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 15s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 54s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 10s{color} | {color:green} master passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 24s{color} | {color:blue} Maven dependency ordering for patch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 48s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 18s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 18s{color} | {color:green} the patch passed {color} |\r\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 40s{color} | {color:red} ql: The patch generated 51 new + 91 unchanged - 2 fixed = 142 total (was 93) {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  6s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 13s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black} 16m 40s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |\r\n| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |\r\n| git revision | master / 419593e |\r\n| Default Java | 1.8.0_111 |\r\n| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8965/yetus/diff-checkstyle-ql.txt |\r\n| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-8965/yetus/patch-asflicense-problems.txt |\r\n| modules | C: common ql itests U: . |\r\n| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8965/yetus.txt |\r\n| Powered by | Apache Yetus    http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2018-02-01T10:26:01.894+0000","updated":"2018-02-01T10:26:01.894+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16348429","id":"16348429","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12908669/HIVE-18301.2.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 21 failed/errored test(s), 12965 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=240)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=49)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_hook] (batchId=13)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=36)\norg.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_move_tbl] (batchId=175)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=152)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=172)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=167)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=171)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=161)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[resourceplan] (batchId=164)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=161)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorization_input_format_excludes] (batchId=163)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=122)\norg.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=221)\norg.apache.hadoop.hive.ql.exec.TestOperators.testNoConditionalTaskSizeForLlap (batchId=282)\norg.apache.hadoop.hive.ql.io.TestDruidRecordWriter.testWrite (batchId=256)\norg.apache.hive.beeline.cli.TestHiveCli.testNoErrorDB (batchId=188)\norg.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=234)\norg.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=234)\norg.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=234)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/8965/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/8965/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8965/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.YetusPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 21 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12908669 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2018-02-01T11:18:40.388+0000","updated":"2018-02-01T11:18:40.388+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13125797/comment/16350077","id":"16350077","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]:\r\n{quote}Instead of putting the input patch in each row, like what the patch is proposing, could we send a serialized IOContext object as a special row whenever the content of the object changes?\r\n{quote}\r\nThis is good idea. IOContext is bind to split not to bind to each record. So I changed to following idea.\r\n{code:java}\r\n inputRDD1                inputRDD2\r\n        |CopyFunction            | CopyFunction\r\n    CopyRDD1                CopyRDD2\r\n        |                       |\r\n       MT_11                       MT_12\r\n        |                       |\r\n       RT_1                         RT_2\r\n         \\                      /\r\n                     Union  \r\n{code}\r\nin the CopyRDD1, I only store IOContext as the tuple_1() of result when the IOContext#getInputPath is changed and store null in other situation. Thus it will reduce the size  of data increment of this solution. In MT_12, it initializes the IOContext when IOContext#getInputPath is null, once IOContext#getInputPath has value, we need not initialize it again in the same thread.  patch is provided in [^HIVE-18301.3.patch].  I guess the file boundary of original rdd is same as the file boundary of cached rdd. Only based on this theory, I guess the solution can works.  Although I don't read the source code of rdd cache, this solution passes in my test env.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-02-02T09:43:49.532+0000","updated":"2018-02-02T09:59:32.534+0000"}],"maxResults":29,"total":29,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-18301/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3o1qv:"}}