{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12735196","self":"https://issues.apache.org/jira/rest/api/2/issue/12735196","key":"HIVE-7787","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-09-17T15:37:19.653+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Feb 13 16:28:57 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-7787/watchers","watchCount":7,"isWatching":false},"created":"2014-08-19T22:04:35.594+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":"Parquet","customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12324312","id":"12324312","description":"released","name":"0.12.0","archived":false,"released":true,"releaseDate":"2013-10-15"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12324986","id":"12324986","description":"released","name":"0.13.0","archived":false,"released":true,"releaseDate":"2014-04-21"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12325279","id":"12325279","description":"Bug fix release for 0.12","name":"0.12.1","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12326450","id":"12326450","description":"released","name":"0.14.0","archived":false,"released":true,"releaseDate":"2014-11-12"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12326829","id":"12326829","description":"0.13 maintenance release 1","name":"0.13.1","archived":false,"released":true,"releaseDate":"2014-06-06"}],"issuelinks":[{"id":"12397382","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12397382","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12693124","key":"HIVE-6367","self":"https://issues.apache.org/jira/rest/api/2/issue/12693124","fields":{"summary":"Implement Decimal in ParquetSerde","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-02-13T16:29:05.629+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/4","description":"This issue was once resolved, but the resolution was deemed incorrect. From here issues are either marked assigned or resolved.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/reopened.png","name":"Reopened","id":"4","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12317906","id":"12317906","name":"Database/Schema","description":"Issues related to support for multiple Datasbases/Schemas"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12314332","id":"12314332","name":"Thrift API","description":"Tracks changes to the Hive Thrift APIs"}],"timeoriginalestimate":null,"description":"When reading Parquet file, where the original Thrift schema contains a struct with an enum, this causes the following error (full stack trace blow): \n{code}\n java.lang.NoSuchFieldError: DECIMAL.\n{code} \n\nExample Thrift Schema:\n{code}\nenum MyEnumType {\n    EnumOne,\n    EnumTwo,\n    EnumThree\n}\n\nstruct MyStruct {\n    1: optional MyEnumType myEnumType;\n    2: optional string field2;\n    3: optional string field3;\n}\n\nstruct outerStruct {\n    1: optional list<MyStruct> myStructs\n}\n{code}\n\nHive Table:\n{code}\nCREATE EXTERNAL TABLE mytable (\n  mystructs array<struct<myenumtype: string, field2: string, field3: string>>\n)\nROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'\nSTORED AS\nINPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'\nOUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'\n; \n{code}\n\nError Stack trace:\n{code}\nJava stack trace for Hive 0.12:\nCaused by: java.lang.NoSuchFieldError: DECIMAL\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.getNewConverter(ETypeConverter.java:146)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:31)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.ArrayWritableGroupConverter.<init>(ArrayWritableGroupConverter.java:45)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:34)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:64)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:47)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:36)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:64)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:40)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter.<init>(DataWritableRecordConverter.java:32)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.prepareForRead(DataWritableReadSupport.java:128)\n\tat parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:142)\n\tat parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:118)\n\tat parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:107)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:92)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:66)\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:51)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:65)\n\t... 16 more\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12672749","id":"12672749","filename":"HIVE-7787.trunk.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-03T07:48:02.373+0000","size":2672,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12672749/HIVE-7787.trunk.1.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Reading Parquet file with enum in Thrift Encoding throws NoSuchFieldError","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rlau","name":"rlau","key":"rlau","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raymond Lau","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rlau","name":"rlau","key":"rlau","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raymond Lau","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Hive 0.12 CDH 5.1.0, Hadoop 2.3.0 CDH 5.1.0","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14103025","id":"14103025","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rlau","name":"rlau","key":"rlau","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raymond Lau","active":true,"timeZone":"Etc/UTC"},"body":"This issue does not occur in Hive 0.12 CDH 5.0.0.  ETypeConverter.getNewConverter in that version does not have checks involving DECIMAL type.\nCDH 5.0.0\n{code}\npublic static Converter getNewConverter(final Class<?> type, final int index, final HiveGroupConverter parent) {\n    for (final ETypeConverter eConverter : values()) {\n      if (eConverter.getType() == type) {\n        return eConverter.getConverter(type, index, parent);\n      }\n    }\n    throw new IllegalArgumentException(\"Converter not found ... for type : \" + type);\n  }\n{code}\n\nCDH 5.1.0\n{code}\npublic static Converter getNewConverter(final PrimitiveType type, final int index, final HiveGroupConverter parent) {\n    if (type.isPrimitive() && (type.asPrimitiveType().getPrimitiveTypeName().equals(PrimitiveType.PrimitiveTypeName.INT96))) {\n      //TODO- cleanup once parquet support Timestamp type annotation.\n      return ETypeConverter.ETIMESTAMP_CONVERTER.getConverter(type, index, parent);\n    }\n    if (OriginalType.DECIMAL == type.getOriginalType()) {\n      return EDECIMAL_CONVERTER.getConverter(type, index, parent);\n    } else if (OriginalType.UTF8 == type.getOriginalType()) {\n      return ESTRING_CONVERTER.getConverter(type, index, parent);\n    }\n\n    Class<?> javaType = type.getPrimitiveTypeName().javaType;\n    for (final ETypeConverter eConverter : values()) {\n      if (eConverter.getType() == javaType) {\n        return eConverter.getConverter(type, index, parent);\n      }\n    }\n\n    throw new IllegalArgumentException(\"Converter not found ... for type : \" + type);\n  }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rlau","name":"rlau","key":"rlau","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raymond Lau","active":true,"timeZone":"Etc/UTC"},"created":"2014-08-19T23:20:55.966+0000","updated":"2014-08-19T23:20:55.966+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14137401","id":"14137401","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svend","name":"svend","key":"svend","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10445","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10445","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10445","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10445"},"displayName":"Svend Vanderveken","active":true,"timeZone":"Europe/Berlin"},"body":"\nI encounter a very similar issue with importing data from a hive external table in raw CSV format into a parquet table with CDH 5.1\n\n{code}\ncreate external table if not exists testsv.objects_raw (\n  objectid string,\n  model string,\n  owner string,\n  attributes map<string,string>)\n ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n STORED AS TEXTFILE\n location '/test/somefolder;\n{code}\n\n(load some data in csv format in /test/somefolder)\n\n{code}\ncreate table if not exists testsv.objects (\n  objectid string,\n  model string,\n  owner string,\n  attributes map<string,string>)\n ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'\n STORED AS\n INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'\n OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat';\n{code}\n\n{code}\n insert overwrite table testsv.objects select source.* from testsv.objects_raw source;\n{code}\n\n\n{code}\n2014-09-17 10:58:39,436 Stage-3 map = 100%,  reduce = 0%\nEnded Job = job_1410534905977_0011 with errors\nError during job, obtaining debugging information...\nExamining task ID: task_1410534905977_0011_m_000000 (and more) from job job_1410534905977_0011\n\nTask with the most failures(4):\n-----\nTask ID:\n  task_1410534905977_0011_m_000000\n\nURL:\n  http://vm28-hulk-priv:8088/taskdetails.jsp?jobid=job_1410534905977_0011&tipid=task_1410534905977_0011_m_000000\n-----\nDiagnostic Messages for this Task:\nError: java.io.IOException: java.lang.reflect.InvocationTargetException\n        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\n        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:346)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.<init>(HadoopShimsSecure.java:293)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:407)\n        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:560)\n        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:168)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:409)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:332)\n        ... 11 more\nCaused by: java.lang.NoSuchFieldError: DECIMAL\n        at org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.getNewConverter(ETypeConverter.java:146)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:31)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:64)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:40)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter.<init>(DataWritableRecordConverter.java:32)\n        at org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.prepareForRead(DataWritableReadSupport.java:128)\n        at parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:142)\n        at parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:118)\n        at parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:107)\n        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:92)\n        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:66)\n        at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:51)\n        at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:65)\n        ... 16 more\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svend","name":"svend","key":"svend","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10445","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10445","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10445","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10445"},"displayName":"Svend Vanderveken","active":true,"timeZone":"Europe/Berlin"},"created":"2014-09-17T15:37:19.653+0000","updated":"2014-09-17T15:37:19.653+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14143159","id":"14143159","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svend","name":"svend","key":"svend","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10445","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10445","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10445","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10445"},"displayName":"Svend Vanderveken","active":true,"timeZone":"Europe/Berlin"},"body":"This might be due to HIVE-6367","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svend","name":"svend","key":"svend","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10445","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10445","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10445","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10445"},"displayName":"Svend Vanderveken","active":true,"timeZone":"Europe/Berlin"},"created":"2014-09-22T12:46:00.734+0000","updated":"2014-09-22T12:46:00.734+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14152139","id":"14152139","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vikram.dixit","name":"vikram.dixit","key":"vikram.dixit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vikram Dixit K","active":true,"timeZone":"America/Los_Angeles"},"body":"[~rlau] Can you verify if this is fixed by HIVE-6367?\n\nThanks\nVikram.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vikram.dixit","name":"vikram.dixit","key":"vikram.dixit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vikram Dixit K","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-09-29T19:36:54.212+0000","updated":"2014-09-29T19:36:54.212+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14155957","id":"14155957","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"body":"I tried building hive from trunk, and running it. But I am seeing the same error:\n\n{code}\nCaused by: java.lang.NoSuchFieldError: DECIMAL\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.getNewConverter(ETypeConverter.java:168)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:31)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:64)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:40)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter.<init>(DataWritableRecordConverter.java:35)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.prepareForRead(DataWritableReadSupport.java:152)\n\tat parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:142)\n\tat parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:118)\n\tat parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:107)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:92)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:66)\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:71)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:65)\n\t... 16 more\n{code}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-02T01:23:15.069+0000","updated":"2014-10-02T01:23:15.069+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14157146","id":"14157146","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"body":"The exception in the above comment was due to the fact that the hadoop cluster I had run had an older version of parquet in. \nI did the following and got rid of the error: {{SET mapreduce.job.user.classpath.first=true}}\n\nBut I hit another issue:\n{code}\nDiagnostic Messages for this Task:\nError: java.io.IOException: java.lang.reflect.InvocationTargetException\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:300)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.<init>(HadoopShimsSecure.java:247)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:371)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:652)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:168)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:409)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:286)\n\t... 11 more\nCaused by: java.lang.IllegalStateException: Field count must be either 1 or 2: 3\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.ArrayWritableGroupConverter.<init>(ArrayWritableGroupConverter.java:38)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:34)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:64)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:47)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:36)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:64)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableGroupConverter.<init>(DataWritableGroupConverter.java:40)\n\tat org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter.<init>(DataWritableRecordConverter.java:35)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.prepareForRead(DataWritableReadSupport.java:152)\n\tat parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:142)\n\tat parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:118)\n\tat parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:107)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:92)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:66)\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:71)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:65)\n{code}\n\nIt appears that ArrayWritableGroupConverter allows either 1 or 2 elements in the structure for an array. Is there a reason for that?\n\nIs the following schema not supported?\n{code}\nenum MyEnumType {\n    EnumOne,\n    EnumTwo,\n    EnumThree\n}\nstruct MyStruct {\n    1: optional MyEnumType myEnumType;\n    2: optional string field2;\n    3: optional string field3;\n}\n\nstruct outerStruct {\n    1: optional list<MyStruct> myStructs\n}\n{code}\n\nI can file another JIRA for this issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-02T20:45:20.699+0000","updated":"2014-10-02T20:45:20.699+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14157771","id":"14157771","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"body":"Looks like {{ArrayWritableGroupConverter}} enforces that the struct should have either 1 or 2 elements. I am not sure the rational behind this, since a struct may have more than two elements. I did a quick patch to omit the check and handle any number of fields. I have tested it and it seems to be working for me for the schema in the description. Given there were explicit checks for the filed count to be either 1 or 2, I am not sure if it is the right approach. Please take a look.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-03T07:48:02.380+0000","updated":"2014-10-03T07:48:02.380+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14236363","id":"14236363","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rdblue","name":"rdblue","key":"rdblue","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=rdblue&avatarId=20387","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=rdblue&avatarId=20387","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=rdblue&avatarId=20387","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=rdblue&avatarId=20387"},"displayName":"Ryan Blue","active":true,"timeZone":"America/Los_Angeles"},"body":"[~amalakar], HIVE-8909 recently fixed the {{ArrayWritableGroupConverter}} problem you ran into on this and I see that you found the error in your classpath that was causing the original issue. Is it okay to close this issue now?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rdblue","name":"rdblue","key":"rdblue","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=rdblue&avatarId=20387","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=rdblue&avatarId=20387","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=rdblue&avatarId=20387","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=rdblue&avatarId=20387"},"displayName":"Ryan Blue","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-06T00:26:24.199+0000","updated":"2014-12-06T00:26:24.199+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14240062","id":"14240062","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"body":"[~rdblue] I haven't tried trunk yet, I am using the patch I submitted here. But lets close this issue. I would reopen if I happen to see the issue after trying hive trunk/0.15. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-09T21:08:17.659+0000","updated":"2014-12-09T21:08:17.659+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14240107","id":"14240107","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rdblue","name":"rdblue","key":"rdblue","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=rdblue&avatarId=20387","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=rdblue&avatarId=20387","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=rdblue&avatarId=20387","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=rdblue&avatarId=20387"},"displayName":"Ryan Blue","active":true,"timeZone":"America/Los_Angeles"},"body":"I'm closing this per discussion with [~amalakar].","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rdblue","name":"rdblue","key":"rdblue","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=rdblue&avatarId=20387","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=rdblue&avatarId=20387","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=rdblue&avatarId=20387","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=rdblue&avatarId=20387"},"displayName":"Ryan Blue","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-09T21:39:22.155+0000","updated":"2014-12-09T21:39:22.155+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12735196/comment/14320356","id":"14320356","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"body":"I tried release 1.0 and still have the same problem, I am going to reopen the JIRA. I will resubmit the patch when I get time.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-13T16:28:57.105+0000","updated":"2015-02-13T16:28:57.105+0000"}],"maxResults":11,"total":11,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-7787/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1z2pr:"}}