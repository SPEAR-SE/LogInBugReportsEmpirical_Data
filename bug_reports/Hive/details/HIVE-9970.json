{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12782127","self":"https://issues.apache.org/jira/rest/api/2/issue/12782127","key":"HIVE-9970","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-03-16T15:50:35.933+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Dec 04 12:45:33 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-9970/watchers","watchCount":15,"isWatching":false},"created":"2015-03-16T04:39:09.121+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tarushg","name":"tarushg","key":"tarushg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tarush Grover","active":true,"timeZone":"Etc/UTC"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-12-04T12:45:33.709+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/4","description":"This issue was once resolved, but the resolution was deemed incorrect. From here issues are either marked assigned or resolved.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/reopened.png","name":"Reopened","id":"4","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"Hi all,\n\nRecently i have configured Spark 1.2.0 and my environment is hadoop\n2.6.0 hive 1.1.0 Here i have tried hive on Spark while executing\ninsert into i am getting the following g error.\n\nQuery ID = hadoop2_20150313162828_8764adad-a8e4-49da-9ef5-35e4ebd6bc63\nTotal jobs = 1\nLaunching Job 1 out of 1\nIn order to change the average load for a reducer (in bytes):\nset hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\nset hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\nset mapreduce.job.reduces=<number>\nFailed to execute spark task, with exception\n'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create\nspark client.)'\nFAILED: Execution Error, return code 1 from\norg.apache.hadoop.hive.ql.exec.spark.SparkTask\n\nHave added the spark-assembly jar in hive lib\nAnd also in hive console using the command add jar followed by the steps\n\nset spark.home=/opt/spark-1.2.1/;\n\nadd jar /opt/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.4.0.jar;\n\nset hive.execution.engine=spark;\n\nset spark.master=spark://xxxxxxx:7077;\n\nset spark.eventLog.enabled=true;\n\nset spark.executor.memory=512m;\n\nset spark.serializer=org.apache.spark.serializer.KryoSerializer;\n\nCan anyone suggest!!!!\n\nThanks & Regards\nAmithsha\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive on spark","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14363377","id":"14363377","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"body":"Any exception in your hive.log? If you can attach your log here, it will be great.  If you see some /tmp/spark-events folder missing in your log, you can try to create that folder, or set spark.eventLog.enabled=false; and run your cli/query again.\n\nBy the way, you don't need to do this: \"add jar /opt/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.4.0.jar;\".\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-16T15:50:35.933+0000","updated":"2015-03-16T15:50:35.933+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14363384","id":"14363384","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"body":"When spark.eventLog.enabled is set to true, you'd set spark.eventLog.dir to an existing folder (default /tmp/spark-events). I added this setting to the Hive on Spark Getting Started page.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-16T15:57:19.386+0000","updated":"2015-03-16T15:57:19.386+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14364950","id":"14364950","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"body":"HIVE LOG (hive.log)\n\n2015-03-17 16:36:52,654 INFO  [main]: ql.Driver (SessionState.java:printInfo(852)) - Launching Job 1 out of 1\n2015-03-17 16:36:52,656 INFO  [main]: ql.Driver (Driver.java:launchTask(1630)) - Starting task [Stage-1:MAPRED] in parallel\n2015-03-17 16:36:52,660 INFO  [Thread-68]: hive.metastore (HiveMetaStoreClient.java:open(365)) - Trying to connect to metastore with URI thrift://nn01:7099\n2015-03-17 16:36:52,665 INFO  [Thread-68]: hive.metastore (HiveMetaStoreClient.java:open(461)) - Connected to metastore.\n2015-03-17 16:36:52,688 INFO  [Thread-68]: session.SessionState (SessionState.java:start(488)) - No Tez session required at this point. hive.execution.engine=mr.\n2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) - In order to change the average load for a reducer (in bytes):\n2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) -   set hive.exec.reducers.bytes.per.reducer=<number>\n2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) - In order to limit the maximum number of reducers:\n2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) -   set hive.exec.reducers.max=<number>\n2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) - In order to set a constant number of reducers:\n2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) -   set mapreduce.job.reduces=<number>\n2015-03-17 16:36:52,696 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.connect.timeout -> 1000).\n2015-03-17 16:36:52,697 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(113)) - load spark property from hive configuration (spark.eventLog.enabled -> false).\n2015-03-17 16:36:52,697 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.rpc.threads -> 8).\n2015-03-17 16:36:52,698 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.secret.bits -> 256).\n2015-03-17 16:36:52,699 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.rpc.max.size -> 52428800).\n2015-03-17 16:36:52,699 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(113)) - load spark property from hive configuration (spark.master -> spark://10.10.10.25:7077).\n2015-03-17 16:36:52,702 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.server.connect.timeout -> 90000).\n2015-03-17 16:36:54,480 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Spark assembly has been built with Hive, including Datanucleus jars on classpath\n2015-03-17 16:36:57,761 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.connect.timeout=1000\n2015-03-17 16:36:57,761 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.rpc.threads=8\n2015-03-17 16:36:57,762 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.rpc.max.size=52428800\n2015-03-17 16:36:57,763 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.secret.bits=256\n2015-03-17 16:36:57,763 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.server.connect.timeout=90000\n2015-03-17 16:36:58,224 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 15/03/17 16:36:58 INFO client.RemoteDriver: Connecting to: nn01:50661\n2015-03-17 16:36:58,240 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Exception in thread \"main\" java.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT\n2015-03-17 16:36:58,241 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat org.apache.hive.spark.client.rpc.RpcConfiguration.<clinit>(RpcConfiguration.java:46)\n2015-03-17 16:36:58,241 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:137)\n2015-03-17 16:36:58,241 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:528)\n2015-03-17 16:36:58,242 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n2015-03-17 16:36:58,242 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n2015-03-17 16:36:58,242 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n2015-03-17 16:36:58,243 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat java.lang.reflect.Method.invoke(Method.java:606)\n2015-03-17 16:36:58,243 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)\n2015-03-17 16:36:58,243 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)\n2015-03-17 16:36:58,243 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)\n2015-03-17 16:36:58,244 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)\n2015-03-17 16:36:58,244 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - \tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n2015-03-17 16:36:58,858 WARN  [Driver]: client.SparkClientImpl (SparkClientImpl.java:run(388)) - Child process exited with code 1.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"created":"2015-03-17T11:08:22.010+0000","updated":"2015-03-17T11:08:22.010+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14364951","id":"14364951","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"body":"\nhive> insert into table test values(6,8797);\nQuery ID = hadoop2_20150317163636_4692aa68-56b6-4ea9-ad21-e0f46efe4bfc\nTotal jobs = 1\nLaunching Job 1 out of 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapreduce.job.reduces=<number>\nFailed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"created":"2015-03-17T11:09:11.775+0000","updated":"2015-03-17T11:09:11.775+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14365446","id":"14365446","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"body":"That means your jar files are not consistent, i.e, not compiled with the same code.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-17T16:33:34.017+0000","updated":"2015-03-17T16:33:34.017+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14366882","id":"14366882","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"Amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10453","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10453","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10453","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10453"},"displayName":"Amith","active":false,"timeZone":"Etc/UTC"},"body":"My working Environment is\nCentos 6.4\nHadoop - 2.6.0\nHive   - 1.1.0\nSpark  - 1.3.0 (Builded spark,sql.yarn)\n\ncould you brief me about the error.Is that because of using higher end verions (or) my mistakes during building the spark.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"Amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10453","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10453","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10453","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10453"},"displayName":"Amith","active":false,"timeZone":"Etc/UTC"},"created":"2015-03-18T09:37:03.821+0000","updated":"2015-03-18T09:37:03.821+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14366945","id":"14366945","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"body":"And also while using beeline i am getting this error\n\n\n\n\n2015-03-18 16:03:21,458 ERROR [pool-3-thread-8]: DataNucleus.Datastore (Log4JLogger.java:error(115)) - An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\n\tat com.mysql.jdbc.Util.getInstance(Util.java:386)\n\tat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1054)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4237)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4169)\n\tat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2617)\n\tat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778)\n\tat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2819)\n\tat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2768)\n\tat com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:949)\n\tat com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:795)\n\tat com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)\n\tat org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatement(AbstractTable.java:760)\n\tat org.datanucleus.store.rdbms.table.TableImpl.createIndices(TableImpl.java:648)\n\tat org.datanucleus.store.rdbms.table.TableImpl.validateIndices(TableImpl.java:593)\n\tat org.datanucleus.store.rdbms.table.TableImpl.validateConstraints(TableImpl.java:390)\n\tat org.datanucleus.store.rdbms.table.ClassTable.validateConstraints(ClassTable.java:3463)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3464)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)\n\tat org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)\n\tat org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)\n\tat org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)\n\tat org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)\n\tat org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)\n\tat org.datanucleus.store.query.Query.executeQuery(Query.java:1744)\n\tat org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)\n\tat org.datanucleus.store.query.Query.execute(Query.java:1654)\n\tat org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)\n\tat org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:172)\n\tat org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:130)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:275)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:238)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:579)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:557)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:933)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:907)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)\n\tat com.sun.proxy.$Proxy1.get_database(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database.getResult(ThriftHiveMetastore.java:8547)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database.getResult(ThriftHiveMetastore.java:8531)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\n2015-03-18 16:03:21,458 ERROR [pool-3-thread-8]: DataNucleus.Datastore (Log4JLogger.java:error(115)) - An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\n\tat com.mysql.jdbc.Util.getInstance(Util.java:386)\n\tat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1054)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4237)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4169)\n\tat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2617)\n\tat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778)\n\tat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2819)\n\tat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2768)\n\tat com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:949)\n\tat com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:795)\n\tat com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)\n\tat org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatement(AbstractTable.java:760)\n\tat org.datanucleus.store.rdbms.table.TableImpl.createIndices(TableImpl.java:648)\n\tat org.datanucleus.store.rdbms.table.TableImpl.validateIndices(TableImpl.java:593)\n\tat org.datanucleus.store.rdbms.table.TableImpl.validateConstraints(TableImpl.java:390)\n\tat org.datanucleus.store.rdbms.table.ClassTable.validateConstraints(ClassTable.java:3463)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3464)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)\n\tat org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)\n\tat org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)\n\tat org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)\n\tat org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)\n\tat org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)\n\tat org.datanucleus.store.query.Query.executeQuery(Query.java:1744)\n\tat org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)\n\tat org.datanucleus.store.query.Query.execute(Query.java:1654)\n\tat org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)\n\tat org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:172)\n\tat org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:130)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:275)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:238)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:579)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:557)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:933)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:907)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)\n\tat com.sun.proxy.$Proxy1.get_database(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database.getResult(ThriftHiveMetastore.java:8547)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database.getResult(ThriftHiveMetastore.java:8531)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n\n2015-03-18 16:03:24,699 INFO  [pool-3-thread-8]: metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:<init>(132)) - Using direct SQL, underlying DB is MYSQL\n2015-03-18 16:03:24,700 INFO  [pool-3-thread-8]: metastore.ObjectStore (ObjectStore.java:setConf(252)) - Initialized ObjectStore\n2015-03-18 16:03:24,713 INFO  [pool-3-thread-8]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(732)) - 5: source:10.10.10.25 get_database: test\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"created":"2015-03-18T10:34:42.697+0000","updated":"2015-03-18T10:34:42.697+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14367048","id":"14367048","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"body":"mysql version\nServer version: 5.1.73-log Source distribution","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"created":"2015-03-18T12:17:46.711+0000","updated":"2015-03-18T12:17:46.711+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14368526","id":"14368526","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"body":"Mysql error solved after updating the version But hive on spark still in error state \nhive version 1.1.0\nSpark 1.3.0\n\nERROR : Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'\norg.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:57)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.getSession(SparkSessionManagerImpl.java:116)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.getSparkSession(SparkUtilities.java:113)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:95)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:75)\nCaused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Timed out waiting for client connection.\n\tat com.google.common.base.Throwables.propagate(Throwables.java:156)\n\tat org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:104)\n\tat org.apache.hive.spark.client.SparkClientFactory.createClient(SparkClientFactory.java:80)\n\tat org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.<init>(RemoteHiveSparkClient.java:88)\n\tat org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.createHiveSparkClient(HiveSparkClientFactory.java:58)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:55)\n\t... 6 more\nCaused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Timed out waiting for client connection.\n\tat io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)\n\tat org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:94)\n\t... 10 more\nCaused by: java.util.concurrent.TimeoutException: Timed out waiting for client connection.\n\tat org.apache.hive.spark.client.rpc.RpcServer$2.run(RpcServer.java:134)\n\tat io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)\n\tat io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:123)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\n\tat java.lang.Thread.run(Thread.java:744)\n\nERROR : Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'\norg.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:57)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.getSession(SparkSessionManagerImpl.java:116)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.getSparkSession(SparkUtilities.java:113)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:95)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:75)\nCaused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Timed out waiting for client connection.\n\tat com.google.common.base.Throwables.propagate(Throwables.java:156)\n\tat org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:104)\n\tat org.apache.hive.spark.client.SparkClientFactory.createClient(SparkClientFactory.java:80)\n\tat org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.<init>(RemoteHiveSparkClient.java:88)\n\tat org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.createHiveSparkClient(HiveSparkClientFactory.java:58)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:55)\n\t... 6 more\nCaused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Timed out waiting for client connection.\n\tat io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)\n\tat org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:94)\n\t... 10 more\nCaused by: java.util.concurrent.TimeoutException: Timed out waiting for client connection.\n\tat org.apache.hive.spark.client.rpc.RpcServer$2.run(RpcServer.java:134)\n\tat io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)\n\tat io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:123)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\n\tat java.lang.Thread.run(Thread.java:744)\nError: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask (state=08S01,code=1)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"created":"2015-03-19T05:07:31.998+0000","updated":"2015-03-19T05:07:31.998+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14370317","id":"14370317","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szehon","name":"szehon","key":"szehon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Szehon Ho","active":true,"timeZone":"Europe/Paris"},"body":"Can you try bumping up \"hive.spark.client.server.connect.timeout\" to a higher value like \"20000ms\"?  Default was bumped in HIVE-9519, not sure if you picked that one up.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szehon","name":"szehon","key":"szehon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Szehon Ho","active":true,"timeZone":"Europe/Paris"},"created":"2015-03-19T23:17:57.488+0000","updated":"2015-03-19T23:17:57.488+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14370731","id":"14370731","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"body":"my default hive.spark.client.server.connect.timeout=90000ms\nnow changed to 20000ms and Also finally to 220000ms\n\nSame Error\n\nLaunching Job 1 out of 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapreduce.job.reduces=<number>\nFailed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"created":"2015-03-20T04:57:50.332+0000","updated":"2015-03-20T04:57:50.332+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14370851","id":"14370851","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"body":"now i tried in pre built spark same error found so is there any problem in hive version (1.1.0)?????","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"created":"2015-03-20T06:38:48.582+0000","updated":"2015-03-20T06:38:48.582+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14375394","id":"14375394","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"body":"Hi all,\n\n\nHave tried with prebuild spark  from Hortonworks \nNow i am able to access spark through Beeline but cannot using hive\nand from Hortonworks documentation i came to know that yarn should be build on spark & they are starting thriftserver as\n\n./sbin/start-thriftserver.sh --master yarn --executor-memory 512m --hiveconf hive.server2.thrift.port=10001\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Amith","name":"Amith","key":"amith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Amithsha","active":true,"timeZone":"Indian/Antananarivo"},"created":"2015-03-23T04:55:23.485+0000","updated":"2015-03-23T04:55:23.485+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14532530","id":"14532530","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manishsethi","name":"manishsethi","key":"manishsethi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish","active":true,"timeZone":"Etc/GMT-5"},"body":"Today, I also ran into the same issue. The issue is caused by using spark that has been built with hive. Hive classes in spark-assembly*.jar seems of older version. Removing all hive specific classes from spark-assembly*.jar resolved the issue or me.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manishsethi","name":"manishsethi","key":"manishsethi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish","active":true,"timeZone":"Etc/GMT-5"},"created":"2015-05-07T12:21:54.419+0000","updated":"2015-05-07T12:21:54.419+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14602243","id":"14602243","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"body":"my hive version is 1.2.0.\nand build spark1.3.1 on hadoop with \"./make-distribution.sh --name \"hadoop2-without-hive\" --tgz \"-Pyarn,hadoop-provided,hadoop-2.5\".\nThats unfortunate,hive on spark still in error state,\"Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\".\n\njava.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT\n\tat org.apache.hive.spark.client.rpc.RpcConfiguration.<clinit>(RpcConfiguration.java:46)\n\tat org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:146)\n\tat org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:480)\n\n\n\nSo,Have these questions had answers?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-26T01:34:56.103+0000","updated":"2015-06-26T01:34:56.103+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14602467","id":"14602467","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"body":"I have resolved the problem.\nfirst hive cli will load $HIVE_HONE\\lib\\*.jar accurately.\nthen spark will load old version hive jar because in $SPARK_HOME\\conf\\spark-ent.sh\n\"export SPARK_CLASSPATH=$SPARK_HOME/lib/*:$HADOOP_HOME/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:$HIVE_HOME/lib/hive-contrib-0.12.0.jar:$HIVE_HOME/lib/hive-common-0.12.0.jar:$HIVE_HOME/bin/hive-cli    -0.12.0.jar:$HIVE_HOME/lib/hive-serde-0.12.0.jar:$HIVE_HOME/lib/:$EXTRA_CLASSPATH\"\n\nhowever HiveConf.class in hive-common-0.12.0.jar does not contain SPARK_RPC_CLIENT_CONNECT_TIMEOUT.\nso, java.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT has occur.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-26T06:28:41.010+0000","updated":"2015-06-26T06:28:41.010+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14627420","id":"14627420","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gallenvara_bg","name":"gallenvara_bg","key":"gallenvara_bg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"GaoLun","active":true,"timeZone":"Etc/UTC"},"body":"Hi , JoneZhang. According to your commitment , how can i resolve this problem ?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gallenvara_bg","name":"gallenvara_bg","key":"gallenvara_bg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"GaoLun","active":true,"timeZone":"Etc/UTC"},"created":"2015-07-15T02:23:53.199+0000","updated":"2015-07-15T02:23:53.199+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14628127","id":"14628127","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pankhuri","name":"pankhuri","key":"pankhuri","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"pankhuri","active":true,"timeZone":"Etc/UTC"},"body":"I am facing same issue when trying to run hive on spark in yarn mode.\n\nhive 1.2.0 /1.1.0 -- tried with both\nspark 1.3.1\nhadoop 2.6.0\n\nI have tried with both the prebuilt version and building with below command to remove any hive dependency\n./make-distribution.sh --name \"hadoop2-without-hive\" --tgz \"-Pyarn,hadoop-provided,hadoop-2.6\"\n\n5/07/15 17:44:09 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1436958102207_0007_000001\n15/07/15 17:44:09 WARN conf.Configuration: mapred-default.xml:an attempt to override final parameter: mapreduce.cluster.local.dir;  Ignoring.\n15/07/15 17:44:09 WARN conf.Configuration: mapred-default.xml:an attempt to override final parameter: mapreduce.cluster.temp.dir;  Ignoring.\n15/07/15 17:44:09 WARN conf.Configuration: mapred-default.xml:an attempt to override final parameter: mapreduce.cluster.local.dir;  Ignoring.\n15/07/15 17:44:09 WARN conf.Configuration: mapred-default.xml:an attempt to override final parameter: mapreduce.cluster.temp.dir;  Ignoring.\n15/07/15 17:44:09 INFO spark.SecurityManager: Changing view acls to: root\n15/07/15 17:44:09 INFO spark.SecurityManager: Changing modify acls to: root\n15/07/15 17:44:09 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)\n15/07/15 17:44:09 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread\n15/07/15 17:44:09 INFO yarn.ApplicationMaster: Waiting for spark context initialization\n15/07/15 17:44:09 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... \n15/07/15 17:44:09 INFO client.RemoteDriver: Connecting to: Impetus-dsrv16:41364\n15/07/15 17:44:09 ERROR yarn.ApplicationMaster: User class threw exception: SPARK_RPC_CLIENT_CONNECT_TIMEOUT\njava.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT\n\tat org.apache.hive.spark.client.rpc.RpcConfiguration.<clinit>(RpcConfiguration.java:46)\n\tat org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:146)\n\tat org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:480)\n15/07/15 17:44:09 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: SPARK_RPC_CLIENT_CONNECT_TIMEOUT)\n15/07/15 17:44:19 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 100000 ms. Please check earlier log output for errors. Failing the application.\n15/07/15 17:44:19 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: SPARK_RPC_CLIENT_CONNECT_TIMEOUT)\n15/07/15 17:44:19 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1436958102207_0007\nPlease let me know if there is a resolution to it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pankhuri","name":"pankhuri","key":"pankhuri","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"pankhuri","active":true,"timeZone":"Etc/UTC"},"created":"2015-07-15T14:24:26.314+0000","updated":"2015-07-15T14:24:26.314+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14629068","id":"14629068","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"body":"1.Make sure  build spark without hive.\nIf there is the following directory in spark-assembly-*-hadoop*.jar,this is not allowed.\nspark-assembly-*-hadoop*.jar\\org\\apache\\hive   \nspark-assembly-*-hadoop*.jar\\org\\apache\\hadoop\\hive \n2. See if there is an older version of hive jar under CLASSPATH.Remove it If there exists.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"created":"2015-07-16T02:04:04.885+0000","updated":"2015-07-16T02:04:04.885+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/14973228","id":"14973228","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"This seems indicating a non-issue. Feel free to reopen with additional information including a meaningful title.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-10-25T13:00:28.654+0000","updated":"2015-10-25T13:00:28.654+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/15001843","id":"15001843","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qiuzhuang.lian","name":"qiuzhuang.lian","key":"qiuzhuang.lian","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Qiuzhuang Lian","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi Xuefu, \n\nI use spark trunk version of 1.6 + hadoop 2.6.0 + hive 1.2.1, when running the HQL in HIVE CLI, We got following error,\n\n2015-11-12 16:31:17,245 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/12 16:31:17 ERROR Utils: uncaught error in thread SparkListenerBus, stopping SparkContext\n2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - java.lang.AbstractMethodError\n2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:62)\n2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)\n\nAny ideas?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qiuzhuang.lian","name":"qiuzhuang.lian","key":"qiuzhuang.lian","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Qiuzhuang Lian","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-11-12T08:35:34.908+0000","updated":"2015-11-12T08:35:34.908+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/15020504","id":"15020504","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tarushg","name":"tarushg","key":"tarushg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tarush Grover","active":true,"timeZone":"Etc/UTC"},"body":"This issue still persists my environment is :\n\nhadoop = 2.6\nhive = 1.1.1\nspark = 1.5.1\n\nbelow are logs in the hive.log :\n\n15/11/21 19:56:27 [HiveServer2-Background-Pool: Thread-39]: ERROR exec.Task: Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'\norg.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:57)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.getSession(SparkSessionManagerImpl.java:116)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.getSparkSession(SparkUtilities.java:113)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:95)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1638)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1397)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1183)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:145)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:70)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:197)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:209)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Cannot run program \"/home/adt/server/spark1.5/spark-1.5.1set mapreduce.input.fileinputformat.split.maxsize=750000000set hive.vectorized.execution.enabled=trueset hive.cbo.enable=trueset hive.optimize.reducededuplication.min.reducer=4set hive.optimize.reducededuplication=trueset hive.orc.splits.include.file.footer=falseset hive.merge.mapfiles=trueset hive.merge.sparkfiles=falseset hive.merge.smallfiles.avgsize=16000000set hive.merge.size.per.task=256000000set hive.merge.orcfile.stripe.level=trueset hive.auto.convert.join=trueset hive.auto.convert.join.noconditionaltask=trueset hive.auto.convert.join.noconditionaltask.size=894435328set hive.optimize.bucketmapjoin.sortedmerge=falseset hive.map.aggr.hash.percentmemory=0.5set hive.map.aggr=trueset hive.optimize.sort.dynamic.partition=falseset hive.stats.autogather=trueset hive.stats.fetch.column.stats=trueset hive.vectorized.execution.reduce.enabled=falseset hive.vectorized.groupby.checkinterval=4096set hive.vectorized.groupby.flush.percent=0.1set hive.compute.query.using.stats=trueset hive.limit.pushdown.memory.usage=0.4set hive.optimize.index.filter=trueset hive.exec.reducers.bytes.per.reducer=67108864set hive.smbjoin.cache.rows=10000set hive.exec.orc.default.stripe.size=67108864set hive.fetch.task.conversion=moreset hive.fetch.task.conversion.threshold=1073741824set hive.fetch.task.aggr=falseset mapreduce.input.fileinputformat.list-status.num-threads=5set spark.kryo.referenceTracking=false#set spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch/bin/spark-submit\": error=36, File name too long\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.hive.spark.client.SparkClientImpl.startDriver(SparkClientImpl.java:376)\n\tat org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:89)\n\tat org.apache.hive.spark.client.SparkClientFactory.createClient(SparkClientFactory.java:80)\n\tat org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.<init>(RemoteHiveSparkClient.java:88)\n\tat org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.createHiveSparkClient(HiveSparkClientFactory.java:58)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:55)\n\t... 22 more\nCaused by: java.io.IOException: error=36, File name too long\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:248)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 28 more\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tarushg","name":"tarushg","key":"tarushg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tarush Grover","active":true,"timeZone":"Etc/UTC"},"created":"2015-11-21T14:36:57.829+0000","updated":"2015-11-21T14:36:57.829+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/15020506","id":"15020506","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tarushg","name":"tarushg","key":"tarushg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tarush Grover","active":true,"timeZone":"Etc/UTC"},"body":"This issue still persists please see logs below :\n\n15/11/21 19:56:27 [HiveServer2-Background-Pool: Thread-39]: ERROR exec.Task: Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'\norg.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:57)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.getSession(SparkSessionManagerImpl.java:116)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.getSparkSession(SparkUtilities.java:113)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:95)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1638)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1397)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1183)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:145)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:70)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:197)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:209)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Cannot run program \"/home/adt/server/spark1.5/spark-1.5.1set mapreduce.input.fileinputformat.split.maxsize=750000000set hive.vectorized.execution.enabled=trueset hive.cbo.enable=trueset hive.optimize.reducededuplication.min.reducer=4set hive.optimize.reducededuplication=trueset hive.orc.splits.include.file.footer=falseset hive.merge.mapfiles=trueset hive.merge.sparkfiles=falseset hive.merge.smallfiles.avgsize=16000000set hive.merge.size.per.task=256000000set hive.merge.orcfile.stripe.level=trueset hive.auto.convert.join=trueset hive.auto.convert.join.noconditionaltask=trueset hive.auto.convert.join.noconditionaltask.size=894435328set hive.optimize.bucketmapjoin.sortedmerge=falseset hive.map.aggr.hash.percentmemory=0.5set hive.map.aggr=trueset hive.optimize.sort.dynamic.partition=falseset hive.stats.autogather=trueset hive.stats.fetch.column.stats=trueset hive.vectorized.execution.reduce.enabled=falseset hive.vectorized.groupby.checkinterval=4096set hive.vectorized.groupby.flush.percent=0.1set hive.compute.query.using.stats=trueset hive.limit.pushdown.memory.usage=0.4set hive.optimize.index.filter=trueset hive.exec.reducers.bytes.per.reducer=67108864set hive.smbjoin.cache.rows=10000set hive.exec.orc.default.stripe.size=67108864set hive.fetch.task.conversion=moreset hive.fetch.task.conversion.threshold=1073741824set hive.fetch.task.aggr=falseset mapreduce.input.fileinputformat.list-status.num-threads=5set spark.kryo.referenceTracking=false#set spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch/bin/spark-submit\": error=36, File name too long\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.hive.spark.client.SparkClientImpl.startDriver(SparkClientImpl.java:376)\n\tat org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:89)\n\tat org.apache.hive.spark.client.SparkClientFactory.createClient(SparkClientFactory.java:80)\n\tat org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.<init>(RemoteHiveSparkClient.java:88)\n\tat org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.createHiveSparkClient(HiveSparkClientFactory.java:58)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:55)\n\t... 22 more\nCaused by: java.io.IOException: error=36, File name too long\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:248)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 28 more\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tarushg","name":"tarushg","key":"tarushg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tarush Grover","active":true,"timeZone":"Etc/UTC"},"created":"2015-11-21T14:38:32.441+0000","updated":"2015-11-21T14:38:32.441+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/15027785","id":"15027785","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~tarushg], the error seems different from the original issue but very strange:\n{code}\nCaused by: java.io.IOException: Cannot run program \"/home/adt/server/spark1.5/spark-1.5.1set mapreduce.input.fileinputformat.split.maxsize=750000000set hive.vectorized.execution.enabled=trueset hive.cbo.enable=trueset hive.optimize.reducededuplication.min.reducer=4set hive.optimize.reducededuplication=trueset hive.orc.splits.include.file.footer=falseset hive.merge.mapfiles=trueset hive.merge.sparkfiles=falseset hive.merge.smallfiles.avgsize=16000000set hive.merge.size.per.task=256000000set hive.merge.orcfile.stripe.level=trueset hive.auto.convert.join=trueset hive.auto.convert.join.noconditionaltask=trueset hive.auto.convert.join.noconditionaltask.size=894435328set hive.optimize.bucketmapjoin.sortedmerge=falseset hive.map.aggr.hash.percentmemory=0.5set hive.map.aggr=trueset hive.optimize.sort.dynamic.partition=falseset hive.stats.autogather=trueset hive.stats.fetch.column.stats=trueset hive.vectorized.execution.reduce.enabled=falseset hive.vectorized.groupby.checkinterval=4096set hive.vectorized.groupby.flush.percent=0.1set hive.compute.query.using.stats=trueset hive.limit.pushdown.memory.usage=0.4set hive.optimize.index.filter=trueset hive.exec.reducers.bytes.per.reducer=67108864set hive.smbjoin.cache.rows=10000set hive.exec.orc.default.stripe.size=67108864set hive.fetch.task.conversion=moreset hive.fetch.task.conversion.threshold=1073741824set hive.fetch.task.aggr=falseset mapreduce.input.fileinputformat.list-status.num-threads=5set spark.kryo.referenceTracking=false#set spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch/bin/spark-submit\": error=36, File name too long\n{code}\nThis is where Hive is building a process to launch a remote spark driver. it usually starts with something like \"/home/adt/server/spark1.5/bin/spark-submit ...\". It seems that the builder gets corrupted with a bunch of set commands. Could you describe how to reproduce this issue?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-11-25T23:21:14.502+0000","updated":"2015-11-25T23:21:14.502+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/15028065","id":"15028065","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=TerrenceYTQ","name":"TerrenceYTQ","key":"terrenceytq","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"TerrenceYTQ","active":true,"timeZone":"Etc/UTC"},"body":"With the same problem \"  ERROR util.Utils: uncaught error in thread SparkListenerBus, stopping SparkContext \", has anyone already solved it ??? \n---My spark 1.5.2   Hive 1.2.1 ,  build by myself with commands : \n   mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Dscala-2.10 -DskipTests clean package e\n\n-------Error Log \n 15/11/26 09:39:40 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n2015-11-26 09:39:40,245 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO exec.Utilities: Processing alias dc_mf_device_one_check\n2015-11-26 09:39:40,245 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO exec.Utilities: Adding input file hdfs://cluster1/user/hive/warehouse/vendorzhhs.db/dc_mf_device_one_check\n2015-11-26 09:39:40,735 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO log.PerfLogger: <PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>\n2015-11-26 09:39:40,735 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO exec.Utilities: Serializing MapWork via kryo\n2015-11-26 09:39:40,902 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO log.PerfLogger: </PERFLOG method=serializePlan start=1448501980734 end=1448501980902 duration=168 from=org.apache.hadoop.hive.ql.exec.Utilities>\n2015-11-26 09:39:41,248 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.MemoryStore: ensureFreeSpace(599952) called with curMem=0, maxMem=555755765\n2015-11-26 09:39:41,250 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 585.9 KB, free 529.4 MB)\n2015-11-26 09:39:41,429 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.MemoryStore: ensureFreeSpace(43801) called with curMem=599952, maxMem=555755765\n2015-11-26 09:39:41,429 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.8 KB, free 529.4 MB)\n2015-11-26 09:39:41,433 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.69:39388 (size: 42.8 KB, free: 530.0 MB)\n2015-11-26 09:39:41,437 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO spark.SparkContext: Created broadcast 0 from hadoopRDD at SparkPlanGenerator.java:188\n2015-11-26 09:39:41,441 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 ERROR util.Utils: uncaught error in thread SparkListenerBus, stopping SparkContext\n2015-11-26 09:39:41,441 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - java.lang.AbstractMethodError\n2015-11-26 09:39:41,441 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:62)\n2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)\n2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:56)\n2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)\n2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:79)\n2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1136)\n2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)\n2015-11-26 09:39:41,467 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=TerrenceYTQ","name":"TerrenceYTQ","key":"terrenceytq","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"TerrenceYTQ","active":true,"timeZone":"Etc/UTC"},"created":"2015-11-26T02:53:21.190+0000","updated":"2015-11-26T02:53:21.190+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782127/comment/15041526","id":"15041526","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bolke","name":"bolke","key":"bolke","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bolke de Bruin","active":true,"timeZone":"Europe/Amsterdam"},"body":"We are seeing the exact same issue as [~TerrenceYTQ] and [~Qiuzhuang] which seems a regression as it has been reported to work with spark 1.4.1 (http://apache-spark-user-list.1001560.n3.nabble.com/Issue-with-spark-on-hive-td25372.html).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bolke","name":"bolke","key":"bolke","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bolke de Bruin","active":true,"timeZone":"Europe/Amsterdam"},"created":"2015-12-04T12:45:33.709+0000","updated":"2015-12-04T12:45:33.709+0000"}],"maxResults":26,"total":26,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-9970/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i26slj:"}}