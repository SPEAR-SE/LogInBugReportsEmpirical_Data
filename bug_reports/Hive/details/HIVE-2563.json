{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12530802","self":"https://issues.apache.org/jira/rest/api/2/issue/12530802","key":"HIVE-2563","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":null,"customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Nov 09 01:52:02 UTC 2011","customfield_12310420":"216540","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-2563/watchers","watchCount":3,"isWatching":false},"created":"2011-11-09T01:35:10.591+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12316336","id":"12316336","description":"released","name":"0.7.1","archived":false,"released":true,"releaseDate":"2011-06-21"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2011-11-09T01:52:02.935+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"I'm trying to use dynamic partition inserts to mimic a legacy file generation process that creates a single file per combination of two record attributes, one with a low cardinality, and one with a high degree of cardinality.  In a small data set, I can do this successfully.  Using a larger data set on the same 11 node cluster, with a combined cardinality resulting in ~1600 partitions, I get out of memory errors in the reduce phase 100% of the time.  \n\nI'm running with the following settings, writing to a textfile-backed table with two partitions of type string:\n\nSET hive.exec.compress.output=true; \nSET io.seqfile.compression.type=BLOCK;\nSET mapred.max.map.failures.percent=100;\nSET hive.exec.dynamic.partition=true;\nSET hive.exec.dynamic.partition.mode=nonstrict;\nSET hive.exec.max.dynamic.partitions=10000;\nSET hive.exec.max.dynamic.partitions.pernode=10000;\n\n(I've also tried gzip compression with the same result)\n\n\nHere's an example of the error:\n\n2011-11-09 00:51:52,425 INFO org.apache.hadoop.hive.ql.exec.FileSinkOperator: New Final Path: FS hdfs://ec2-50-19-131-121.compute-1.amazonaws.com/tmp/hive-hdfs/hive_2011-11-09_00-48-57_840_6003656718210084497/_tmp.-ext-10000/requestday=2011-09-29/clientname=XXXX-JA/000008_0.deflate\n2011-11-09 00:51:52,461 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1\n2011-11-09 00:51:52,464 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.OutOfMemoryError: unable to create new native thread\n\tat java.lang.Thread.start0(Native Method)\n\tat java.lang.Thread.start(Thread.java:640)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:2931)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:544)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:219)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:584)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:565)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:472)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:464)\n\tat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:80)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:247)\n\tat org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:235)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:458)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutWriters(FileSinkOperator.java:599)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:539)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:959)\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:798)\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:724)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:469)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:270)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:264)","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"13515","customfield_12312823":null,"summary":"OutOfMemory errors when using dynamic partition inserts with large number of partitions","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=epollan","name":"epollan","key":"epollan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Pollan","active":true,"timeZone":"America/New_York"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=epollan","name":"epollan","key":"epollan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Pollan","active":true,"timeZone":"America/New_York"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Cloudera CDH3 Update 2 distro on Ubuntu 10.04 64 bit cluster nodes","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530802/comment/13146723","id":"13146723","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=epollan","name":"epollan","key":"epollan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Pollan","active":true,"timeZone":"America/New_York"},"body":"By the way, the total number of records in this table (if were able to insert successfully :), is just over 5 million.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=epollan","name":"epollan","key":"epollan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Pollan","active":true,"timeZone":"America/New_York"},"created":"2011-11-09T01:52:02.848+0000","updated":"2011-11-09T01:52:02.848+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-2563/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i02o5r:"}}