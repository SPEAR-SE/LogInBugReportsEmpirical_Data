{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12751801","self":"https://issues.apache.org/jira/rest/api/2/issue/12751801","key":"HIVE-8677","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326450","id":"12326450","description":"released","name":"0.14.0","archived":false,"released":true,"releaseDate":"2014-11-12"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2014-10-31T06:15:34.575+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Nov 13 19:42:20 UTC 2014","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_47590979_*|*_1_*:*_1_*:*_30294413_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-10-31T19:29:09.448+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-8677/watchers","watchCount":4,"isWatching":false},"created":"2014-10-30T21:51:04.151+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326450","id":"12326450","description":"released","name":"0.14.0","archived":false,"released":true,"releaseDate":"2014-11-12"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hagleitn","name":"hagleitn","key":"hagleitn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035"},"displayName":"Gunther Hagleitner","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-11-13T19:42:20.828+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"TPC-DS Q51 fails with the exception below \n{code}\n, TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n{code}\n\nQuery\n{code}\nset hive.cbo.enable=true;\nset hive.stats.fetch.column.stats=true;\nset hive.exec.dynamic.partition.mode=nonstrict;\nset hive.tez.auto.reducer.parallelism=true;\nset hive.tez.exec.print.summary=true;\nset hive.auto.convert.join.noconditionaltask.size=1280000000;\nset hive.exec.reducers.bytes.per.reducer=100000000;\nset hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;\nset hive.support.concurrency=false;\n \nWITH web_v1 as (\nselect\n  ws_item_sk item_sk, d_date, sum(ws_sales_price),\n  sum(sum(ws_sales_price))\n      over (partition by ws_item_sk order by d_date rows between unbounded preceding and current row) cume_sales\nfrom web_sales\n    ,date_dim\nwhere ws_sold_date_sk=d_date_sk\n  and d_month_seq between 1193 and 1193+11\n  and ws_item_sk is not NULL\ngroup by ws_item_sk, d_date),\nstore_v1 as (\nselect\n  ss_item_sk item_sk, d_date, sum(ss_sales_price),\n  sum(sum(ss_sales_price))\n      over (partition by ss_item_sk order by d_date rows between unbounded preceding and current row) cume_sales\nfrom store_sales\n    ,date_dim\nwhere ss_sold_date_sk=d_date_sk\n  and d_month_seq between 1193 and 1193+11\n  and ss_item_sk is not NULL\ngroup by ss_item_sk, d_date)\n select  *\nfrom (select item_sk\n     ,d_date\n     ,web_sales\n     ,store_sales\n     ,max(web_sales)\n         over (partition by item_sk order by d_date rows between unbounded preceding and current row) web_cumulative\n     ,max(store_sales)\n         over (partition by item_sk order by d_date rows between unbounded preceding and current row) store_cumulative\n     from (select case when web.item_sk is not null then web.item_sk else store.item_sk end item_sk\n                 ,case when web.d_date is not null then web.d_date else store.d_date end d_date\n                 ,web.cume_sales web_sales\n                 ,store.cume_sales store_sales\n           from web_v1 web full outer join store_v1 store on (web.item_sk = store.item_sk\n                                                          and web.d_date = store.d_date)\n          )x )y\nwhere web_cumulative > store_cumulative\norder by item_sk\n        ,d_date\nlimit 100\n{code}\n\nPlan\n{code}\nOK\nSTAGE DEPENDENCIES:\n  Stage-1 is a root stage\n  Stage-0 depends on stages: Stage-1\n\nSTAGE PLANS:\n  Stage: Stage-1\n    Tez\n      Edges:\n        Map 2 <- Map 8 (BROADCAST_EDGE)\n        Map 9 <- Map 1 (BROADCAST_EDGE)\n        Reducer 10 <- Map 9 (SIMPLE_EDGE)\n        Reducer 11 <- Reducer 10 (SIMPLE_EDGE)\n        Reducer 3 <- Map 2 (SIMPLE_EDGE)\n        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)\n        Reducer 5 <- Reducer 11 (SIMPLE_EDGE), Reducer 4 (SIMPLE_EDGE)\n        Reducer 6 <- Reducer 5 (SIMPLE_EDGE)\n        Reducer 7 <- Reducer 6 (SIMPLE_EDGE)\n      DagName: mmokhtar_20141030010808_11af3ba0-8b28-4a33-9f4d-73618503e272:1\n      Vertices:\n        Map 1 \n            Map Operator Tree:\n                TableScan\n                  alias: date_dim\n                  filterExpr: (d_date_sk is not null and d_month_seq BETWEEN 1193 AND 1204) (type: boolean)\n                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: (d_date_sk is not null and d_month_seq BETWEEN 1193 AND 1204) (type: boolean)\n                    Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                    Reduce Output Operator\n                      key expressions: d_date_sk (type: int)\n                      sort order: +\n                      Map-reduce partition columns: d_date_sk (type: int)\n                      Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                      value expressions: d_date (type: string), d_month_seq (type: int)\n                    Select Operator\n                      expressions: d_date_sk (type: int)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                      Group By Operator\n                        keys: _col0 (type: int)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                        Dynamic Partitioning Event Operator\n                          Target Input: store_sales\n                          Partition key expr: ss_sold_date_sk\n                          Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                          Target column: ss_sold_date_sk\n                          Target Vertex: Map 9\n            Execution mode: vectorized\n        Map 2 \n            Map Operator Tree:\n                TableScan\n                  alias: web_sales\n                  filterExpr: ws_item_sk is not null (type: boolean)\n                  Statistics: Num rows: 21594638446 Data size: 2850189889652 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: ws_item_sk is not null (type: boolean)\n                    Statistics: Num rows: 21594638446 Data size: 259124859072 Basic stats: COMPLETE Column stats: COMPLETE\n                    Map Join Operator\n                      condition map:\n                           Inner Join 0 to 1\n                      condition expressions:\n                        0 {ws_item_sk} {ws_sales_price} {ws_sold_date_sk}\n                        1 {d_date_sk} {d_date} {d_month_seq}\n                      keys:\n                        0 ws_sold_date_sk (type: int)\n                        1 d_date_sk (type: int)\n                      outputColumnNames: _col2, _col20, _col33, _col37, _col39, _col40\n                      input vertices:\n                        1 Map 8\n                      Statistics: Num rows: 24145061366 Data size: 2752536995724 Basic stats: COMPLETE Column stats: COMPLETE\n                      Filter Operator\n                        predicate: (((_col33 = _col37) and _col40 BETWEEN 1193 AND 1204) and _col2 is not null) (type: boolean)\n                        Statistics: Num rows: 6036265341 Data size: 688134248874 Basic stats: COMPLETE Column stats: COMPLETE\n                        Select Operator\n                          expressions: _col2 (type: int), _col39 (type: string), _col20 (type: float)\n                          outputColumnNames: _col2, _col39, _col20\n                          Statistics: Num rows: 6036265341 Data size: 688134248874 Basic stats: COMPLETE Column stats: COMPLETE\n                          Group By Operator\n                            aggregations: sum(_col20)\n                            keys: _col2 (type: int), _col39 (type: string)\n                            mode: hash\n                            outputColumnNames: _col0, _col1, _col2\n                            Statistics: Num rows: 6036265341 Data size: 639844126146 Basic stats: COMPLETE Column stats: COMPLETE\n                            Reduce Output Operator\n                              key expressions: _col0 (type: int), _col1 (type: string)\n                              sort order: ++\n                              Map-reduce partition columns: _col0 (type: int), _col1 (type: string)\n                              Statistics: Num rows: 6036265341 Data size: 639844126146 Basic stats: COMPLETE Column stats: COMPLETE\n                              value expressions: _col2 (type: double)\n            Execution mode: vectorized\n        Map 8 \n            Map Operator Tree:\n                TableScan\n                  alias: date_dim\n                  filterExpr: (d_date_sk is not null and d_month_seq BETWEEN 1193 AND 1204) (type: boolean)\n                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: (d_date_sk is not null and d_month_seq BETWEEN 1193 AND 1204) (type: boolean)\n                    Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                    Reduce Output Operator\n                      key expressions: d_date_sk (type: int)\n                      sort order: +\n                      Map-reduce partition columns: d_date_sk (type: int)\n                      Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                      value expressions: d_date (type: string), d_month_seq (type: int)\n                    Select Operator\n                      expressions: d_date_sk (type: int)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                      Group By Operator\n                        keys: _col0 (type: int)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                        Dynamic Partitioning Event Operator\n                          Target Input: web_sales\n                          Partition key expr: ws_sold_date_sk\n                          Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                          Target column: ws_sold_date_sk\n                          Target Vertex: Map 2\n            Execution mode: vectorized\n        Map 9 \n            Map Operator Tree:\n                TableScan\n                  alias: store_sales\n                  filterExpr: ss_item_sk is not null (type: boolean)\n                  Statistics: Num rows: 82510879939 Data size: 6873789738208 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: ss_item_sk is not null (type: boolean)\n                    Statistics: Num rows: 82510879939 Data size: 982359338028 Basic stats: COMPLETE Column stats: COMPLETE\n                    Map Join Operator\n                      condition map:\n                           Inner Join 0 to 1\n                      condition expressions:\n                        0 {ss_item_sk} {ss_sales_price} {ss_sold_date_sk}\n                        1 {d_date_sk} {d_date} {d_month_seq}\n                      keys:\n                        0 ss_sold_date_sk (type: int)\n                        1 d_date_sk (type: int)\n                      outputColumnNames: _col1, _col12, _col22, _col26, _col28, _col29\n                      input vertices:\n                        1 Map 1\n                      Statistics: Num rows: 92255782124 Data size: 10517159162136 Basic stats: COMPLETE Column stats: COMPLETE\n                      Filter Operator\n                        predicate: (((_col22 = _col26) and _col29 BETWEEN 1193 AND 1204) and _col1 is not null) (type: boolean)\n                        Statistics: Num rows: 23063945531 Data size: 2629289790534 Basic stats: COMPLETE Column stats: COMPLETE\n                        Select Operator\n                          expressions: _col1 (type: int), _col28 (type: string), _col12 (type: float)\n                          outputColumnNames: _col1, _col28, _col12\n                          Statistics: Num rows: 23063945531 Data size: 2629289790534 Basic stats: COMPLETE Column stats: COMPLETE\n                          Group By Operator\n                            aggregations: sum(_col12)\n                            keys: _col1 (type: int), _col28 (type: string)\n                            mode: hash\n                            outputColumnNames: _col0, _col1, _col2\n                            Statistics: Num rows: 23063945531 Data size: 2444778226286 Basic stats: COMPLETE Column stats: COMPLETE\n                            Reduce Output Operator\n                              key expressions: _col0 (type: int), _col1 (type: string)\n                              sort order: ++\n                              Map-reduce partition columns: _col0 (type: int), _col1 (type: string)\n                              Statistics: Num rows: 23063945531 Data size: 2444778226286 Basic stats: COMPLETE Column stats: COMPLETE\n                              value expressions: _col2 (type: double)\n            Execution mode: vectorized\n        Reducer 10 \n            Reduce Operator Tree:\n              Group By Operator\n                aggregations: sum(VALUE._col0)\n                keys: KEY._col0 (type: int), KEY._col1 (type: string)\n                mode: mergepartial\n                outputColumnNames: _col0, _col1, _col2\n                Statistics: Num rows: 23063945531 Data size: 2537034008410 Basic stats: COMPLETE Column stats: COMPLETE\n                Reduce Output Operator\n                  key expressions: _col0 (type: int), _col1 (type: string)\n                  sort order: ++\n                  Map-reduce partition columns: _col0 (type: int)\n                  Statistics: Num rows: 23063945531 Data size: 2537034008410 Basic stats: COMPLETE Column stats: COMPLETE\n                  value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: double)\n            Execution mode: vectorized\n        Reducer 11 \n            Reduce Operator Tree:\n              Extract\n                Statistics: Num rows: 23063945531 Data size: 2537034008410 Basic stats: COMPLETE Column stats: COMPLETE\n                PTF Operator\n                  Statistics: Num rows: 23063945531 Data size: 2537034008410 Basic stats: COMPLETE Column stats: COMPLETE\n                  Select Operator\n                    expressions: _col0 (type: int), _col1 (type: string), _wcol0 (type: double)\n                    outputColumnNames: _col0, _col1, _col3\n                    Statistics: Num rows: 23063945531 Data size: 184511564248 Basic stats: COMPLETE Column stats: COMPLETE\n                    Reduce Output Operator\n                      key expressions: _col0 (type: int), _col1 (type: string)\n                      sort order: ++\n                      Map-reduce partition columns: _col0 (type: int), _col1 (type: string)\n                      Statistics: Num rows: 23063945531 Data size: 184511564248 Basic stats: COMPLETE Column stats: COMPLETE\n                      value expressions: _col3 (type: double)\n        Reducer 3 \n            Reduce Operator Tree:\n              Group By Operator\n                aggregations: sum(VALUE._col0)\n                keys: KEY._col0 (type: int), KEY._col1 (type: string)\n                mode: mergepartial\n                outputColumnNames: _col0, _col1, _col2\n                Statistics: Num rows: 6036265341 Data size: 663989187510 Basic stats: COMPLETE Column stats: COMPLETE\n                Reduce Output Operator\n                  key expressions: _col0 (type: int), _col1 (type: string)\n                  sort order: ++\n                  Map-reduce partition columns: _col0 (type: int)\n                  Statistics: Num rows: 6036265341 Data size: 663989187510 Basic stats: COMPLETE Column stats: COMPLETE\n                  value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: double)\n            Execution mode: vectorized\n        Reducer 4 \n            Reduce Operator Tree:\n              Extract\n                Statistics: Num rows: 6036265341 Data size: 663989187510 Basic stats: COMPLETE Column stats: COMPLETE\n                PTF Operator\n                  Statistics: Num rows: 6036265341 Data size: 663989187510 Basic stats: COMPLETE Column stats: COMPLETE\n                  Select Operator\n                    expressions: _col0 (type: int), _col1 (type: string), _wcol0 (type: double)\n                    outputColumnNames: _col0, _col1, _col3\n                    Statistics: Num rows: 6036265341 Data size: 48290122728 Basic stats: COMPLETE Column stats: COMPLETE\n                    Reduce Output Operator\n                      key expressions: _col0 (type: int), _col1 (type: string)\n                      sort order: ++\n                      Map-reduce partition columns: _col0 (type: int), _col1 (type: string)\n                      Statistics: Num rows: 6036265341 Data size: 48290122728 Basic stats: COMPLETE Column stats: COMPLETE\n                      value expressions: _col3 (type: double)\n        Reducer 5 \n            Reduce Operator Tree:\n              Merge Join Operator\n                condition map:\n                     Outer Join 0 to 1\n                condition expressions:\n                  0 {KEY.reducesinkkey0} {KEY.reducesinkkey1} {VALUE._col1}\n                  1 {KEY.reducesinkkey0} {KEY.reducesinkkey1} {VALUE._col1}\n                outputColumnNames: _col0, _col1, _col3, _col4, _col5, _col7\n                Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                Select Operator\n                  expressions: CASE WHEN (_col0 is not null) THEN (_col0) ELSE (_col4) END (type: int), CASE WHEN (_col1 is not null) THEN (_col1) ELSE (_col5) END (type: string), _col3 (type: double), _col7 (type: double)\n                  outputColumnNames: _col0, _col1, _col2, _col3\n                  Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                  Reduce Output Operator\n                    key expressions: _col0 (type: int), _col1 (type: string)\n                    sort order: ++\n                    Map-reduce partition columns: _col0 (type: int)\n                    Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                    value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: double), _col3 (type: double)\n        Reducer 6 \n            Reduce Operator Tree:\n              Extract\n                Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                PTF Operator\n                  Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: (_wcol0 > _wcol1) (type: boolean)\n                    Statistics: Num rows: 3074457345618258602 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                    Select Operator\n                      expressions: _col0 (type: int), _col1 (type: string), _col2 (type: double), _col3 (type: double), _wcol0 (type: double), _wcol1 (type: double)\n                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                      Statistics: Num rows: 3074457345618258602 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                      Reduce Output Operator\n                        key expressions: _col0 (type: int), _col1 (type: string)\n                        sort order: ++\n                        Statistics: Num rows: 3074457345618258602 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                        TopN Hash Memory Usage: 0.04\n                        value expressions: _col2 (type: double), _col3 (type: double), _col4 (type: double), _col5 (type: double)\n        Reducer 7 \n            Reduce Operator Tree:\n              Select Operator\n                expressions: KEY.reducesinkkey0 (type: int), KEY.reducesinkkey1 (type: string), VALUE._col0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: double), VALUE._col3 (type: double)\n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                Statistics: Num rows: 3074457345618258602 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                Limit\n                  Number of rows: 100\n                  Statistics: Num rows: 100 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                  File Output Operator\n                    compressed: false\n                    Statistics: Num rows: 100 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                    table:\n                        input format: org.apache.hadoop.mapred.TextInputFormat\n                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n            Execution mode: vectorized\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: 100\n      Processor Tree:\n        ListSink\n{code}\n\nThe full exception \n{code}\nStatus: Failed\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Status: Failed\nVertex failed, vertexName=Reducer 11, vertexId=vertex_1414029100044_0733_1_05, diagnostics=[Task failed, taskId=task_1414029100044_0733_1_05_000027, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:262)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)\n\t... 13 more\nCaused by: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.tez.runtime.library.common.ValuesIterator$1$1.next(ValuesIterator.java:142)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processKeyValues(ReduceRecordSource.java:288)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)\n\t... 15 more\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum Error:  CurrentOffset=166741, offset=4, off=0, dataLength=167232, origLen=495, len=491, length=167236, checksumSize=4\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.doRead(IFileInputStream.java:254)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.read(IFileInputStream.java:184)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.close(IFileInputStream.java:131)\n\tat org.apache.hadoop.io.compress.DecompressorStream.close(DecompressorStream.java:205)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFile$Reader.close(IFile.java:784)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.closeReader(TezMerger.java:332)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.close(TezMerger.java:338)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.adjustPriorityQueue(TezMerger.java:489)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.next(TezMerger.java:503)\n\tat org.apache.tez.runtime.library.common.shuffle.orderedgrouped.MergeManager$RawKVIteratorReader.readRawKey(MergeManager.java:765)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.readRawKey(TezMerger.java:319)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.adjustPriorityQueue(TezMerger.java:481)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.next(TezMerger.java:503)\n\tat org.apache.tez.runtime.library.common.ValuesIterator.readNextKey(ValuesIterator.java:181)\n\tat org.apache.tez.runtime.library.common.ValuesIterator.access$300(ValuesIterator.java:47)\n\tat org.apache.tez.runtime.library.common.ValuesIterator$1$1.next(ValuesIterator.java:140)\n\t... 17 more\n], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1414029100044_0733_1_05 [Reducer 11] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex failed, vertexName=Reducer 11, vertexId=vertex_1414029100044_0733_1_05, diagnostics=[Task failed, taskId=task_1414029100044_0733_1_05_000027, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:262)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)\n\t... 13 more\nCaused by: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.tez.runtime.library.common.ValuesIterator$1$1.next(ValuesIterator.java:142)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processKeyValues(ReduceRecordSource.java:288)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)\n\t... 15 more\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum Error:  CurrentOffset=166741, offset=4, off=0, dataLength=167232, origLen=495, len=491, length=167236, checksumSize=4\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.doRead(IFileInputStream.java:254)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.read(IFileInputStream.java:184)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.close(IFileInputStream.java:131)\n\tat org.apache.hadoop.io.compress.DecompressorStream.close(DecompressorStream.java:205)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFile$Reader.close(IFile.java:784)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.closeReader(TezMerger.java:332)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.close(TezMerger.java:338)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.adjustPriorityQueue(TezMerger.java:489)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.next(TezMerger.java:503)\n\tat org.apache.tez.runtime.library.common.shuffle.orderedgrouped.MergeManager$RawKVIteratorReader.readRawKey(MergeManager.java:765)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.readRawKey(TezMerger.java:319)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.adjustPriorityQueue(TezMerger.java:481)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.next(TezMerger.java:503)\n\tat org.apache.tez.runtime.library.common.ValuesIterator.readNextKey(ValuesIterator.java:181)\n\tat org.apache.tez.runtime.library.common.ValuesIterator.access$300(ValuesIterator.java:47)\n\tat org.apache.tez.runtime.library.common.ValuesIterator$1$1.next(ValuesIterator.java:140)\n\t... 17 more\n], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1414029100044_0733_1_05 [Reducer 11] killed/failed due to:null]\nVertex killed, vertexName=Reducer 7, vertexId=vertex_1414029100044_0733_1_10, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_10 [Reducer 7] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex killed, vertexName=Reducer 7, vertexId=vertex_1414029100044_0733_1_10, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_10 [Reducer 7] killed/failed due to:null]\nVertex killed, vertexName=Reducer 6, vertexId=vertex_1414029100044_0733_1_09, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_09 [Reducer 6] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex killed, vertexName=Reducer 6, vertexId=vertex_1414029100044_0733_1_09, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_09 [Reducer 6] killed/failed due to:null]\nVertex killed, vertexName=Reducer 4, vertexId=vertex_1414029100044_0733_1_07, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_07 [Reducer 4] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex killed, vertexName=Reducer 4, vertexId=vertex_1414029100044_0733_1_07, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_07 [Reducer 4] killed/failed due to:null]\nVertex killed, vertexName=Reducer 5, vertexId=vertex_1414029100044_0733_1_08, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_08 [Reducer 5] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex killed, vertexName=Reducer 5, vertexId=vertex_1414029100044_0733_1_08, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_08 [Reducer 5] killed/failed due to:null]\nDAG failed due to vertex failure. failedVertices:1 killedVertices:4\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: DAG failed due to vertex failure. failedVertices:1 killedVertices:4\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask\n14/10/30 01:19:19 [main]: ERROR ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask\n{code}\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12678422","id":"12678422","filename":"HIVE-8677.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hagleitn","name":"hagleitn","key":"hagleitn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035"},"displayName":"Gunther Hagleitner","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-31T06:10:39.028+0000","size":650,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12678422/HIVE-8677.1.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"TPC-DS Q51 : fails with \"init not supported\" exception in GenericUDAFStreamingEvaluator.init","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmokhtar","name":"mmokhtar","key":"mmokhtar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmokhtar&avatarId=21863","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmokhtar&avatarId=21863","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmokhtar&avatarId=21863","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmokhtar&avatarId=21863"},"displayName":"Mostafa Mokhtar","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmokhtar","name":"mmokhtar","key":"mmokhtar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmokhtar&avatarId=21863","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmokhtar&avatarId=21863","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmokhtar&avatarId=21863","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmokhtar&avatarId=21863"},"displayName":"Mostafa Mokhtar","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12751801/comment/14191443","id":"14191443","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hagleitn","name":"hagleitn","key":"hagleitn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035"},"displayName":"Gunther Hagleitner","active":true,"timeZone":"America/Los_Angeles"},"body":"The problem is that with container reuse the PTFOperator gets initialized multiple times. With the streaming support we now update the PTFDesc in a way that can't be repeated. We replace a GenericUDAFEvaluator with a wrapper class - GenericUDAFStreamingEvaluator. The second time around we call init it throws an exception.\n\nThe fix is simple, just forward the init call to the wrapped class. The wrapping does not happen twice, everything else seems to get initialized correctly (second time around). I tested it on a few queries - seems ok.\n\n[~rhbutani] can you take a look? [~gopalv] if you have some time for review that'd be great too.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hagleitn","name":"hagleitn","key":"hagleitn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035"},"displayName":"Gunther Hagleitner","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-31T06:15:34.575+0000","updated":"2014-10-31T06:15:34.575+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12751801/comment/14191897","id":"14191897","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"body":"lgtm +1\n(probably can avoid the reconstructQueryDef processing after the first time; will look into this as a follow up)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-31T15:03:40.035+0000","updated":"2014-10-31T15:03:40.035+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12751801/comment/14192169","id":"14192169","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 at least one tests failed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12678422/HIVE-8677.1.patch\n\n{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6608 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1577/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1577/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1577/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 1 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12678422 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2014-10-31T18:01:43.873+0000","updated":"2014-10-31T18:01:43.873+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12751801/comment/14192302","id":"14192302","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hagleitn","name":"hagleitn","key":"hagleitn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035"},"displayName":"Gunther Hagleitner","active":true,"timeZone":"America/Los_Angeles"},"body":"Committed to both.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hagleitn","name":"hagleitn","key":"hagleitn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035"},"displayName":"Gunther Hagleitner","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-31T19:29:09.480+0000","updated":"2014-10-31T19:29:09.480+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12751801/comment/14210742","id":"14210742","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"This has been fixed in 0.14 release. Please open new jira if you see any issues.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-11-13T19:42:20.770+0000","updated":"2014-11-13T19:42:20.770+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-8677/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i21sfj:"}}