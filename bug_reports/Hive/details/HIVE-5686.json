{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12676508","self":"https://issues.apache.org/jira/rest/api/2/issue/12676508","key":"HIVE-5686","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12324986","id":"12324986","description":"released","name":"0.13.0","archived":false,"released":true,"releaseDate":"2014-04-21"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2013-11-07T16:18:49.699+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Dec 06 15:00:18 UTC 2013","customfield_12310420":"355940","customfield_12312320":null,"customfield_12310222":"10002_*:*_5_*:*_2328610916_*|*_1_*:*_5_*:*_928198801_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2013-12-06T15:00:18.893+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-5686/watchers","watchCount":3,"isWatching":false},"created":"2013-10-29T22:20:09.211+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"7.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2013-12-06T15:00:18.922+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Another interesting issue...\n{noformat}\nhive> create table zzzzz(c string) partitioned by (i date,j date);\nOK\nTime taken: 0.099 seconds\nhive> alter table zzzzz add partition (i='2012-01-01', j='foo');          \nFAILED: SemanticException [Error 10248]: Cannot add partition column j of type string as it cannot be converted to type date\nhive> alter table zzzzz add partition (i='2012-01-01', j=date 'foo');\nOK\nTime taken: 0.119 seconds\n{noformat}\n\nThe fake date is caught in normal queries:\n{noformat}\nhive> select * from zzzzz where j == date 'foo';\nFAILED: SemanticException Unable to convert date literal string to date value.\n{noformat}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12612864","id":"12612864","filename":"HIVE-5686.01.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-08T18:52:25.704+0000","size":4063,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12612864/HIVE-5686.01.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12613289","id":"12613289","filename":"HIVE-5686.02.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-12T01:15:17.575+0000","size":5976,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12613289/HIVE-5686.02.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12613966","id":"12613966","filename":"HIVE-5686.03.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-14T23:14:27.890+0000","size":5957,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12613966/HIVE-5686.03.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12613945","id":"12613945","filename":"HIVE-5686.03.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-14T21:55:13.754+0000","size":4921,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12613945/HIVE-5686.03.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12613715","id":"12613715","filename":"HIVE-5686.03.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-13T22:13:20.685+0000","size":5976,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12613715/HIVE-5686.03.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12616676","id":"12616676","filename":"HIVE-5686.04.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-12-03T02:21:13.991+0000","size":5989,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12616676/HIVE-5686.04.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12612528","id":"12612528","filename":"HIVE-5686.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-07T04:21:45.473+0000","size":4063,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12612528/HIVE-5686.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"356228","customfield_12312823":null,"summary":"partition column type validation doesn't quite work for dates","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13815606","id":"13815606","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"I think this is the culprit:\n{code}\n    try {\n      getPartExprNodeDesc(astNode, astExprNodeMap);\n    } catch (HiveException e) {\n      return;\n    }\n{code}\n\nWhen date-blah syntax is used, getPartExprNodeDesc throws exception about not being able to convert date literal which is silently gone.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-07T03:32:11.283+0000","updated":"2013-11-07T03:32:11.283+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13815631","id":"13815631","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"trivial patch","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-07T04:25:44.212+0000","updated":"2013-11-07T04:25:44.212+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13816095","id":"13816095","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"body":"+1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-07T16:18:49.699+0000","updated":"2013-11-07T16:18:49.699+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13816153","id":"13816153","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 no tests executed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12612528/HIVE-5686.patch\n\nTest results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/180/testReport\nConsole output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/180/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-180/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ svn = \\s\\v\\n ]]\n+ [[ -n '' ]]\n+ [[ -d apache-svn-trunk-source ]]\n+ [[ ! -d apache-svn-trunk-source/.svn ]]\n+ [[ ! -d apache-svn-trunk-source ]]\n+ cd apache-svn-trunk-source\n+ svn revert -R .\nReverted 'ant/src/org/apache/hadoop/hive/ant/GenVectorCode.java'\nReverted 'ant/src/org/apache/hadoop/hive/ant/GenVectorTestCode.java'\n++ awk '{print $2}'\n++ egrep -v '^X|^Performing status on external'\n++ svn status --no-ignore\n+ rm -rf target datanucleus.log ant/target shims/0.20/target shims/assembly/target shims/0.20S/target shims/0.23/target shims/common/target shims/common-secure/target metastore/target common/target common/src/gen serde/target\n+ svn update\n\nFetching external item into 'hcatalog/src/test/e2e/harness'\nExternal at revision 1539725.\n\nAt revision 1539725.\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nGoing to apply patch with: patch -p0\npatching file ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\npatching file ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java\npatching file ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\npatching file ql/src/test/queries/clientnegative/illegal_partition_type3.q\npatching file ql/src/test/results/clientnegative/illegal_partition_type3.q.out\n+ [[ maven == \\m\\a\\v\\e\\n ]]\n+ rm -rf /data/hive-ptest/working/maven/org/apache/hive\n+ mvn -B clean install -DskipTests -Dmaven.repo.local=/data/hive-ptest/working/maven\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO] \n[INFO] Hive\n[INFO] Hive Ant Utilities\n[INFO] Hive Shims Common\n[INFO] Hive Shims 0.20\n[INFO] Hive Shims Secure Common\n[INFO] Hive Shims 0.20S\n[INFO] Hive Shims 0.23\n[INFO] Hive Shims\n[INFO] Hive Common\n[INFO] Hive Serde\n[INFO] Hive Metastore\n[INFO] Hive Query Language\n[INFO] Hive Service\n[INFO] Hive JDBC\n[INFO] Hive Beeline\n[INFO] Hive CLI\n[INFO] Hive Contrib\n[INFO] Hive HBase Handler\n[INFO] Hive HCatalog\n[INFO] Hive HCatalog Core\n[INFO] Hive HCatalog Pig Adapter\n[INFO] Hive HCatalog Server Extensions\n[INFO] Hive HCatalog Webhcat Java Client\n[INFO] Hive HCatalog Webhcat\n[INFO] Hive HCatalog HBase Storage Handler\n[INFO] Hive HWI\n[INFO] Hive ODBC\n[INFO] Hive Shims Aggregator\n[INFO] Hive TestUtils\n[INFO] Hive Packaging\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive/0.13.0-SNAPSHOT/hive-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Ant Utilities 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-ant ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/ant (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-ant ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/ant/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-ant ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-ant ---\n[INFO] Compiling 5 source files to /data/hive-ptest/working/apache-svn-trunk-source/ant/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/org/apache/hadoop/hive/ant/DistinctElementsClassPath.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-ant ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/ant/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-ant ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-ant ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-ant ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-ant ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/hive-ant-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-ant ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/ant/target/hive-ant-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-ant/0.13.0-SNAPSHOT/hive-ant-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/ant/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-ant/0.13.0-SNAPSHOT/hive-ant-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-common ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/common (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-common ---\n[INFO] Compiling 15 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-common ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-common ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-common ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-common ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-common ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common/0.13.0-SNAPSHOT/hive-shims-common-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common/0.13.0-SNAPSHOT/hive-shims-common-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.20 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.20 ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20 (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.20 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.20 ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.20 ---\n[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.20 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.20 ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.20 ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.20 ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.20 ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.20 ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20/0.13.0-SNAPSHOT/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20/0.13.0-SNAPSHOT/hive-shims-0.20-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims Secure Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-common-secure ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-common-secure ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-common-secure ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-common-secure ---\n[INFO] Compiling 12 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-common-secure ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-common-secure ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-common-secure ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-common-secure ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-common-secure ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-common-secure ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common-secure/0.13.0-SNAPSHOT/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common-secure/0.13.0-SNAPSHOT/hive-shims-common-secure-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.20S 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.20S ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.20S ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.20S ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.20S ---\n[INFO] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.20S ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.20S ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.20S ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.20S ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.20S ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.20S ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20S/0.13.0-SNAPSHOT/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20S/0.13.0-SNAPSHOT/hive-shims-0.20S-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.23 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.23 ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23 (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.23 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.23 ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.23 ---\n[INFO] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.23 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.23 ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.23 ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.23 ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.23 ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.23 ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.23/0.13.0-SNAPSHOT/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.23/0.13.0-SNAPSHOT/hive-shims-0.23-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims ---\n[WARNING] JAR will be empty - no content was marked for inclusion!\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-assembly-plugin:2.3:single (uberjar) @ hive-shims ---\n[INFO] Reading assembly descriptor: src/assemble/uberjar.xml\n[WARNING] Artifact: org.apache.hive:hive-shims:jar:0.13.0-SNAPSHOT references the same file as the assembly destination file. Moving it to a temporary location for inclusion.\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[WARNING] Configuration options: 'appendAssemblyId' is set to false, and 'classifier' is missing.\nInstead of attaching the assembly file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar, it will become the file for main project artifact.\nNOTE: If multiple descriptors or descriptor-formats are provided for this project, the value of this file will be non-deterministic!\n[WARNING] Replacing pre-existing project main-artifact file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/archive-tmp/hive-shims-0.13.0-SNAPSHOT.jar\nwith assembly file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.13.0-SNAPSHOT/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.13.0-SNAPSHOT/hive-shims-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-common ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/common (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (generate-version-annotation) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-common ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/common/src/gen added.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-common ---\n[INFO] Compiling 31 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 4 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-common ---\n[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/test-classes\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-common ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-common ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-common ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.13.0-SNAPSHOT/hive-common-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.13.0-SNAPSHOT/hive-common-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Serde 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-serde ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/serde (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-serde ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/protobuf/gen-java added.\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/thrift/gen-javabean added.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-serde ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-serde ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-serde ---\n[INFO] Compiling 351 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-serde ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-serde ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-serde ---\n[INFO] Compiling 41 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/test-classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-serde ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-serde ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-serde ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.13.0-SNAPSHOT/hive-serde-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.13.0-SNAPSHOT/hive-serde-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Metastore 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-metastore ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/metastore (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-metastore ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/model added.\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/gen/thrift/gen-javabean added.\n[INFO] \n[INFO] --- antlr3-maven-plugin:3.4:antlr (default) @ hive-metastore ---\n[INFO] ANTLR: Processing source directory /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java\nANTLR Parser Generator  Version 3.4\norg/apache/hadoop/hive/metastore/parser/Filter.g\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-metastore ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-metastore ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-metastore ---\n[INFO] Compiling 132 source files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- datanucleus-maven-plugin:3.3.0-release:enhance (default) @ hive-metastore ---\n[INFO] DataNucleus Enhancer (version 3.2.2) for API \"JDO\" using JRE \"1.6\"\nDataNucleus Enhancer : Classpath\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-maven-plugin/3.3.0-release/datanucleus-maven-plugin-3.3.0-release.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-core/3.2.2/datanucleus-core-3.2.2.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/plexus/plexus-utils/3.0.8/plexus-utils-3.0.8.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/plexus/plexus-component-annotations/1.5.5/plexus-component-annotations-1.5.5.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-inject-bean/2.3.0/sisu-inject-bean-2.3.0.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-guice/3.1.0/sisu-guice-3.1.0-no_aop.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-guava/0.9.9/sisu-guava-0.9.9.jar\n>>  /data/hive-ptest/working/maven/org/apache/xbean/xbean-reflect/3.4/xbean-reflect-3.4.jar\n>>  /data/hive-ptest/working/maven/log4j/log4j/1.2.12/log4j-1.2.12.jar\n>>  /data/hive-ptest/working/maven/commons-logging/commons-logging-api/1.1/commons-logging-api-1.1.jar\n>>  /data/hive-ptest/working/maven/com/google/collections/google-collections/1.0/google-collections-1.0.jar\n>>  /data/hive-ptest/working/maven/junit/junit/3.8.2/junit-3.8.2.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/classes\n>>  /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar\n>>  /data/hive-ptest/working/maven/org/tukaani/xz/1.0/xz-1.0.jar\n>>  /data/hive-ptest/working/maven/commons-codec/commons-codec/1.4/commons-codec-1.4.jar\n>>  /data/hive-ptest/working/maven/org/apache/avro/avro/1.7.1/avro-1.7.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar\n>>  /data/hive-ptest/working/maven/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar\n>>  /data/hive-ptest/working/maven/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.3/zookeeper-3.4.3.jar\n>>  /data/hive-ptest/working/maven/jline/jline/0.9.94/jline-0.9.94.jar\n>>  /data/hive-ptest/working/maven/org/jboss/netty/netty/3.2.2.Final/netty-3.2.2.Final.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/com/google/guava/guava/11.0.2/guava-11.0.2.jar\n>>  /data/hive-ptest/working/maven/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar\n>>  /data/hive-ptest/working/maven/commons-cli/commons-cli/1.2/commons-cli-1.2.jar\n>>  /data/hive-ptest/working/maven/commons-lang/commons-lang/2.4/commons-lang-2.4.jar\n>>  /data/hive-ptest/working/maven/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/derby/derby/10.4.2.0/derby-10.4.2.0.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-api-jdo/3.2.1/datanucleus-api-jdo-3.2.1.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-rdbms/3.2.1/datanucleus-rdbms-3.2.1.jar\n>>  /data/hive-ptest/working/maven/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar\n>>  /data/hive-ptest/working/maven/javax/transaction/jta/1.1/jta-1.1.jar\n>>  /data/hive-ptest/working/maven/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar\n>>  /data/hive-ptest/working/maven/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar\n>>  /data/hive-ptest/working/maven/antlr/antlr/2.7.7/antlr-2.7.7.jar\n>>  /data/hive-ptest/working/maven/org/apache/thrift/libfb303/0.9.0/libfb303-0.9.0.jar\n>>  /data/hive-ptest/working/maven/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar\n>>  /data/hive-ptest/working/maven/org/apache/httpcomponents/httpclient/4.1.3/httpclient-4.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/httpcomponents/httpcore/4.1.3/httpcore-4.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/hadoop/hadoop-core/1.2.1/hadoop-core-1.2.1.jar\n>>  /data/hive-ptest/working/maven/xmlenc/xmlenc/0.52/xmlenc-0.52.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar\n>>  /data/hive-ptest/working/maven/stax/stax-api/1.0.1/stax-api-1.0.1.jar\n>>  /data/hive-ptest/working/maven/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar\n>>  /data/hive-ptest/working/maven/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar\n>>  /data/hive-ptest/working/maven/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar\n>>  /data/hive-ptest/working/maven/javax/activation/activation/1.1/activation-1.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-jaxrs/1.7.1/jackson-jaxrs-1.7.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar\n>>  /data/hive-ptest/working/maven/asm/asm/3.1/asm-3.1.jar\n>>  /data/hive-ptest/working/maven/commons-io/commons-io/2.1/commons-io-2.1.jar\n>>  /data/hive-ptest/working/maven/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.jar\n>>  /data/hive-ptest/working/maven/org/apache/commons/commons-math/2.1/commons-math-2.1.jar\n>>  /data/hive-ptest/working/maven/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar\n>>  /data/hive-ptest/working/maven/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar\n>>  /data/hive-ptest/working/maven/commons-digester/commons-digester/1.8/commons-digester-1.8.jar\n>>  /data/hive-ptest/working/maven/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar\n>>  /data/hive-ptest/working/maven/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar\n>>  /data/hive-ptest/working/maven/commons-net/commons-net/1.4.1/commons-net-1.4.1.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar\n>>  /data/hive-ptest/working/maven/tomcat/jasper-runtime/5.5.12/jasper-runtime-5.5.12.jar\n>>  /data/hive-ptest/working/maven/tomcat/jasper-compiler/5.5.12/jasper-compiler-5.5.12.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar\n>>  /data/hive-ptest/working/maven/ant/ant/1.6.5/ant-1.6.5.jar\n>>  /data/hive-ptest/working/maven/commons-el/commons-el/1.0/commons-el-1.0.jar\n>>  /data/hive-ptest/working/maven/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar\n>>  /data/hive-ptest/working/maven/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar\n>>  /data/hive-ptest/working/maven/oro/oro/2.0.8/oro-2.0.8.jar\n>>  /data/hive-ptest/working/maven/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar\n>>  /data/hive-ptest/working/maven/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar\n>>  /data/hive-ptest/working/maven/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar\n>>  /data/hive-ptest/working/maven/log4j/log4j/1.2.16/log4j-1.2.16.jar\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDatabase\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MFieldSchema\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MType\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTable\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MOrder\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MStringList\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartition\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MIndex\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MRole\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MRoleMap\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MMasterKey\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDelegationToken\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MVersionTable\nDataNucleus Enhancer completed with success for 25 classes. Timings : input=577 ms, enhance=920 ms, total=1497 ms. Consult the log for full details\n\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-metastore ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-metastore ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-metastore ---\n[INFO] Compiling 10 source files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/test-classes\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-metastore ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-metastore ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:test-jar (default) @ hive-metastore ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT-tests.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-metastore ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT.pom\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT-tests.jar\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Query Language 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Hive .............................................. SUCCESS [2.509s]\n[INFO] Hive Ant Utilities ................................ SUCCESS [6.684s]\n[INFO] Hive Shims Common ................................. SUCCESS [2.792s]\n[INFO] Hive Shims 0.20 ................................... SUCCESS [1.813s]\n[INFO] Hive Shims Secure Common .......................... SUCCESS [3.050s]\n[INFO] Hive Shims 0.20S .................................. SUCCESS [1.334s]\n[INFO] Hive Shims 0.23 ................................... SUCCESS [3.098s]\n[INFO] Hive Shims ........................................ SUCCESS [3.495s]\n[INFO] Hive Common ....................................... SUCCESS [5.126s]\n[INFO] Hive Serde ........................................ SUCCESS [11.459s]\n[INFO] Hive Metastore .................................... SUCCESS [23.881s]\n[INFO] Hive Query Language ............................... FAILURE [0.215s]\n[INFO] Hive Service ...................................... SKIPPED\n[INFO] Hive JDBC ......................................... SKIPPED\n[INFO] Hive Beeline ...................................... SKIPPED\n[INFO] Hive CLI .......................................... SKIPPED\n[INFO] Hive Contrib ...................................... SKIPPED\n[INFO] Hive HBase Handler ................................ SKIPPED\n[INFO] Hive HCatalog ..................................... SKIPPED\n[INFO] Hive HCatalog Core ................................ SKIPPED\n[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED\n[INFO] Hive HCatalog Server Extensions ................... SKIPPED\n[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED\n[INFO] Hive HCatalog Webhcat ............................. SKIPPED\n[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED\n[INFO] Hive HWI .......................................... SKIPPED\n[INFO] Hive ODBC ......................................... SKIPPED\n[INFO] Hive Shims Aggregator ............................. SKIPPED\n[INFO] Hive TestUtils .................................... SKIPPED\n[INFO] Hive Packaging .................................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 1:07.801s\n[INFO] Finished at: Thu Nov 07 12:25:27 EST 2013\n[INFO] Final Memory: 43M/202M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project hive-exec: Could not resolve dependencies for project org.apache.hive:hive-exec:jar:0.13.0-SNAPSHOT: Could not find artifact org.apache.hive:hive-shims:jar:uberjar:0.13.0-SNAPSHOT -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-exec\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12612528","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-11-07T17:25:40.880+0000","updated":"2013-11-07T17:25:40.880+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13816332","id":"13816332","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"build failures is a known issue, already fixed elsewhere, not related","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-07T19:40:24.583+0000","updated":"2013-11-07T19:40:24.583+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13817467","id":"13817467","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"body":"You need to re-upload the patch for Hive QA to kick in.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-08T17:06:31.583+0000","updated":"2013-11-08T17:06:31.583+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13817569","id":"13817569","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"same patch","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-08T18:52:25.708+0000","updated":"2013-11-08T18:52:25.708+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13817954","id":"13817954","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 at least one tests failed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12612864/HIVE-5686.01.patch\n\n{color:red}ERROR:{color} -1 due to 52 failed/errored test(s), 4599 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_partition_skip_default\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape1\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_escape2\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part1\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part10\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part11\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part12\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part13\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part2\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part3\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part4\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part6\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part8\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part9\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock3\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_lock4\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge4\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition2\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition3\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition4\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_merge_dynamic_partition5\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mi\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge1\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_rcfile_merge2\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample10\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats12\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats13\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats14\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats15\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats19\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats2\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats4\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats6\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats7\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats8\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_noscan_1\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_updateAccessTime\norg.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats\norg.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats2\norg.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_stats3\norg.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers\norg.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part\norg.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_num_buckets\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_archive_insert4\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part2\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dyn_part4\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partscan_norcfile\n{noformat}\n\nTest results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/217/testReport\nConsole output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/217/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests failed with: TestsFailedException: 52 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12612864","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-11-09T02:39:55.655+0000","updated":"2013-11-09T02:39:55.655+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13819597","id":"13819597","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"This is due to dynamic partitioning. I will add some code to not do that, and also to validate partially-dynamic partitioning","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-11T23:36:22.189+0000","updated":"2013-11-11T23:36:22.189+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13819700","id":"13819700","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"add support for dynamic partitioning with some static columns; remove the corresponding exception","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-12T01:15:17.580+0000","updated":"2013-11-12T01:15:17.580+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13819704","id":"13819704","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"posted https://reviews.facebook.net/D13989 , Apache RB gives me 502 errors","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-12T01:21:21.414+0000","updated":"2013-11-12T01:21:21.414+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13821908","id":"13821908","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"exact same patch again","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-13T22:13:20.690+0000","updated":"2013-11-13T22:13:20.690+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13821909","id":"13821909","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"o HiveQA, please accept my humble offering of a patch!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-13T22:13:29.418+0000","updated":"2013-11-13T22:13:29.418+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13821915","id":"13821915","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"LOL - I don't think the patch from monday was picked up because apache jenkins was not healthy on monday","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-13T22:19:00.033+0000","updated":"2013-11-13T22:19:00.033+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13822849","id":"13822849","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 no tests executed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12613715/HIVE-5686.03.patch\n\nTest results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/281/testReport\nConsole output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/281/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-281/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ svn = \\s\\v\\n ]]\n+ [[ -n '' ]]\n+ [[ -d apache-svn-trunk-source ]]\n+ [[ ! -d apache-svn-trunk-source/.svn ]]\n+ [[ ! -d apache-svn-trunk-source ]]\n+ cd apache-svn-trunk-source\n+ svn revert -R .\nReverted 'common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/HiveDecimalUtils.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/PrimitiveObjectInspector.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/AbstractPrimitiveObjectInspector.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java'\nReverted 'serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java'\nReverted 'ql/src/test/results/clientnegative/udf_assert_true2.q.out'\nReverted 'ql/src/test/results/clientnegative/invalid_arithmetic_type.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_join13.q.out'\nReverted 'ql/src/test/results/clientpositive/decimal_udf.q.out'\nReverted 'ql/src/test/results/clientpositive/rcfile_createas1.q.out'\nReverted 'ql/src/test/results/clientpositive/udf_pmod.q.out'\nReverted 'ql/src/test/results/clientpositive/orc_createas1.q.out'\nReverted 'ql/src/test/results/clientpositive/rcfile_merge2.q.out'\nReverted 'ql/src/test/results/clientpositive/ql_rewrite_gbtoidx.q.out'\nReverted 'ql/src/test/results/clientpositive/input8.q.out'\nReverted 'ql/src/test/results/clientpositive/vectorization_short_regress.q.out'\nReverted 'ql/src/test/results/clientpositive/decimal_6.q.out'\nReverted 'ql/src/test/results/clientpositive/rcfile_merge1.q.out'\nReverted 'ql/src/test/results/clientpositive/bucketmapjoin_negative3.q.out'\nReverted 'ql/src/test/results/clientpositive/windowing_expressions.q.out'\nReverted 'ql/src/test/results/clientpositive/vectorization_5.q.out'\nReverted 'ql/src/test/results/clientpositive/num_op_type_conv.q.out'\nReverted 'ql/src/test/results/clientpositive/skewjoin.q.out'\nReverted 'ql/src/test/results/clientpositive/vectorization_15.q.out'\nReverted 'ql/src/test/results/clientpositive/ppd_constant_expr.q.out'\nReverted 'ql/src/test/results/clientpositive/vectorized_math_funcs.q.out'\nReverted 'ql/src/test/results/clientpositive/auto_join2.q.out'\nReverted 'ql/src/test/results/compiler/plan/join2.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input8.q.xml'\nReverted 'ql/src/test/results/compiler/plan/udf4.q.xml'\nReverted 'ql/src/test/results/compiler/plan/input20.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample1.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample2.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample3.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample4.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample5.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample6.q.xml'\nReverted 'ql/src/test/results/compiler/plan/sample7.q.xml'\nReverted 'ql/src/test/results/compiler/plan/cast1.q.xml'\nReverted 'ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java'\nReverted 'ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorSelectOperator.java'\nReverted 'ql/src/test/org/apache/hadoop/hive/ql/udf/TestUDFPosMod.java'\nReverted 'ql/src/test/org/apache/hadoop/hive/ql/udf/TestUDFOPDivide.java'\nReverted 'ql/src/test/org/apache/hadoop/hive/ql/udf/TestUDFOPMod.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMultiply.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericOp.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPDivide.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPlus.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMinus.java'\n++ egrep -v '^X|^Performing status on external'\n++ awk '{print $2}'\n++ svn status --no-ignore\n+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/assembly/target shims/0.20S/target shims/0.23/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPDivide.java ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPMinus.java ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPMultiply.java ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPMod.java ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFPosMod.java ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFOPPlus.java ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPPlus.java ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBaseNumeric.java ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMod.java ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMinus.java ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFPosMod.java ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDivide.java ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMultiply.java\n+ svn update\n\nFetching external item into 'hcatalog/src/test/e2e/harness'\nExternal at revision 1542057.\n\nAt revision 1542057.\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nGoing to apply patch with: patch -p0\npatching file ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java\npatching file ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java\npatching file ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nHunk #1 succeeded at 5222 (offset 63 lines).\npatching file ql/src/test/queries/clientnegative/illegal_partition_type3.q\npatching file ql/src/test/results/clientnegative/illegal_partition_type3.q.out\n+ [[ maven == \\m\\a\\v\\e\\n ]]\n+ rm -rf /data/hive-ptest/working/maven/org/apache/hive\n+ mvn -B clean install -DskipTests -Dmaven.repo.local=/data/hive-ptest/working/maven\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO] \n[INFO] Hive\n[INFO] Hive Ant Utilities\n[INFO] Hive Shims Common\n[INFO] Hive Shims 0.20\n[INFO] Hive Shims Secure Common\n[INFO] Hive Shims 0.20S\n[INFO] Hive Shims 0.23\n[INFO] Hive Shims\n[INFO] Hive Common\n[INFO] Hive Serde\n[INFO] Hive Metastore\n[INFO] Hive Query Language\n[INFO] Hive Service\n[INFO] Hive JDBC\n[INFO] Hive Beeline\n[INFO] Hive CLI\n[INFO] Hive Contrib\n[INFO] Hive HBase Handler\n[INFO] Hive HCatalog\n[INFO] Hive HCatalog Core\n[INFO] Hive HCatalog Pig Adapter\n[INFO] Hive HCatalog Server Extensions\n[INFO] Hive HCatalog Webhcat Java Client\n[INFO] Hive HCatalog Webhcat\n[INFO] Hive HCatalog HBase Storage Handler\n[INFO] Hive HWI\n[INFO] Hive ODBC\n[INFO] Hive Shims Aggregator\n[INFO] Hive TestUtils\n[INFO] Hive Packaging\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive/0.13.0-SNAPSHOT/hive-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Ant Utilities 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-ant ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/ant (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-ant ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/ant/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-ant ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-ant ---\n[INFO] Compiling 5 source files to /data/hive-ptest/working/apache-svn-trunk-source/ant/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/org/apache/hadoop/hive/ant/DistinctElementsClassPath.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-ant ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/ant/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-ant ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/ant/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-ant ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-ant ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-ant ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/ant/target/hive-ant-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-ant ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/ant/target/hive-ant-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-ant/0.13.0-SNAPSHOT/hive-ant-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/ant/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-ant/0.13.0-SNAPSHOT/hive-ant-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-common ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/common (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-common ---\n[INFO] Compiling 15 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-common ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-common ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-common ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-common ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-common ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common/0.13.0-SNAPSHOT/hive-shims-common-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common/0.13.0-SNAPSHOT/hive-shims-common-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.20 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.20 ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20 (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.20 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.20 ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.20 ---\n[INFO] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.20 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.20 ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.20 ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.20 ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.20 ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.20 ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20/0.13.0-SNAPSHOT/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20/0.13.0-SNAPSHOT/hive-shims-0.20-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims Secure Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-common-secure ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-common-secure ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-common-secure ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-common-secure ---\n[INFO] Compiling 12 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-common-secure ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-common-secure ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-common-secure ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-common-secure ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-common-secure ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-common-secure ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common-secure/0.13.0-SNAPSHOT/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-common-secure/0.13.0-SNAPSHOT/hive-shims-common-secure-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.20S 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.20S ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.20S ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.20S ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.20S ---\n[INFO] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.20S ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.20S ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.20S ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.20S ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.20S ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.20S ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20S/0.13.0-SNAPSHOT/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.20S/0.13.0-SNAPSHOT/hive-shims-0.20S-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.23 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims-0.23 ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23 (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims-0.23 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims-0.23 ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims-0.23 ---\n[INFO] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java uses or overrides a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims-0.23 ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims-0.23 ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims-0.23 ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims-0.23 ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims-0.23 ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims-0.23 ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.23/0.13.0-SNAPSHOT/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/shims/hive-shims-0.23/0.13.0-SNAPSHOT/hive-shims-0.23-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Shims 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-shims ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-shims ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-shims ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-shims ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims ---\n[WARNING] JAR will be empty - no content was marked for inclusion!\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-assembly-plugin:2.3:single (uberjar) @ hive-shims ---\n[INFO] Reading assembly descriptor: src/assemble/uberjar.xml\n[WARNING] Artifact: org.apache.hive:hive-shims:jar:0.13.0-SNAPSHOT references the same file as the assembly destination file. Moving it to a temporary location for inclusion.\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[INFO] META-INF/MANIFEST.MF already added, skipping\n[WARNING] Configuration options: 'appendAssemblyId' is set to false, and 'classifier' is missing.\nInstead of attaching the assembly file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar, it will become the file for main project artifact.\nNOTE: If multiple descriptors or descriptor-formats are provided for this project, the value of this file will be non-deterministic!\n[WARNING] Replacing pre-existing project main-artifact file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/archive-tmp/hive-shims-0.13.0-SNAPSHOT.jar\nwith assembly file: /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.13.0-SNAPSHOT/hive-shims-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.13.0-SNAPSHOT/hive-shims-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Common 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-common ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/common (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (generate-version-annotation) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-common ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/common/src/gen added.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-common ---\n[INFO] Compiling 31 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/classes\n[WARNING] Note: /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java uses unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-common ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 4 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-common ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-common ---\n[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-svn-trunk-source/common/target/test-classes\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-common ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-common ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-common ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.13.0-SNAPSHOT/hive-common-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/common/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-common/0.13.0-SNAPSHOT/hive-common-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Serde 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-serde ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/serde (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-serde ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/protobuf/gen-java added.\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/gen/thrift/gen-javabean added.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-serde ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/main/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-serde ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-serde ---\n[INFO] Compiling 351 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-serde ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-serde ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-serde ---\n[INFO] Compiling 41 source files to /data/hive-ptest/working/apache-svn-trunk-source/serde/target/test-classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-serde ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-serde ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-serde ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.13.0-SNAPSHOT/hive-serde-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/serde/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-serde/0.13.0-SNAPSHOT/hive-serde-0.13.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Metastore 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-metastore ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/metastore (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-metastore ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/model added.\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/gen/thrift/gen-javabean added.\n[INFO] \n[INFO] --- antlr3-maven-plugin:3.4:antlr (default) @ hive-metastore ---\n[INFO] ANTLR: Processing source directory /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java\nANTLR Parser Generator  Version 3.4\norg/apache/hadoop/hive/metastore/parser/Filter.g\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-metastore ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-metastore ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-metastore ---\n[INFO] Compiling 132 source files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/classes\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] \n[INFO] --- datanucleus-maven-plugin:3.3.0-release:enhance (default) @ hive-metastore ---\n[INFO] DataNucleus Enhancer (version 3.2.2) for API \"JDO\" using JRE \"1.6\"\nDataNucleus Enhancer : Classpath\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-maven-plugin/3.3.0-release/datanucleus-maven-plugin-3.3.0-release.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-core/3.2.2/datanucleus-core-3.2.2.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/plexus/plexus-utils/3.0.8/plexus-utils-3.0.8.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/plexus/plexus-component-annotations/1.5.5/plexus-component-annotations-1.5.5.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-inject-bean/2.3.0/sisu-inject-bean-2.3.0.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-guice/3.1.0/sisu-guice-3.1.0-no_aop.jar\n>>  /data/hive-ptest/working/maven/org/sonatype/sisu/sisu-guava/0.9.9/sisu-guava-0.9.9.jar\n>>  /data/hive-ptest/working/maven/org/apache/xbean/xbean-reflect/3.4/xbean-reflect-3.4.jar\n>>  /data/hive-ptest/working/maven/log4j/log4j/1.2.12/log4j-1.2.12.jar\n>>  /data/hive-ptest/working/maven/commons-logging/commons-logging-api/1.1/commons-logging-api-1.1.jar\n>>  /data/hive-ptest/working/maven/com/google/collections/google-collections/1.0/google-collections-1.0.jar\n>>  /data/hive-ptest/working/maven/junit/junit/3.8.2/junit-3.8.2.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/classes\n>>  /data/hive-ptest/working/apache-svn-trunk-source/serde/target/hive-serde-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/common/target/hive-common-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar\n>>  /data/hive-ptest/working/maven/org/tukaani/xz/1.0/xz-1.0.jar\n>>  /data/hive-ptest/working/maven/commons-codec/commons-codec/1.4/commons-codec-1.4.jar\n>>  /data/hive-ptest/working/maven/org/apache/avro/avro/1.7.1/avro-1.7.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar\n>>  /data/hive-ptest/working/maven/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar\n>>  /data/hive-ptest/working/maven/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/assembly/target/hive-shims-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/common/target/hive-shims-common-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20/target/hive-shims-0.20-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/common-secure/target/hive-shims-common-secure-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.3/zookeeper-3.4.3.jar\n>>  /data/hive-ptest/working/maven/jline/jline/0.9.94/jline-0.9.94.jar\n>>  /data/hive-ptest/working/maven/org/jboss/netty/netty/3.2.2.Final/netty-3.2.2.Final.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.20S/target/hive-shims-0.20S-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/apache-svn-trunk-source/shims/0.23/target/hive-shims-0.23-0.13.0-SNAPSHOT.jar\n>>  /data/hive-ptest/working/maven/com/google/guava/guava/11.0.2/guava-11.0.2.jar\n>>  /data/hive-ptest/working/maven/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar\n>>  /data/hive-ptest/working/maven/commons-cli/commons-cli/1.2/commons-cli-1.2.jar\n>>  /data/hive-ptest/working/maven/commons-lang/commons-lang/2.4/commons-lang-2.4.jar\n>>  /data/hive-ptest/working/maven/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/derby/derby/10.4.2.0/derby-10.4.2.0.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-api-jdo/3.2.1/datanucleus-api-jdo-3.2.1.jar\n>>  /data/hive-ptest/working/maven/org/datanucleus/datanucleus-rdbms/3.2.1/datanucleus-rdbms-3.2.1.jar\n>>  /data/hive-ptest/working/maven/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar\n>>  /data/hive-ptest/working/maven/javax/transaction/jta/1.1/jta-1.1.jar\n>>  /data/hive-ptest/working/maven/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar\n>>  /data/hive-ptest/working/maven/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar\n>>  /data/hive-ptest/working/maven/antlr/antlr/2.7.7/antlr-2.7.7.jar\n>>  /data/hive-ptest/working/maven/org/apache/thrift/libfb303/0.9.0/libfb303-0.9.0.jar\n>>  /data/hive-ptest/working/maven/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar\n>>  /data/hive-ptest/working/maven/org/apache/httpcomponents/httpclient/4.1.3/httpclient-4.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/httpcomponents/httpcore/4.1.3/httpcore-4.1.3.jar\n>>  /data/hive-ptest/working/maven/org/apache/hadoop/hadoop-core/1.2.1/hadoop-core-1.2.1.jar\n>>  /data/hive-ptest/working/maven/xmlenc/xmlenc/0.52/xmlenc-0.52.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar\n>>  /data/hive-ptest/working/maven/stax/stax-api/1.0.1/stax-api-1.0.1.jar\n>>  /data/hive-ptest/working/maven/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar\n>>  /data/hive-ptest/working/maven/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar\n>>  /data/hive-ptest/working/maven/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar\n>>  /data/hive-ptest/working/maven/javax/activation/activation/1.1/activation-1.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-jaxrs/1.7.1/jackson-jaxrs-1.7.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar\n>>  /data/hive-ptest/working/maven/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar\n>>  /data/hive-ptest/working/maven/asm/asm/3.1/asm-3.1.jar\n>>  /data/hive-ptest/working/maven/commons-io/commons-io/2.1/commons-io-2.1.jar\n>>  /data/hive-ptest/working/maven/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.jar\n>>  /data/hive-ptest/working/maven/org/apache/commons/commons-math/2.1/commons-math-2.1.jar\n>>  /data/hive-ptest/working/maven/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar\n>>  /data/hive-ptest/working/maven/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar\n>>  /data/hive-ptest/working/maven/commons-digester/commons-digester/1.8/commons-digester-1.8.jar\n>>  /data/hive-ptest/working/maven/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar\n>>  /data/hive-ptest/working/maven/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar\n>>  /data/hive-ptest/working/maven/commons-net/commons-net/1.4.1/commons-net-1.4.1.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar\n>>  /data/hive-ptest/working/maven/tomcat/jasper-runtime/5.5.12/jasper-runtime-5.5.12.jar\n>>  /data/hive-ptest/working/maven/tomcat/jasper-compiler/5.5.12/jasper-compiler-5.5.12.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar\n>>  /data/hive-ptest/working/maven/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar\n>>  /data/hive-ptest/working/maven/ant/ant/1.6.5/ant-1.6.5.jar\n>>  /data/hive-ptest/working/maven/commons-el/commons-el/1.0/commons-el-1.0.jar\n>>  /data/hive-ptest/working/maven/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar\n>>  /data/hive-ptest/working/maven/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar\n>>  /data/hive-ptest/working/maven/oro/oro/2.0.8/oro-2.0.8.jar\n>>  /data/hive-ptest/working/maven/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar\n>>  /data/hive-ptest/working/maven/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar\n>>  /data/hive-ptest/working/maven/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar\n>>  /data/hive-ptest/working/maven/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar\n>>  /data/hive-ptest/working/maven/log4j/log4j/1.2.16/log4j-1.2.16.jar\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDatabase\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MFieldSchema\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MType\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTable\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MOrder\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MStringList\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartition\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MIndex\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MRole\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MRoleMap\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MMasterKey\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDelegationToken\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics\nENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MVersionTable\nDataNucleus Enhancer completed with success for 25 classes. Timings : input=584 ms, enhance=926 ms, total=1510 ms. Consult the log for full details\n\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ hive-metastore ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/test/resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-metastore ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp/conf\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-metastore ---\n[INFO] Compiling 10 source files to /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/test-classes\n[INFO] \n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-metastore ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-metastore ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-jar-plugin:2.2:test-jar (default) @ hive-metastore ---\n[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT-tests.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-metastore ---\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT.pom\n[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/metastore/target/hive-metastore-0.13.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-metastore/0.13.0-SNAPSHOT/hive-metastore-0.13.0-SNAPSHOT-tests.jar\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Query Language 0.13.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-exec ---\n[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/ql (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (generate-sources) @ hive-exec ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ql/target/generated-sources/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ql/target/generated-sources/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/gen\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/ql/target/generated-test-sources/java/org/apache/hadoop/hive/ql/exec/vector/expressions/gen\nGenerating vector expression code\nGenerating vector expression test code\n[INFO] Executed tasks\n[INFO] \n[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-exec ---\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/ql/src/gen/protobuf/gen-java added.\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/ql/src/gen/thrift/gen-javabean added.\n[INFO] Source directory: /data/hive-ptest/working/apache-svn-trunk-source/ql/target/generated-sources/java added.\n[INFO] \n[INFO] --- antlr3-maven-plugin:3.4:antlr (default) @ hive-exec ---\n[INFO] ANTLR: Processing source directory /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java\nANTLR Parser Generator  Version 3.4\norg/apache/hadoop/hive/ql/parse/HiveLexer.g\norg/apache/hadoop/hive/ql/parse/HiveParser.g\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:872:5: \nDecision can match input such as \"Identifier KW_RENAME KW_TO\" using multiple alternatives: 1, 10\n\nAs a result, alternative(s) 10 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1177:5: \nDecision can match input such as \"KW_TEXTFILE\" using multiple alternatives: 2, 6\n\nAs a result, alternative(s) 6 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1177:5: \nDecision can match input such as \"KW_SEQUENCEFILE\" using multiple alternatives: 1, 6\n\nAs a result, alternative(s) 6 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1177:5: \nDecision can match input such as \"KW_ORCFILE\" using multiple alternatives: 4, 6\n\nAs a result, alternative(s) 6 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1177:5: \nDecision can match input such as \"KW_RCFILE\" using multiple alternatives: 3, 6\n\nAs a result, alternative(s) 6 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1190:23: \nDecision can match input such as \"KW_ELEM_TYPE\" using multiple alternatives: 1, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1190:23: \nDecision can match input such as \"KW_KEY_TYPE\" using multiple alternatives: 2, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1190:23: \nDecision can match input such as \"KW_VALUE_TYPE\" using multiple alternatives: 3, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1197:23: \nDecision can match input such as \"KW_ELEM_TYPE\" using multiple alternatives: 1, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1197:23: \nDecision can match input such as \"KW_VALUE_TYPE\" using multiple alternatives: 3, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1197:23: \nDecision can match input such as \"KW_KEY_TYPE\" using multiple alternatives: 2, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1215:29: \nDecision can match input such as \"KW_PRETTY {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE..KW_CASCADE, KW_CHANGE, KW_CLUSTER..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITIONED..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER..KW_UNARCHIVE, KW_UNDO..KW_UNIONTYPE, KW_UNLOCK..KW_VALUE_TYPE, KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 3, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1215:29: \nDecision can match input such as \"KW_FORMATTED {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE..KW_CASCADE, KW_CHANGE, KW_CLUSTER..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITIONED..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER..KW_UNARCHIVE, KW_UNDO..KW_UNIONTYPE, KW_UNLOCK..KW_VALUE_TYPE, KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1215:29: \nDecision can match input such as \"KW_PRETTY Identifier\" using multiple alternatives: 3, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1215:29: \nDecision can match input such as \"KW_FORMATTED Identifier\" using multiple alternatives: 1, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1215:29: \nDecision can match input such as \"KW_PRETTY KW_PARTITION\" using multiple alternatives: 3, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1215:29: \nDecision can match input such as \"KW_FORMATTED KW_PARTITION\" using multiple alternatives: 1, 4\n\nAs a result, alternative(s) 4 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1486:116: \nDecision can match input such as \"KW_STORED KW_AS KW_DIRECTORIES\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1609:5: \nDecision can match input such as \"KW_STORED KW_AS KW_RCFILE\" using multiple alternatives: 3, 7\n\nAs a result, alternative(s) 7 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1609:5: \nDecision can match input such as \"KW_STORED KW_AS KW_SEQUENCEFILE\" using multiple alternatives: 1, 7\n\nAs a result, alternative(s) 7 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1609:5: \nDecision can match input such as \"KW_STORED KW_AS KW_ORCFILE\" using multiple alternatives: 4, 7\n\nAs a result, alternative(s) 7 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1609:5: \nDecision can match input such as \"KW_STORED KW_AS KW_TEXTFILE\" using multiple alternatives: 2, 7\n\nAs a result, alternative(s) 7 were disabled for that input\nwarning(200): org/apache/hadoop/hive/ql/parse/HiveParser.g:1609:5: \nDecision can match input such as \"KW_STORED KW_AS KW_INPUTFORMAT\" using multiple alternatives: 5, 7\n\nAs a result, alternative(s) 7 were disabled for that input\nwarning(200): SelectClauseParser.g:149:5: \nDecision can match input such as \"KW_NULL DOT {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE..KW_CASCADE, KW_CHANGE, KW_CLUSTER..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER..KW_UNARCHIVE, KW_UNDO..KW_UNIONTYPE, KW_UNLOCK..KW_VALUE_TYPE, KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): SelectClauseParser.g:149:5: \nDecision can match input such as \"KW_NULL DOT Identifier\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:127:2: \nDecision can match input such as \"KW_LATERAL KW_VIEW KW_OUTER\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:25: \nDecision can match input such as \"LPAREN StringLiteral EQUAL\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:25: \nDecision can match input such as \"LPAREN StringLiteral COMMA\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:25: \nDecision can match input such as \"LPAREN StringLiteral RPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN BigintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_CAST\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_IF\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN CharSetName\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_EXISTS\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_CASE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN TinyintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_NULL\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN SmallintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_FALSE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_UNIONTYPE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN Number\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN Identifier\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN StringLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN DecimalLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE, KW_CLUSTER..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE, KW_EXPLAIN..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VALUE_TYPE, KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_STRUCT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_DATE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_NOT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_MAP\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_TRUE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:179:68: \nDecision can match input such as \"Identifier LPAREN KW_ARRAY\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN BigintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_CAST\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_IF\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN CharSetName\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_EXISTS\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_CASE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN TinyintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_NULL\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN SmallintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_FALSE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_UNIONTYPE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN Number\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN Identifier\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN StringLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN DecimalLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE, KW_CLUSTER..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE, KW_EXPLAIN..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VALUE_TYPE, KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_STRUCT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_DATE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_NOT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_MAP\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_TRUE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): FromClauseParser.g:237:16: \nDecision can match input such as \"Identifier LPAREN KW_ARRAY\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT CharSetName\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE CharSetName\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN CharSetName\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT Number\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_UNIONTYPE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL {KW_LIKE, KW_REGEXP, KW_RLIKE}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_DATE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_NOT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_NOT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_NOT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE Identifier\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL AMPERSAND\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN SmallintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN TinyintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN BigintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_FALSE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_STRUCT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_TRUE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_ARRAY\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN Number\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL KW_IS\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT StringLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_MAP\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_MAP\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_MAP\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN DecimalLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL RPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN StringLiteral StringLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_IF\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_TRUE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_IF\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_IF\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL BITWISEOR\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL DOT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_EXISTS\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE StringLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_FALSE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT Identifier\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL {DIV..DIVIDE, MOD, STAR}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE, KW_CLUSTER..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE, KW_EXPLAIN..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VALUE_TYPE, KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL NOTEQUAL\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_EXISTS\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL EQUAL_NS\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_EXISTS LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL LESSTHAN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL LESSTHANOREQUALTO\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_ARRAY\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_NULL\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_UNIONTYPE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL KW_OR\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_DATE StringLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_DATE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_STRUCT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL EQUAL\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CAST LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_STRUCT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_UNIONTYPE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL GREATERTHAN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_ARRAY\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_DATE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL GREATERTHANOREQUALTO\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_NULL\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_EXISTS\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE TinyintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL KW_BETWEEN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE SmallintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL {MINUS, PLUS}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL KW_NOT\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE BigintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL LSQUARE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE, KW_CLUSTER..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE, KW_EXPLAIN..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VALUE_TYPE, KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_FALSE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_NULL\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE, KW_CLUSTER..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE, KW_EXPLAIN..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VALUE_TYPE, KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_TRUE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE Number\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL BITWISEXOR\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT SmallintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN CharSetName CharSetLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_CAST\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_CAST\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_CAST\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT BigintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL KW_AND\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN KW_CASE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NULL KW_IN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT KW_CASE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_CASE\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT DecimalLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_NOT TinyintLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE KW_WHEN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN StringLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN LPAREN Identifier\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:68:4: \nDecision can match input such as \"LPAREN KW_CASE DecimalLiteral\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:108:5: \nDecision can match input such as \"KW_ORDER KW_BY LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:121:5: \nDecision can match input such as \"KW_CLUSTER KW_BY LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:133:5: \nDecision can match input such as \"KW_PARTITION KW_BY LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:144:5: \nDecision can match input such as \"KW_DISTRIBUTE KW_BY LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:155:5: \nDecision can match input such as \"KW_SORT KW_BY LPAREN\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:172:7: \nDecision can match input such as \"STAR\" using multiple alternatives: 1, 2\n\nAs a result, alternative(s) 2 were disabled for that input\nwarning(200): IdentifiersParser.g:185:5: \nDecision can match input such as \"KW_UNIONTYPE\" using multiple alternatives: 5, 6\n\nAs a result, alternative(s) 6 were disabled for that input\nwarning(200): IdentifiersParser.g:185:5: \nDecision can match input such as \"KW_STRUCT\" using multiple alternatives: 4, 6\n\nAs a result, alternative(s) 6 were disabled for that input\nwarning(200): IdentifiersParser.g:185:5: \nDecision can match input such as \"KW_ARRAY\" using multiple alternatives: 2, 6\n\nAs a result, alternative(s) 6 were disabled for that input\nwarning(200): IdentifiersParser.g:267:5: \nDecision can match input such as \"KW_TRUE\" using multiple alternatives: 3, 8\n\nAs a result, alternative(s) 8 were disabled for that input\nwarning(200): IdentifiersParser.g:267:5: \nDecision can match input such as \"KW_DATE StringLiteral\" using multiple alternatives: 2, 3\n\nAs a result, alternative(s) 3 were disabled for that input\nwarning(200): IdentifiersParser.g:267:5: \nDecision can match input such as \"KW_FALSE\" using multiple alternatives: 3, 8\n\nAs a result, alternative(s) 8 were disabled for that input\nwarning(200): IdentifiersParser.g:267:5: \nDecision can match input such as \"KW_NULL\" using multiple alternatives: 1, 8\n\nAs a result, alternative(s) 8 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_OVERWRITE\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_CLUSTER KW_BY\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_SORT KW_BY\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"KW_BETWEEN KW_MAP LPAREN\" using multiple alternatives: 8, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:399:5: \nDecision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_DISTRIBUTE KW_BY\" using multiple alternatives: 2, 9\n\nAs a result, alternative(s) 9 were disabled for that input\nwarning(200): IdentifiersParser.g:524:5: \nDecision can match input such as \"{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}\" using multiple alternatives: 1, 3\n\nAs a result, alternative(s) 3 were disabled for that input\n[INFO] \n[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-exec ---\n[debug] execute contextualize\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-exec ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-exec ---\n[INFO] Compiling 1387 source files to /data/hive-ptest/working/apache-svn-trunk-source/ql/target/classes\n[INFO] -------------------------------------------------------------\n[WARNING] COMPILATION WARNING : \n[INFO] -------------------------------------------------------------\n[WARNING] Note: Some input files use or override a deprecated API.\n[WARNING] Note: Recompile with -Xlint:deprecation for details.\n[WARNING] Note: Some input files use unchecked or unsafe operations.\n[WARNING] Note: Recompile with -Xlint:unchecked for details.\n[INFO] 4 warnings \n[INFO] -------------------------------------------------------------\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR : \n[INFO] -------------------------------------------------------------\n[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java:[1158,46] cannot find symbol\nsymbol  : variable partValASTChild\nlocation: class org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer\n[INFO] 1 error\n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Hive .............................................. SUCCESS [2.523s]\n[INFO] Hive Ant Utilities ................................ SUCCESS [7.754s]\n[INFO] Hive Shims Common ................................. SUCCESS [2.777s]\n[INFO] Hive Shims 0.20 ................................... SUCCESS [1.633s]\n[INFO] Hive Shims Secure Common .......................... SUCCESS [3.017s]\n[INFO] Hive Shims 0.20S .................................. SUCCESS [1.333s]\n[INFO] Hive Shims 0.23 ................................... SUCCESS [3.083s]\n[INFO] Hive Shims ........................................ SUCCESS [3.000s]\n[INFO] Hive Common ....................................... SUCCESS [4.608s]\n[INFO] Hive Serde ........................................ SUCCESS [11.256s]\n[INFO] Hive Metastore .................................... SUCCESS [25.883s]\n[INFO] Hive Query Language ............................... FAILURE [37.753s]\n[INFO] Hive Service ...................................... SKIPPED\n[INFO] Hive JDBC ......................................... SKIPPED\n[INFO] Hive Beeline ...................................... SKIPPED\n[INFO] Hive CLI .......................................... SKIPPED\n[INFO] Hive Contrib ...................................... SKIPPED\n[INFO] Hive HBase Handler ................................ SKIPPED\n[INFO] Hive HCatalog ..................................... SKIPPED\n[INFO] Hive HCatalog Core ................................ SKIPPED\n[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED\n[INFO] Hive HCatalog Server Extensions ................... SKIPPED\n[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED\n[INFO] Hive HCatalog Webhcat ............................. SKIPPED\n[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED\n[INFO] Hive HWI .......................................... SKIPPED\n[INFO] Hive ODBC ......................................... SKIPPED\n[INFO] Hive Shims Aggregator ............................. SKIPPED\n[INFO] Hive TestUtils .................................... SKIPPED\n[INFO] Hive Packaging .................................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 1:46.863s\n[INFO] Finished at: Thu Nov 14 15:17:28 EST 2013\n[INFO] Final Memory: 41M/504M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure\n[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java:[1158,46] cannot find symbol\n[ERROR] symbol  : variable partValASTChild\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-exec\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12613715","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-11-14T20:17:42.395+0000","updated":"2013-11-14T20:17:42.395+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13822973","id":"13822973","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"fix patch","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-14T21:55:13.758+0000","updated":"2013-11-14T21:55:13.758+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13824069","id":"13824069","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 no tests executed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12613966/HIVE-5686.03.patch\n\nTest results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/316/testReport\nConsole output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/316/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-316/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ svn = \\s\\v\\n ]]\n+ [[ -n '' ]]\n+ [[ -d apache-svn-trunk-source ]]\n+ [[ ! -d apache-svn-trunk-source/.svn ]]\n+ [[ ! -d apache-svn-trunk-source ]]\n+ cd apache-svn-trunk-source\n+ svn revert -R .\nReverted 'ql/src/test/results/clientnegative/notable_alias3.q.out'\nReverted 'ql/src/test/results/compiler/plan/groupby1.q.xml'\nReverted 'ql/src/test/results/compiler/plan/groupby5.q.xml'\nReverted 'ql/src/test/results/compiler/errors/nonkey_groupby.q.out'\nReverted 'ql/src/test/queries/clientnegative/notable_alias3.q'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/PTFTranslator.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java'\n++ awk '{print $2}'\n++ egrep -v '^X|^Performing status on external'\n++ svn status --no-ignore\n+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/assembly/target shims/0.20S/target shims/0.23/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/storage-handlers/hbase/target hcatalog/server-extensions/target hcatalog/core/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientpositive/notable_alias3.q.out ql/src/test/results/clientpositive/groupby_resolution.q.out ql/src/test/queries/clientpositive/notable_alias3.q ql/src/test/queries/clientpositive/groupby_resolution.q\n+ svn update\nU    pom.xml\n\nFetching external item into 'hcatalog/src/test/e2e/harness'\nUpdated external to revision 1542390.\n\nUpdated to revision 1542390.\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12613966","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-11-15T20:44:46.043+0000","updated":"2013-11-15T20:44:46.043+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13837302","id":"13837302","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:green}Overall{color}: +1 all checks pass\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12616676/HIVE-5686.04.patch\n\n{color:green}SUCCESS:{color} +1 4450 tests passed\n\nTest results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/494/testReport\nConsole output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/494/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12616676","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-12-03T03:59:13.330+0000","updated":"2013-12-03T03:59:13.330+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13837946","id":"13837946","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"also updated fb review","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-12-03T17:47:33.172+0000","updated":"2013-12-03T17:47:33.172+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13840829","id":"13840829","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"body":"+1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-12-06T02:12:04.762+0000","updated":"2013-12-06T02:12:04.762+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12676508/comment/13841328","id":"13841328","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"body":"Committed to trunk. Thanks, Sergey!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-12-06T15:00:18.919+0000","updated":"2013-12-06T15:00:18.919+0000"}],"maxResults":21,"total":21,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-5686/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1pd27:"}}