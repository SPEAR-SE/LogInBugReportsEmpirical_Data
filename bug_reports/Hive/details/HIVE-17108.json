{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13087504","self":"https://issues.apache.org/jira/rest/api/2/issue/13087504","key":"HIVE-17108","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":null,"customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jul 20 03:36:31 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17108/watchers","watchCount":5,"isWatching":false},"created":"2017-07-17T08:30:08.697+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-20T05:16:35.470+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"in [parquet_analyze.q|https://github.com/apache/hive/blob/master/ql/src/test/queries/clientpositive/parquet_analyze.q#L27], we need run \"ANALYZE TABLE parquet_create_people COMPUTE STATISTICS noscan\" to update the statistic. \n\nIn [orc_analyze.q|https://github.com/apache/hive/blob/master/ql/src/test/queries/clientpositive/orc_analyze.q#L45], we need not do that if we set hive.stats.autogather as true.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12878095","id":"12878095","filename":"HIVE-17018.wip.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-20T05:16:33.211+0000","size":2653,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12878095/HIVE-17018.wip.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Parquet file does not gather statistic such as \"RAW DATA SIZE\" automatically ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13087504/comment/16089491","id":"16089491","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~csun] or [~xuefuz]: can you help to view it, thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-17T08:32:12.872+0000","updated":"2017-07-17T08:32:12.872+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13087504/comment/16090965","id":"16090965","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~csun], [~xuefuz]:  If we must use \"ANALYZE TABLE parquet_create_people COMPUTE STATISTICS noscan\" to get the statistics such as \"RAW DATA SIZE\",  we need update parquet*q to add \"analyze table xxxx\"  such as [parquet_join.q|https://github.com/apache/hive/blob/master/ql/src/test/queries/clientpositive/parquet_join.q].\nlet's use part code of parquet_join.q to explain:\nafter use \"analyze table parquet_jointable1 compute statistics nocan\", the raw data size is changed from 4 to 108.\n{code}\nset hive.mapred.mode=nonstrict;\n\ndrop table if exists staging;\ndrop table if exists parquet_jointable1;\ndrop table if exists parquet_jointable2;\n\ncreate table staging (key int, value string) stored as textfile;\ninsert into table staging select distinct key, value from src order by key limit 2;\n\ncreate table parquet_jointable1 stored as parquet as select * from staging;\ncreate table parquet_jointable2 stored as parquet as select key,key+1,concat(value,\"value\") as myvalue from staging;\n\n\n-- MR join\ndescribe formatted parquet_jointable1;\nexplain select p2.myvalue from parquet_jointable1 p1 join parquet_jointable2 p2 on p1.key=p2.key;\n--update the statistics of parquet_jointable1\nanalyze table parquet_jointable1 COMPUTE STATISTICS noscan;\ndescribe formatted parquet_jointable1;\n--now the datasize of parquet_jointable1 changes from 4 to 108\nexplain select p2.myvalue from parquet_jointable1 p1 join parquet_jointable2 p2 on p1.key=p2.key;\n{code}\n\nthe output of the script\n{code}\n# Detailed Table Information\t \t \nDatabase:           \tdefault             \t \nOwner:              \troot                \t \nCreateTime:         \tMon Jul 17 21:34:52 EDT 2017\t \nLastAccessTime:     \tUNKNOWN             \t \nRetention:          \t0                   \t \nLocation:           \thdfs://bdpe42:8020/user/hive/warehouse/parquet_jointable1\t \nTable Type:         \tMANAGED_TABLE       \t \nTable Parameters:\t \t \n\tCOLUMN_STATS_ACCURATE\t{\\\"BASIC_STATS\\\":\\\"true\\\"}\n\tnumFiles            \t1                   \n\tnumRows             \t2                   \n\trawDataSize         \t4                   \n\ttotalSize           \t345                 \n\ttransient_lastDdlTime\t1500341692          \n\t \t \n# Storage Information\t \t \nSerDe Library:      \torg.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\t \nInputFormat:        \torg.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\t \nOutputFormat:       \torg.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\t \nCompressed:         \tNo                  \t \nNum Buckets:        \t-1                  \t \nBucket Columns:     \t[]                  \t \nSort Columns:       \t[]                  \t \nStorage Desc Params:\t \t \n\tserialization.format\t1                   \nTime taken: 0.202 seconds, Fetched: 31 row(s)\nOK\nSTAGE DEPENDENCIES:\n  Stage-2 is a root stage\n  Stage-1 depends on stages: Stage-2\n  Stage-0 depends on stages: Stage-1\n\nSTAGE PLANS:\n  Stage: Stage-2\n    Spark\n      DagName: root_20170717213454_eadfaac1-9d9d-4bc0-bb65-cb4beef4505a:5\n      Vertices:\n        Map 1 \n            Map Operator Tree:\n                TableScan\n                  alias: p1\n                  Statistics: Num rows: 2 Data size: 4 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: key is not null (type: boolean)\n                    Statistics: Num rows: 2 Data size: 4 Basic stats: COMPLETE Column stats: NONE\n                    Spark HashTable Sink Operator\n                      keys:\n                        0 key (type: int)\n                        1 key (type: int)\n            Local Work:\n              Map Reduce Local Work\n\n  Stage: Stage-1\n    Spark\n      DagName: root_20170717213454_eadfaac1-9d9d-4bc0-bb65-cb4beef4505a:4\n      Vertices:\n        Map 2 \n            Map Operator Tree:\n                TableScan\n                  alias: p2\n                  Statistics: Num rows: 2 Data size: 6 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: key is not null (type: boolean)\n                    Statistics: Num rows: 2 Data size: 6 Basic stats: COMPLETE Column stats: NONE\n                    Map Join Operator\n                      condition map:\n                           Inner Join 0 to 1\n                      keys:\n                        0 key (type: int)\n                        1 key (type: int)\n                      outputColumnNames: _col7\n                      input vertices:\n                        0 Map 1\n                      Statistics: Num rows: 2 Data size: 4 Basic stats: COMPLETE Column stats: NONE\n                      Select Operator\n                        expressions: _col7 (type: string)\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 2 Data size: 4 Basic stats: COMPLETE Column stats: NONE\n                        File Output Operator\n                          compressed: false\n                          Statistics: Num rows: 2 Data size: 4 Basic stats: COMPLETE Column stats: NONE\n                          table:\n                              input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n            Local Work:\n              Map Reduce Local Work\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n      Processor Tree:\n        ListSink\n\nTime taken: 0.314 seconds, Fetched: 67 row(s)\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\nTable default.parquet_jointable1 stats: [numFiles=1, numRows=2, totalSize=345, rawDataSize=108]\nOK\nTime taken: 0.394 seconds\nOK\n# col_name            \tdata_type           \tcomment             \n\t \t \nkey                 \tint                 \t                    \nvalue               \tstring              \t                    \n\t \t \n# Detailed Table Information\t \t \nDatabase:           \tdefault             \t \nOwner:              \troot                \t \nCreateTime:         \tMon Jul 17 21:34:52 EDT 2017\t \nLastAccessTime:     \tUNKNOWN             \t \nRetention:          \t0                   \t \nLocation:           \thdfs://bdpe42:8020/user/hive/warehouse/parquet_jointable1\t \nTable Type:         \tMANAGED_TABLE       \t \nTable Parameters:\t \t \n\tCOLUMN_STATS_ACCURATE\t{\\\"BASIC_STATS\\\":\\\"true\\\"}\n\tnumFiles            \t1                   \n\tnumRows             \t2                   \n\trawDataSize         \t108                 \n\ttotalSize           \t345                 \n\ttransient_lastDdlTime\t1500341695          \n\t \t \n# Storage Information\t \t \nSerDe Library:      \torg.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\t \nInputFormat:        \torg.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\t \nOutputFormat:       \torg.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\t \nCompressed:         \tNo                  \t \nNum Buckets:        \t-1                  \t \nBucket Columns:     \t[]                  \t \nSort Columns:       \t[]                  \t \nStorage Desc Params:\t \t \n\tserialization.format\t1                   \nTime taken: 0.173 seconds, Fetched: 31 row(s)\nOK\nSTAGE DEPENDENCIES:\n  Stage-2 is a root stage\n  Stage-1 depends on stages: Stage-2\n  Stage-0 depends on stages: Stage-1\n\nSTAGE PLANS:\n  Stage: Stage-2\n    Spark\n      DagName: root_20170717213455_bdd3a4bd-9642-46a2-b95f-4ab6fbd322f0:8\n      Vertices:\n        Map 2 \n            Map Operator Tree:\n                TableScan\n                  alias: p2\n                  Statistics: Num rows: 2 Data size: 6 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: key is not null (type: boolean)\n                    Statistics: Num rows: 2 Data size: 6 Basic stats: COMPLETE Column stats: NONE\n                    Spark HashTable Sink Operator\n                      keys:\n                        0 key (type: int)\n                        1 key (type: int)\n            Local Work:\n              Map Reduce Local Work\n\n  Stage: Stage-1\n    Spark\n      DagName: root_20170717213455_bdd3a4bd-9642-46a2-b95f-4ab6fbd322f0:7\n      Vertices:\n        Map 1 \n            Map Operator Tree:\n                TableScan\n                  alias: p1\n                  Statistics: Num rows: 2 Data size: 108 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: key is not null (type: boolean)\n                    Statistics: Num rows: 2 Data size: 108 Basic stats: COMPLETE Column stats: NONE\n                    Map Join Operator\n                      condition map:\n                           Inner Join 0 to 1\n                      keys:\n                        0 key (type: int)\n                        1 key (type: int)\n                      outputColumnNames: _col7\n                      input vertices:\n                        1 Map 2\n                      Statistics: Num rows: 2 Data size: 118 Basic stats: COMPLETE Column stats: NONE\n                      Select Operator\n                        expressions: _col7 (type: string)\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 2 Data size: 118 Basic stats: COMPLETE Column stats: NONE\n                        File Output Operator\n                          compressed: false\n                          Statistics: Num rows: 2 Data size: 118 Basic stats: COMPLETE Column stats: NONE\n                          table:\n                              input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n            Local Work:\n              Map Reduce Local Work\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n      Processor Tree:\n        ListSink\n{code}\n\nIf we do not use \"analyze table xxx\" to update the statistic like \"raw data size\", the statistic is wrong which will cause the parallelism invalid in [SetSparkReducerParallelism#numberOfBytes|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java#L147]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-18T01:52:29.182+0000","updated":"2017-07-18T01:52:29.182+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13087504/comment/16091284","id":"16091284","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~pxiong]: when I view the code about [orc_analyze.q https://github.com/apache/hive/blob/master/ql/src/test/queries/clientpositive/orc_analyze.q#L45], it seems that orc will automatically gather statics like \"RAW DATA SIZE\" without executing \"analyze table compute statistics\". But reading the code, whether orc or parquet, it calls [org.apache.hadoop.hive.ql.exec.StatsNoJobTask#aggregateStats|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java#L266] to update the statistics. The query {{INSERT OVERWRITE TABLE orc_create_people SELECT * FROM orc_create_people_staging ORDER BY id}} will only call StatsJobTask but not calls StatsNoJobTask. So why orc updates statistics automatically?\n\nLet's use an example to explain more detailed\n{noformat}\nset hive.execution.engine=mr;\nuse default;\ndrop table if exists orc_create_people_staging;\ndrop table if exists orc_create_people;\n\nCREATE TABLE orc_create_people_staging (\n  id int,\n  first_name string,\n  last_name string,\n  address string,\n  salary decimal,\n  start_date timestamp,\n  state string);\n\nLOAD DATA LOCAL INPATH './orc_create_people.txt' OVERWRITE INTO TABLE orc_create_people_staging;\n\nCREATE TABLE orc_create_people (\n  id int,\n  first_name string,\n  last_name string,\n  address string,\n  salary decimal,\n  start_date timestamp,\n  state string)\nSTORED AS orc;\n\nexplain extended INSERT OVERWRITE TABLE orc_create_people SELECT * FROM orc_create_people_staging ORDER BY id;\nINSERT OVERWRITE TABLE orc_create_people SELECT * FROM orc_create_people_staging ORDER BY id;\ndesc formatted orc_create_people;\n\n{noformat}\n\n\nthe result of \"desc formatted orc_create_people\" is following , the rawDataSize is correct(value 336)\n{code}\n# col_name            \tdata_type           \tcomment             \n\t \t \nid                  \tint                 \t                    \nfirst_name          \tstring              \t                    \nlast_name           \tstring              \t                    \naddress             \tstring              \t                    \nsalary              \tdecimal(10,0)       \t                    \nstart_date          \ttimestamp           \t                    \nstate               \tstring              \t                    \n\t \t \n# Detailed Table Information\t \t \nDatabase:           \tdefault             \t \nOwner:              \troot                \t \nCreateTime:         \tTue Jul 18 04:29:16 EDT 2017\t \nLastAccessTime:     \tUNKNOWN             \t \nRetention:          \t0                   \t \nLocation:           \thdfs://bdpe42:8020/user/hive/warehouse/orc_create_people\t \nTable Type:         \tMANAGED_TABLE       \t \nTable Parameters:\t \t \n\tCOLUMN_STATS_ACCURATE\t{\\\"BASIC_STATS\\\":\\\"true\\\"}\n\tnumFiles            \t1                   \n\tnumRows             \t3                   \n\trawDataSize         \t336                 \n\ttotalSize           \t537                 \n\ttransient_lastDdlTime\t1500366574          \n\t \t \n# Storage Information\t \t \nSerDe Library:      \torg.apache.hadoop.hive.ql.io.orc.OrcSerde\t \nInputFormat:        \torg.apache.hadoop.hive.ql.io.orc.OrcInputFormat\t \nOutputFormat:       \torg.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\t \nCompressed:         \tNo                  \t \nNum Buckets:        \t-1                  \t \nBucket Columns:     \t[]                  \t \nSort Columns:       \t[]                  \t \nStorage Desc Params:\t \t \n\tserialization.format\t\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-18T08:35:07.278+0000","updated":"2017-07-18T08:35:07.278+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13087504/comment/16094128","id":"16094128","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"the detail reason why parquet file does not gather statistic such as \"RAW DATA SIZE\" automatically:\nwhen executing \"INSERT OVERWRITE TABLE xxx SELECT * xxx\",\nhive with orc will update statistics from orc footer in [FileSinkOperator#closeOp|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L1060] while hive with parquet will not. \nOrcRecordWriter implements StatsProvidingRecordWriter.\nParquetRecordWriterWrapper not implements StatsProvidingRecordWriter.\n\nBut i guess even ParquetRecordWriterWrapper implements [StatsProvidingRecordWriter|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/StatsProvidingRecordWriter.java], statistics like \"RAW DATA SIZE\" can not be updated because org.apache.parquet.hadoop.ParquetWriter does not provide interface like getRawDataSize() or getRawCount().","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-20T03:36:31.586+0000","updated":"2017-07-20T03:36:31.586+0000"}],"maxResults":4,"total":4,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17108/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3hl27:"}}