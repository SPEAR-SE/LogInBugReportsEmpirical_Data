{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12940629","self":"https://issues.apache.org/jira/rest/api/2/issue/12940629","key":"HIVE-13098","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-09-28T04:54:28.186+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Sep 30 19:46:46 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-13098/watchers","watchCount":8,"isWatching":false},"created":"2016-02-19T20:22:38.367+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-10-04T01:19:27.979+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"When e.g. 999999 is selected as decimal(5,0), the result is null. This can be problematic, esp. if the data is written to a table and lost without the user realizing it. There should be an option to error out in such cases instead; it should probably be on by default and the error message should instruct the user on how to disable it.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12830622","id":"12830622","filename":"HIVE-13098.WIP.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-28T02:58:30.556+0000","size":475911,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12830622/HIVE-13098.WIP.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12830837","id":"12830837","filename":"HIVE-13098.WIP2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-29T02:32:36.726+0000","size":605728,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12830837/HIVE-13098.WIP2.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Add a strict check for when the decimal gets converted to null due to insufficient width","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15317581","id":"15317581","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"This is especially problematic for implicit conversions...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-07T00:48:33.120+0000","updated":"2016-06-07T00:48:33.120+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15528204","id":"15528204","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"The epic WIP patch... still need to take care of some paths.\n\nPropagating config to all decimals is difficult thanks to massive static use in Hive...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-28T02:58:30.569+0000","updated":"2016-09-28T02:58:30.569+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15528392","id":"15528392","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12830622/HIVE-13098.WIP.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1325/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1325/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1325/\n\nMessages:\n{noformat}\n**** This message was trimmed, see log for full details ****\n     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/custom-udfs/udf-vectorized-badexample/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ udf-vectorized-badexample ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ udf-vectorized-badexample ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ udf-vectorized-badexample ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/custom-udfs/udf-vectorized-badexample/target/udf-vectorized-badexample-2.2.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ udf-vectorized-badexample ---\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ udf-vectorized-badexample ---\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/custom-udfs/udf-vectorized-badexample/target/udf-vectorized-badexample-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-custom-udfs/udf-vectorized-badexample/2.2.0-SNAPSHOT/udf-vectorized-badexample-2.2.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/custom-udfs/udf-vectorized-badexample/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-custom-udfs/udf-vectorized-badexample/2.2.0-SNAPSHOT/udf-vectorized-badexample-2.2.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Integration - HCatalog Unit Tests 2.2.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hcatalog-it-unit ---\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-hcatalog-it-unit ---\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-it-unit ---\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-it-unit ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/main/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf\n     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---\n[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/test-classes\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-hcatalog-it-unit ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-hcatalog-it-unit ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:test-jar (default) @ hive-hcatalog-it-unit ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.pom\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Integration - Testing Utilities 2.2.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util/target\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-util ---\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-util ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---\n[INFO] Compiling 66 source files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/classes\n[INFO] -------------------------------------------------------------\n[WARNING] COMPILATION WARNING : \n[INFO] -------------------------------------------------------------\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Recompile with -Xlint:deprecation for details.\n[WARNING] Some messages have been simplified; recompile with -Xdiags:verbose to get full output\n[INFO] 3 warnings \n[INFO] -------------------------------------------------------------\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR : \n[INFO] -------------------------------------------------------------\n[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,60] no suitable method found for create(java.lang.String)\n    method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable\n      (argument mismatch; java.lang.String cannot be converted to int)\n    method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable\n      (argument mismatch; java.lang.String cannot be converted to long)\n[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,91] no suitable method found for create(java.lang.String)\n    method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable\n      (argument mismatch; java.lang.String cannot be converted to int)\n    method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable\n      (argument mismatch; java.lang.String cannot be converted to long)\n[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,122] no suitable method found for create(java.lang.String)\n    method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable\n      (argument mismatch; java.lang.String cannot be converted to int)\n    method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable\n      (argument mismatch; java.lang.String cannot be converted to long)\n[INFO] 3 errors \n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Hive Integration - Parent ......................... SUCCESS [1.672s]\n[INFO] Hive Integration - Custom Serde ................... SUCCESS [3.930s]\n[INFO] Hive Integration - Custom udfs .................... SUCCESS [0.921s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-util  SUCCESS [0.796s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-udf1  SUCCESS [0.864s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-udf2  SUCCESS [0.801s]\n[INFO] Hive Integration - Custom UDFs - udf-vectorized-badexample  SUCCESS [0.717s]\n[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [4.335s]\n[INFO] Hive Integration - Testing Utilities .............. FAILURE [4.740s]\n[INFO] Hive Integration - Unit Tests ..................... SKIPPED\n[INFO] Hive Integration - Test Serde ..................... SKIPPED\n[INFO] Hive Integration - QFile Tests .................... SKIPPED\n[INFO] Hive Integration - QFile Accumulo Tests ........... SKIPPED\n[INFO] JMH benchmark: Hive ............................... SKIPPED\n[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED\n[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED\n[INFO] Hive Integration - QFile Spark Tests .............. SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 19.516s\n[INFO] Finished at: Wed Sep 28 04:54:48 UTC 2016\n[INFO] Final Memory: 87M/716M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-it-util: Compilation failure: Compilation failure:\n[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,60] no suitable method found for create(java.lang.String)\n[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable\n[ERROR] (argument mismatch; java.lang.String cannot be converted to int)\n[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable\n[ERROR] (argument mismatch; java.lang.String cannot be converted to long)\n[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,91] no suitable method found for create(java.lang.String)\n[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable\n[ERROR] (argument mismatch; java.lang.String cannot be converted to int)\n[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable\n[ERROR] (argument mismatch; java.lang.String cannot be converted to long)\n[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,122] no suitable method found for create(java.lang.String)\n[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable\n[ERROR] (argument mismatch; java.lang.String cannot be converted to int)\n[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable\n[ERROR] (argument mismatch; java.lang.String cannot be converted to long)\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-it-util\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12830622 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-28T04:54:28.186+0000","updated":"2016-09-28T04:54:28.186+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15531581","id":"15531581","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"This patch handles many more paths.\nI think it's close to being ready to review/commit..\nThe main question is how to handle OIs, cause OI static giant global methods are just too general. I almost wonder if I should give up and add a threadlocal just for that case in the static OI class that would read config once per thread... however, that will make the config global for that case. We can document that config is HS2-wide and cannot be changed per query and do a followup.\nAs for write paths, these would need to be handled on case by case basis cause their initialization is obscure. I started on the ORC write path, but put the config in the wrong place for master (it's the right place for branch-1). I will finish that.\n\n[~jdere] [~ashutoshc] do you want to take a look? 95% of the changes are just propagating the stupid config. Main changes are in HiveDecimal and HiveDecimalOverflow classes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-29T02:32:36.732+0000","updated":"2016-09-29T02:32:36.732+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15533799","id":"15533799","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12830837/HIVE-13098.WIP2.patch\n\n{color:green}SUCCESS:{color} +1 due to 57 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 97 failed/errored test(s), 10645 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[annotate_stats_select]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_1]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_2]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_5]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_precision]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_skewjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_stats]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_ppd_decimal]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_ppd_decimal]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_format_number]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_greatest]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_least]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_to_byte]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_to_long]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_to_short]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_aggregate_9]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_between_in]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_cast_constant]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_1]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_2]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_3]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_aggregate]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_precision]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_udf]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_struct_in]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_0]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_13]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_17]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_short_regress]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[tez_union_decimal]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[tez_vector_dynpart_hashjoin_1]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_aggregate_9]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_between_in]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_cast_constant]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_char_mapjoin1]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_2]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_3]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_aggregate]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_precision]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_udf]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_inner_join]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_interval_mapjoin]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_join_filters]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_left_outer_join2]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_left_outer_join]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_leftsemi_mapjoin]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_mapjoin_reduce]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join0]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join1]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join2]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join3]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join4]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join5]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join6]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_varchar_mapjoin1]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorization_0]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorization_13]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorization_17]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorization_short_regress]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorized_context]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorized_mapjoin]\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorized_nested_mapjoin]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_inner_join]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join0]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join1]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join2]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join3]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join4]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join5]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query21]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_between_in]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_cast_constant]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_decimal_aggregate]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_left_outer_join]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_0]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_13]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_17]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_short_regress]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_mapjoin]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_nested_mapjoin]\norg.apache.hadoop.hive.common.type.TestHiveDecimal.testMultiply\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgDecimal\norg.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgDecimalNegative\norg.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumDecimal\norg.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumDecimalHive6508\norg.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.testVectorSerDeRow\norg.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastTimestampToDecimal\norg.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testByteDivideShort\norg.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDecimalDivideDecimal\norg.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testLongDivideDecimal\norg.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPrintf.testDecimalArgs\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1345/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1345/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1345/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 97 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12830837 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-29T19:29:27.465+0000","updated":"2016-09-29T19:29:27.465+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15534285","id":"15534285","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"Upon looking further on OI path I don't think it's possible to propagate it there without major changes, in fact OI-related parts of this patch are not valid, since OIs are assumed to be stateless and are cached process-wide, ditto for TypeInfo-s. There are lots of static method paths accessing those...\nI think I might scrape a lot of the patch and add a globally accessible static that would have to be initialize on CLI/HS2/task startup.. The only exception would be write path that happens outside of Hive services... \n\nThis will reduce size of the patch a lot (but also make it a global setting not modifiable per query...)\n\nUpdate: another alternative would be a (TADA!) threadlocal.\nWe could set it at compile time and change the patch to have only compile paths use it, whereas runtime paths would use the fields in OIs and fns that compile populates. As much as I hate threadlocals, I think that's the best approach as it will make patch smaller (right now 700kb of code changes is not even everything, OI changes would be massive), also allow one to set it per query and remove the requirement to initialize it for everyone using Hive libs, since APIs would not use it beyond compilation.\n\n[~ashutoshc] [~hagleitn] [~jdere] opinions?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-29T22:33:27.691+0000","updated":"2016-09-29T22:49:18.831+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15534325","id":"15534325","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"I will stop working on this for now cause it;s a giant annoying time sink. If there are no objections to threadlocal I will go with that.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-29T22:50:04.757+0000","updated":"2016-09-29T22:50:04.757+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15535331","id":"15535331","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"body":"What if we added some function(s) to help people explore their data?\n\nWhat about a function that takes a column value or expression and a target data type and reports on how that conversion would go.\n\nFor example, for string to int, it could report:\n   string doesn't parse to a number,\n   string has decimal digits that would be thrown away,\n   number parses but would overflow an int\n\nFor string to decimal, it could report:\n   (parse errors)\n   Integer digits will not fit in decimal precision\n   Decimal digits would require rounding given the precision.\n\nWe could even go further and have function(s) that examine a string column and speculate on good possible data types that would be appropriate for a conversion.\n\nWe could borrow ideas from the schema discovery folks (Drill?).\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"created":"2016-09-30T07:44:43.133+0000","updated":"2016-09-30T07:44:43.133+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15535333","id":"15535333","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"body":"[~hagleitn] Perhaps a different approach.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"created":"2016-09-30T07:46:05.184+0000","updated":"2016-09-30T07:46:05.184+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15536662","id":"15536662","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"[~mmccline] the main concern here is automated nulls, where people get them after they import a large amount of data. If the ETL runs every day they cannot be expected to look at the data every day (arguably it should be cleaned up by someone else before Hive in this case, but people make mistakes and there are bugs in other code...)\n\nOne way to handle this for most cases would be to break the existing behavior to always throw, and add a separate UDF (\"trycast\"?) for people who don't care. However that would only work in queries, not for automated pipelines/writers.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-30T18:29:36.773+0000","updated":"2016-09-30T18:29:36.773+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15536749","id":"15536749","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"body":"There are other industry solutions.  E.g. Greenplum added an ERROR TABLE feature a long time ago for saving rejected rows so they could be cleaned and add later.  Also, see Teradata.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"created":"2016-09-30T18:59:38.740+0000","updated":"2016-09-30T18:59:38.740+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15536835","id":"15536835","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"Well, the crux of the matter is, whatever solution we do in Hive would be super unwieldy code-wise, because decimals, decimal OIs, etc. are created in 1000000 places in giant static methods. I was going to add a global for compilation (thread-local since that is single threaded) to be able to populate it  everywhere. At runtime (or during import) it can be used from the fields in runtime objects (see DecimalUdf interface in the patch and its usage). That would reduce the impact a lot compared to 700Kb of code changes...\nThen we can choose what to do with the config once all the requisite code has it.\nThe question is whether to do it at all, esp. since as Gopal noted other types are also converted to null on overflow (unless they are bugged like decimal-to-int cast ;)), so the proper fix that covers all the cases uniformly would be very um, impactful in the codebase.\n\nIt is much easier to make the solution that requires user to actively participate (e.g. trycast or functions to explore data), but it doesn't cover the main case of the automated nullification.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-30T19:37:21.555+0000","updated":"2016-09-30T19:41:05.213+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15536856","id":"15536856","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. However that would only work in queries, not for automated pipelines/writers.\n\nThere's already a config for this problem for SQOOP, right? {{sqoop.bigdecimal.format.string}}?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-09-30T19:43:10.285+0000","updated":"2016-09-30T19:43:10.285+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12940629/comment/15536866","id":"15536866","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"Not sure how that config is relevant? Different columns could have different widths, etc. Sqoop could have a config that would ensure that data fits in each column, but I don't think it does.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-30T19:46:46.478+0000","updated":"2016-09-30T19:46:46.478+0000"}],"maxResults":14,"total":14,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-13098/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2t31r:"}}