{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13035930","self":"https://issues.apache.org/jira/rest/api/2/issue/13035930","key":"HIVE-15658","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-01-20T21:53:41.929+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sat Jan 21 06:01:37 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15658/watchers","watchCount":6,"isWatching":false},"created":"2017-01-18T17:01:02.150+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329363","id":"12329363","name":"1.1.0","archived":false,"released":true,"releaseDate":"2015-03-07"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12332384","id":"12332384","name":"1.2.1","archived":false,"released":true,"releaseDate":"2015-06-26"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12332641","id":"12332641","description":"Hive 2.0.0","name":"2.0.0","archived":false,"released":true,"releaseDate":"2016-02-15"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12334886","id":"12334886","name":"2.0.1","archived":false,"released":true,"releaseDate":"2016-05-25"}],"issuelinks":[{"id":"12528090","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12528090","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13141044","key":"HIVE-18808","self":"https://issues.apache.org/jira/rest/api/2/issue/13141044","fields":{"summary":"Make compaction more robust when stats update fails","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-02-26T21:10:26.977+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12324726","id":"12324726","name":"API"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12320409","id":"12320409","name":"HCatalog","description":"Tracks issues related to the HCatalog"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12322671","id":"12322671","name":"Transactions","description":"Transaction management and ACID"}],"timeoriginalestimate":null,"description":"Method start() in hive.ql.session.SessionState is supposed to setup needed preconditions, like HDFS scratch directories for session.\nThis happens to be not an atomic operation with setting thread local variable, which can later be obtained by calling SessionState.get().\nTherefore, even is the start() method itself fails, the SessionState.get() does not return null and further re-use of the thread which previously invoked start() may lead to obtaining SessionState object in inconsistent state.\n\nI have observed this using Flume Hive Sink, which uses Hive Streaming interface. When the directory /tmp/hive is not writable by session user, the start() method fails (throwing RuntimeException). If the thread is re-used (like it is in Flume), further executions work with wrongly initialized SessionState object (HDFS dirs are non-existent). In Flume, this happens to me when Flume should create partition if not exists (but the code doing this is in Hive Streaming).\n\nSteps to reproduce:\n0. create test spooldir and allow flume to write to it, in my case /home/ubuntu/flume_test, 775, ubuntu:flume\n1. create Flume config (see attachment)\n2. create Hive table\n{code}\ncreate table default.flume_test (column1 string, column2 string) partitioned by (dt string) clustered by (column1) INTO 2 BUCKETS STORED AS ORC;\n{code}\n3. start flume agent:\n{code}\nbin/flume-ng agent -n a1 -c conf -f conf/flume-config.txt\n{code}\n4. hdfs dfs -chmod 600 /tmp/hive\n5. put this file into spooldir:\n{code}\necho value1,value2 > file1\n{code}\n\nExpected behavior:\nException regarding scratch dir permissions to be thrown repeatedly.\nexample (note that the line numbers are wrong as Cloudera is cloning the source codes here https://github.com/cloudera/flume-ng/ and here https://github.com/cloudera/hive):\n{code}\n2017-01-18 12:39:38,926 WARN org.apache.flume.sink.hive.HiveSink: sink_hive_1 : Failed connecting to EndPoint {metaStoreUri='thrift://n02.cdh.ideata:9083', database='default', table='flume_test', partitionVals=[20170118] }\norg.apache.flume.sink.hive.HiveWriter$ConnectException: Failed connecting to EndPoint {metaStoreUri='thrift://n02.cdh.ideata:9083', database='default', table='flume_test', partitionVals=[20170118] } \n        at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:99)\n        at org.apache.flume.sink.hive.HiveSink.getOrCreateWriter(HiveSink.java:344)\n        at org.apache.flume.sink.hive.HiveSink.drainOneBatch(HiveSink.java:296)\n        at org.apache.flume.sink.hive.HiveSink.process(HiveSink.java:254)\n        at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)\n        at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.flume.sink.hive.HiveWriter$ConnectException: Failed connecting to EndPoint {metaStoreUri='thrift://n02.cdh.ideata:9083', database='default', table='flume_test', partitionVals=[20170118] }\n        at org.apache.flume.sink.hive.HiveWriter.newConnection(HiveWriter.java:380)\n        at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:86)\n        ... 6 more\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rw-------\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:540)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.createPartitionIfNotExists(HiveEndPoint.java:358)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:276)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:243)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(HiveEndPoint.java:180)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:157)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:110)\n        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:376)\n        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:373)\n        at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:425)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        ... 1 more\nCaused by: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rw-------\n        at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:625)\n        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:574)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:518)\n        ... 13 more\n{code}\n\nActual behavior:\nException regarding scratch dir permissions thrown once, meaningless exceptions from code, which should be unreachable, are re-thrown again and again, obfuscating the\nsource of the problem to the user.\nexceptions thrown repeatedly:\n{code}\njava.lang.NullPointerException: Non-local session path expected to be non-null\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:204)\n        at org.apache.hadoop.hive.ql.session.SessionState.getHDFSSessionPath(SessionState.java:686)\n        at org.apache.hadoop.hive.ql.Context.<init>(Context.java:131)\n        at org.apache.hadoop.hive.ql.Context.<init>(Context.java:118)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:411)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.runDDL(HiveEndPoint.java:404)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.createPartitionIfNotExists(HiveEndPoint.java:369)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:276)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:243)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(HiveEndPoint.java:180)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:157)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:110)\n        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:376)\n        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:373)\n        at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:425)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n{code}\n{code}\n2017-01-18 12:39:44,453 WARN org.apache.flume.sink.hive.HiveSink: sink_hive_1 : Failed connecting to EndPoint {metaStoreUri='thrift://n02.cdh.ideata:9083', database='default', table='flume_test', partitionVals=[20170118] }\norg.apache.flume.sink.hive.HiveWriter$ConnectException: Failed connecting to EndPoint {metaStoreUri='thrift://n02.cdh.ideata:9083', database='default', table='flume_test', partitionVals=[20170118] }\n        at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:99)\n        at org.apache.flume.sink.hive.HiveSink.getOrCreateWriter(HiveSink.java:344)\n        at org.apache.flume.sink.hive.HiveSink.drainOneBatch(HiveSink.java:296)\n        at org.apache.flume.sink.hive.HiveSink.process(HiveSink.java:254)\n        at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)\n        at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hive.hcatalog.streaming.StreamingException: partition values=[20170118]. Unable to get path for end point: [20170118]\n        at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.getPathForEndPoint(AbstractRecordWriter.java:162)\n        at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.<init>(AbstractRecordWriter.java:66)\n        at org.apache.hive.hcatalog.streaming.DelimitedInputWriter.<init>(DelimitedInputWriter.java:115)\n        at org.apache.flume.sink.hive.HiveDelimitedTextSerializer.createRecordWriter(HiveDelimitedTextSerializer.java:67)\n        at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:89)\n        ... 6 more\nCaused by: NoSuchObjectException(message:partition values=[20170118])\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result$get_partition_resultStandardScheme.read(ThriftHiveMetastore.java:60283)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result$get_partition_resultStandardScheme.read(ThriftHiveMetastore.java:60251)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result.read(ThriftHiveMetastore.java:60182)\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_partition(ThriftHiveMetastore.java:1892)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_partition(ThriftHiveMetastore.java:1877)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartition(HiveMetaStoreClient.java:1171)\n        at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.getPathForEndPoint(AbstractRecordWriter.java:157)\n        ... 10 more\n{code}\n\nDetailed description on whats going on:\nFlume, as the Hive Streaming client, does the streaming in the HiveSink class, main part is done on line\nhttps://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveSink.java#L253\nwhere one \"Batch\" is drained (batch in sense of flume batch of incoming messages from channel).\nMain for loop for batch drain is:\nhttps://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveSink.java#L282\nFlume creates hive endpoint for each line it tries to insert into Hive (https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveSink.java#L290), not very effective, but, the .equals in HiveEndPoint is properly written, so everything works.\nThen, it creates the helper HiveWriter (https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveSink.java#L295), which\nis cached - one for each HiveEndPoint, if no HiveWriter for endpoint exists, it is created on line\nhttps://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveSink.java#L343\n\nInspecting the constructor of HiveWriter, brings us to creating new connection to Hive using the Streaming API:\nhttps://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L86\nThe connection is created in a separate thread:\nhttps://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L376\nas the submitted Future (https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L425)\ninto the thread pool callTimeoutPool (the pool comes from HiveWriter https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveSink.java#L493 and is of constant size 1, which seems like Flume is using 1 thread per Hive Sink to talk with Hive.\n\nWhen creating newConnection (https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L379),\nwith the request of autoCreatePartitions=true, the HiveEndPoint, the entry point to Hive Streaming is called : https://github.com/apache/hive/blob/master/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java#L105\nAs I was testing non-authenticated, it boils to https://github.com/apache/hive/blob/master/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java#L192\nand finally to https://github.com/apache/hive/blob/master/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java#L215\n\nConstructor for inner private class ConnectionImpl then tries to create partition if it not exists, on the line https://github.com/apache/hive/blob/master/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java#L318\nAnd the trouble starts in method createPartitionIfNotExists on line\nhttps://github.com/apache/hive/blob/master/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java#L455\nas the SessionState.get() returns null - we did not started the session yet, we try to create a new one.\nIn SessionState.start() first thing done is registering the object itself as the threadlocal variable:\nhttps://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L526\n\nThereafter, the directories (scratchdir and subdirs) are tried to be created:\nhttps://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L548\nbut if this fails, the RuntimeException (from https://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L619 and https://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L677) is not caught in the catch blocks (nor there is any finally block).\n\nSo basically, SessionState.start() has failed with proper initialization (e.g. HDFS dirs are not created, nor is the SessionState.hdfsSessionPath set to non-null) and yet the execution continues.\nWith RuntimeException thrown from .start() method, the caller (HiveEndPoint) propagates the exception back to the HiveWriter https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L379\n\nThe exception is caught https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L442 but handled only as do logging and go on: https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L456\nThis is the moment this exception is logged:\n{code}\nCaused by: java.lang.RuntimeException: The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rw-------\n        at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:625)\n        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:574)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:518)\n        ... 13 more\n{code}\n\nWhat happens next? Flume re-runs the delivery, calling HiveSink.process, boiling into newConnection again. But Flume uses the SAME and exact one thread it used before to do this.\nThis time, the if clause: https://github.com/apache/hive/blob/master/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java#L454\nreturns true, as the SessionState.get() return the incorrectly initialized SessionState from previous attempt.\nThen, it goes into https://github.com/apache/hive/blob/master/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java#L466 and down to the \nhttps://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L738 which fails on null value of hdfsSessionPath in SessionState.\n\nBut this RuntimeException (NullPointerException) is not caught by https://github.com/apache/hive/blob/master/hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java#L470 and so it is logged:\n{code}\n2017-01-18 12:39:44,194 ERROR org.apache.hadoop.hive.ql.Driver: FAILED: NullPointerException Non-local session path expected to be non-null\njava.lang.NullPointerException: Non-local session path expected to be non-null\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:204)\n        at org.apache.hadoop.hive.ql.session.SessionState.getHDFSSessionPath(SessionState.java:686)\n        at org.apache.hadoop.hive.ql.Context.<init>(Context.java:131)\n        at org.apache.hadoop.hive.ql.Context.<init>(Context.java:118)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:411)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.runDDL(HiveEndPoint.java:404)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.createPartitionIfNotExists(HiveEndPoint.java:369)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:276)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:243)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(HiveEndPoint.java:180)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:157)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:110)\n        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:376)\n        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:373)\n        at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:425)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n{code}\n\nSometimes, Flume manages to run through the https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L86 as the newConnection is created in separate thread, the Flume rushes into https://github.com/apache/flume/blob/release-1.7.0/flume-ng-sinks/flume-hive-sink/src/main/java/org/apache/flume/sink/hive/HiveWriter.java#L89 creating another meaningless exception:\n{code}\n2017-01-18 12:39:44,453 WARN org.apache.flume.sink.hive.HiveSink: sink_hive_1 : Failed connecting to EndPoint {metaStoreUri='thrift://n02.cdh.ideata:9083', database='default', table='flume_test', partitionVals=[20170118] }\norg.apache.flume.sink.hive.HiveWriter$ConnectException: Failed connecting to EndPoint {metaStoreUri='thrift://n02.cdh.ideata:9083', database='default', table='flume_test', partitionVals=[20170118] }\n        at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:99)\n        at org.apache.flume.sink.hive.HiveSink.getOrCreateWriter(HiveSink.java:344)\n        at org.apache.flume.sink.hive.HiveSink.drainOneBatch(HiveSink.java:296)\n        at org.apache.flume.sink.hive.HiveSink.process(HiveSink.java:254)\n        at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)\n        at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hive.hcatalog.streaming.StreamingException: partition values=[20170118]. Unable to get path for end point: [20170118]\n        at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.getPathForEndPoint(AbstractRecordWriter.java:162)\n        at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.<init>(AbstractRecordWriter.java:66)\n        at org.apache.hive.hcatalog.streaming.DelimitedInputWriter.<init>(DelimitedInputWriter.java:115)\n        at org.apache.flume.sink.hive.HiveDelimitedTextSerializer.createRecordWriter(HiveDelimitedTextSerializer.java:67)\n        at org.apache.flume.sink.hive.HiveWriter.<init>(HiveWriter.java:89)\n        ... 6 more\nCaused by: NoSuchObjectException(message:partition values=[20170118])\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result$get_partition_resultStandardScheme.read(ThriftHiveMetastore.java:60283)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result$get_partition_resultStandardScheme.read(ThriftHiveMetastore.java:60251)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partition_result.read(ThriftHiveMetastore.java:60182)\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_partition(ThriftHiveMetastore.java:1892)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_partition(ThriftHiveMetastore.java:1877)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getPartition(HiveMetaStoreClient.java:1171)\n        at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.getPathForEndPoint(AbstractRecordWriter.java:157)\n        ... 10 more\n{code}\n\nProposing solution:\nIf Hive Streaming API is allowed to be used with same thread again (which probably is), then the threadlocal set in \nhttps://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L526\nhas to be unset in case of any exception in proceeding blocks:\nhttps://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L539\nso set the thread local back to null before rethrowing exceptions here:\nhttps://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L568\nand here:\nhttps://github.com/apache/hive/blob/release-2.0.1/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L602\n\nLinks to source codes are from latest version, although I have been doing testing on Hive 1.1.0. From code, it seems like\nbug has to be present also in recent versions.\n\nIf Hive Streaming API is not allowed to be called by reusing threads, then not only Flume, but probably also NiFi client (https://github.com/apache/nifi/blob/master/nifi-nar-bundles/nifi-hive-bundle/nifi-hive-processors/src/main/java/org/apache/nifi/util/hive/HiveWriter.java#L237) has to be fixed (well, NiFi just copy&pasted the Flume codebase, is there any other copy of this HiveWriter out there?).\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12848498","id":"12848498","filename":"HIVE-15658_branch-1.2_1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michal.klempa","name":"michal.klempa","key":"michal.klempa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=michal.klempa&avatarId=26061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=michal.klempa&avatarId=26061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=michal.klempa&avatarId=26061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=michal.klempa&avatarId=26061"},"displayName":"Michal Klempa","active":true,"timeZone":"Europe/Vienna"},"created":"2017-01-20T09:42:39.051+0000","size":2440,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12848498/HIVE-15658_branch-1.2_1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12848499","id":"12848499","filename":"HIVE-15658_branch-2.1_1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michal.klempa","name":"michal.klempa","key":"michal.klempa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=michal.klempa&avatarId=26061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=michal.klempa&avatarId=26061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=michal.klempa&avatarId=26061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=michal.klempa&avatarId=26061"},"displayName":"Michal Klempa","active":true,"timeZone":"Europe/Vienna"},"created":"2017-01-20T09:42:39.070+0000","size":2447,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12848499/HIVE-15658_branch-2.1_1.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"hive.ql.session.SessionState start() is not atomic, SessionState thread local variable can get into inconsistent state","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michal.klempa","name":"michal.klempa","key":"michal.klempa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=michal.klempa&avatarId=26061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=michal.klempa&avatarId=26061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=michal.klempa&avatarId=26061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=michal.klempa&avatarId=26061"},"displayName":"Michal Klempa","active":true,"timeZone":"Europe/Vienna"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michal.klempa","name":"michal.klempa","key":"michal.klempa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=michal.klempa&avatarId=26061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=michal.klempa&avatarId=26061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=michal.klempa&avatarId=26061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=michal.klempa&avatarId=26061"},"displayName":"Michal Klempa","active":true,"timeZone":"Europe/Vienna"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"CDH5.8.0, Flume 1.6.0, Hive 1.1.0","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13035930/comment/15831627","id":"15831627","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michal.klempa","name":"michal.klempa","key":"michal.klempa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=michal.klempa&avatarId=26061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=michal.klempa&avatarId=26061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=michal.klempa&avatarId=26061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=michal.klempa&avatarId=26061"},"displayName":"Michal Klempa","active":true,"timeZone":"Europe/Vienna"},"body":"Another exception type, which can cause this error to happen is, when during creation of directories in HDFS the InterruptedException occurs. See the stack trace (once again, line numbers are off by, cause of https://github.com/cloudera/flume-ng and https://github.com/cloudera/hive):\n\n{code}\n2017-01-19 08:36:19,917 WARN org.apache.hadoop.ipc.Client: interrupted waiting to send rpc request to server\njava.lang.InterruptedException\n        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:187)\n        at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1055)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1450)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1408)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n        at com.sun.proxy.$Proxy24.getFileInfo(Unknown Source) \n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:762)\n        at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source) \n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n        at com.sun.proxy.$Proxy25.getFileInfo(Unknown Source) \n        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2102)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1215)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1211)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1211)\n        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1412)\n        at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:616)\n        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:574)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:518)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.createPartitionIfNotExists(HiveEndPoint.java:358)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:276)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:243)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(HiveEndPoint.java:180)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:157)\n        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:110)\n        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:376)\n        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:373)\n        at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:425)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n{code}\n\nIts the same problem, only the exception that caused it is different.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michal.klempa","name":"michal.klempa","key":"michal.klempa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=michal.klempa&avatarId=26061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=michal.klempa&avatarId=26061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=michal.klempa&avatarId=26061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=michal.klempa&avatarId=26061"},"displayName":"Michal Klempa","active":true,"timeZone":"Europe/Vienna"},"created":"2017-01-20T12:17:36.150+0000","updated":"2017-01-20T12:18:54.334+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13035930/comment/15832465","id":"15832465","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"In the last catch of the SessionState.java part of the patch, can you add a call to tezSessionState.close, if present, protected by try-catch? \nDoesn't look like it should fail leaving it in open state, but I wonder if open (and esp. beginOpen) calls are atomic... we'd rather not have that leak. \n\nAlso why is HiveSessionImpl change necessary? As far as I understand, the session at that point is already valid. At least, it's retained in the field (for future use?) even though we detach it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-20T21:53:41.929+0000","updated":"2017-01-20T21:54:00.070+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13035930/comment/15832835","id":"15832835","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michal.klempa","name":"michal.klempa","key":"michal.klempa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=michal.klempa&avatarId=26061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=michal.klempa&avatarId=26061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=michal.klempa&avatarId=26061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=michal.klempa&avatarId=26061"},"displayName":"Michal Klempa","active":true,"timeZone":"Europe/Vienna"},"body":"Hi Sergey, thank you for response, unfortunately, I am not the author of the original code and I have problems figuring out all the places where this should be fixed. I analyzed all the calls of .setCurrentSession and modified those parts, where I was it is not paired with .detach().\nIf, in HiveSessionImpl the session is already valid, why there is call to setSessionState, though. Maybe it may be reomved.\n\nMaybe we need more complex change of this, to centralize the point where the Session is created & initialized atomically and then only call this one procedure. Standard way of doing this is in constructors. Detaching/closing is then done in .close(). I do not know why this time, the .start was chosen to decouple construction and starting of object, and why classes using SessionState are calling .detach() directly and not only .close(). The outer API of SessionState class is not clear to me.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michal.klempa","name":"michal.klempa","key":"michal.klempa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=michal.klempa&avatarId=26061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=michal.klempa&avatarId=26061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=michal.klempa&avatarId=26061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=michal.klempa&avatarId=26061"},"displayName":"Michal Klempa","active":true,"timeZone":"Europe/Vienna"},"created":"2017-01-21T06:01:37.767+0000","updated":"2017-01-21T06:01:37.767+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15658/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i38vu7:"}}