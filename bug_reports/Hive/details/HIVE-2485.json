{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12525946","self":"https://issues.apache.org/jira/rest/api/2/issue/12525946","key":"HIVE-2485","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2012-07-07T00:32:57.179+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sun Jul 08 14:45:42 UTC 2012","customfield_12310420":"46570","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-2485/watchers","watchCount":4,"isWatching":false},"created":"2011-10-05T19:30:12.427+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12316336","id":"12316336","description":"released","name":"0.7.1","archived":false,"released":true,"releaseDate":"2011-06-21"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2012-07-08T14:45:42.704+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12313604","id":"12313604","name":"CLI","description":"Command-line interpreter for Hive.\n"}],"timeoriginalestimate":null,"description":"When loading multiple files using the statement:\n\nDROP TABLE partition_test;\nCREATE TABLE partition_test (key INT, value STRING) PARTITIONED BY (ts STRING) STORED AS TEXTFILE;\nLOAD DATA LOCAL INPATH 'file:/myfile.txt' OVERWRITE INTO TABLE partition_test PARTITION(ts='1');\nLOAD DATA LOCAL INPATH 'file:/myfile2.txt' OVERWRITE INTO TABLE partition_test PARTITION(ts='2');\nLOAD DATA LOCAL INPATH 'file:/myfile3.txt' OVERWRITE INTO TABLE partition_test PARTITION(ts='3');\netc. up to 250...\n\nHive CLI fails with error:\n\nFailed with exception null\n\nhive.log:\n\n2011-10-05 15:07:11,899 WARN  hdfs.DFSClient (DFSClient.java:processDatanodeError(2667)) - Error Recovery for block blk_-7990368440974156305_1721 bad datanode[0] nodes == null\n2011-10-05 15:07:11,899 WARN  hdfs.DFSClient (DFSClient.java:processDatanodeError(2695)) - Could not get block locations. Source file \"/tmp/hive-cfolsom/hive_2011-10-05_15-06-47_812_4664863850423838867/-ext-10000/kv1.txt\" - Aborting...\n2011-10-05 15:07:11,900 ERROR exec.Task (SessionState.java:printError(365)) - Failed with exception null\njava.io.EOFException\n        at java.io.DataInputStream.readShort(DataInputStream.java:298)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:3065)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2988)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2260)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2451)\n\n2011-10-05 15:07:11,901 ERROR hdfs.DFSClient (DFSClient.java:close(1125)) - Exception closing file /tmp/hive-cfolsom/hive_2011-10-05_15-06-47_812_4664863850423838867/-ext-10000/kv1.txt : java.io.EOFException\njava.io.EOFException\n        at java.io.DataInputStream.readShort(DataInputStream.java:298)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:3065)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2988)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2260)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2451)\n2011-10-05 15:07:12,002 ERROR ql.Driver (SessionState.java:printError(365)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.CopyTask\n\n\nHadoop datanode log: there are too many open files:\n\njava.io.FileNotFoundException: /tmp/hadoop-hadoop/dfs/data/current/subdir4/blk_207900366872942737 (Too many open files)\n        at java.io.RandomAccessFile.open(Native Method)\n        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.getBlockInputStream(FSDataset.java:862)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:166)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:189)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n\n2011-10-05 15:06:14,532 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1718046113-127.0.0.1-50010-1317838257821, infoPort=50075, ipcPort=50020):DataXceiver\njava.io.FileNotFoundException: /tmp/hadoop-hadoop/dfs/data/current/subdir4/blk_207900366872942737 (Too many open files)\n        at java.io.RandomAccessFile.open(Native Method)\n        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.getBlockInputStream(FSDataset.java:862)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:166)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:189)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n2011-10-05 15:06:14,772 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Call to localhost/127.0.0.1:9000 failed on local exception: java.io.IOException: Too many open files\n        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1065)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1033)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:224)\n        at $Proxy5.sendHeartbeat(Unknown Source)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:853)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1356)\n        at java.lang.Thread.run(Thread.java:619)\nCaused by: java.io.IOException: Too many open files\n        at sun.nio.ch.EPollArrayWrapper.epollCreate(Native Method)\n        at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:69)\n        at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:52)\n        at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:18)\n        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.get(SocketIOWithTimeout.java:407)\n        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:322)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)\n        at java.io.FilterInputStream.read(FilterInputStream.java:116)\n        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:343)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)\n        at java.io.DataInputStream.readInt(DataInputStream.java:370)\n        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:767)\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:712)\n\n2011-10-05 15:06:17,535 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1718046113-127.0.0.1-50010-1317838257821, infoPort=50075, ipcPort=50020):Got exception while serving blk_207900366872942737_1719 to /127.0.0.1:\njava.io.FileNotFoundException: /tmp/hadoop-hadoop/dfs/data/current/subdir4/blk_207900366872942737_1719.meta (Too many open files)\n        at java.io.FileInputStream.open(Native Method)\n        at java.io.FileInputStream.<init>(FileInputStream.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaDataInputStream(FSDataset.java:751)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:189)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n\n2011-10-05 15:06:17,535 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1718046113-127.0.0.1-50010-1317838257821, infoPort=50075, ipcPort=50020):DataXceiver\njava.io.FileNotFoundException: /tmp/hadoop-hadoop/dfs/data/current/subdir4/blk_207900366872942737_1719.meta (Too many open files)\n        at java.io.FileInputStream.open(Native Method)\n        at java.io.FileInputStream.<init>(FileInputStream.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaDataInputStream(FSDataset.java:751)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:189)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n2011-10-05 15:06:18,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s).\n2011-10-05 15:06:19,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s).\n2011-10-05 15:06:20,539 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1718046113-127.0.0.1-50010-1317838257821, infoPort=50075, ipcPort=50020):Got exception while serving blk_207900366872942737_1719 to /127.0.0.1:\njava.io.FileNotFoundException: /tmp/hadoop-hadoop/dfs/data/current/subdir4/blk_207900366872942737_1719.meta (Too many open files)\n        at java.io.FileInputStream.open(Native Method)\n        at java.io.FileInputStream.<init>(FileInputStream.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.getMetaDataInputStream(FSDataset.java:751)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:189)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n\na quick lsof reveals that the datanode pid has 1092 open files.\n\nAt first, I suspected that this was because the CopyTask was opening instances of FileSystem that it did not close, but updating CopyTask to make sure that the source and destination FileSystems were closed did not resolve the issue. I suspect that some other task has left files hanging open elsewhere. In any case, this causes more than a bit of a problem as the datanode is completely unavailable after a number of file loads. The process is still running, but the node is effectively dead.\n\n\n\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12497875","id":"12497875","filename":"CopyTask.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jcfolsom","name":"jcfolsom","key":"jcfolsom","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J. Chris Folsom","active":true,"timeZone":"Etc/UTC"},"created":"2011-10-05T19:34:07.124+0000","size":4166,"mimeType":"text/x-java","content":"https://issues.apache.org/jira/secure/attachment/12497875/CopyTask.java"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"123923","customfield_12312823":null,"summary":"LOAD DATA LOCAL Leaves Files Hanging Open","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jcfolsom","name":"jcfolsom","key":"jcfolsom","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J. Chris Folsom","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jcfolsom","name":"jcfolsom","key":"jcfolsom","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J. Chris Folsom","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Linux - Hadoop 0.20.204 Hive 0.7.0, Hive 0.7.1, Hive 0.8.0, trunk","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12525946/comment/13121390","id":"13121390","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jcfolsom","name":"jcfolsom","key":"jcfolsom","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J. Chris Folsom","active":true,"timeZone":"Etc/UTC"},"body":"A version of CopyTask that closes the source and destination FileSystem after opening them.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jcfolsom","name":"jcfolsom","key":"jcfolsom","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J. Chris Folsom","active":true,"timeZone":"Etc/UTC"},"created":"2011-10-05T19:34:07.183+0000","updated":"2011-10-05T19:34:07.183+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12525946/comment/13408485","id":"13408485","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=c0decracker","name":"c0decracker","key":"c0decracker","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alex N.","active":true,"timeZone":"America/New_York"},"body":"This issue still occurring when loading large number of files via Hive CLI.\n\nThe file handlers that are being opened and remain so are of the following type:\n\njava      15930     hadoop 3438u     IPv6            2677372        0t0        TCP ip-10-34-139-215.ec2.internal:52216->ip-10-244-182-97.ec2.internal:9200 (ESTABLISHED)\n\nJust a sample, note this is taken from EMR with running Hive during the process of running hive -f loadfiles.q command, where loadfiles.q contained about 60 thousand lines of LOAD DATA commands. The number of open handlers such as example above was continiously growing through the process, until it failed with the exception in CopyTask.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=c0decracker","name":"c0decracker","key":"c0decracker","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alex N.","active":true,"timeZone":"America/New_York"},"created":"2012-07-07T00:32:57.179+0000","updated":"2012-07-07T00:32:57.179+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12525946/comment/13408743","id":"13408743","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"If there is a problem it is in the copyutil. Hadoop filesystems should not he explicitly closed. I will take a look.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-07T18:53:26.500+0000","updated":"2012-07-07T18:53:26.500+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12525946/comment/13408750","id":"13408750","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kannakar%40microsoft.com","name":"kannakar@microsoft.com","key":"kannakar@microsoft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kanna Karanam","active":true,"timeZone":"Etc/UTC"},"body":"@Edward - I fixed one resource leak in Hadoop sequence file constructor (HADOOP-8486). It may be related to this issue. Thanks\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kannakar%40microsoft.com","name":"kannakar@microsoft.com","key":"kannakar@microsoft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kanna Karanam","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-07T19:24:10.897+0000","updated":"2012-07-07T19:24:10.897+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12525946/comment/13408792","id":"13408792","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"It could be but the copy utils do not use sequence file constructor.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-07T22:15:14.123+0000","updated":"2012-07-07T22:15:14.123+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12525946/comment/13408841","id":"13408841","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kannakar%40microsoft.com","name":"kannakar@microsoft.com","key":"kannakar@microsoft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kanna Karanam","active":true,"timeZone":"Etc/UTC"},"body":"I noticed this problem with MoveTask::execute (Not with Copy Task) at the following code. While checking the file format, we are attempting to create the SequenceFile to validate the file format. If the given file is not a valid sequence file then it is resulting into a resource leak with Hadoop v1.x or lower.\n\nif (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n            // Check if the file format of the file matches that of the table.\n            boolean flag = HiveFileFormatUtils.checkInputFormat(\n\nThanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kannakar%40microsoft.com","name":"kannakar@microsoft.com","key":"kannakar@microsoft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kanna Karanam","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-08T03:16:55.049+0000","updated":"2012-07-08T03:16:55.049+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12525946/comment/13408932","id":"13408932","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"if HIVECHECKFILEFORMAT is the problem for a hot fix this problem can be disabled in hive-site.xml","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-08T14:29:04.339+0000","updated":"2012-07-08T14:29:04.339+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12525946/comment/13408935","id":"13408935","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"If HIVECHECKFILEFORMAT is the issue.It seems like the correct way to handle this is to create a shim. Hadoop 20 versions could simply not validate, not check or we could package a back ported sequenceFile under a new name to do the checking.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-08T14:45:42.663+0000","updated":"2012-07-08T14:45:42.663+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-2485/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0lk7b:"}}