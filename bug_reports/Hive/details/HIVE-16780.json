{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13075336","self":"https://issues.apache.org/jira/rest/api/2/issue/13075336","key":"HIVE-16780","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340268","id":"12340268","name":"3.0.0","archived":false,"released":true,"releaseDate":"2018-05-21"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-06-05T04:37:58.972+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue May 22 23:58:32 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_779596041_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_395957634","customfield_12312321":null,"resolutiondate":"2017-06-09T22:23:54.417+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-16780/watchers","watchCount":6,"isWatching":false},"created":"2017-05-27T07:51:20.810+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12506081","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12506081","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13078451","key":"HIVE-16862","self":"https://issues.apache.org/jira/rest/api/2/issue/13078451","fields":{"summary":"Implement a similar feature like \"hive.tez.dynamic.semijoin.reduction\" in hive on spark","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-05-22T23:58:32.755+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"script.q\n{code}\nset hive.optimize.ppd=true;\nset hive.ppd.remove.duplicatefilters=true;\nset hive.spark.dynamic.partition.pruning=true;\nset hive.optimize.metadataonly=false;\nset hive.optimize.index.filter=true;\nset hive.strict.checks.cartesian.product=false;\nset hive.spark.dynamic.partition.pruning=true;\n\n-- multiple sources, single key\nselect count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)\n{code}\n\nif disabling \"hive.optimize.index.filter\", case passes otherwise it always hang out in the first job. Exception\n{code}\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 PerfLogger: </PERFLOG method=SparkInitializeOperators start=1495899585574 end=1495899585933 duration=359 from=org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler>\n17/05/27 23:39:45 INFO Executor task launch worker-0 Utilities: PLAN PATH = hdfs://bdpe41:8020/tmp/hive/root/029a2d8a-c6e5-4ea9-adea-ef8fbea3cde2/hive_2017-05-27_23-39-06_464_5915518562441677640-1/-mr-10007/617d9dd6-9f9a-4786-8131-a7b98e8abc3e/map.xml\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 Utilities: Found plan in cache for name: map.xml\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 DFSClient: Connecting to datanode 10.239.47.162:50010\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 MapOperator: Processing alias(es) srcpart_hour for file hdfs://bdpe41:8020/user/hive/warehouse/srcpart_hour/000008_0\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 ObjectCache: Creating root_20170527233906_ac2934e1-2e58-4116-9f0d-35dee302d689_DynamicValueRegistry\n17/05/27 23:39:45 ERROR Executor task launch worker-0 SparkMapRecordHandler: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"hr\":\"11\",\"hour\":\"11\"}\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"hr\":\"11\",\"hour\":\"11\"}\n     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562)\n     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\n     at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\n     at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n     at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\n     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\n     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)\n     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)\n     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n     at org.apache.spark.scheduler.Task.run(Task.scala:85)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n     at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_7_srcpart__col3_min\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126)\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.evaluate(GenericUDFBetween.java:57)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead._evaluate(ExprNodeEvaluatorHead.java:44)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)\n     at org.apache.hadoop.hive.ql.exec.FilterOperator.process(FilterOperator.java:112)\n     at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)\n     at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)\n     at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148)\n     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547)\n     ... 17 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:62)\n     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:51)\n     at org.apache.hadoop.hive.ql.exec.ObjectCacheWrapper.retrieve(ObjectCacheWrapper.java:40)\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:119)\n     ... 41 more\nCaused by: java.lang.NullPointerException\n     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:60)\n     ... 44 more\n17/05/27 23:39:45 ERROR Executor task launch worker-0 Executor: Exception in task 1.0 in stage 0.0 (TID 1)\njava.lang.RuntimeException: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"hr\":\"11\",\"hour\":\"11\"}\n     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:149)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\n     at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\n     at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n     at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\n     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\n     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)\n     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)\n     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n     at org.apache.spark.scheduler.Task.run(Task.scala:85)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n     at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"hr\":\"11\",\"hour\":\"11\"}\n     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562)\n     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\n     ... 16 more\nCaused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_7_srcpart__col3_min\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126)\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.evaluate(GenericUDFBetween.java:57)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive\n{code} ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12872156","id":"12872156","filename":"HIVE-16780.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-08T22:20:48.446+0000","size":1130,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12872156/HIVE-16780.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12871199","id":"12871199","filename":"HIVE-16780.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-05T08:24:28.604+0000","size":1016,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12871199/HIVE-16780.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Case \"multiple sources, single key\" in spark_dynamic_pruning.q fails ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16027342","id":"16027342","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~csun]:  I found that if i disable \"hive.optimize.index.filter\", the case pass. \nif enable hive.optimize.index.filter, case fail, the exception is \n{code}\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 PerfLogger: </PERFLOG method=SparkInitializeOperators start=1495899585574 end=1495899585933 duration=359 from=org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler>\n17/05/27 23:39:45 INFO Executor task launch worker-0 Utilities: PLAN PATH = hdfs://bdpe41:8020/tmp/hive/root/029a2d8a-c6e5-4ea9-adea-ef8fbea3cde2/hive_2017-05-27_23-39-06_464_5915518562441677640-1/-mr-10007/617d9dd6-9f9a-4786-8131-a7b98e8abc3e/map.xml\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 Utilities: Found plan in cache for name: map.xml\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 DFSClient: Connecting to datanode 10.239.47.162:50010\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 MapOperator: Processing alias(es) srcpart_hour for file hdfs://bdpe41:8020/user/hive/warehouse/srcpart_hour/000008_0\n17/05/27 23:39:45 DEBUG Executor task launch worker-0 ObjectCache: Creating root_20170527233906_ac2934e1-2e58-4116-9f0d-35dee302d689_DynamicValueRegistry\n17/05/27 23:39:45 ERROR Executor task launch worker-0 SparkMapRecordHandler: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"hr\":\"11\",\"hour\":\"11\"}\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"hr\":\"11\",\"hour\":\"11\"}\n     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562)\n     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\n     at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\n     at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n     at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\n     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\n     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)\n     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)\n     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n     at org.apache.spark.scheduler.Task.run(Task.scala:85)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n     at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_7_srcpart__col3_min\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126)\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.evaluate(GenericUDFBetween.java:57)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead._evaluate(ExprNodeEvaluatorHead.java:44)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)\n     at org.apache.hadoop.hive.ql.exec.FilterOperator.process(FilterOperator.java:112)\n     at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)\n     at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)\n     at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148)\n     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547)\n     ... 17 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:62)\n     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:51)\n     at org.apache.hadoop.hive.ql.exec.ObjectCacheWrapper.retrieve(ObjectCacheWrapper.java:40)\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:119)\n     ... 41 more\nCaused by: java.lang.NullPointerException\n     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:60)\n     ... 44 more\n17/05/27 23:39:45 ERROR Executor task launch worker-0 Executor: Exception in task 1.0 in stage 0.0 (TID 1)\njava.lang.RuntimeException: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"hr\":\"11\",\"hour\":\"11\"}\n     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:149)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\n     at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\n     at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\n     at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n     at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\n     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\n     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)\n     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)\n     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n     at org.apache.spark.scheduler.Task.run(Task.scala:85)\n     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n     at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"hr\":\"11\",\"hour\":\"11\"}\n     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562)\n     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\n     ... 16 more\nCaused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_7_srcpart__col3_min\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126)\n     at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.evaluate(GenericUDFBetween.java:57)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\n     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\n     at org.apache.hadoop.hive\n{code}\n\n\nCan you help to verify whether this pass or not in your environment?  in my enviroment, hive version:54dbca69c9ea630b9cccd5550bdb455b9bbc240c  spark:2.0.0.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-05-27T07:54:55.480+0000","updated":"2017-05-27T16:17:37.307+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16030853","id":"16030853","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz],[~csun]:  I guess this exception is caused by the modification of [DynamicPartitonPruning|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java#L266] after HIVE-15269.  NPE is caused by\norg.apache.hadoop.hive.ql.exec.mr.ObjectCache#retrieve(java.lang.String)\n{code}\n@Override\n  public <T> T retrieve(String key) throws HiveException {\n    return retrieve(key, null);\n  }\n\n  @Override\n  public <T> T retrieve(String key, Callable<T> fn) throws HiveException {\n    try {\n      if (isDebugEnabled) {\n        LOG.debug(\"Creating \" + key);\n      }\n      return fn.call();  // NPE is thrown here\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }\n{code}\nComparing with org.apache.hadoop.hive.ql.exec.tez.ObjectCache#retrieve(java.lang.String)\n{code}\n public <T> T retrieve(String key) throws HiveException {\n    T value = null;\n    try {\n      value = (T) registry.get(key);\n      if ( value != null) {\n        LOG.info(\"Found \" + key + \" in cache with value: \" + value);\n      }\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n    return value;\n  }\n{code}\n\n if we want to fix this, we need add a similar code as what hive on tez does( DynamicValueRegistryTez, RegistryConfTez), appreciate if you can give some suggestions.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-05-31T08:51:11.151+0000","updated":"2017-05-31T08:51:11.151+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16036520","id":"16036520","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the findings [~kellyzly]! I wonder if the issue still happen when {{hive.tez.dynamic.semijoin.reduction}} is set to false.\n\nIt seems this config affects Spark branch too, which should not happen. Maybe we should first disable this optimization for Spark in {{DynamicPartitionPruningOptimization}}, which is shared by both engines. In future we can investigate on how to enable this optimization for Spark.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-05T04:37:58.972+0000","updated":"2017-06-05T04:37:58.972+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16036670","id":"16036670","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~csun]:  case \"multiple sources, single key\" pass if hive.tez.dynamic.semijoin.reduction is false.\nbq.Maybe we should first disable this optimization for Spark in DynamicPartitionPruningOptimization\nagree, update HIVE-16780.1.patch.\n\nthe explain when enabling hive.tez.dynamic.semijoin.reduction\n{noformat}\nTAGE DEPENDENCIES:\n  Stage-2 is a root stage\n  Stage-3 depends on stages: Stage-2\n  Stage-1 depends on stages: Stage-3\n  Stage-0 depends on stages: Stage-1\n\nSTAGE PLANS:\n  Stage: Stage-2\n    Spark\n      DagName: root_20170605152828_4c4f4f82-d08f-41e9-9a07-4147b8529dd0:2\n      Vertices:\n        Map 4 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart_date\n                  filterExpr: ds is not null (type: boolean)\n                  Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: ds is not null (type: boolean)\n                    Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                    Spark HashTable Sink Operator\n                      keys:\n                        0 ds (type: string)\n                        1 ds (type: string)\n                    Select Operator\n                      expressions: ds (type: string)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        keys: _col0 (type: string)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                        Spark Partition Pruning Sink Operator\n                          partition key expr: ds\n                          Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                          target column name: ds\n                          target work: Map 1\n            Local Work:\n              Map Reduce Local Work\n        Map 5 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart_hour\n                  filterExpr: (hr is not null and (hr BETWEEN DynamicValue(RS_7_srcpart__col3_min) AND DynamicValue(RS_7_srcpart__col3_max) and in_bloom_filter(hr, DynamicValue(RS_7_srcpart__col3_bloom_filter)))) (type: boolean)\n                  Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: (hr is not null and (hr BETWEEN DynamicValue(RS_7_srcpart__col3_min) AND DynamicValue(RS_7_srcpart__col3_max) and in_bloom_filter(hr, DynamicValue(RS_7_srcpart__col3_bloom_filter)))) (type: boolean)\n                    Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                    Spark HashTable Sink Operator\n                      keys:\n                        0 _col3 (type: string)\n                        1 hr (type: string)\n                    Select Operator\n                      expressions: hr (type: string)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        keys: _col0 (type: string)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                        Spark Partition Pruning Sink Operator\n                          partition key expr: hr\n                          Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                          target column name: hr\n                          target work: Map 1\n            Local Work:\n              Map Reduce Local Work\n\n  Stage: Stage-3\n    Spark\n      DagName: root_20170605152828_4c4f4f82-d08f-41e9-9a07-4147b8529dd0:3\n      Vertices:\n        Map 4 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart_date\n                  filterExpr: ds is not null (type: boolean)\n                  Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: ds is not null (type: boolean)\n                    Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                    Spark HashTable Sink Operator\n                      keys:\n                        0 ds (type: string)\n                        1 ds (type: string)\n                    Select Operator\n                      expressions: ds (type: string)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        keys: _col0 (type: string)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                        Spark Partition Pruning Sink Operator\n                          partition key expr: ds\n                          Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                          target column name: ds\n                          target work: Map 1\n            Local Work:\n              Map Reduce Local Work\n        Map 5 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart_hour\n                  filterExpr: (hr is not null and (hr BETWEEN DynamicValue(RS_7_srcpart__col3_min) AND DynamicValue(RS_7_srcpart__col3_max) and in_bloom_filter(hr, DynamicValue(RS_7_srcpart__col3_bloom_filter)))) (type: boolean)\n                  Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: (hr is not null and (hr BETWEEN DynamicValue(RS_7_srcpart__col3_min) AND DynamicValue(RS_7_srcpart__col3_max) and in_bloom_filter(hr, DynamicValue(RS_7_srcpart__col3_bloom_filter)))) (type: boolean)\n                    Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                    Spark HashTable Sink Operator\n                      keys:\n                        0 _col3 (type: string)\n                        1 hr (type: string)\n                    Select Operator\n                      expressions: hr (type: string)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        keys: _col0 (type: string)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                        Spark Partition Pruning Sink Operator\n                          partition key expr: hr\n                          Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                          target column name: hr\n                          target work: Map 1\n            Local Work:\n              Map Reduce Local Work\n\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 2 <- Map 6 (GROUP, 1)\n        Reducer 3 <- Map 7 (GROUP, 1)\n      DagName: root_20170605152828_4c4f4f82-d08f-41e9-9a07-4147b8529dd0:1\n      Vertices:\n        Map 6 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart\n                  Statistics: Num rows: 1 Data size: 23248 Basic stats: PARTIAL Column stats: NONE\n                  Map Join Operator\n                    condition map:\n                         Inner Join 0 to 1\n                    keys:\n                      0 ds (type: string)\n                      1 ds (type: string)\n                    outputColumnNames: _col3\n                    input vertices:\n                      1 Map 4\n                    Statistics: Num rows: 2 Data size: 46 Basic stats: COMPLETE Column stats: NONE\n                    Select Operator\n                      expressions: _col3 (type: string)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 2 Data size: 46 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        aggregations: min(_col0), max(_col0), bloom_filter(_col0, expectedEntries=1000000)\n                        mode: hash\n                        outputColumnNames: _col0, _col1, _col2\n                        Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE\n                        Reduce Output Operator\n                          sort order: \n                          Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE\n                          value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)\n            Local Work:\n              Map Reduce Local Work\n        Map 7 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart\n                  Statistics: Num rows: 1 Data size: 23248 Basic stats: PARTIAL Column stats: NONE\n                  Map Join Operator\n                    condition map:\n                         Inner Join 0 to 1\n                    keys:\n                      0 ds (type: string)\n                      1 ds (type: string)\n                    outputColumnNames: _col3\n                    input vertices:\n                      1 Map 4\n                    Statistics: Num rows: 2 Data size: 46 Basic stats: COMPLETE Column stats: NONE\n                    Map Join Operator\n                      condition map:\n                           Inner Join 0 to 1\n                      keys:\n                        0 _col3 (type: string)\n                        1 hr (type: string)\n                      input vertices:\n                        1 Map 5\n                      Statistics: Num rows: 2 Data size: 50 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        aggregations: count()\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                        Reduce Output Operator\n                          sort order: \n                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                          value expressions: _col0 (type: bigint)\n            Local Work:\n              Map Reduce Local Work\n        Reducer 2 \n            Reduce Operator Tree:\n              Group By Operator\n                aggregations: min(VALUE._col0), max(VALUE._col1), bloom_filter(VALUE._col2, expectedEntries=1000000)\n                mode: final\n                outputColumnNames: _col0, _col1, _col2\n                Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE\n                Reduce Output Operator\n                  sort order: \n                  Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE\n                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)\n        Reducer 3 \n            Reduce Operator Tree:\n              Group By Operator\n                aggregations: count(VALUE._col0)\n                mode: mergepartial\n                outputColumnNames: _col0\n                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                File Output Operator\n                  compressed: false\n                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                  table:\n                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n      Processor Tree:\n        ListSink\n{noformat}\n\n\nthe explain when disabling hive.tez.dynamic.semijoin.reduction\n{noformat}\nSTAGE DEPENDENCIES:\n  Stage-2 is a root stage\n  Stage-1 depends on stages: Stage-2\n  Stage-0 depends on stages: Stage-1\n\nSTAGE PLANS:\n  Stage: Stage-2\n    Spark\n      DagName: root_20170605153113_6d0bbb45-8a4a-4b10-9b2c-4585abdf2a79:2\n      Vertices:\n        Map 3 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart_date\n                  filterExpr: ds is not null (type: boolean)\n                  Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: ds is not null (type: boolean)\n                    Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                    Spark HashTable Sink Operator\n                      keys:\n                        0 ds (type: string)\n                        1 ds (type: string)\n                    Select Operator\n                      expressions: ds (type: string)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        keys: _col0 (type: string)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                        Spark Partition Pruning Sink Operator\n                          partition key expr: ds\n                          Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE\n                          target column name: ds\n                          target work: Map 1\n            Local Work:\n              Map Reduce Local Work\n        Map 4 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart_hour\n                  filterExpr: hr is not null (type: boolean)\n                  Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                  Filter Operator\n                    predicate: hr is not null (type: boolean)\n                    Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                    Spark HashTable Sink Operator\n                      keys:\n                        0 _col3 (type: string)\n                        1 hr (type: string)\n                    Select Operator\n                      expressions: hr (type: string)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        keys: _col0 (type: string)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                        Spark Partition Pruning Sink Operator\n                          partition key expr: hr\n                          Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE\n                          target column name: hr\n                          target work: Map 1\n            Local Work:\n              Map Reduce Local Work\n\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 2 <- Map 1 (GROUP, 1)\n      DagName: root_20170605153113_6d0bbb45-8a4a-4b10-9b2c-4585abdf2a79:1\n      Vertices:\n        Map 1 \n            Map Operator Tree:\n                TableScan\n                  alias: srcpart\n                  Statistics: Num rows: 1 Data size: 23248 Basic stats: PARTIAL Column stats: NONE\n                  Map Join Operator\n                    condition map:\n                         Inner Join 0 to 1\n                    keys:\n                      0 ds (type: string)\n                      1 ds (type: string)\n                    outputColumnNames: _col3\n                    input vertices:\n                      1 Map 3\n                    Statistics: Num rows: 2 Data size: 46 Basic stats: COMPLETE Column stats: NONE\n                    Map Join Operator\n                      condition map:\n                           Inner Join 0 to 1\n                      keys:\n                        0 _col3 (type: string)\n                        1 hr (type: string)\n                      input vertices:\n                        1 Map 4\n                      Statistics: Num rows: 2 Data size: 50 Basic stats: COMPLETE Column stats: NONE\n                      Group By Operator\n                        aggregations: count()\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                        Reduce Output Operator\n                          sort order: \n                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                          value expressions: _col0 (type: bigint)\n            Local Work:\n              Map Reduce Local Work\n        Reducer 2 \n            Reduce Operator Tree:\n              Group By Operator\n                aggregations: count(VALUE._col0)\n                mode: mergepartial\n                outputColumnNames: _col0\n                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                File Output Operator\n                  compressed: false\n                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                  table:\n                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n      Processor Tree:\n        ListSink\n\n{noformat}\n\nOne interesting thing is when enabling {{hive.tez.dynamic.semijoin.reduction}}, there is an extra reduce Reducer 2 <- Map 6 (GROUP, 1).  But what's purpose of Reducer 2?\n{noformat}\n   Reducer 2 \n            Reduce Operator Tree:\n              Group By Operator\n                aggregations: min(VALUE._col0), max(VALUE._col1), bloom_filter(VALUE._col2, expectedEntries=1000000)\n                mode: final\n                outputColumnNames: _col0, _col1, _col2\n                Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE\n                Reduce Output Operator\n                  sort order: \n                  Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE\n                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)\n     \n{noformat}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-05T08:24:10.667+0000","updated":"2017-06-05T08:24:10.667+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16036722","id":"16036722","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12871199/HIVE-16780.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10820 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_reverse] (batchId=83)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)\norg.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)\norg.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)\norg.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query78] (batchId=232)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/5529/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/5529/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5529/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 6 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12871199 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-06-05T09:35:47.689+0000","updated":"2017-06-05T09:35:47.689+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16037480","id":"16037480","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"body":"{quote}\nOne interesting thing is when enabling hive.tez.dynamic.semijoin.reduction, there is an extra reduce Reducer 2 <- Map 6 (GROUP, 1). But what's purpose of Reducer 2?\n{quote}\nI think that's for the aggregation of min/max and bloom filter. See [here|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java#L489].","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-05T20:03:30.062+0000","updated":"2017-06-05T20:03:30.062+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16038380","id":"16038380","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~csun]: Reduce2 is calculate the aggregation(mix/max) of specified key? then save the result to the temp file or source file?  If have time, please help review [HIVE-16780.patch|https://issues.apache.org/jira/secure/attachment/12871199/HIVE-16780.patch].","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-06T08:37:37.711+0000","updated":"2017-06-06T08:37:37.711+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16042240","id":"16042240","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"body":"Looks OK. Did tests pass? Can you add a TODO to enable this optimization for Spark in future?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-08T05:55:47.491+0000","updated":"2017-06-08T05:55:47.491+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16043537","id":"16043537","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~csun]:\nthe test in jira description pass.\n have filed jira HIVE-16862 to track a similar feature like \"hive.tez.dynamic.semijoin.reduction\" in hos. and add a TODO in HIVE-16780.2.patch. help review, thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-08T22:20:48.454+0000","updated":"2017-06-08T22:20:48.454+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16043818","id":"16043818","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12872156/HIVE-16780.2.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10816 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)\norg.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)\norg.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)\norg.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query78] (batchId=232)\norg.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=239)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/5591/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/5591/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5591/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 7 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12872156 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-06-09T02:08:19.347+0000","updated":"2017-06-09T02:08:19.347+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16043878","id":"16043878","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"body":"+1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-09T03:35:16.966+0000","updated":"2017-06-09T03:35:16.966+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16043914","id":"16043914","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~csun] : thanks for review, [~Ferd]: can you help commit it to upstream?  ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-09T04:28:33.400+0000","updated":"2017-06-09T04:28:33.400+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16043955","id":"16043955","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Thanks [~kellyzly] for the patch. We need to wait for 24 hrs to see if any other people have further comments.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2017-06-09T05:25:07.980+0000","updated":"2017-06-09T05:25:07.980+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16045129","id":"16045129","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Committed to the upstream. Thanks [~kellyzly] for the patch and [~csun] for the reivew.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2017-06-09T22:23:54.458+0000","updated":"2017-06-09T22:23:54.458+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13075336/comment/16485979","id":"16485979","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgarg","name":"vgarg","key":"vgarg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=vgarg&avatarId=30430","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=vgarg&avatarId=30430","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=vgarg&avatarId=30430","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=vgarg&avatarId=30430"},"displayName":"Vineet Garg","active":true,"timeZone":"America/Los_Angeles"},"body":"Hive 3.0.0 has been released so closing this jira.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgarg","name":"vgarg","key":"vgarg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=vgarg&avatarId=30430","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=vgarg&avatarId=30430","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=vgarg&avatarId=30430","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=vgarg&avatarId=30430"},"displayName":"Vineet Garg","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-22T23:58:32.753+0000","updated":"2018-05-22T23:58:32.753+0000"}],"maxResults":15,"total":15,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-16780/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3fjs7:"}}