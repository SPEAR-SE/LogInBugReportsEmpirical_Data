{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12682296","self":"https://issues.apache.org/jira/rest/api/2/issue/12682296","key":"HIVE-5922","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-02-15T08:53:04.125+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Jul 01 19:08:58 UTC 2016","customfield_12310420":"361553","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-5922/watchers","watchCount":6,"isWatching":false},"created":"2013-12-03T03:03:59.762+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-07-01T19:08:58.937+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12320633","id":"12320633","name":"File Formats","description":"File Formats"}],"timeoriginalestimate":null,"description":"Two stack traces ...\n{code}\njava.io.IOException: IO error in map input file hdfs://10.38.55.204:8020/user/hive/warehouse/ssdb_bin_compress_orc_large_0_13.db/cycle/000004_0\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:236)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:210)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.io.IOException: java.io.IOException: Seek outside of data in compressed stream Stream for column 9 kind DATA position: 21496054 length: 33790900 range: 2 offset: 1048588 limit: 1048588 range 0 = 13893791 to 1048588;  range 1 = 17039555 to 1310735;  range 2 = 20447466 to 1048588;  range 3 = 23855377 to 1048588;  range 4 = 27263288 to 1048588;  range 5 = 30409052 to 1310735 uncompressed: 262144 to 262144 to 21496054\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:276)\n\tat org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)\n\tat org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:108)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:230)\n\t... 9 more\nCaused by: java.io.IOException: Seek outside of data in compressed stream Stream for column 9 kind DATA position: 21496054 length: 33790900 range: 2 offset: 1048588 limit: 1048588 range 0 = 13893791 to 1048588;  range 1 = 17039555 to 1310735;  range 2 = 20447466 to 1048588;  range 3 = 23855377 to 1048588;  range 4 = 27263288 to 1048588;  range 5 = 30409052 to 1310735 uncompressed: 262144 to 262144 to 21496054\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.seek(InStream.java:328)\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:161)\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:205)\n\tat org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readInts(SerializationUtils.java:450)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readDirectValues(RunLengthIntegerReaderV2.java:240)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:53)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:288)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$IntTreeReader.next(RecordReaderImpl.java:510)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.next(RecordReaderImpl.java:1581)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:2707)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:110)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:86)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:274)\n\t... 13 more\n{\\code}\n\n{code}\njava.io.IOException: IO error in map input file hdfs://10.38.55.204:8020/user/hive/warehouse/ssdb_bin_compress_orc_large_0_13.db/cycle/000095_0\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:236)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:210)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.io.IOException: java.lang.IllegalStateException: Can't read header at compressed stream Stream for column 9 kind DATA position: 20447466 length: 20958101 range: 6 offset: 1835029 limit: 1835029 range 0 = 0 to 524294;  range 1 = 1835029 to 2097176;  range 2 = 5242940 to 1835029;  range 3 = 8650851 to 1835029;  range 4 = 11796615 to 2097176;  range 5 = 15204526 to 2097176;  range 6 = 18612437 to 1835029 uncompressed: 262144 to 262144\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:276)\n\tat org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)\n\tat org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:108)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:230)\n\t... 9 more\nCaused by: java.lang.IllegalStateException: Can't read header at compressed stream Stream for column 9 kind DATA position: 20447466 length: 20958101 range: 6 offset: 1835029 limit: 1835029 range 0 = 0 to 524294;  range 1 = 1835029 to 2097176;  range 2 = 5242940 to 1835029;  range 3 = 8650851 to 1835029;  range 4 = 11796615 to 2097176;  range 5 = 15204526 to 2097176;  range 6 = 18612437 to 1835029 uncompressed: 262144 to 262144\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:195)\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:205)\n\tat org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readInts(SerializationUtils.java:450)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readDirectValues(RunLengthIntegerReaderV2.java:240)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:53)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:288)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$IntTreeReader.next(RecordReaderImpl.java:510)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.next(RecordReaderImpl.java:1581)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:2707)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:110)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.next(OrcInputFormat.java:86)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:274)\n\t... 13 more\n{\\code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"361851","customfield_12312823":null,"summary":"In orc.InStream.CompressedStream, the desired position passed to seek can equal offsets[i] + bytes[i].remaining() when ORC predicate pushdown is enabled","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhuai","name":"yhuai","key":"yhuai","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yhuai&avatarId=23452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yhuai&avatarId=23452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yhuai&avatarId=23452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yhuai&avatarId=23452"},"displayName":"Yin Huai","active":true,"timeZone":"America/New_York"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhuai","name":"yhuai","key":"yhuai","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yhuai&avatarId=23452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yhuai&avatarId=23452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yhuai&avatarId=23452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yhuai&avatarId=23452"},"displayName":"Yin Huai","active":true,"timeZone":"America/New_York"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682296/comment/13837275","id":"13837275","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhuai","name":"yhuai","key":"yhuai","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yhuai&avatarId=23452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yhuai&avatarId=23452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yhuai&avatarId=23452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yhuai&avatarId=23452"},"displayName":"Yin Huai","active":true,"timeZone":"America/New_York"},"body":"For the first trace, the desired position is 21496054 and the second range is \"range 2 = 20447466 to 1048588\". For the second trace, the desired position is 20447466 and the sixth range is \"range 6 = 18612437 to 1835029\". \n\nWhen I turned off predicate pushdown or I used predicate pushdown with uncompressed data, I did not see this problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhuai","name":"yhuai","key":"yhuai","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yhuai&avatarId=23452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yhuai&avatarId=23452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yhuai&avatarId=23452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yhuai&avatarId=23452"},"displayName":"Yin Huai","active":true,"timeZone":"America/New_York"},"created":"2013-12-03T03:08:54.666+0000","updated":"2013-12-03T03:08:54.666+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682296/comment/13902353","id":"13902353","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Puneetkgupta","name":"Puneetkgupta","key":"puneetkgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Puneet Gupta","active":true,"timeZone":"Asia/Kolkata"},"body":"I got a similar Exception  ( on seeking to row 9,103,258 )\n\njava.io.IOException: Seek outside of data in compressed stream Stream for column 65 kind DATA position: 1572882 length: 2116178 range: 1 offset: 1048588 limit: 1048588 range 0 = 0 to 0;  range 1 = 524294 to 1048588;  range 2 = 1835029 to 262147 uncompressed: 1048588 to 1048588 to 1572882\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.seek(InStream.java:277)\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:153)\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:197)\n\tat org.apache.hadoop.hive.ql.io.orc.SerializationUtils.readInts(SerializationUtils.java:450)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readPatchedBaseValues(RunLengthIntegerReaderV2.java:161)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:54)\n\tat org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.skip(RunLengthIntegerReaderV2.java:318)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$IntTreeReader.skipRows(RecordReaderImpl.java:427)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.skipRows(RecordReaderImpl.java:1181)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:2183)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.seekToRow(RecordReaderImpl.java:2284)\n\nSome observations\n----\n1. I have used Snappy for compression \n\n2. there are 75 columns in the file (mostly numbers - int,long,byte,short  and a few strings). Exception always happens for column 65 which is an int. If I remove this column from include column list, seek works fine . \n\n2. This issue happens only when I am seeking to row using RecordReader.seekToRow(long). In this flow the RecordReader is created using  Reader.rows(long, long, boolean[], SearchArgument, String[]). The SearchArgument is using \"IN\" construct with 200 long values which are actually the row numbers I want to retrieve   (SearchArgument.FACTORY.newBuilder().startOr().in(colName, 200 Long Values).end().build()). Exception happens for seek to row 9103258 (file has about 13 million rows). I tried SearchArgument with just one IN value of 9103258.... BINGO .. got the same Exception. This problem can be reproduced for any rowSeek between 9103258 and 9103279. Rows after this seem to work fine .\n\n3. I face no Exceptions if the RecordReader is created using Reader.rows(null), and the entire file is iterated using RecordReader.hasNext() and RecordReader.next()\n\n4. I face no Exceptions if the RecordReader is created using  Reader.rows(long, long, boolean[], SearchArgument, String[]) and SearchArgument is passed null. Then the required data (about 200 rows) is retrieved using RecordReader.seekToRow(long) and RecordReader.next()\n\n5.Obvious WorkAround is not to use predicate push down . In may case since I know the row numbers to be seeked to, the performance let down is not very drastic. \n\tRead/SeekTO 167 rows in (ms)3609   : Existing usage with predicate push down in ORC  \n\tRead/SeekTo 167 rows in (ms)4626   : WorkAround without predicate/Search-Argument pushdown\n\t   >>>> Difference of  1017 ms  = approx 7 ms per row let down in performance (arounf 80% values are fetched from different strides)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Puneetkgupta","name":"Puneetkgupta","key":"puneetkgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Puneet Gupta","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-02-15T08:53:04.125+0000","updated":"2014-02-15T08:53:04.125+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682296/comment/13902355","id":"13902355","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prasanth_j","name":"prasanth_j","key":"prasanth_j","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanth Jayachandran","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Puneeth\n\nThe issue might be related to https://issues.apache.org/jira/browse/HIVE-6320 or https://issues.apache.org/jira/browse/HIVE-6287 depends on whether you have enable vectorization or not. Is this issue happening in trunk?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prasanth_j","name":"prasanth_j","key":"prasanth_j","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanth Jayachandran","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-02-15T09:00:44.272+0000","updated":"2014-02-15T09:00:44.272+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682296/comment/13902358","id":"13902358","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Puneetkgupta","name":"Puneetkgupta","key":"puneetkgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Puneet Gupta","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi Prasanth\n\nI am using Hive Binary from \"hive-0.12.0-bin.tar.gz\"\nhttp://apache.claz.org/hive/hive-0.12.0/\n\nI am using only the ORC file format part to store my data . Its not used along with Hive . ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Puneetkgupta","name":"Puneetkgupta","key":"puneetkgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Puneet Gupta","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-02-15T09:16:20.376+0000","updated":"2014-02-15T09:16:20.376+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682296/comment/13902956","id":"13902956","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Puneetkgupta","name":"Puneetkgupta","key":"puneetkgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Puneet Gupta","active":true,"timeZone":"Asia/Kolkata"},"body":"From what is know 0.12.0 does not have vectorization support .So that can not be the issue.  Also this happens only on seeking while predicate push-down is enabled . Normal iteration is fine . ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Puneetkgupta","name":"Puneetkgupta","key":"puneetkgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Puneet Gupta","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-02-17T04:52:58.401+0000","updated":"2014-02-17T04:52:58.401+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682296/comment/15357581","id":"15357581","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frankluo","name":"frankluo","key":"frankluo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frank Luo","active":true,"timeZone":"America/Chicago"},"body":"same issue here and I am using Apache Hive 1.2.1.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frankluo","name":"frankluo","key":"frankluo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frank Luo","active":true,"timeZone":"America/Chicago"},"created":"2016-06-30T18:03:24.871+0000","updated":"2016-06-30T18:03:24.871+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682296/comment/15359489","id":"15359489","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prasanth_j","name":"prasanth_j","key":"prasanth_j","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanth Jayachandran","active":true,"timeZone":"America/Los_Angeles"},"body":"[~puneet884]/[~frankluo] Are you using the ORC writer APIs directly or using custom MapReduce program to write ORC files? If so can you make sure the row that you are writing to ORC is not null. Recently observed similar issue with a customer and looks like they were using a custom MR program that writes to ORC file. The value passed to writer.addRow() seems to have been null. Orc writer does not expect null as a row. The columns within a row can be null though. This can be verified from orcfiledump. If you guys can provide me orcfiledump output I can confirm if that's the case. \n\n{code}\nwriter.addRow(null); // invalid\nwriter.addRow([a, b, null, d]); // valid\nwriter.addRow([null, null, null, null]); // valid\n{code}\nWe are planning to fix this in the writer API documentation and code to make sure users does not pass null row.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prasanth_j","name":"prasanth_j","key":"prasanth_j","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanth Jayachandran","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-01T19:08:58.937+0000","updated":"2016-07-01T19:08:58.937+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-5922/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1qbpb:"}}