{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12920958","self":"https://issues.apache.org/jira/rest/api/2/issue/12920958","key":"HIVE-12650","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334255","id":"12334255","name":"2.1.0","archived":false,"released":true,"releaseDate":"2016-06-20"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2016-02-01T13:08:53.961+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Apr 01 06:42:41 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_9340793224_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_345843786","customfield_12312321":null,"resolutiondate":"2016-04-01T06:42:41.619+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-12650/watchers","watchCount":6,"isWatching":false},"created":"2015-12-11T03:58:44.660+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329557","id":"12329557","name":"1.1.1","archived":false,"released":true,"releaseDate":"2015-05-20"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12332384","id":"12332384","name":"1.2.1","archived":false,"released":true,"releaseDate":"2015-06-26"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-06-21T15:53:13.775+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"I think hive.spark.client.server.connect.timeout should be set greater than spark.yarn.am.waitTime. The default value for \nspark.yarn.am.waitTime is 100s, and the default value for hive.spark.client.server.connect.timeout is 90s, which is not good. We can increase it to a larger value such as 120s.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12795582","id":"12795582","filename":"HIVE-12650.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-28T06:38:24.017+0000","size":5398,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12795582/HIVE-12650.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12795584","id":"12795584","filename":"HIVE-12650.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-28T07:55:52.025+0000","size":6785,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12795584/HIVE-12650.2.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Improve error messages for Hive on Spark in case the cluster has no resources available","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15126219","id":"15126219","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~vanzin], any idea on this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-02-01T13:08:53.961+0000","updated":"2016-02-01T13:08:53.961+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15126293","id":"15126293","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~lirui], since application master in the context of Hive on Spark takes a container from yarn. In a busy cluster, spark-submit may wait up to spark.yarn.am.waitTime to launch the master. On the other hand, Hive waits for  hive.spark.client.server.connect.timeout  before declaring that the remote driver is not connecting back. If the latter is less than the former, it's possible that Hive prematurely disconnects, causing an unstable condition. [~JoyoungZhang@gmail.com] had a description of the problem in the user list.\n\nI think we need at least to make hive.spark.client.server.connect.timeout greater than spark.yarn.am.waitTime by default. To further guard against the problem, Hive can increase hive.spark.client.server.connect.timeout automatically based on the value of spark.yarn.am.waitTime;\n\n[~vanzin], please share your thoughts as well.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-01T14:26:08.826+0000","updated":"2016-02-01T14:26:08.826+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15126779","id":"15126779","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"{{spark.yarn.am.waitTime}} is not the time Spark waits for the master to launch. It's the time the Spark AM waits for the SparkContext to be created after the AM has been launched.\n\nThat being said, it's ok for the Hive timeout to be larger. 90s already seems like a really long time to wait, so I doubt the extra 30s will help, but it won't hurt.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-01T18:47:59.408+0000","updated":"2016-02-01T18:47:59.408+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15126866","id":"15126866","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the clarification, [~vanzin]. I agree with you. Do you know what factors (such as a lack of available executors) might make Spark AM wait for SparkContext to be initialized for longer period of time (say, a minute)? The problem seems to be that Hive times out first while the AM still appears running, waiting for the context to be initialized. It will eventually fail either the context gets initialized for timeout occurs. This might look a bit confusing. I'm think if we make Hive waits longer than that, then we can avoid the scenario. Any further thoughts?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-01T19:29:24.204+0000","updated":"2016-02-01T19:29:24.204+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15126881","id":"15126881","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"bq.  Do you know what factors (such as a lack of available executors) might make Spark AM wait for SparkContext to be initialized for longer period of time (say, a minute)?\n\nThe only factor is possible problems in the user's {{main}} method, since that's the code that creates the SparkContext. The AM container is *already running* at that time, so it can't really fail for not being able to allocate the container...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-01T19:38:12.052+0000","updated":"2016-02-01T19:38:12.052+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15127492","id":"15127492","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks guys for your inputs. My understanding is that {{hive.spark.client.server.connect.timeout}} is the timeout between RPC server and client handshake. In {{RemoteDriver}}, RPC client is created before SparkContext. And if {{spark.yarn.am.waitTime}} is the timeout waiting for SparkContext to be created, maybe it won't help here. I mean we can try increasing {{hive.spark.client.server.connect.timeout}}, but according to something else.\nBTW, is it possible the timeout is caused by the schedule delay within yarn? Is the issue only encountered with yarn-cluster?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-02-02T02:41:30.442+0000","updated":"2016-02-02T02:41:30.442+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15127507","id":"15127507","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Here is the log that provided the the JIRA creator:\n{code}\nLogs of Application_1448873753366_121022 as follows(same as application_1448873753366_121055):\nContainer: container_1448873753366_121022_03_000001 on 10.226.136.122_8041\n============================================================================\nLogType: stderr\nLogLength: 4664\nLog Contents:\nPlease use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future\nPlease use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future\n15/12/09 16:29:45 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]\n15/12/09 16:29:46 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1448873753366_121022_000003\n15/12/09 16:29:47 INFO spark.SecurityManager: Changing view acls to: mqq\n15/12/09 16:29:47 INFO spark.SecurityManager: Changing modify acls to: mqq\n15/12/09 16:29:47 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mqq); users with modify permissions: Set(mqq)\n15/12/09 16:29:47 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread\n15/12/09 16:29:47 INFO yarn.ApplicationMaster: Waiting for spark context initialization\n15/12/09 16:29:47 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... \n15/12/09 16:29:47 INFO client.RemoteDriver: Connecting to: 10.179.12.140:38842\n15/12/09 16:29:48 WARN rpc.Rpc: Invalid log level null, reverting to default.\n15/12/09 16:29:48 ERROR yarn.ApplicationMaster: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.\njava.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)\n        at org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:156)\n        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:483)\nCaused by: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.\n        at org.apache.hive.spark.client.rpc.Rpc$SaslClientHandler.dispose(Rpc.java:449)\n        at org.apache.hive.spark.client.rpc.SaslHandler.channelInactive(SaslHandler.java:90)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at org.apache.hive.spark.client.rpc.KryoMessageCodec.channelInactive(KryoMessageCodec.java:127)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)\n        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)\n        at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:769)\n        at io.netty.channel.AbstractChannel$AbstractUnsafe$5.run(AbstractChannel.java:567)\n        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\n        at java.lang.Thread.run(Thread.java:745)\n15/12/09 16:29:48 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.)\n15/12/09 16:29:57 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 150000 ms. Please check earlier log output for errors. Failing the application.\n15/12/09 16:29:57 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.)\n15/12/09 16:29:57 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1448873753366_121022\n15/12/09 16:29:57 INFO util.Utils: Shutdown hook called\n\nLogType: stdout\nLogLength: 216\nLog Contents:\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0\n{code}\n\nThe interesting part of the log is:\n{code}\n15/12/09 16:29:57 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 150000 ms. Please check earlier log output for errors. Failing the application.\n{code}\n\nI have shared with the thread in the user mailing list for reference via email.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-02T02:47:55.204+0000","updated":"2016-02-02T02:47:55.204+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15127564","id":"15127564","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I'm specially interested in case where Hive calls spark-submit to submit the application while there is no container available. I'm not sure if spark-submit will wait. If it does, then Hive can time out first before the AM starts to run.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-02T03:09:46.005+0000","updated":"2016-02-02T03:09:46.005+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15127747","id":"15127747","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~xuefuz], the exception you posted doesn't seem to be a timeout, at least it's not related to {{hive.spark.client.server.connect.timeout}}, because the elapsed time is much less than 90s. I found the code that prints the log you mentioned:\n{code}\n      while (sparkContextRef.get() == null && System.currentTimeMillis < deadline && !finished) {\n        logInfo(\"Waiting for spark context initialization ... \")\n        sparkContextRef.wait(10000L)\n      }\n\n      val sparkContext = sparkContextRef.get()\n      if (sparkContext == null) {\n        logError((\"SparkContext did not initialize after waiting for %d ms. Please check earlier\"\n          + \" log output for errors. Failing the application.\").format(totalWaitTime))\n      }\n{code}\nYou can see the while loop can exit either on timeout or finished being set to true. Since time elapsed is short, it must because user thread (RemoteDriver) has finished abnormally:\n{code}\n    val userThread = new Thread {\n      override def run() {\n        try {\n          mainMethod.invoke(null, userArgs.toArray)\n          finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)\n          logDebug(\"Done running users class\")\n        } catch {\n          case e: InvocationTargetException =>\n            e.getCause match {\n              case _: InterruptedException =>\n                // Reporter thread can interrupt to stop user class\n              case SparkUserAppException(exitCode) =>\n                val msg = s\"User application exited with status $exitCode\"\n                logError(msg)\n                finish(FinalApplicationStatus.FAILED, exitCode, msg)\n              case cause: Throwable =>\n                logError(\"User class threw exception: \" + cause, cause)\n                finish(FinalApplicationStatus.FAILED,\n                  ApplicationMaster.EXIT_EXCEPTION_USER_CLASS,\n                  \"User class threw exception: \" + cause)\n            }\n        }\n      }\n    }\n{code}\nIn conclusion, the problem here is not we timed out creating SparkContext. My guess is that something goes wrong before we create SparkContext (you can refer to the constructor of RemoteDriver). Also found another property {{hive.spark.client.connect.timeout}} which defaults to 1000ms. It's used when RemoteDriver creates RPC client so it could be related, although I'm a little confused about the difference between the 2 configurations.\n\nRegarding your last question, I tried submitting application when no container is available. Spark-submit will wait until timeout (90s).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-02-02T06:23:45.103+0000","updated":"2016-02-02T06:23:45.103+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15128361","id":"15128361","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~lirui], thanks for your analysis. Yeah, I saw the actually elapsed time is very short, while the message says timeout 150s, which is very confusing.\n\n[~vanzin], could you please explain a little bit the use of the two timeout? Also, what timeout value does spark-submit use if the application cannot be submitted?\n\n[~JoyoungZhang@gmail.com], could you please reproduce the problem and provide more info such as hive.log?\n\nThanks, folks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-02T14:55:45.916+0000","updated":"2016-02-02T14:55:45.916+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15128913","id":"15128913","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. could you please explain a little bit the use of the two timeout?\n\nThere's nothing complicated about them.\n\n- RSC timeout: time between the RSC launching the Spark app and the Spark driver connecting back.\n- Spark AM timeout: time between Spark AM launching the user's \"main\" method and a SparkContext being created.\n\nBoth overlap but one is not necessarily contained in the other.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-02T20:00:38.631+0000","updated":"2016-02-02T20:00:38.631+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15128949","id":"15128949","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks, [~vanzin]. I guess the question is the difference between the follow two (both defined in Hive):\n1. hive.spark.client.connect.timeout\n2. hive.spark.client.server.connect.timeout\n\nThe second question is: what's the timeout value that spark-submit uses in case of no available containers?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-02T20:19:26.261+0000","updated":"2016-02-02T20:19:26.261+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15128979","id":"15128979","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"body":"* hive.spark.client.connect.timeout\n\nThat's the socket connect timeout when the driver connects to the RSC server. Equivalent to this:\nhttp://docs.oracle.com/javase/7/docs/api/java/net/Socket.html#connect(java.net.SocketAddress,%20int)\n\n* hive.spark.client.server.connect.timeout\n\nThat's the timeout explained in my previous comment.\n\n* what's the timeout value that spark-submit uses in case of no available containers?\n\nI don't believe there is one.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vanzin","name":"vanzin","key":"vanzin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Marcelo Vanzin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-02T20:37:19.801+0000","updated":"2016-02-02T20:37:19.801+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15129392","id":"15129392","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks, [~vanzin].\n\nIf there is no timeout in spark-submit (wait indefinitely), I'm wondering what happens if the cluster is busy. Here is my speculation. Hive will time out first (also corresponding to Rui's observation), but spark-submit will continue to run. If a container becomes available, Spark AM will start and connect to Hive. Hive of course refuses. Then, AM will error out.\n\nI'm not sure if this what the user experienced. It would be good if we can cancel the submit. However, it doesn't look too bad even if we decide to live with it.\n\nUnless [~JoyoungZhang@gmail.com] can provide more info, it doesn't seem we can do much here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-03T00:01:54.420+0000","updated":"2016-02-03T00:01:54.420+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15129595","id":"15129595","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"bq. Regarding your last question, I tried submitting application when no container is available. Spark-submit will wait until timeout (90s).\nSorry this comment is misleading. Actually I mean hive will timeout after 90s. But after this, we'll interrupt the driver thread:\n{code}\n    try {\n      // The RPC server will take care of timeouts here.\n      this.driverRpc = rpcServer.registerClient(clientId, secret, protocol).get();\n    } catch (Throwable e) {\n      LOG.warn(\"Error while waiting for client to connect.\", e);\n      driverThread.interrupt();\n      try {\n        driverThread.join();\n      } catch (InterruptedException ie) {\n        // Give up.\n        LOG.debug(\"Interrupted before driver thread was finished.\");\n      }\n      throw Throwables.propagate(e);\n    }\n{code}\nwhich in turn will destroy the SparkSubmit process:\n{code}\n        public void run() {\n          try {\n            int exitCode = child.waitFor();\n            if (exitCode != 0) {\n              rpcServer.cancelClient(clientId, \"Child process exited before connecting back\");\n              LOG.warn(\"Child process exited with code {}.\", exitCode);\n            }\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Waiting thread interrupted, killing child process.\");\n            Thread.interrupted();\n            child.destroy();\n          } catch (Exception e) {\n            LOG.warn(\"Exception while waiting for child process.\", e);\n          }\n        }\n{code}\nSo on my machine, after the timeout, SparkSubmit is terminated.\nI think the {{Client closed before SASL negotiation finished.}} exception is worth investigating and should be root cause here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-02-03T01:56:19.638+0000","updated":"2016-02-03T01:56:19.638+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15129655","id":"15129655","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~lirui], thanks for the info. It's good that spark-submit is killed when Hive times out. Now the user's problem seems more interesting, though we cannot do much unless we have more information.\n\n\"Client closed before SASL negotiation finished\" could be caused by the fact that AM tries to connect back to Hive, but Hive has already timed out. While Spark-submit is killed, is possible that YARN RM still has the request which will be eventually served?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-03T02:33:30.000+0000","updated":"2016-02-03T02:33:30.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15129682","id":"15129682","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks Xuefu. Yeah I tried again and found the application is served (AM launched) and failed eventually, even after SparkSubmit is killed. Although I didn't get the AM log due to some env issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-02-03T03:00:18.216+0000","updated":"2016-02-03T03:00:18.216+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15129761","id":"15129761","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I see. I think that's what the [~JoyoungZhang@gmail.com] experienced as well. Killing spark-submit doesn't cancel AM request. When AM is finally launched, it tries to connect back to Hive and gets refused. As a result, it quickly errors out. (However, on spark side, the message, saying \"spark context initialization times out in xxx seconds\", is very confusing.) I'm not sure if we can do anything here.\n\nNevertheless, it seems spark.yarn.am.waitTime isn't relevant after all.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-03T04:21:02.126+0000","updated":"2016-02-03T04:21:02.126+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15148177","id":"15148177","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"body":"Hi all,\nI'm sorrry reply you so late.\n\nYes\nhive.spark.client.server.connect.timeout and spark.yarn.am.waitTime does not have any relations.\nhive.spark.client.server.connect.timeout is the timeout between RPC server and client handshake.When no container is available, hive cient  will exit after hive.spark.client.server.connect.timeout.\nspark.yarn.am.waitTime is the time the Spark AM waits for the SparkContext to be created after the AM has been launched.\n\nThere are two types of error log\n1.Client closed before SASL negotiation finished was happened on resubmitted. See https://issues.apache.org/jira/browse/HIVE-12649.\n2.Connection refused: /hiveclientip:port was happend when am tries to connect back to Hive.\n\nContainer: container_1448873753366_113453_01_000001 on 10.247.169.134_8041\n============================================================================\nLogType: stderr\nLogLength: 3302\nLog Contents:\nPlease use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future\nPlease use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future\n15/12/09 02:11:48 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]\n15/12/09 02:11:48 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1448873753366_113453_000001\n15/12/09 02:11:49 INFO spark.SecurityManager: Changing view acls to: mqq\n15/12/09 02:11:49 INFO spark.SecurityManager: Changing modify acls to: mqq\n15/12/09 02:11:49 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mqq); users with modify permissions: Set(mqq)\n15/12/09 02:11:49 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread\n15/12/09 02:11:49 INFO yarn.ApplicationMaster: Waiting for spark context initialization\n15/12/09 02:11:49 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... \n15/12/09 02:11:49 INFO client.RemoteDriver: Connecting to: 10.179.12.140:58013\n15/12/09 02:11:49 ERROR yarn.ApplicationMaster: User class threw exception: java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013\njava.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)\n        at org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:156)\n        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:483)\nCaused by: java.net.ConnectException: Connection refused: /10.179.12.140:58013\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n        at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)\n        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\n        at java.lang.Thread.run(Thread.java:745)\n15/12/09 02:11:49 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013)\n15/12/09 02:11:59 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 150000 ms. Please check earlier log output for errors. Failing the application.\n15/12/09 02:11:59 INFO util.Utils: Shutdown hook called","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=JoyoungZhang%40gmail.com","name":"JoyoungZhang@gmail.com","key":"joyoungzhang@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"JoneZhang","active":true,"timeZone":"Etc/UTC"},"created":"2016-02-16T07:04:32.737+0000","updated":"2016-02-16T07:04:32.737+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15192912","id":"15192912","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xhao1","name":"xhao1","key":"xhao1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xin Hao","active":true,"timeZone":"Asia/Shanghai"},"body":"Encountered the issue based on Apache Hive 2.0.0.\n\nBTW, suggest to change the issue title to a new one to avoid confusion. E.g. \"Spark-submit is killed when Hive times out.  Killing spark-submit doesn't cancel AM request. When AM is finally launched, it tries to connect back to Hive and gets refused.\". Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xhao1","name":"xhao1","key":"xhao1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xin Hao","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-14T08:32:50.328+0000","updated":"2016-03-14T08:32:50.328+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15193174","id":"15193174","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"The timeout is necessary in case the RSC crashes due to some errors. But the issue here shows that it could also because the RSC is just waiting for resources from a busy cluster. I think we need a way to distinguish these two scenarios and don't timeout on the latter.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-14T12:25:18.242+0000","updated":"2016-03-14T12:25:18.242+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15201029","id":"15201029","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Here're my findings so far (for yarn-client mode).\n\n# If the cluster has no resources available, the {{RemoteDriver}} is blocked at creating the SparkContext. Rpc has been set up at this point. So {{SparkClientImpl}} believes that driver is up.\n# There're 2 points where hive can get timeout. If we enable container pre-warm, we'll timeout at {{RemoteHiveSparkClient#createRemoteClient#getExecutorCount}}. If pre-warm is off, we'll timeout at {{RemoteSparkJobMonitor#startMonitor}}. Both are because SparkContext is not created and RemoteDriver can't respond to requests. For the latter, the job status remains {{SENT}} until timeout. Ideally it should be {{QUEUE}} instead. My understanding is that the Rpc handler is blocked at RemoteDriver for {{ADD JAR/FILE}} calls, which we submitted before the real job.\n# Currently YARN doesn't timeout a starving application, which means the app will eventually get served. Refer to YARN-3813 and YARN-2266.\n\nBased on these findings, I think we have to decide whether hive should timeout in this situation. Waiting is reasonable for busy clusters. But on the other hand, it seems difficult to tell whether we're blocked for lack of resources. I'm not sure if spark has such facilities for it. For yarn-cluster mode, this may be even more difficult because RemoteDriver is not running in that case and we'll have less information.\nWhat do you guys think?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-18T05:14:17.272+0000","updated":"2016-03-18T05:14:17.272+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15205200","id":"15205200","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Since Hive cannot really differentiate the scenarios, I'm not sure if there is anything we can do better except for better error message and documentation. RPC timeout is necessary due to network.\n\nOn a side note, Yarn queues are more appropriate for solving the starvation problem. The problem here seems more like an uncommon scenario.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-21T21:30:53.335+0000","updated":"2016-03-21T21:30:53.335+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15205603","id":"15205603","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Regarding better error message, do you think we can throw a timeout exception if SparkContext is not up after certain amount of time? Otherwise user only gets a timeout on the future and doesn't know the cause. On the other hand, this means adding another property and I think it only works for yarn-client.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-22T01:46:55.852+0000","updated":"2016-03-22T01:46:55.852+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15208398","id":"15208398","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I was thinking of just improving the current message, maybe by naming different possibilities when a timeout occurs. The new timeout you mentioned doesn't seem very helpful as yarn-cluster is what we recommend.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-23T13:33:54.393+0000","updated":"2016-03-23T13:33:54.393+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15211335","id":"15211335","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I think the difficult part is that we really don't know the possible reasons. Anyway all we get is a timeout, it could be due to network issue, exceptions, or the RSC is just busy.\n\nAnother possible refinement is that we can make the behavior more consistent. Like I said, there're now 2 paths that can lead to timeout/failure and user will see different error messages. How about remove the timeout at {{RemoteHiveSparkClient#createRemoteClient#getExecutorCount}}? I mean after certain amount of time, we can give up the pre-warm and eventually fail the job at job monitor.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-25T02:54:45.993+0000","updated":"2016-03-25T02:54:45.993+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15211391","id":"15211391","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks, Rui. I think it's fine to list all possible causes in an error message when we don't actually know the exact one. We can also suggest user where to look further (such as yarn logs).\n\nI understand that prewarming containers complicates the things a bit, but I'm not sure of your proposal. Could you provide a patch showing the changes you have in mind?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-25T04:13:03.276+0000","updated":"2016-03-25T04:13:03.276+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15213875","id":"15213875","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Assigned this to me and upload a patch.\nThe main change in the patch is that we don't error out when timeout pre-warming. I think it makes sense because we already allow timeout in pre-warm so it shouldn't be a fatal error.\nThe patch also adds some explanations in error messages so it should be more user friendly.\n\nOne thing I noticed when I tested the patch is that, with yarn-client mode, we may end up with hanging spark AM trying to connect to the driver that has already timed out. But I think we have to live with that and the AM will eventually give up and exit.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-28T06:38:24.021+0000","updated":"2016-03-28T06:38:24.021+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15213935","id":"15213935","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Update patch to also improve messages in yarn-cluster mode. Here's the summary of behaviors under these two modes.\n||   ||Error users will see||Will spark-submit be killed after timeout||\n|yarn-cluster|Failed to create spark client|Y|\n|yarn-client|Job hasn't been submitted|N|\n\nI think the bottom line here is that when the starving app gets served, the aborted query won't be executed so that resources won't be wasted.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-28T07:55:52.029+0000","updated":"2016-03-28T07:55:52.029+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15216086","id":"15216086","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12795584/HIVE-12650.2.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 9888 tests executed\n*Failed tests:*\n{noformat}\nTestSparkCliDriver-groupby3_map.q-sample2.q-auto_join14.q-and-12-more - did not produce a TEST-*.xml file\nTestSparkCliDriver-groupby_map_ppr_multi_distinct.q-table_access_keys_stats.q-groupby4_noskew.q-and-12-more - did not produce a TEST-*.xml file\nTestSparkCliDriver-join_rc.q-insert1.q-vectorized_rcfile_columnar.q-and-12-more - did not produce a TEST-*.xml file\nTestSparkCliDriver-ppd_join4.q-join9.q-ppd_join3.q-and-12-more - did not produce a TEST-*.xml file\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7402/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7402/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7402/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 4 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12795584 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-03-29T14:27:10.409+0000","updated":"2016-03-29T14:27:10.409+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15221049","id":"15221049","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I tried several failed tests locally and they were not reproduced.\n[~xuefuz] would you mind take a look at the patch when you have time? Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-04-01T02:49:54.102+0000","updated":"2016-04-01T02:49:54.102+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15221149","id":"15221149","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"+1. Patch looks good to me. Thanks for working on this, Rui!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-01T04:45:40.640+0000","updated":"2016-04-01T04:45:40.640+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12920958/comment/15221257","id":"15221257","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Committed to master. Thanks Xuefu for the review and guys for the discussions.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-04-01T06:42:41.647+0000","updated":"2016-04-01T06:42:41.647+0000"}],"maxResults":33,"total":33,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-12650/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2pr1z:"}}