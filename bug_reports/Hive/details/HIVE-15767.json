{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13039168","self":"https://issues.apache.org/jira/rest/api/2/issue/13039168","key":"HIVE-15767","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340268","id":"12340268","name":"3.0.0","archived":false,"released":true,"releaseDate":"2018-05-21"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-07-11T14:46:10.464+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue May 22 23:58:12 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_247296_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_16604777971","customfield_12312321":null,"resolutiondate":"2017-08-11T16:52:19.339+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15767/watchers","watchCount":9,"isWatching":false},"created":"2017-01-31T12:21:54.132+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"3.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332384","id":"12332384","name":"1.2.1","archived":false,"released":true,"releaseDate":"2015-06-26"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12335838","id":"12335838","description":"Maintenance branch for 2.1 ","name":"2.1.1","archived":false,"released":true,"releaseDate":"2016-12-08"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-05-22T23:58:12.046+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12323200","id":"12323200","name":"Spark","description":"Hive on Spark"}],"timeoriginalestimate":null,"description":"When a HiveAction is launched form Oozie with Hive On Spark enabled, we're getting errors:\n{noformat}\nCaused by: java.io.IOException: Exception reading file:/yarn/nm/usercache/yshi/appcache/application_1485271416004_0022/container_1485271416004_0022_01_000002/container_tokens\n        at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:188)\n        at org.apache.hadoop.mapreduce.security.TokenCache.mergeBinaryTokens(TokenCache.java:155)\n{noformat}\n\nThis is caused by passing the {{mapreduce.job.credentials.binary}} property to the Spark configuration in RemoteHiveSparkClient.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12877383","id":"12877383","filename":"HIVE-15767.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-14T20:07:25.978+0000","size":1329,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12877383/HIVE-15767.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12850187","id":"12850187","filename":"HIVE-15767-001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-01-31T12:25:53.566+0000","size":1328,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12850187/HIVE-15767-001.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12876635","id":"12876635","filename":"HIVE-15767-002.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-11T14:48:51.271+0000","size":1329,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12876635/HIVE-15767-002.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive On Spark is not working on secure clusters from Oozie","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/15846742","id":"15846742","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"Attaching fix","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-01-31T12:25:53.573+0000","updated":"2017-01-31T12:25:53.573+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16082308","id":"16082308","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"body":"[~gezapeti]: There is a typo in the comment (missing 'n' from locatioN)\n\n[~stakiar], [~xuefuz]: Could you please take a look at this?\n\nThanks,\nPeter","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-11T14:46:10.464+0000","updated":"2017-07-11T14:46:10.464+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16082315","id":"16082315","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"Addressing typo","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-11T14:49:09.506+0000","updated":"2017-07-11T14:49:09.506+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16082411","id":"16082411","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"Overall LGTM. Just a few questions:\n* Are these errors thrown by HiveServer2 or by the HoS Remote Driver?\n* Is that same thing required for Hive-on-MR?\n* Is it possible to add a test for this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-11T15:46:52.786+0000","updated":"2017-07-11T15:46:52.786+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16082948","id":"16082948","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"This happens with HiveCLI, not with HS2. \nThe exception is coming from the spark driver.\n\nWhen the HiveCLI is executed from shell, the mapreduce.job.credentials.binary is empty in the configuration as spark-submit is called from the RemoteClient.\nWhen it's executed from Oozie's LauncherMapper, Hive picks up this property from the Oozie launcher's configuration which is correct, but passes it to Spark. Spark runs in yarn-cluster mode so the Spark driver gets it's own container (which may be on an other machine). It look for the credential files in the folder where the Oozie Launcher ran. That's on a different machine, so it can't pick up the conatiner_tokens file which leaves the spark driver with no tokens so it fails.\n\nI don't know how Hive-on-MR works in this regards, but we had no similar issues with the HiveAction before, so I assume it works differently.\n\nI don't think it's possible to reproduce it using MiniClusters as the local folders will be available in the test so the Spark driver will be able to access it. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-11T20:44:45.990+0000","updated":"2017-07-11T20:44:45.990+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16083374","id":"16083374","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Yibing","name":"Yibing","key":"yibing","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yibing Shi","active":true,"timeZone":"Australia/Sydney"},"body":"[~peterceluch], can the tokens in Oozie launcher application still be passed to Spark job when property {{mapreduce.job.credentials.binary}} is unset? For example, in an environment where HDFS transparent encryption is enabled, is Spark job still able to connect to KMS servers?\n\n(The change is in {{RemoteHiveSparkClient}}. Hive on MR shouldn't be affected. Oozie actions have already make sure the tokens are added to action configuration, which then should be passed to MR jobs).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Yibing","name":"Yibing","key":"yibing","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yibing Shi","active":true,"timeZone":"Australia/Sydney"},"created":"2017-07-12T03:33:31.993+0000","updated":"2017-07-12T03:33:31.993+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16085448","id":"16085448","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"I think the Spark driver will get the tokens afterwards, this property is pointing to an invalid location.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-13T09:36:12.876+0000","updated":"2017-07-13T09:36:12.876+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16085612","id":"16085612","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Yibing","name":"Yibing","key":"yibing","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yibing Shi","active":true,"timeZone":"Australia/Sydney"},"body":"bq. I think the Spark driver will get the tokens afterwards\nI really doubt that Spark driver can do this. In Oozie environment, it is Oozie server that obtains all the tokens *on behalf of the end user*. When the Hive actions starts a Spark job, the Spark driver has no access to end user ticket or keytab file. I don't think it can obtain necessary tokens. \nI believe we should somehow extract all the tokens from existing toke file, and pass it on to the Spark driver.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Yibing","name":"Yibing","key":"yibing","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yibing Shi","active":true,"timeZone":"Australia/Sydney"},"created":"2017-07-13T12:07:47.923+0000","updated":"2017-07-13T12:07:47.923+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16085625","id":"16085625","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"The Spark driver will get the correct tokens from the parent application - it's in the local folder created for it's container. I'm not sure how it get's them, but they are there. \nThe driver will pick it up from the correct container_tokens file using the HADOOP_TOKEN_FILE_LOCATION env variable or something like that. The issue is that Hadoop's TokenCache is looking for the mapreduce.job.credentials.binary property as well, while it's not needed and this invalid reference causes the job to fail.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-13T12:20:49.874+0000","updated":"2017-07-13T12:20:49.874+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16085634","id":"16085634","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Yibing","name":"Yibing","key":"yibing","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yibing Shi","active":true,"timeZone":"Australia/Sydney"},"body":"Thanks for the explanation!\nThis may be done by YARN instead of Spark. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Yibing","name":"Yibing","key":"yibing","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yibing Shi","active":true,"timeZone":"Australia/Sydney"},"created":"2017-07-13T12:28:49.133+0000","updated":"2017-07-13T12:28:49.133+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16087605","id":"16087605","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"[~aihuaxu] may have some input, he knows more about security.\n\nOther than that, LGTM.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-14T17:10:28.538+0000","updated":"2017-07-14T17:10:28.538+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16087611","id":"16087611","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aihuaxu","name":"aihuaxu","key":"aihuaxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aihua Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"[~gezapeti] I will take a look. Can you rename your patch to the format of HIVE-15767.1.patch to kick off the build? Looks like that's the reason why the build is not run.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aihuaxu","name":"aihuaxu","key":"aihuaxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aihua Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-14T17:14:17.849+0000","updated":"2017-07-14T17:14:17.849+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16087957","id":"16087957","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"Thanks for the comments and reviews!\n\nRenaming patch -002 to .1 to kick off pre-commit job. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-14T20:08:20.911+0000","updated":"2017-07-14T20:08:20.911+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16088177","id":"16088177","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12877383/HIVE-15767.1.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 10907 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=238)\norg.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=238)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning] (batchId=167)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_2] (batchId=169)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_explainuser_1] (batchId=168)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_op_stats] (batchId=167)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_ts_stats_for_mapjoin] (batchId=168)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=167)\norg.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=233)\norg.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)\norg.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)\norg.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testHttpRetryOnServerIdleTimeout (batchId=227)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/6041/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/6041/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6041/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 14 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12877383 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-07-14T22:02:23.075+0000","updated":"2017-07-14T22:02:23.075+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16090171","id":"16090171","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aihuaxu","name":"aihuaxu","key":"aihuaxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aihua Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"[~gezapeti] Logically seems it's correct to set proper mapreduce.job.credentials.binary and pass to Spark. And MR is also doing the same thing. Can you find out why it makes the difference when oozie calls HiveCLI MR vs. Spark actions? \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aihuaxu","name":"aihuaxu","key":"aihuaxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aihua Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-17T17:52:30.024+0000","updated":"2017-07-17T17:52:30.024+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16103843","id":"16103843","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"The problem is that we're not setting the _proper_ mapreduce.job.credentials.binary, but [here|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java#L235], were passing every property from the HiveConf conf to the configuration for Spark.\nIf HiveCLI is called from the Oozie LauncherMapper, that HiveConf will contain the \"mapreduce.job.credentials.binary\" property for the LauncherMapper. e.g /yarn/nm/usercache/systest/appcache/application_1501079366372_0045/container_1501079366372_0045_01_000001/container_tokens\nThis property have to be there so HiveCLI can access the tokens properly.\n\nPassing this folder to the Spark driver is problematic as the driver often will be executed on an other machine in the cluster where it won't be able to read this file as it's not there. There are a couple ways to define the location of the container_tokens file and Yarn takes care of Spark getting the correct location on the node the driver will be executed on.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-07-27T20:25:27.921+0000","updated":"2017-07-27T20:25:27.921+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16117614","id":"16117614","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"From what I see, the patch seems logical, harmless at least. What I don't understand is that why Spark would attempt reading this file. As a side note, I didn't find the source in Spark code base that does this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-08-08T00:28:18.532+0000","updated":"2017-08-08T00:28:18.532+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16117956","id":"16117956","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"I don't remember all the details, but here is a longer stack trace:{code}\njava.lang.RuntimeException: java.io.IOException: Exception reading file:/yarn/nm/usercache/yshi/appcache/application_1485271416004_0001/container_1485271416004_0001_01_000002/container_tokens\n\tat org.apache.hadoop.mapreduce.security.TokenCache.mergeBinaryTokens(TokenCache.java:160)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:138)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)\n\tat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:243)\n{code}\nSpark does not refer the {{mapreduce.job.credentials.binary}} directly, it is hard-coded in Hadoop's TokenCache [here|https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java#L148]. I think this TokenCache is used by Hadoop's FileSystem implementation too so when Spark talks to HDFS it does through this class.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2017-08-08T07:12:01.502+0000","updated":"2017-08-08T07:12:01.502+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16118589","id":"16118589","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"+1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-08-08T16:35:10.230+0000","updated":"2017-08-08T16:35:10.230+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16123621","id":"16123621","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"body":"Pushed to master.\nThanks [~gezapeti] for your contribution!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pvary","name":"pvary","key":"pvary","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Vary","active":true,"timeZone":"Europe/Budapest"},"created":"2017-08-11T16:52:19.386+0000","updated":"2017-08-11T16:52:19.386+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16449392","id":"16449392","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linwukang","name":"linwukang","key":"linwukang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"linwukang","active":true,"timeZone":"Asia/Chongqing"},"body":"Hi [~gezapeti] , after apply this patch, i find that the Hive On Spark worked with yarn, all tasks is finished successfully. but there's another error throws at the end of the progress:\r\n\r\n \r\n{code:java}\r\n2018-04-24T14:28:46,409 INFO [116dbf89-2982-407d-9b64-4206b3bbe105 main] lockmgr.DbTxnManager: Stopped heartbeat for query: flowagent_20180424142839_be68e2b9-aca9-4023-89f8-6a18d53dd0c5\r\n2018-04-24T14:28:46,409 INFO [116dbf89-2982-407d-9b64-4206b3bbe105 main] lockmgr.DbLockManager: releaseLocks: [lockid:438 queryId=flowagent_20180424142839_be68e2b9-aca9-4023-89f8-6a18d53dd0c5 txnid:0]\r\n2018-04-24T14:28:46,422 ERROR [116dbf89-2982-407d-9b64-4206b3bbe105 main] CliDriver: Failed with exception java.io.IOException:org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication\r\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6635)\r\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:563)\r\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:988)\r\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1727)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\r\n\r\njava.io.IOException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication\r\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6635)\r\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:563)\r\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:988)\r\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1727)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\r\n\r\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:521)\r\nat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:428)\r\nat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:147)\r\nat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2208)\r\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:253)\r\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)\r\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)\r\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)\r\nat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)\r\nat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)\r\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)\r\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linwukang","name":"linwukang","key":"linwukang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"linwukang","active":true,"timeZone":"Asia/Chongqing"},"created":"2018-04-24T06:44:26.491+0000","updated":"2018-04-24T06:44:26.491+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16450073","id":"16450073","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"body":"[~linwukang], we haven't seen this exception after the fix.\r\nThis might have to do something in regards the command you're executing or the time frame where the job runs. Without knowing those it's hard to give meaningful suggestions.\r\nCan you check the expiry date for the HDFS token in the job? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gezapeti","name":"gezapeti","key":"gezapeti","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34050","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"},"displayName":"Peter Cseh","active":true,"timeZone":"Europe/Budapest"},"created":"2018-04-24T15:38:27.070+0000","updated":"2018-04-24T15:38:27.070+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13039168/comment/16485906","id":"16485906","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgarg","name":"vgarg","key":"vgarg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=vgarg&avatarId=30430","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=vgarg&avatarId=30430","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=vgarg&avatarId=30430","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=vgarg&avatarId=30430"},"displayName":"Vineet Garg","active":true,"timeZone":"America/Los_Angeles"},"body":"Hive 3.0.0 has been released so closing this jira.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgarg","name":"vgarg","key":"vgarg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=vgarg&avatarId=30430","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=vgarg&avatarId=30430","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=vgarg&avatarId=30430","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=vgarg&avatarId=30430"},"displayName":"Vineet Garg","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-22T23:58:12.038+0000","updated":"2018-05-22T23:58:12.038+0000"}],"maxResults":23,"total":23,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15767/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i39eyn:"}}