{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12737919","self":"https://issues.apache.org/jira/rest/api/2/issue/12737919","key":"HIVE-7916","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2014-09-01T05:37:55.317+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Oct 24 19:08:16 UTC 2014","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_4826449909_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-10-24T17:59:50.488+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-7916/watchers","watchCount":4,"isWatching":false},"created":"2014-08-29T21:19:00.637+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["Spark-M1"],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12395531","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12395531","type":{"id":"12310010","name":"Incorporates","inward":"is part of","outward":"incorporates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310010"},"inwardIssue":{"id":"12723734","key":"HIVE-7292","self":"https://issues.apache.org/jira/rest/api/2/issue/12723734","fields":{"summary":"Hive on Spark","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12395592","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12395592","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12732338","key":"SPARK-2881","self":"https://issues.apache.org/jira/rest/api/2/issue/12732338","fields":{"summary":"Snappy is now default codec - could lead to conflicts since uses /tmp","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-10-24T19:08:16.195+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12323200","id":"12323200","name":"Spark","description":"Hive on Spark"}],"timeoriginalestimate":null,"description":"Recently spark branch upgraded its dependency on Spark to 1.1.0-SNAPSHOT. While the new version addressed some lib conflicts (such as guava), I'm afraid that it also introduced new problems. The following might be one, when I set the master URL to be a spark standalone cluster:\n\n{code}\nhive> set hive.execution.engine=spark;\nhive> set spark.serializer=org.apache.spark.serializer.KryoSerializer;\nhive> set spark.master=spark://xzdt:7077;\nhive> select name, avg(value) from dec group by name;\n\n14/08/28 16:41:52 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 333.0 KB, free 128.0 MB)\njava.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:601)\n    at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:317)\n    at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:219)\n    at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)\n    at org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)\n    at org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:124)\n    at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:207)\n    at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:83)\n    at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:68)\n    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)\n    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)\n    at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)\n    at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)\n    at org.apache.spark.rdd.HadoopRDD.<init>(HadoopRDD.scala:116)\n    at org.apache.spark.SparkContext.hadoopRDD(SparkContext.scala:541)\n    at org.apache.spark.api.java.JavaSparkContext.hadoopRDD(JavaSparkContext.scala:318)\n    at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateRDD(SparkPlanGenerator.java:160)\n    at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate(SparkPlanGenerator.java:88)\n    at org.apache.hadoop.hive.ql.exec.spark.SparkClient.execute(SparkClient.java:156)\n    at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.submit(SparkSessionImpl.java:52)\n    at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:77)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1537)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1304)\n    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1116)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:940)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:930)\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)\n    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)\n    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:601)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path\n    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)\n    at java.lang.Runtime.loadLibrary0(Runtime.java:845)\n    at java.lang.System.loadLibrary(System.java:1084)\n    at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)\n    ... 42 more\norg.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null\n    at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)\n    at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)\n    at org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)\n    at org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:124)\n    at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:207)\n    at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:83)\n    at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:68)\n    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)\n    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)\n    at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)\n    at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)\n    at org.apache.spark.rdd.HadoopRDD.<init>(HadoopRDD.scala:116)\n    at org.apache.spark.SparkContext.hadoopRDD(SparkContext.scala:541)\n    at org.apache.spark.api.java.JavaSparkContext.hadoopRDD(JavaSparkContext.scala:318)\n    at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateRDD(SparkPlanGenerator.java:160)\n    at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate(SparkPlanGenerator.java:88)\n    at org.apache.hadoop.hive.ql.exec.spark.SparkClient.execute(SparkClient.java:156)\n    at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.submit(SparkSessionImpl.java:52)\n    at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:77)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1537)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1304)\n    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1116)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:940)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:930)\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)\n    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)\n    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:601)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nFAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. [FAILED_TO_LOAD_NATIVE_LIBRARY] null\n14/08/28 16:41:52 ERROR ql.Driver: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. [FAILED_TO_LOAD_NATIVE_LIBRARY] null\n{code}\n\nIt could be a setup issue. Nevertheless, we need to take a look to be sure.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Snappy-java error when running hive query on spark [Spark Branch]","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14117050","id":"14117050","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~xuefuz], I tried on my cluster but cannot reproduce the problem. I removed the spark jars from local maven repo before building hive, so that the jars are downloaded from the AWS server we maintain. After hive is built, I linked the spark-assembly jar to {{lib}} of the hive home directory. The spark-assembly jar is built with {{mvn -Pyarn -Phadoop-2.4 -DskipTests clean package}} of the spark 1.1 branch.\nCould you provide more info about your environment, e.g. the spark jars you used or if the table is snappy compressed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2014-09-01T05:37:55.317+0000","updated":"2014-09-01T05:37:55.317+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14117057","id":"14117057","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I noted this may be related to SPARK-2881. Snappy-java is bumped to 1.0.5.3 in the 1.1 branch and to 1.1.1.3 in the master branch. Hadoop-2.4.0 seems to use snappy-java-1.0.4.1.\nWhile the snappy-java version is different, I don't see any conflicts on my side.\n[~xuefuz], I found the following in the description of SPARK-2881:\n{quote}\nThe issue was that someone else had run with snappy and it created /tmp/snappy-*.so but it had restrictive permissions so I was not able to use it or remove it. This caused my spark job to not start.\n{quote}\nCould you check if this is the case in your environment?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2014-09-01T05:49:31.708+0000","updated":"2014-09-01T05:49:31.708+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14117872","id":"14117872","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~lirui] Thanks for looking into this. I had noticed the version difference, and thought for a moment it was the cause. However, I don't see any /tmp/snappy*.so file in my machine. I also built spark-assembly myself, but with slightly different params:\n{code}\nmvn -Dhadoop.version=2.3.0-cdh5.0.1 -Phadoop-2.3 -DskipTests clean install.\n{code}\n\nAslo, I'm not sure if my machine has ever run snappy before. Is there anything that needs to be done in order for it to work?\n\nI can try again to reproduce the problem.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-09-02T04:21:31.233+0000","updated":"2014-09-02T04:21:31.233+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14117911","id":"14117911","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~xuefuz], do you use the latest code of spark 1.1 branch? SPARK-2881 is resolved with [#1999|https://github.com/apache/spark/pull/1999] for branch-1.1. You can check the pom file of your spark to verify that, latest code uses snappy-java-1.0.5.3.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2014-09-02T05:39:07.573+0000","updated":"2014-09-02T05:39:07.573+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14119247","id":"14119247","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~lirui], I tried both branch 1.1 and master. Spark 1.1 branch doesn't have the guava lib fix, right? So, using that branch, I hit the guava lib conflict. Using master gives the snappy error.\n\nI'm thinking if you can take a look at the guide at https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started#HiveonSpark:GettingStarted-HiveonSpark:GettingStarted and update it if necessary. Then, I can try to follow the guide to see if I still get the snappy problem or guava problem.\n\nThanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-09-03T03:28:45.552+0000","updated":"2014-09-03T03:28:45.552+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14175712","id":"14175712","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ssatish","name":"ssatish","key":"ssatish","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Suhas Satish","active":true,"timeZone":"America/Los_Angeles"},"body":"I also hit the following snappy lib exceptions - I am using snappy snappy-java-1.0.5.jar. Let me try upgrading to snappy 1.1.1.3\n\n2014-10-17 16:18:01,977 ERROR [Executor task launch worker-0]: executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 0.0 (TID 0)\norg.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null\n\tat org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)\n\tat org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)\n\tat org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)\n\tat org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:125)\n\tat org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager$$anonfun$7.apply(BlockManager.scala:579)\n\tat org.apache.spark.storage.BlockManager$$anonfun$7.apply(BlockManager.scala:579)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:126)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:192)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4$$anonfun$apply$2.apply(ExternalSorter.scala:732)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4$$anonfun$apply$2.apply(ExternalSorter.scala:731)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:789)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4.apply(ExternalSorter.scala:731)\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4.apply(ExternalSorter.scala:727)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:727)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n----------\n2014-10-17 16:18:02,021 INFO  [main]: scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 0 failed: foreach at SparkPlan.java:80, took 3.389683 s\n2014-10-17 16:18:02,021 ERROR [main]: spark.SparkClient (SparkClient.java:execute(166)) - Error executing Spark Plan\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null\n        org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)\n        org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)\n        org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)\n        org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:125)\n        org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1083)\n        org.apache.spark.storage.BlockManager$$anonfun$7.apply(BlockManager.scala:579)\n        org.apache.spark.storage.BlockManager$$anonfun$7.apply(BlockManager.scala:579)\n        org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:126)\n        org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:192)\n        org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4$$anonfun$apply$2.apply(ExternalSorter.scala:732)\n        org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4$$anonfun$apply$2.apply(ExternalSorter.scala:731)\n        scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:789)\n        org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4.apply(ExternalSorter.scala:731)\n        org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4.apply(ExternalSorter.scala:727)\n        scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:727)\n        org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:70)\n        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n        org.apache.spark.scheduler.Task.run(Task.scala:56)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        java.lang.Thread.run(Thread.java:745)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1180)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1179)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1179)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:694)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:694)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:694)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1397)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ssatish","name":"ssatish","key":"ssatish","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Suhas Satish","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-17T23:47:06.513+0000","updated":"2014-10-17T23:47:06.513+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14175771","id":"14175771","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ssatish","name":"ssatish","key":"ssatish","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Suhas Satish","active":true,"timeZone":"America/Los_Angeles"},"body":"Hitting the same problem with snappy 1.1.1.3 as well. Using hive tar ball as of today (fri, oct 17, 2014) with spark.master=local\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ssatish","name":"ssatish","key":"ssatish","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Suhas Satish","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-18T01:11:01.119+0000","updated":"2014-10-18T01:11:01.119+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14183152","id":"14183152","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"With latest Spark-Hive integration, the problem seems disappeared. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-24T17:59:50.521+0000","updated":"2014-10-24T17:59:50.521+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14183290","id":"14183290","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ssatish","name":"ssatish","key":"ssatish","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Suhas Satish","active":true,"timeZone":"America/Los_Angeles"},"body":"Not sure what solved it for you, but setting this seems to work for me on a Mac OS X -\nexport HADOOP_OPTS=\"-Dorg.xerial.snappy.tempdir=/tmp -Dorg.xerial.snappy.lib.name=libsnappyjava.jnilib $HADOOP_OPTS\"\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ssatish","name":"ssatish","key":"ssatish","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Suhas Satish","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-24T19:02:19.957+0000","updated":"2014-10-24T19:02:19.957+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737919/comment/14183309","id":"14183309","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Yeah. I'm on Ubuntu, and I didn't set up anything to HADOOP_OPTS. It just magically works now. :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-24T19:08:16.195+0000","updated":"2014-10-24T19:08:16.195+0000"}],"maxResults":10,"total":10,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-7916/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1zi8n:"}}