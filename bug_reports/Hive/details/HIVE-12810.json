{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12928591","self":"https://issues.apache.org/jira/rest/api/2/issue/12928591","key":"HIVE-12810","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-01-08T09:43:17.736+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Jan 18 12:18:03 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-12810/watchers","watchCount":3,"isWatching":false},"created":"2016-01-08T09:26:50.316+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332384","id":"12332384","name":"1.2.1","archived":false,"released":true,"releaseDate":"2015-06-26"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-01-18T12:18:03.704+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12324409","id":"12324409","name":"Beeline"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12313604","id":"12313604","name":"CLI","description":"Command-line interpreter for Hive.\n"}],"timeoriginalestimate":null,"description":"Hadoop HDP 2.3 (Hadoop 2.7.1.2.3.0.0-2557)\nHive 1.2.1.2.3.0.0-2557\n\nWe are loading orc tables in hive with sqoop from hana db.\n\nEverything works fine, count and select with ie. 16.000.000 entries in the table, but when we load 34.000.000 entries query select does not work anymore and we get the followong error (select count(*) is working in both cases):\n\n{code}\nselect count(*) from tablename;\nINFO  : Session is already open\nINFO  :\n\nINFO  : Status: Running (Executing on YARN cluster with App id application_1452091205505_0032)\n\nINFO  : Map 1: -/-      Reducer 2: 0/1\nINFO  : Map 1: 0/96     Reducer 2: 0/1\n.\n.\n.\nINFO  : Map 1: 96/96    Reducer 2: 0(+1)/1\nINFO  : Map 1: 96/96    Reducer 2: 1/1\n+-----------+--+\n|    _c0    |\n+-----------+--+\n| 34146816  |\n+-----------+--+\n1 row selected (45.455 seconds)\n\n{code}\n\n{code}\n\"select originalxml from tablename where messageid = 'd0b3c872-435d-499b-a65c-619d9e732bbb'\n\n0: jdbc:hive2://10.4.zz.xx:10000/default> select originalxml from tablename where messageid = 'd0b3c872-435d-499b-a65c-619d9e732bbb';\nINFO  : Session is already open\nINFO  : Tez session was closed. Reopening...\nINFO  : Session re-established.\nINFO  :\n\nINFO  : Status: Running (Executing on YARN cluster with App id application_1452091205505_0032)\n\nINFO  : Map 1: -/-\nERROR : Status: Failed\nERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1452091205505_0032_1_00, diagnostics=[Vertex vertex_1452091205505_0032_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: tablename initializer failed, vertex=vertex_1452091205505_0032_1_00 [Map 1], java.lang.RuntimeException: serious problem\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:306)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:408)\n        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:245)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:239)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:239)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:226)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1016)\n        ... 15 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.Collections$EmptyList.get(Collections.java:4454)\n        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:649)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:632)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:927)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        ... 4 more\n]\nERROR : DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0\nError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1452091205505_0032_1_00, diagnostics=[Vertex vertex_1452091205505_0032_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: tablename initializer failed, vertex=vertex_1452091205505_0032_1_00 [Map 1], java.lang.RuntimeException: serious problem\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:306)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:408)\n        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:245)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:239)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:239)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:226)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1016)\n        ... 15 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.Collections$EmptyList.get(Collections.java:4454)\n        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:649)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:632)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:927)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        ... 4 more\n]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0 (state=08S01,code=2)\n0: jdbc:hive2://10.4.zz.xx:10000/default>\n{code}\n\nIf anybody can help regarding this issue I will appreciate.\n\nthanks,\nmaske\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive select fails - java.lang.IndexOutOfBoundsException","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"HDP 2.3.0","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15088973","id":"15088973","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"[~maske]: is the table marked as \"transactional\"=\"true\" in the describe?\n\nIf it is, this might be a dupe of HIVE-11102","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-08T09:43:17.736+0000","updated":"2016-01-08T09:43:17.736+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089031","id":"15089031","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"How can I see that?\n\ndescribe formatted tablename;\n.\n.\nTable Type:                   | MANAGED_TABLE\nTable Parameters:             | NULL    \norc.compress \ntransient_lastDdlTime\n.\n.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T10:31:36.803+0000","updated":"2016-01-08T10:31:36.803+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089040","id":"15089040","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"The word \"transactional\" should be in the Table parameters?\n\nAn alternate way is that you can do a \"dfs -ls \" of the dir to find that as well. The ACID compliant tables have different naming conventions for the files inside it.\n\nThe other bug reported was shipped in 2.3.2 release.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-08T10:40:47.348+0000","updated":"2016-01-08T10:40:47.348+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089065","id":"15089065","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"Gopal,\nI do not find \"transactional\" anywhere on suggested places.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T11:04:03.915+0000","updated":"2016-01-08T11:04:03.915+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089089","id":"15089089","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"I also have two other tables where select is working fine.\nIt is also impossible to print last 5 rows for example on this table, on other tables no problem.\nThe problem started when we loaded 34mio of records in that table, with 16mio there was no problem.\n\n0: jdbc:hive2://10.4.zz.xx:10000/default> select * from tablename limit 5;\nError: java.io.IOException: java.lang.RuntimeException: serious problem (state=,code=0)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T11:39:35.172+0000","updated":"2016-01-08T11:39:35.172+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089091","id":"15089091","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"You need to post a {{dfs -ls}} on the dir, because this doesn't make sense right now.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-08T11:43:40.120+0000","updated":"2016-01-08T11:43:40.120+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089093","id":"15089093","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"[hdfs@hdp-node1 ~]$ hdfs dfs -ls /apps/hive/warehouse/archive.db/tablename\nFound 36 items\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00000\n-rw-rw-rw-   3 hdfs hdfs 1708846898 2016-01-07 22:48 /apps/hive/warehouse/archive.db/tablename/part-m-00001\n-rw-rw-rw-   3 hdfs hdfs 1707780193 2016-01-07 22:53 /apps/hive/warehouse/archive.db/tablename/part-m-00002\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00003\n-rw-rw-rw-   3 hdfs hdfs 1708664903 2016-01-07 22:52 /apps/hive/warehouse/archive.db/tablename/part-m-00004\n-rw-rw-rw-   3 hdfs hdfs 1707965289 2016-01-07 22:46 /apps/hive/warehouse/archive.db/tablename/part-m-00005\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00006\n-rw-rw-rw-   3 hdfs hdfs 1706666181 2016-01-07 22:46 /apps/hive/warehouse/archive.db/tablename/part-m-00007\n-rw-rw-rw-   3 hdfs hdfs 1707111247 2016-01-07 22:46 /apps/hive/warehouse/archive.db/tablename/part-m-00008\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00009\n-rw-rw-rw-   3 hdfs hdfs 1708573731 2016-01-07 22:48 /apps/hive/warehouse/archive.db/tablename/part-m-00010\n-rw-rw-rw-   3 hdfs hdfs 1706909984 2016-01-07 22:51 /apps/hive/warehouse/archive.db/tablename/part-m-00011\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00012\n-rw-rw-rw-   3 hdfs hdfs 1706116939 2016-01-07 22:53 /apps/hive/warehouse/archive.db/tablename/part-m-00013\n-rw-rw-rw-   3 hdfs hdfs 1708793297 2016-01-07 22:46 /apps/hive/warehouse/archive.db/tablename/part-m-00014\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00015\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00016\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00017\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00018\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00019\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00020\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00021\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00022\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00023\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00024\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00025\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00026\n-rw-rw-rw-   3 hdfs hdfs 1708762502 2016-01-07 22:54 /apps/hive/warehouse/archive.db/tablename/part-m-00027\n-rw-rw-rw-   3 hdfs hdfs 1706241325 2016-01-07 22:46 /apps/hive/warehouse/archive.db/tablename/part-m-00028\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:16 /apps/hive/warehouse/archive.db/tablename/part-m-00029\n-rw-rw-rw-   3 hdfs hdfs 1706505399 2016-01-07 22:46 /apps/hive/warehouse/archive.db/tablename/part-m-00030\n-rw-rw-rw-   3 hdfs hdfs 1710595521 2016-01-07 22:51 /apps/hive/warehouse/archive.db/tablename/part-m-00031\n-rw-rw-rw-   3 hdfs hdfs         49 2016-01-07 21:17 /apps/hive/warehouse/archive.db/tablename/part-m-00032\n-rw-rw-rw-   3 hdfs hdfs 1708213829 2016-01-07 22:48 /apps/hive/warehouse/archive.db/tablename/part-m-00033\n-rw-rw-rw-   3 hdfs hdfs 1706986531 2016-01-07 22:52 /apps/hive/warehouse/archive.db/tablename/part-m-00034\n-rw-rw-rw-   3 hdfs hdfs      11425 2016-01-07 21:17 /apps/hive/warehouse/archive.db/tablename/part-m-00035","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T11:47:48.974+0000","updated":"2016-01-08T11:47:48.974+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089110","id":"15089110","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"That looks like it is not using ACID schemas.\n\nYou can attempt the following as a work around to the problem.\n\n{code}\nset hive.execution.engine=mr;\n\ninsert overwrite tablename as select * from tablename sort by messageid;\n{code}\n\nThat should reload the table organized by messageid by reading via the legacy input format (org.apache.hadoop.hive.ql.io.CombineHiveInputFormat) & get to a file layout which is produced by the hive's writer (instead of SQOOP's writer).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-08T12:02:16.700+0000","updated":"2016-01-08T12:02:16.700+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089112","id":"15089112","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"And once it is reloaded, you can revert to using Tez to run queries on the same.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-08T12:02:54.230+0000","updated":"2016-01-08T12:02:54.230+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089142","id":"15089142","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"I'm concerned about this procedure since we are just in the beginning of loadnig few meilion of entries every day to this table...it should work as expected not with workarounds.\n\nWhy do you think, select is working with 16 milion of entries and does not work with 34 milion of entries?\nIt is some kind of limitin parameter maybe to be set?\n\nIf I look into aplication log I can  see:\nfirst few warnings: \nWARN [ORC_GET_SPLITS #0] ipc.Client: interrupted waiting to send rpc request to server\n\nand later on error:\n\nERROR [Dispatcher thread: Central] impl.VertexImpl: Vertex Input: tablename initializer failed, vertex=vertex_1452091205505_0030_1_00 [Map 1]\norg.apache.tez.dag.app.dag.impl.AMUserCodeException: java.lang.RuntimeException: serious problem\n\nand java exception:\n\njava.lang.IndexOutOfBoundsException: Index:\n\n----------\n#hdfs dfs -ls /apps/hive/warehouse/archive.db.db/tablename/part-m-00035\n\nContainer: container_e09_1452091205505_0030_01_000001 on hdp-node3.something.com_45454\n===============================================================================\nLogType:dag_1452091205505_0030_1.dot\nLog Upload Time:Fri Jan 08 09:37:16 +0100 2016\nLogLength:1102\nLog Contents:\ndigraph hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32 {\ngraph [ label=\"hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32\", fontsize=24, fontname=Helvetica];\nnode [fontsize=12, fontname=Helvetica];\nedge [fontsize=9, fontcolor=blue, fontname=Arial];\n\"hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32.Map_1\" [ label = \"Map_1[MapTezProcessor]\" ];\n\"hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32.Map_1\" -> \"hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32.Map_1_out_Map_1\" [ label = \"Output [outputClass=MROutput,\\n committer=]\" ];\n\"hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32.Map_1_tablename\" [ label = \"Map_1[tablename]\", shape = \"box\" ];\n\"hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32.Map_1_tablename\" -> \"hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32.Map_1\" [ label = \"Input [inputClass=MRInputLegacy,\\n initializer=HiveSplitGenerator]\" ];\n\"hive_20160108092603_9b21f8a6_0a4a_45aa_9006_2e88328cd361_32.Map_1_out_Map_1\" [ label = \"Map_1[out_Map_1]\", shape = \"box\" ];\n}\nEnd of LogType:dag_1452091205505_0030_1.dot\n\nLogType:stderr\nLog Upload Time:Fri Jan 08 09:37:16 +0100 2016\nLogLength:118\nLog Contents:\n2016-01-08 09:26:10 Running Dag: dag_1452091205505_0030_1\n2016-01-08 09:26:12 Completed Dag: dag_1452091205505_0030_1\nEnd of LogType:stderr\n\nLogType:stdout\nLog Upload Time:Fri Jan 08 09:37:16 +0100 2016\nLogLength:8503\nLog Contents:\n0.839: [GC pause (G1 Evacuation Pause) (young), 0.0122097 secs]\n   [Parallel Time: 7.9 ms, GC Workers: 4]\n      [GC Worker Start (ms): Min: 838.9, Avg: 839.0, Max: 839.1, Diff: 0.2]\n      [Ext Root Scanning (ms): Min: 0.0, Avg: 2.2, Max: 4.1, Diff: 4.1, Sum: 8.7]\n      [Update RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]\n         [Processed Buffers: Min: 0, Avg: 0.0, Max: 0, Diff: 0, Sum: 0]\n      [Scan RS (ms): Min: 0.0, Avg: 0.0, Max: 0.1, Diff: 0.1, Sum: 0.2]\n      [Code Root Scanning (ms): Min: 0.0, Avg: 0.2, Max: 0.7, Diff: 0.7, Sum: 0.7]\n      [Object Copy (ms): Min: 0.0, Avg: 2.8, Max: 4.4, Diff: 4.4, Sum: 11.4]\n      [Termination (ms): Min: 0.0, Avg: 0.3, Max: 0.4, Diff: 0.4, Sum: 1.0]\n      [GC Worker Other (ms): Min: 0.0, Avg: 2.1, Max: 7.5, Diff: 7.5, Sum: 8.6]\n      [GC Worker Total (ms): Min: 7.5, Avg: 7.6, Max: 7.8, Diff: 0.2, Sum: 30.6]\n      [GC Worker End (ms): Min: 846.7, Avg: 846.7, Max: 846.7, Diff: 0.0]\n   [Code Root Fixup: 0.2 ms]\n   [Code Root Purge: 0.0 ms]\n   [Clear CT: 0.4 ms]\n   [Other: 3.6 ms]\n      [Choose CSet: 0.0 ms]\n      [Ref Proc: 2.6 ms]\n      [Ref Enq: 0.0 ms]\n      [Redirty Cards: 0.6 ms]\n      [Humongous Reclaim: 0.0 ms]\n      [Free CSet: 0.1 ms]\n   [Eden: 18.0M(18.0M)->0.0B(23.0M) Survivors: 0.0B->3072.0K Heap: 18.0M(368.0M)->2914.3K(368.0M)]\n [Times: user=0.02 sys=0.00, real=0.01 secs]\n1.503: [GC pause (G1 Evacuation Pause) (young), 0.0107880 secs]\n   [Parallel Time: 6.9 ms, GC Workers: 4]\n      [GC Worker Start (ms): Min: 1502.7, Avg: 1502.8, Max: 1502.9, Diff: 0.2]\n      [Ext Root Scanning (ms): Min: 0.5, Avg: 1.3, Max: 1.8, Diff: 1.3, Sum: 5.4]\n      [Update RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]\n         [Processed Buffers: Min: 0, Avg: 0.0, Max: 0, Diff: 0, Sum: 0]\n      [Scan RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]\n      [Code Root Scanning (ms): Min: 0.0, Avg: 1.2, Max: 3.1, Diff: 3.1, Sum: 4.8]\n      [Object Copy (ms): Min: 2.1, Avg: 4.0, Max: 4.8, Diff: 2.8, Sum: 16.2]\n      [Termination (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 0.6]\n      [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]\n      [GC Worker Total (ms): Min: 6.7, Avg: 6.8, Max: 6.9, Diff: 0.2, Sum: 27.1]\n      [GC Worker End (ms): Min: 1509.6, Avg: 1509.6, Max: 1509.6, Diff: 0.0]\n   [Code Root Fixup: 0.4 ms]\n   [Code Root Purge: 0.0 ms]\n   [Clear CT: 0.4 ms]\n   [Other: 3.0 ms]\n      [Choose CSet: 0.0 ms]\n      [Ref Proc: 2.0 ms]\n      [Ref Enq: 0.1 ms]\n      [Redirty Cards: 0.5 ms]\n      [Humongous Reclaim: 0.0 ms]\n      [Free CSet: 0.1 ms]\n   [Eden: 23.0M(23.0M)->0.0B(216.0M) Survivors: 3072.0K->4096.0K Heap: 25.8M(368.0M)->5120.0K(368.0M)]\n [Times: user=0.03 sys=0.00, real=0.01 secs]\n3.070: [GC pause (Metadata GC Threshold) (young) (initial-mark), 0.0228356 secs]\n   [Parallel Time: 10.6 ms, GC Workers: 4]\n      [GC Worker Start (ms): Min: 3070.7, Avg: 3071.2, Max: 3072.8, Diff: 2.1]\n      [Ext Root Scanning (ms): Min: 0.1, Avg: 1.7, Max: 2.5, Diff: 2.4, Sum: 6.9]\n      [Update RS (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 0.3]\n         [Processed Buffers: Min: 0, Avg: 0.2, Max: 1, Diff: 1, Sum: 1]\n      [Scan RS (ms): Min: 0.0, Avg: 0.1, Max: 0.1, Diff: 0.1, Sum: 0.2]\n      [Code Root Scanning (ms): Min: 0.0, Avg: 1.1, Max: 2.2, Diff: 2.2, Sum: 4.4]\n      [Object Copy (ms): Min: 5.7, Avg: 6.9, Max: 7.8, Diff: 2.1, Sum: 27.5]\n      [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]\n      [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]\n      [GC Worker Total (ms): Min: 8.3, Avg: 9.9, Max: 10.4, Diff: 2.1, Sum: 39.5]\n      [GC Worker End (ms): Min: 3081.1, Avg: 3081.1, Max: 3081.1, Diff: 0.0]\n   [Code Root Fixup: 0.8 ms]\n   [Code Root Purge: 0.0 ms]\n   [Clear CT: 0.6 ms]\n   [Other: 10.9 ms]\n      [Choose CSet: 0.0 ms]\n      [Ref Proc: 9.4 ms]\n      [Ref Enq: 0.1 ms]\n      [Redirty Cards: 0.5 ms]\n      [Humongous Reclaim: 0.0 ms]\n      [Free CSet: 0.4 ms]\n   [Eden: 97.0M(216.0M)->0.0B(204.0M) Survivors: 4096.0K->16.0M Heap: 102.0M(368.0M)->16.5M(368.0M)]\n [Times: user=0.04 sys=0.01, real=0.03 secs]\n3.094: [GC concurrent-root-region-scan-start]\n3.106: [GC concurrent-root-region-scan-end, 0.0122353 secs]\n3.106: [GC concurrent-mark-start]\n3.107: [GC concurrent-mark-end, 0.0008803 secs]\n3.107: [GC remark 3.108: [Finalize Marking, 0.0010414 secs] 3.109: [GC ref-proc, 0.0001344 secs] 3.109: [Unloading, 0.0099256 secs], 0.0116719 secs]\n [Times: user=0.02 sys=0.00, real=0.01 secs]\n3.120: [GC cleanup 17M->15M(368M), 0.0033051 secs]\n [Times: user=0.00 sys=0.00, real=0.01 secs]\n3.123: [GC concurrent-cleanup-start]\n3.123: [GC concurrent-cleanup-end, 0.0000432 secs]\n5.768: [GC pause (G1 Evacuation Pause) (young), 0.0419872 secs]\n   [Parallel Time: 32.1 ms, GC Workers: 4]\n      [GC Worker Start (ms): Min: 5768.7, Avg: 5774.6, Max: 5776.9, Diff: 8.2]\n      [Ext Root Scanning (ms): Min: 2.2, Avg: 7.9, Max: 17.6, Diff: 15.4, Sum: 31.5]\n      [Update RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]\n         [Processed Buffers: Min: 0, Avg: 0.2, Max: 1, Diff: 1, Sum: 1]\n      [Scan RS (ms): Min: 0.0, Avg: 0.2, Max: 0.4, Diff: 0.4, Sum: 0.7]\n      [Code Root Scanning (ms): Min: 0.0, Avg: 1.3, Max: 2.6, Diff: 2.6, Sum: 5.0]\n      [Object Copy (ms): Min: 14.2, Avg: 16.6, Max: 18.9, Diff: 4.7, Sum: 66.4]\n      [Termination (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 0.5]\n      [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.1, Diff: 0.0, Sum: 0.1]\n      [GC Worker Total (ms): Min: 23.8, Avg: 26.1, Max: 32.0, Diff: 8.2, Sum: 104.2]\n      [GC Worker End (ms): Min: 5800.7, Avg: 5800.7, Max: 5800.7, Diff: 0.0]\n   [Code Root Fixup: 0.9 ms]\n   [Code Root Purge: 0.0 ms]\n   [Clear CT: 0.2 ms]\n   [Other: 8.8 ms]\n      [Choose CSet: 0.0 ms]\n      [Ref Proc: 7.8 ms]\n      [Ref Enq: 0.1 ms]\n      [Redirty Cards: 0.2 ms]\n      [Humongous Reclaim: 0.0 ms]\n      [Free CSet: 0.3 ms]\n   [Eden: 204.0M(204.0M)->0.0B(200.0M) Survivors: 16.0M->20.0M Heap: 219.5M(368.0M)->20.0M(368.0M)]\n [Times: user=0.11 sys=0.01, real=0.04 secs]\n2016-01-08 09:26:10 Running Dag: dag_1452091205505_0030_1\n6.426: [GC pause (Metadata GC Threshold) (young) (initial-mark), 0.0310678 secs]\n   [Parallel Time: 24.8 ms, GC Workers: 4]\n      [GC Worker Start (ms): Min: 6426.8, Avg: 6428.0, Max: 6431.4, Diff: 4.6]\n      [Ext Root Scanning (ms): Min: 0.1, Avg: 3.4, Max: 5.7, Diff: 5.6, Sum: 13.7]\n      [Update RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]\n         [Processed Buffers: Min: 0, Avg: 0.0, Max: 0, Diff: 0, Sum: 0]\n      [Scan RS (ms): Min: 0.0, Avg: 0.1, Max: 0.1, Diff: 0.1, Sum: 0.2]\n      [Code Root Scanning (ms): Min: 0.0, Avg: 1.1, Max: 2.1, Diff: 2.1, Sum: 4.4]\n      [Object Copy (ms): Min: 17.9, Avg: 18.8, Max: 19.5, Diff: 1.6, Sum: 75.1]\n      [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.1, Diff: 0.0, Sum: 0.2]\n      [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.1, Diff: 0.0, Sum: 0.2]\n      [GC Worker Total (ms): Min: 20.1, Avg: 23.5, Max: 24.6, Diff: 4.6, Sum: 93.8]\n      [GC Worker End (ms): Min: 6451.5, Avg: 6451.5, Max: 6451.5, Diff: 0.0]\n   [Code Root Fixup: 1.2 ms]\n   [Code Root Purge: 0.0 ms]\n   [Clear CT: 0.6 ms]\n   [Other: 4.5 ms]\n      [Choose CSet: 0.0 ms]\n      [Ref Proc: 3.0 ms]\n      [Ref Enq: 0.0 ms]\n      [Redirty Cards: 0.6 ms]\n      [Humongous Reclaim: 0.0 ms]\n      [Free CSet: 0.2 ms]\n   [Eden: 46.0M(200.0M)->0.0B(206.0M) Survivors: 20.0M->14.0M Heap: 65.9M(368.0M)->13.5M(368.0M)]\n [Times: user=0.09 sys=0.01, real=0.03 secs]\n6.458: [GC concurrent-root-region-scan-start]\n6.486: [GC concurrent-root-region-scan-end, 0.0282857 secs]\n6.486: [GC concurrent-mark-start]\n6.487: [GC concurrent-mark-end, 0.0004830 secs]\n6.488: [GC remark 6.488: [Finalize Marking, 0.0004329 secs] 6.488: [GC ref-proc, 0.0002030 secs] 6.488: [Unloading, 0.0107436 secs], 0.0122303 secs]\n [Times: user=0.04 sys=0.00, real=0.01 secs]\n6.500: [GC cleanup 14M->14M(368M), 0.0027615 secs]\n [Times: user=0.00 sys=0.01, real=0.00 secs]\n6.503: [GC concurrent-cleanup-start]\n6.503: [GC concurrent-cleanup-end, 0.0000442 secs]\n2016-01-08 09:26:12 Completed Dag: dag_1452091205505_0030_1\nHeap\n garbage-first heap   total 376832K, used 140799K [0x00000006f3400000, 0x00000006f3500b80, 0x00000007c0000000)\n  region size 1024K, 139 young (142336K), 14 survivors (14336K)\n Metaspace       used 42234K, capacity 42538K, committed 43008K, reserved 1087488K\n  class space    used 4993K, capacity 5067K, committed 5120K, reserved 1048576K\nEnd of LogType:stdout\n\nLogType:syslog\nLog Upload Time:Fri Jan 08 09:37:16 +0100 2016\nLogLength:10261\nLog Contents:\n2016-01-08 09:26:07,147 INFO [main] app.DAGAppMaster: Created DAGAppMaster for application appattempt_1452091205505_0030_000001, versionInfo=[ component=tez-dag, version=0.7.0.2.3.0.0-2557, revision=ab9a0295b8de3f8711cf2bd4d837cd0b66be7cfa, SCM-URL=scm:git:https://git-wip-us.apache.org/repos/asf/tez.git, buildTime=20150714-0943 ]\n2016-01-08 09:26:07,181 INFO [main] app.DAGAppMaster: Comparing client version with AM version, clientVersion=0.7.0.2.3.0.0-2557, AMVersion=0.7.0.2.3.0.0-2557\n2016-01-08 09:26:08,093 INFO [main] app.DAGAppMaster: Adding session token to jobTokenSecretManager for application\n2016-01-08 09:26:08,101 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.rm.container.AMContainerEventType for class org.apache.tez.dag.app.rm.container.AMContainerMap\n2016-01-08 09:26:08,102 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.rm.node.AMNodeEventType for class org.apache.tez.dag.app.rm.node.AMNodeTracker\n2016-01-08 09:26:08,104 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.DAGAppMasterEventType for class org.apache.tez.dag.app.DAGAppMaster$DAGAppMasterEventHandler\n2016-01-08 09:26:08,105 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.DAGEventType for class org.apache.tez.dag.app.DAGAppMaster$DagEventDispatcher\n2016-01-08 09:26:08,105 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.VertexEventType for class org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher\n2016-01-08 09:26:08,106 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.TaskEventType for class org.apache.tez.dag.app.DAGAppMaster$TaskEventDispatcher\n2016-01-08 09:26:08,107 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.TaskAttemptEventType for class org.apache.tez.dag.app.DAGAppMaster$TaskAttemptEventDispatcher\n2016-01-08 09:26:08,109 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.SpeculatorEventType for independent dispatch using: class org.apache.tez.dag.app.DAGAppMaster$SpeculatorEventHandler\n2016-01-08 09:26:08,109 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.dag.event.SpeculatorEventType for class org.apache.tez.dag.app.DAGAppMaster$SpeculatorEventHandler\n2016-01-08 09:26:08,184 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.rm.AMSchedulerEventType for class org.apache.tez.dag.app.rm.TaskSchedulerEventHandler\n2016-01-08 09:26:08,187 INFO [main] common.AsyncDispatcher: Registering class org.apache.tez.dag.app.rm.NMCommunicatorEventType for class org.apache.tez.dag.app.launcher.ContainerLauncherImpl\n2016-01-08 09:26:08,344 INFO [main] node.AMNodeTracker: blacklistDisablePercent is 33, blacklistingEnabled: true, maxTaskFailuresPerNode: 10\n2016-01-08 09:26:08,347 INFO [main] web.WebUIService: Tez UI History URL: http://hdp-master1.something.com:8080/#/main/views/TEZ/0.7.0.2.3.0.0-236/TEZ_CLUSTER_INSTANCE?viewPath=%2F%23%2Ftez-app%2Fapplication_1452091205505_0030\n2016-01-08 09:26:08,349 INFO [main] launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500\n2016-01-08 09:26:08,349 INFO [main] history.HistoryEventHandler: Initializing HistoryEventHandler\n2016-01-08 09:26:08,369 INFO [main] ats.ATSHistoryLoggingService: Initializing ATSService\n2016-01-08 09:26:08,932 INFO [main] impl.TimelineClientImpl: Timeline service address: http://hdp-master1.something.com:8188/ws/v1/timeline/\n2016-01-08 09:26:08,932 INFO [main] ats.ATSHistoryLoggingService: Using org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager to manage Timeline ACLs\n2016-01-08 09:26:09,126 INFO [main] impl.TimelineClientImpl: Timeline service address: http://hdp-master1.something.com:8188/ws/v1/timeline/\n2016-01-08 09:26:09,126 INFO [main] recovery.RecoveryService: Initializing RecoveryService\n2016-01-08 09:26:09,130 INFO [main] history.HistoryEventHandler: [HISTORY][DAG:N/A][Event:APP_LAUNCHED]: applicationId=application_1452091205505_0030, appSubmitTime=1452241564250, launchTime=1452241567142\n2016-01-08 09:26:09,131 INFO [main] history.HistoryEventHandler: [HISTORY][DAG:N/A][Event:AM_LAUNCHED]: appAttemptId=appattempt_1452091205505_0030_000001, appSubmitTime=1452241564250, launchTime=1452241567142\n2016-01-08 09:26:09,141 INFO [ServiceThread:org.apache.tez.dag.history.HistoryEventHandler] history.HistoryEventHandler: Starting HistoryEventHandler\n2016-01-08 09:26:09,142 INFO [ServiceThread:org.apache.tez.dag.history.HistoryEventHandler] ats.ATSHistoryLoggingService: Starting ATSService\n2016-01-08 09:26:09,152 INFO [ServiceThread:org.apache.tez.dag.history.HistoryEventHandler] recovery.RecoveryService: Starting RecoveryService\n2016-01-08 09:26:09,153 INFO [ServiceThread:org.apache.tez.dag.app.launcher.ContainerLauncherImpl] impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\n2016-01-08 09:26:09,156 INFO [ServiceThread:org.apache.tez.dag.app.TaskAttemptListenerImpTezDag] ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2016-01-08 09:26:09,156 INFO [ServiceThread:DAGClientRPCServer] ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2016-01-08 09:26:09,179 INFO [Socket Reader #1 for port 55777] ipc.Server: Starting Socket Reader #1 for port 55777\n2016-01-08 09:26:09,186 INFO [Socket Reader #1 for port 48707] ipc.Server: Starting Socket Reader #1 for port 48707\n2016-01-08 09:26:09,228 INFO [IPC Server Responder] ipc.Server: IPC Server Responder: starting\n2016-01-08 09:26:09,232 INFO [IPC Server listener on 55777] ipc.Server: IPC Server listener on 55777: starting\n2016-01-08 09:26:09,253 INFO [IPC Server Responder] ipc.Server: IPC Server Responder: starting\n2016-01-08 09:26:09,254 INFO [ServiceThread:DAGClientRPCServer] client.DAGClientServer: Instantiated DAGClientRPCServer at hdp-node3.something.com/10.4.31.59:48707\n2016-01-08 09:26:09,254 INFO [IPC Server listener on 48707] ipc.Server: IPC Server listener on 48707: starting\n2016-01-08 09:26:09,354 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n2016-01-08 09:26:09,369 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n2016-01-08 09:26:09,377 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpRequestLog: Http request log for http.requests. is not defined\n2016-01-08 09:26:09,387 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n2016-01-08 09:26:09,394 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context\n2016-01-08 09:26:09,394 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static\n2016-01-08 09:26:09,402 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: adding path spec: /*\n2016-01-08 09:26:09,418 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] http.HttpServer2: Jetty bound to port 52136\n2016-01-08 09:26:09,418 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: jetty-6.1.26.hwx\n2016-01-08 09:26:09,489 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: Extract jar:file:/hadoop/hadoop/yarn/local/filecache/11/tez.tar.gz/lib/hadoop-yarn-common-2.7.1.2.3.0.0-2557.jar!/webapps/ to /tmp/Jetty_0_0_0_0_52136_webapps____hhonjy/webapp\n2016-01-08 09:26:09,880 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:52136\n2016-01-08 09:26:09,881 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] webapp.WebApps: Web app / started at 52136\n2016-01-08 09:26:10,217 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] webapp.WebApps: Registered webapp guice modules\n2016-01-08 09:26:10,219 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] web.WebUIService: Instantiated WebUIService at http://hdp-node3.something.com:52136/ui/\n2016-01-08 09:26:10,261 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] rm.YarnTaskSchedulerService: TaskScheduler initialized with configuration: maxRMHeartbeatInterval: 250, containerReuseEnabled: true, reuseRackLocal: true, reuseNonLocal: false, localitySchedulingDelay: 250, preemptionPercentage: 10, numHeartbeatsBetweenPreemptions: 3, idleContainerMinTimeout: 10000, idleContainerMaxTimeout: 20000, sessionMinHeldContainers: 0\n2016-01-08 09:26:10,317 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] client.RMProxy: Connecting to ResourceManager at hdp-master1.something.com/10.4.31.56:8030\n2016-01-08 09:26:10,401 INFO [main] history.HistoryEventHandler: [HISTORY][DAG:N/A][Event:AM_STARTED]: appAttemptId=appattempt_1452091205505_0030_000001, startTime=1452241570399\n2016-01-08 09:26:10,401 INFO [main] app.DAGAppMaster: In Session mode. Waiting for DAG over RPC\n2016-01-08 09:26:10,451 INFO [AMRM Callback Handler Thread] rm.YarnTaskSchedulerService: App total resource memory: 65536 cpu: 1 taskAllocations: 0\n2016-01-08 09:26:10,457 INFO [Dispatcher thread: Central] node.AMNodeTracker: Num cluster nodes = 3\n2016-01-08 09:26:10,696 INFO [IPC Server handler 0 on 48707] app.DAGAppMaster: Starting DAG submitted via RPC: hive_20160108092603_9b21f8a6-0a4a-45aa-9006-2e88328cd361:32\n2016-01-08 09:26:10,924 INFO [IPC Server handler 0 on 48707] app.DAGAppMaster: Generating DAG graphviz file, dagId=dag_1452091205505_0030_1, filePath=/hadoop/hadoop/yarn/log/application_1452091205505_0030/container_e09_1452091205505_0030_01_000001/dag_1452091205505_0030_1.dot\n2016-01-08 09:26:10,929 INFO [IPC Server handler 0 on 48707] common.TezUtilsInternal: Redirecting log file based on addend: dag_1452091205505_0030_1\nEnd of LogType:syslog\n\nLogType:syslog_dag_1452091205505_0030_1\nLog Upload Time:Fri Jan 08 09:37:16 +0100 2016\nLogLength:30256\nLog Contents:\n2016-01-08 09:26:10,930 INFO [IPC Server handler 0 on 48707] app.DAGAppMaster: Running DAG: hive_20160108092603_9b21f8a6-0a4a-45aa-9006-2e88328cd361:32\n2016-01-08 09:26:11,261 INFO [IPC Server handler 0 on 48707] history.HistoryEventHandler: [HISTORY][DAG:dag_1452091205505_0030_1][Event:DAG_SUBMITTED]: dagID=dag_1452091205505_0030_1, submitTime=1452241570697\n2016-01-08 09:26:11,332 WARN [HistoryEventHandlingThread] ats.ATSHistoryLoggingService: Could not post history event to ATS, atsPutError=6, entityId=dag_1452091205505_0030_1\n2016-01-08 09:26:11,338 WARN [HistoryEventHandlingThread] ats.ATSHistoryLoggingService: Could not post history event to ATS, atsPutError=6, entityId=dag_1452091205505_0030_1\n2016-01-08 09:26:11,368 INFO [IPC Server handler 0 on 48707] impl.VertexImpl: setting additional outputs for vertex Map 1\n2016-01-08 09:26:11,370 INFO [IPC Server handler 0 on 48707] impl.DAGImpl: Using DAG Scheduler: org.apache.tez.dag.app.dag.impl.DAGSchedulerNaturalOrder\n2016-01-08 09:26:11,373 INFO [IPC Server handler 0 on 48707] history.HistoryEventHandler: [HISTORY][DAG:dag_1452091205505_0030_1][Event:DAG_INITIALIZED]: dagID=dag_1452091205505_0030_1, initTime=1452241571309\n2016-01-08 09:26:11,374 INFO [IPC Server handler 0 on 48707] impl.DAGImpl: dag_1452091205505_0030_1 transitioned from NEW to INITED\n2016-01-08 09:26:11,378 INFO [Dispatcher thread: Central] history.HistoryEventHandler: [HISTORY][DAG:dag_1452091205505_0030_1][Event:DAG_STARTED]: dagID=dag_1452091205505_0030_1, startTime=1452241571377\n2016-01-08 09:26:11,379 INFO [Dispatcher thread: Central] impl.DAGImpl: Added additional resources : [[]] to classpath\n2016-01-08 09:26:11,384 INFO [Dispatcher thread: Central] impl.DAGImpl: dag_1452091205505_0030_1 transitioned from INITED to RUNNING\n2016-01-08 09:26:11,385 INFO [Dispatcher thread: Central] impl.VertexImpl: Root Inputs exist for Vertex: Map 1 : {tablename={InputName=tablename}, {Descriptor=ClassName=org.apache.tez.mapreduce.input.MRInputLegacy, hasPayload=true}, {ControllerDescriptor=ClassName=org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator, hasPayload=false}}\n2016-01-08 09:26:11,386 INFO [Dispatcher thread: Central] impl.VertexImpl: Starting root input initializer for input: tablename, with class: [org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator]\n2016-01-08 09:26:11,386 INFO [Dispatcher thread: Central] impl.VertexImpl: Setting vertexManager to RootInputVertexManager for vertex_1452091205505_0030_1_00 [Map 1]\n2016-01-08 09:26:11,402 INFO [Dispatcher thread: Central] impl.VertexImpl: Num tasks is -1. Expecting VertexManager/InputInitializers/1-1 split to set #tasks for the vertex vertex_1452091205505_0030_1_00 [Map 1]\n2016-01-08 09:26:11,402 INFO [Dispatcher thread: Central] impl.VertexImpl: Vertex will initialize from input initializer. vertex_1452091205505_0030_1_00 [Map 1]\n2016-01-08 09:26:11,405 INFO [Dispatcher thread: Central] impl.VertexImpl: Vertex will initialize via inputInitializers vertex_1452091205505_0030_1_00 [Map 1]. Starting root input initializers: 1\n2016-01-08 09:26:11,454 INFO [Dispatcher thread: Central] Configuration.deprecation: mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed\n2016-01-08 09:26:11,456 INFO [Dispatcher thread: Central] Configuration.deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n2016-01-08 09:26:11,674 INFO [Dispatcher thread: Central] exec.Utilities: PLAN PATH = hdfs://hdp-master1.something.com:8020/tmp/hive/hive/ce1315c6-a203-459a-a5d2-116a46a1c14e/hive_2016-01-08_09-26-03_418_3119952734722954293-34/hive/_tez_scratch_dir/919e0d70-60c0-4f67-9199-99b02a9c67e2/map.xml\n2016-01-08 09:26:11,674 INFO [Dispatcher thread: Central] exec.Utilities: ***************non-local mode***************\n2016-01-08 09:26:11,675 INFO [Dispatcher thread: Central] exec.Utilities: local path = hdfs://hdp-master1.something.com:8020/tmp/hive/hive/ce1315c6-a203-459a-a5d2-116a46a1c14e/hive_2016-01-08_09-26-03_418_3119952734722954293-34/hive/_tez_scratch_dir/919e0d70-60c0-4f67-9199-99b02a9c67e2/map.xml\n2016-01-08 09:26:11,691 INFO [Dispatcher thread: Central] log.PerfLogger: <PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>\n2016-01-08 09:26:11,691 INFO [Dispatcher thread: Central] exec.Utilities: Deserializing MapWork via kryo\n2016-01-08 09:26:12,120 INFO [Dispatcher thread: Central] log.PerfLogger: </PERFLOG method=deserializePlan start=1452241571690 end=1452241572120 duration=430 from=org.apache.hadoop.hive.ql.exec.Utilities>\n2016-01-08 09:26:12,134 INFO [InputInitializer [Map 1] #0] dag.RootInputInitializerManager: Starting InputInitializer for Input: tablename on vertex vertex_1452091205505_0030_1_00 [Map 1]\n2016-01-08 09:26:12,154 INFO [Dispatcher thread: Central] impl.VertexImpl: vertex_1452091205505_0030_1_00 [Map 1] transitioned from NEW to INITIALIZING due to event V_INIT\n2016-01-08 09:26:12,167 INFO [InputInitializer [Map 1] #0] tez.HiveSplitGenerator: The preferred split size is 16777216\n2016-01-08 09:26:12,168 INFO [InputInitializer [Map 1] #0] log.PerfLogger: <PERFLOG method=getSplits from=org.apache.hadoop.hive.ql.io.HiveInputFormat>\n2016-01-08 09:26:12,169 INFO [InputInitializer [Map 1] #0] exec.Utilities: PLAN PATH = hdfs://hdp-master1.something.com:8020/tmp/hive/hive/ce1315c6-a203-459a-a5d2-116a46a1c14e/hive_2016-01-08_09-26-03_418_3119952734722954293-34/hive/_tez_scratch_dir/919e0d70-60c0-4f67-9199-99b02a9c67e2/map.xml\n2016-01-08 09:26:12,171 INFO [InputInitializer [Map 1] #0] exec.Utilities: Processing alias tablename\n2016-01-08 09:26:12,171 INFO [InputInitializer [Map 1] #0] exec.Utilities: Adding input file hdfs://hdp-master1.something.com:8020/apps/hive/warehouse/archive.db/tablename\n2016-01-08 09:26:12,199 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: hive.io.file.readcolumn.ids=1,6\n2016-01-08 09:26:12,199 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: hive.io.file.readcolumn.names=messageid,originalxml\n2016-01-08 09:26:12,199 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: Generating splits\n2016-01-08 09:26:12,238 INFO [InputInitializer [Map 1] #0] log.PerfLogger: <PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>\n2016-01-08 09:26:12,241 INFO [InputInitializer [Map 1] #0] Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n2016-01-08 09:26:12,605 WARN [ORC_GET_SPLITS #0] ipc.Client: interrupted waiting to send rpc request to server\njava.lang.InterruptedException\n        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:191)\n        at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1057)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1400)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n        at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy12.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:358)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:314)\n        at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:237)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:924)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n2016-01-08 09:26:12,615 INFO [InputInitializer [Map 1] #0] dag.RootInputInitializerManager: Failed InputInitializer for Input: tablename on vertex vertex_1452091205505_0030_1_00 [Map 1]\n2016-01-08 09:26:12,613 WARN [ORC_GET_SPLITS #7] ipc.Client: interrupted waiting to send rpc request to server\njava.lang.InterruptedException\n        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:191)\n        at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1057)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1400)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n        at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy12.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:358)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:314)\n        at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:237)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:924)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n2016-01-08 09:26:12,613 WARN [ORC_GET_SPLITS #1] ipc.Client: interrupted waiting to send rpc request to server\njava.lang.InterruptedException\n        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:191)\n        at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1057)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1400)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n        at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n        at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy12.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:358)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:314)\n        at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:237)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:924)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n2016-01-08 09:26:12,609 WARN [ORC_GET_SPLITS #4] ipc.Client: interrupted waiting to send rpc request to server\njava.lang.InterruptedException\n        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:191)\n        at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1057)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1400)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n        at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy12.getFileInfo(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:358)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:314)\n        at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:237)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:924)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n2016-01-08 09:26:12,627 ERROR [Dispatcher thread: Central] impl.VertexImpl: Vertex Input: tablename initializer failed, vertex=vertex_1452091205505_0030_1_00 [Map 1]\norg.apache.tez.dag.app.dag.impl.AMUserCodeException: java.lang.RuntimeException: serious problem\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallback.onFailure(RootInputInitializerManager.java:291)\n        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1140)\n        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)\n        at com.google.common.util.concurrent.ExecutionList$RunnableExecutorPair.execute(ExecutionList.java:150)\n        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:135)\n        at com.google.common.util.concurrent.ListenableFutureTask.done(ListenableFutureTask.java:91)\n        at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:384)\n        at java.util.concurrent.FutureTask.setException(FutureTask.java:251)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:271)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: serious problem\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:306)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:408)\n        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:245)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:239)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:239)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:226)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        ... 3 more\nCaused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1016)\n        ... 15 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.Collections$EmptyList.get(Collections.java:4454)\n        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:649)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:632)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:927)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        ... 4 more\n2016-01-08 09:26:12,643 INFO [Dispatcher thread: Central] impl.VertexImpl: Invoking committer abort for vertex, vertexId=vertex_1452091205505_0030_1_00 [Map 1]\n2016-01-08 09:26:12,700 INFO [Dispatcher thread: Central] history.HistoryEventHandler: [HISTORY][DAG:dag_1452091205505_0030_1][Event:VERTEX_FINISHED]: vertexName=Map 1, vertexId=vertex_1452091205505_0030_1_00, initRequestedTime=1452241571385, initedTime=0, startRequestedTime=1452241572166, startedTime=0, finishTime=1452241572645, timeTaken=1452241572645, status=FAILED, diagnostics=Vertex vertex_1452091205505_0030_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE\nVertex Input: tablename initializer failed, vertex=vertex_1452091205505_0030_1_00 [Map 1], java.lang.RuntimeException: serious problem\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:306)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:408)\n        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:245)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:239)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:239)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:226)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1016)\n        ... 15 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.Collections$EmptyList.get(Collections.java:4454)\n        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:649)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:632)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:927)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        ... 4 more\n, counters=Counters: 0, vertexStats=firstTaskStartTime=-1, firstTasksToStart=[  ], lastTaskFinishTime=-1, lastTasksToFinish=[  ], minTaskDuration=-1, maxTaskDuration=-1, avgTaskDuration=-1.0, numSuccessfulTasks=0, shortestDurationTasks=[  ], longestDurationTasks=[  ], vertexTaskStats={numFailedTaskAttempts=0, numKilledTaskAttempts=0, numCompletedTasks=0, numSucceededTasks=0, numKilledTasks=0, numFailedTasks=0}\n2016-01-08 09:26:12,701 INFO [Dispatcher thread: Central] impl.VertexImpl: vertex_1452091205505_0030_1_00 [Map 1] transitioned from INITIALIZING to FAILED due to event V_ROOT_INPUT_FAILED\n2016-01-08 09:26:12,703 INFO [Dispatcher thread: Central] impl.DAGImpl: Vertex vertex_1452091205505_0030_1_00 [Map 1] completed., numCompletedVertices=1, numSuccessfulVertices=0, numFailedVertices=1, numKilledVertices=0, numVertices=1\n2016-01-08 09:26:12,703 INFO [Dispatcher thread: Central] impl.DAGImpl: Checking vertices for DAG completion, numCompletedVertices=1, numSuccessfulVertices=0, numFailedVertices=1, numKilledVertices=0, numVertices=1, commitInProgress=0, terminationCause=VERTEX_FAILURE\n2016-01-08 09:26:12,703 INFO [Dispatcher thread: Central] impl.DAGImpl: DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0\n2016-01-08 09:26:12,720 INFO [ORC_GET_SPLITS #9] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (EQUALS messageid d0b3c872-435d-499b-a65c-619d9e732bbb)\nexpr = leaf-0\n2016-01-08 09:26:12,720 INFO [ORC_GET_SPLITS #2] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (EQUALS messageid d0b3c872-435d-499b-a65c-619d9e732bbb)\nexpr = leaf-0\n2016-01-08 09:26:12,721 INFO [ORC_GET_SPLITS #6] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (EQUALS messageid d0b3c872-435d-499b-a65c-619d9e732bbb)\nexpr = leaf-0\n2016-01-08 09:26:12,722 INFO [ORC_GET_SPLITS #3] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (EQUALS messageid d0b3c872-435d-499b-a65c-619d9e732bbb)\nexpr = leaf-0\n2016-01-08 09:26:12,720 INFO [ORC_GET_SPLITS #5] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (EQUALS messageid d0b3c872-435d-499b-a65c-619d9e732bbb)\nexpr = leaf-0\n2016-01-08 09:26:12,723 INFO [ORC_GET_SPLITS #8] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (EQUALS messageid d0b3c872-435d-499b-a65c-619d9e732bbb)\nexpr = leaf-0\n2016-01-08 09:26:12,801 INFO [Dispatcher thread: Central] counters.Limits: Counter limits initialized with parameters:  GROUP_NAME_MAX=256, MAX_GROUPS=1000, COUNTER_NAME_MAX=64, MAX_COUNTERS=2000\n2016-01-08 09:26:12,887 INFO [Dispatcher thread: Central] recovery.RecoveryService: DAG completed, dagId=dag_1452091205505_0030_1, queueSize=0\n2016-01-08 09:26:12,905 INFO [Dispatcher thread: Central] history.HistoryEventHandler: [HISTORY][DAG:dag_1452091205505_0030_1][Event:DAG_FINISHED]: dagId=dag_1452091205505_0030_1, startTime=1452241571377, finishTime=1452241572808, timeTaken=1431, status=FAILED, diagnostics=Vertex failed, vertexName=Map 1, vertexId=vertex_1452091205505_0030_1_00, diagnostics=[Vertex vertex_1452091205505_0030_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: tablename initializer failed, vertex=vertex_1452091205505_0030_1_00 [Map 1], java.lang.RuntimeException: serious problem\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:306)\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:408)\n        at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:155)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:245)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:239)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:239)\n        at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:226)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1016)\n        ... 15 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0\n        at java.util.Collections$EmptyList.get(Collections.java:4454)\n        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:649)\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:632)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:927)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:836)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:702)\n        ... 4 more\n]\nDAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0, counters=Counters: 2, org.apache.tez.common.counters.DAGCounter, AM_CPU_MILLISECONDS=4460, AM_GC_TIME_MILLIS=32\n2016-01-08 09:26:12,905 INFO [Dispatcher thread: Central] impl.VertexImpl: Ignoring multiple aborts for vertex: vertex_1452091205505_0030_1_00 [Map 1]\n2016-01-08 09:26:12,906 INFO [Dispatcher thread: Central] impl.DAGImpl: DAG: dag_1452091205505_0030_1 finished with state: FAILED\n2016-01-08 09:26:12,906 INFO [Dispatcher thread: Central] impl.DAGImpl: dag_1452091205505_0030_1 transitioned from RUNNING to FAILED\n2016-01-08 09:26:12,907 INFO [Dispatcher thread: Central] app.DAGAppMaster: DAG completed, dagId=dag_1452091205505_0030_1, dagState=FAILED\n2016-01-08 09:26:12,907 INFO [Dispatcher thread: Central] common.TezUtilsInternal: Redirecting log file based on addend: dag_1452091205505_0030_1_post\nEnd of LogType:syslog_dag_1452091205505_0030_1\n\nLogType:syslog_dag_1452091205505_0030_1_post\nLog Upload Time:Fri Jan 08 09:37:16 +0100 2016\nLogLength:4819\nLog Contents:\n2016-01-08 09:26:12,908 INFO [Dispatcher thread: Central] app.DAGAppMaster: Central Dispatcher queue size after DAG completion, before cleanup: 0\n2016-01-08 09:26:12,908 INFO [Dispatcher thread: Central] app.DAGAppMaster: Waiting for next DAG to be submitted.\n2016-01-08 09:26:12,911 INFO [Dispatcher thread: Central] app.DAGAppMaster: Cleaning up DAG: name=hive_20160108092603_9b21f8a6-0a4a-45aa-9006-2e88328cd361:32, with id=dag_1452091205505_0030_1\n2016-01-08 09:26:12,919 INFO [Dispatcher thread: Central] app.DAGAppMaster: Completed cleanup for DAG: name=hive_20160108092603_9b21f8a6-0a4a-45aa-9006-2e88328cd361:32, with id=dag_1452091205505_0030_1\n2016-01-08 09:37:10,402 INFO [Timer-1] app.DAGAppMaster: Session timed out, lastDAGCompletionTime=1452241572907 ms, sessionTimeoutInterval=600000 ms\n2016-01-08 09:37:10,404 INFO [Timer-1] rm.TaskSchedulerEventHandler: TaskScheduler notified that it should unregister from RM\n2016-01-08 09:37:10,405 INFO [Timer-1] app.DAGAppMaster: No current running DAG, shutting down the AM\n2016-01-08 09:37:10,405 INFO [Timer-1] app.DAGAppMaster: DAGAppMasterShutdownHandler invoked\n2016-01-08 09:37:10,405 INFO [Timer-1] app.DAGAppMaster: Handling DAGAppMaster shutdown\n2016-01-08 09:37:10,407 INFO [AMShutdownThread] app.DAGAppMaster: Sleeping for 5 seconds before shutting down\n2016-01-08 09:37:15,408 INFO [AMShutdownThread] app.DAGAppMaster: Calling stop for all the services\n2016-01-08 09:37:15,411 INFO [AMShutdownThread] history.HistoryEventHandler: Stopping HistoryEventHandler\n2016-01-08 09:37:15,411 INFO [AMShutdownThread] recovery.RecoveryService: Stopping RecoveryService\n2016-01-08 09:37:15,411 INFO [AMShutdownThread] recovery.RecoveryService: Handle the remaining events in queue, queue size=0\n2016-01-08 09:37:15,412 INFO [RecoveryEventHandlingThread] recovery.RecoveryService: EventQueue take interrupted. Returning\n2016-01-08 09:37:15,412 INFO [AMShutdownThread] recovery.RecoveryService: Closing Summary Stream\n2016-01-08 09:37:15,423 INFO [AMShutdownThread] ats.ATSHistoryLoggingService: Stopping ATSService, eventQueueBacklog=0\n2016-01-08 09:37:15,425 INFO [DelayedContainerManager] rm.YarnTaskSchedulerService: AllocatedContainerManager Thread interrupted\n2016-01-08 09:37:15,431 INFO [AMShutdownThread] rm.YarnTaskSchedulerService: Unregistering application from RM, exitStatus=SUCCEEDED, exitMessage=Session stats:submittedDAGs=1, successfulDAGs=0, failedDAGs=1, killedDAGs=0\n, trackingURL=http://hdp-master1.something.com:8080/#/main/views/TEZ/0.7.0.2.3.0.0-236/TEZ_CLUSTER_INSTANCE?viewPath=%2F%23%2Ftez-app%2Fapplication_1452091205505_0030\n2016-01-08 09:37:15,443 INFO [AMShutdownThread] impl.AMRMClientImpl: Waiting for application to be successfully unregistered.\n2016-01-08 09:37:15,545 INFO [AMShutdownThread] rm.YarnTaskSchedulerService: Successfully unregistered application from RM\n2016-01-08 09:37:15,548 INFO [AMRM Callback Handler Thread] impl.AMRMClientAsyncImpl: Interrupted while waiting for queue\njava.lang.InterruptedException\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)\n        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n        at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:287)\n2016-01-08 09:37:15,576 INFO [AMShutdownThread] mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:0\n2016-01-08 09:37:15,580 INFO [AMShutdownThread] ipc.Server: Stopping server on 55777\n2016-01-08 09:37:15,585 INFO [IPC Server listener on 55777] ipc.Server: Stopping IPC Server listener on 55777\n2016-01-08 09:37:15,585 INFO [AMShutdownThread] ipc.Server: Stopping server on 48707\n2016-01-08 09:37:15,586 INFO [IPC Server Responder] ipc.Server: Stopping IPC Server Responder\n2016-01-08 09:37:15,586 INFO [IPC Server listener on 48707] ipc.Server: Stopping IPC Server listener on 48707\n2016-01-08 09:37:15,587 INFO [IPC Server Responder] ipc.Server: Stopping IPC Server Responder\n2016-01-08 09:37:15,592 INFO [Thread-2] app.DAGAppMaster: DAGAppMasterShutdownHook invoked\n2016-01-08 09:37:15,592 INFO [Thread-2] app.DAGAppMaster: The shutdown handler is still running, waiting for it to complete\n2016-01-08 09:37:15,601 INFO [AMShutdownThread] app.DAGAppMaster: Completed deletion of tez scratch data dir, path=hdfs://hdp-master1.something.com:8020/tmp/hive/hive/_tez_session_dir/76022374-fe62-4835-a80d-6f20ee57c7ba/.tez/application_1452091205505_0030\n2016-01-08 09:37:15,602 INFO [AMShutdownThread] app.DAGAppMaster: Exiting DAGAppMaster..GoodBye!\n2016-01-08 09:37:15,602 INFO [Thread-2] app.DAGAppMaster: The shutdown handler has completed\nEnd of LogType:syslog_dag_1452091205505_0030_1_post\n\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T12:33:14.449+0000","updated":"2016-01-08T12:33:14.449+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089154","id":"15089154","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. I'm concerned about this procedure since we are just in the beginning of loadnig few meilion of entries every day to this table...it should work as expected not with workarounds.\n\nYeah. This is already fixed, I will probably close this bug as a duplicate.\n\nbq. Why do you think, select is working with 16 milion of entries and does not work with 34 milion of entries?\nbq. It is some kind of limitin parameter maybe to be set?\n\nYes, there are codepaths for data-size < 1Gb and > 1Gb.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-08T12:38:16.608+0000","updated":"2016-01-08T12:38:16.608+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15089166","id":"15089166","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"Hello,\n\ncan you please be more specific. How can I set the codepath?\n\nthanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T12:56:24.358+0000","updated":"2016-01-08T12:56:24.358+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15091607","id":"15091607","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"Hello again,\n\nI find out that suggested workaround blows up file system space. When I perform \"insert overwrite\" dik size will grow for twice amount of table size I work with.\nThe other problem with workaround is that I'm able to perform select but output is full of spaces.\n\nset hive.execution.engine=mr;\n\ninsert overwrite tablename as select * from tablename sort by messageid;\n\n\nIs there any parameter to overcome the limitation I mentioned? Everything works fine with 16mio entries, but you are unable to select when there is 36mio entries in the table.\nIs it some java problem maybe?\n\nThanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-11T08:35:37.036+0000","updated":"2016-01-11T08:35:37.036+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15091610","id":"15091610","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. When I perform \"insert overwrite\" dik size will grow for twice amount of table size I work with.\n\nyes, HDFS is an immutable filesystem.\n\nbq. Is there any parameter to overcome the limitation I mentioned?\n\nYes, try HDP-2.3.2 which contains the fix.\n\nbq. Is it some java problem maybe?\n\nThis looks like a SQOOP issue in the data write-path, which is why I recommended doing a reinsert via Hive to fix the data.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-11T08:43:41.507+0000","updated":"2016-01-11T08:43:41.507+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15094026","id":"15094026","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"Hi,\n\ntoo bad, I did an upgrade to 2.3.2, everything went ok, all services are up, but I can not connect to hive now...any idea maybe?\n\n[root@hdp-master ~]# beeline\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/spark/lib/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nWARNING: Use \"yarn jar\" to launch YARN applications.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/spark/lib/spark-assembly-1.4.1.2.3.2.0-2950-hadoop2.7.1.2.3.2.0-2950.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/hdp/2.3.2.0-2950/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nBeeline version 1.2.1.2.3.2.0-2950 by Apache Hive\nbeeline> !connect jdbc:hive2://ipaddr:10000/default\nConnecting to jdbc:hive2://ipaddr:10000/default\nEnter username for jdbc:hive2://ipaddr:10000/default: hive\nEnter password for jdbc:hive2://ipaddr:10000/default: *******\nError: Could not open client transport with JDBC Uri: jdbc:hive2://ipaddr:10000/default: java.net.ConnectException: Connection refused (state=08S01,code=0)\n0: jdbc:hive2://ipaddr:10000/default (closed)>\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-12T15:10:37.931+0000","updated":"2016-01-12T15:10:37.931+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15095298","id":"15095298","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=taksaito","name":"taksaito","key":"taksaito","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10438","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10438","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10438","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10438"},"displayName":"Takahiko Saito","active":true,"timeZone":"America/Los_Angeles"},"body":"[~maske], Have you checked the hiveserver2 log to see if hiveserver2 has started properly? Do you seen any error there?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=taksaito","name":"taksaito","key":"taksaito","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10438","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10438","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10438","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10438"},"displayName":"Takahiko Saito","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-01-13T00:31:11.017+0000","updated":"2016-01-13T00:31:11.017+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15095769","id":"15095769","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"I get the following error in hive-server2.log all the time:\n\nivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,file:/usr/hdp/2.3.2.0-2950/hadoop/lib/hadoop-lzo-0.6.0.2.3.2.0-2950-sources.jar!/ivysettings.xml w\nill be used\n\nan the following error in hiveserver2.log, looks like some permissions problem?:\n\n2016-01-13 00:01:45,186 INFO  [HiveServer2-Handler-Pool: Thread-41]: thrift.ThriftCLIService (ThriftCLIService.java:OpenSession(294)) - Client protocol version\n: HIVE_CLI_SERVICE_PROTOCOL_V8\n2016-01-13 00:01:45,188 INFO  [HiveServer2-Handler-Pool: Thread-41]: DependencyResolver (SessionState.java:printInfo(951)) - ivysettings.xml file not found in\nHIVE_HOME or HIVE_CONF_DIR,file:/usr/hdp/2.3.2.0-2950/hadoop/lib/hadoop-lzo-0.6.0.2.3.2.0-2950-sources.jar!/ivysettings.xml will be used\n2016-01-13 00:01:45,200 WARN  [HiveServer2-Handler-Pool: Thread-41]: thrift.ThriftCLIService (ThriftCLIService.java:OpenSession(308)) - Error opening session:\norg.apache.hive.service.cli.HiveSQLException: Failed to open new session: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteE\nxception(org.apache.hadoop.security.authorize.AuthorizationException): User: hive is not allowed to impersonate anonymous\n        at org.apache.hive.service.cli.session.SessionManager.openSession(SessionManager.java:266)\n        at org.apache.hive.service.cli.CLIService.openSessionWithImpersonation(CLIService.java:202)\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.getSessionHandle(ThriftCLIService.java:402)\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:297)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1253)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1238)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationExce\nption): User: hive is not allowed to impersonate anonymous\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:83)\n        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\n        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\n        at com.sun.proxy.$Proxy22.open(Unknown Source)\n        at org.apache.hive.service.cli.session.SessionManager.openSession(SessionManager.java:258)\n        ... 12 more\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-13T07:35:20.744+0000","updated":"2016-01-13T07:35:20.744+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15096700","id":"15096700","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=taksaito","name":"taksaito","key":"taksaito","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10438","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10438","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10438","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10438"},"displayName":"Takahiko Saito","active":true,"timeZone":"America/Los_Angeles"},"body":"If you have hive.server2.enable.doAs=false, you can set it to true, restart hs2 and try again. That may help.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=taksaito","name":"taksaito","key":"taksaito","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10438","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10438","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10438","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10438"},"displayName":"Takahiko Saito","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-01-13T18:03:54.701+0000","updated":"2016-01-13T18:03:54.701+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15097909","id":"15097909","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"Hello,\nThank you.\nyes I tried that and many other possible options yesteraday with no success. \nSince my hdfs is still empty I decided to reinstall everything - I have to get system up and running asap.\n\nWill go for last update now hdp 2.3.4.0 with a hope that problem with select will be solved.\n\nWill come back later with results...\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-14T09:55:19.969+0000","updated":"2016-01-14T09:55:19.969+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12928591/comment/15105210","id":"15105210","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"body":"Hi,\n\nI did a complete install from scratch with last available version of hdp 2.3.4.0-3485 and select is working proprer now with more than 34mio of records.\n\nref. http://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_Installing_HDP_AMB/content/index.html\n\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=maske","name":"maske","key":"maske","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matjaz Skerjanec","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-18T12:18:03.704+0000","updated":"2016-01-18T12:18:03.704+0000"}],"maxResults":20,"total":20,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-12810/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2r14n:"}}