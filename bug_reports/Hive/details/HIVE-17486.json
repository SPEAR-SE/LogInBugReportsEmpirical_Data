{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13100713","self":"https://issues.apache.org/jira/rest/api/2/issue/13100713","key":"HIVE-17486","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-09-08T16:39:10.895+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jan 03 06:22:22 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17486/watchers","watchCount":6,"isWatching":false},"created":"2017-09-08T08:28:15.416+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"9.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12522537","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12522537","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"inwardIssue":{"id":"13125511","key":"HIVE-18289","self":"https://issues.apache.org/jira/rest/api/2/issue/13125511","fields":{"summary":"Support Parquet/Orc format when enable rdd cache in Hive on Spark","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12522536","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12522536","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"inwardIssue":{"id":"13125797","key":"HIVE-18301","self":"https://issues.apache.org/jira/rest/api/2/issue/13125797","fields":{"summary":"Investigate to enable MapInput cache in Hive on Spark","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-01-03T06:22:22.392+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[],"timeoriginalestimate":null,"description":"in HIVE-16602, Implement shared scans with Tez.\n\nGiven a query plan, the goal is to identify scans on input tables that can be merged so the data is read only once. Optimization will be carried out at the physical level.  In Hive on Spark, it caches the result of spark work if the spark work is used by more than 1 child spark work. After sharedWorkOptimizer is enabled in physical plan in HoS, the identical table scans are merged to 1 table scan. This result of table scan will be used by more 1 child spark work. Thus we need not do the same computation because of cache mechanism.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12900461","id":"12900461","filename":"explain.28.share.false","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-04T09:40:42.524+0000","size":17524,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12900461/explain.28.share.false"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12900460","id":"12900460","filename":"explain.28.share.true","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-04T09:40:42.524+0000","size":45388,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12900460/explain.28.share.true"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12900619","id":"12900619","filename":"HIVE-17486.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-05T06:27:13.884+0000","size":55086,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12900619/HIVE-17486.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12902243","id":"12902243","filename":"HIVE-17486.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-15T08:40:15.187+0000","size":81259,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12902243/HIVE-17486.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12902412","id":"12902412","filename":"HIVE-17486.3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-15T17:15:01.035+0000","size":78871,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12902412/HIVE-17486.3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12902486","id":"12902486","filename":"HIVE-17486.4.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-16T09:52:38.394+0000","size":78479,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12902486/HIVE-17486.4.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12904307","id":"12904307","filename":"HIVE-17486.5.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-03T01:46:24.103+0000","size":80703,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12904307/HIVE-17486.5.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12895149","id":"12895149","filename":"scanshare.after.svg","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-01T09:00:44.937+0000","size":100469,"mimeType":"image/svg+xml","content":"https://issues.apache.org/jira/secure/attachment/12895149/scanshare.after.svg"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12895148","id":"12895148","filename":"scanshare.before.svg","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-01T09:00:44.938+0000","size":109338,"mimeType":"image/svg+xml","content":"https://issues.apache.org/jira/secure/attachment/12895148/scanshare.before.svg"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Enable SharedWorkOptimizer in tez on HOS","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16158893","id":"16158893","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"Is the {{SharedWorkOptimizer}} different from HoS's {{CombineEquivalentWorkResolver}}?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2017-09-08T16:39:10.895+0000","updated":"2017-09-08T16:39:10.895+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16160943","id":"16160943","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~stakiar]: thanks for interesting on it.  I guess this optimization have following effect.\n{code}\nset hive.strict.checks.cartesian.product=false;\nset hive.join.emit.interval=2;\nset hive.auto.convert.join=false;\n\nexplain SELECT *\nFROM (\n  SELECT test1.key AS key1, test1.value AS value1, test1.col_1 AS col_1,\n         test2.key AS key2, test2.value AS value2, test2.col_2 AS col_2\n  FROM test1 RIGHT OUTER JOIN test2\n  ON (test1.value=test2.value\n    AND (test1.key between 100 and 102\n      OR test2.key between 100 and 102))\n  ) sq1\nFULL OUTER JOIN (\n  SELECT test1.key AS key3, test1.value AS value3, test1.col_1 AS col_3,\n         test2.key AS key4, test2.value AS value4, test2.col_2 AS col_4\n  FROM test1 LEFT OUTER JOIN test2\n  ON (test1.value=test2.value\n    AND (test1.key between 100 and 102\n      OR test2.key between 100 and 102))\n  ) sq2\nON (sq1.value1 is null or sq2.value4 is null and sq2.value3 != sq1.value2);\n\n{code}\n\nthe spark explain\n{code}\n STAGE DEPENDENCIES:\n  Stage-1 is a root stage\n  Stage-0 depends on stages: Stage-1\n\nSTAGE PLANS:\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 12), Map 4 (PARTITION-LEVEL SORT, 12)\n        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1), Reducer 6 (PARTITION-LEVEL SORT, 1)\n        Reducer 6 <- Map 5 (PARTITION-LEVEL SORT, 12), Map 7 (PARTITION-LEVEL SORT, 12)\n      DagName: root_20170911043433_e314705a-beca-41a0-b28a-c85c5f811a67:1\n      Vertices:\n        Map 1 \n            Map Operator Tree:\n                TableScan\n                  alias: test1\n                  Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                  Select Operator\n                    expressions: key (type: int), value (type: int), col_1 (type: string)\n                    outputColumnNames: _col0, _col1, _col2\n                    Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                    Reduce Output Operator\n                      key expressions: _col1 (type: int)\n                      sort order: +\n                      Map-reduce partition columns: _col1 (type: int)\n                      Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                      value expressions: _col0 (type: int), _col2 (type: string)\n        Map 4 \n            Map Operator Tree:\n                TableScan\n                  alias: test2\n                  Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                  Select Operator\n                    expressions: key (type: int), value (type: int), col_2 (type: string)\n                    outputColumnNames: _col0, _col1, _col2\n                    Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                    Reduce Output Operator\n                      key expressions: _col1 (type: int)\n                      sort order: +\n                      Map-reduce partition columns: _col1 (type: int)\n                      Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                      value expressions: _col0 (type: int), _col2 (type: string)\n        Map 5 \n            Map Operator Tree:\n                TableScan\n                  alias: test1\n                  Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                  Select Operator\n                    expressions: key (type: int), value (type: int), col_1 (type: string)\n                    outputColumnNames: _col0, _col1, _col2\n                    Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                    Reduce Output Operator\n                      key expressions: _col1 (type: int)\n                      sort order: +\n                      Map-reduce partition columns: _col1 (type: int)\n                      Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                      value expressions: _col0 (type: int), _col2 (type: string)\n        Map 7 \n            Map Operator Tree:\n                TableScan\n                  alias: test2\n                  Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                  Select Operator\n                    expressions: key (type: int), value (type: int), col_2 (type: string)\n                    outputColumnNames: _col0, _col1, _col2\n                    Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                    Reduce Output Operator\n                      key expressions: _col1 (type: int)\n                      sort order: +\n                      Map-reduce partition columns: _col1 (type: int)\n                      Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                      value expressions: _col0 (type: int), _col2 (type: string)\n        Reducer 2 \n            Reduce Operator Tree:\n              Join Operator\n                condition map:\n                     Right Outer Join 0 to 1\n                keys:\n                  0 _col1 (type: int)\n                  1 _col1 (type: int)\n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                residual filter predicates: {(_col0 BETWEEN 100 AND 102 or _col3 BETWEEN 100 AND 102)}\n                Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE\n                Reduce Output Operator\n                  sort order: \n                  Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE\n                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: int), _col4 (type: int), _col5 (type: string)\n        Reducer 3 \n            Reduce Operator Tree:\n              Join Operator\n                condition map:\n                     Outer Join 0 to 1\n                keys:\n                  0 \n                  1 \n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11\n                residual filter predicates: {(_col1 is null or (_col10 is null and (_col7 <> _col4)))}\n                Statistics: Num rows: 36 Data size: 768 Basic stats: COMPLETE Column stats: NONE\n                File Output Operator\n                  compressed: false\n                  Statistics: Num rows: 36 Data size: 768 Basic stats: COMPLETE Column stats: NONE\n                  table:\n                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n        Reducer 6 \n            Reduce Operator Tree:\n              Join Operator\n                condition map:\n                     Left Outer Join 0 to 1\n                keys:\n                  0 _col1 (type: int)\n                  1 _col1 (type: int)\n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                residual filter predicates: {(_col0 BETWEEN 100 AND 102 or _col3 BETWEEN 100 AND 102)}\n                Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE\n                Reduce Output Operator\n                  sort order: \n                  Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE\n                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: int), _col4 (type: int), _col5 (type: string)\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n      Processor Tree:\n        ListSink\n\n{code}\n\n\nthe explain tez:\n{code}\nSTAGE PLANS:\n  Stage: Stage-1\n    Tez\n#### A masked pattern was here ####\n      Edges:\n        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 5 (SIMPLE_EDGE)\n        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE), Reducer 4 (CUSTOM_SIMPLE_EDGE)\n        Reducer 4 <- Map 1 (SIMPLE_EDGE), Map 5 (SIMPLE_EDGE)\n#### A masked pattern was here ####\n      Vertices:\n        Map 1 \n            Map Operator Tree:\n                TableScan\n                  alias: test1\n                  Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                  Select Operator\n                    expressions: key (type: int), value (type: int), col_1 (type: string)\n                    outputColumnNames: _col0, _col1, _col2\n                    Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                    Reduce Output Operator\n                      key expressions: _col1 (type: int)\n                      sort order: +\n                      Map-reduce partition columns: _col1 (type: int)\n                      Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                      value expressions: _col0 (type: int), _col2 (type: string)\n                  Select Operator\n                    expressions: key (type: int), value (type: int), col_1 (type: string)\n                    outputColumnNames: _col0, _col1, _col2\n                    Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                    Reduce Output Operator\n                      key expressions: _col1 (type: int)\n                      sort order: +\n                      Map-reduce partition columns: _col1 (type: int)\n                      Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE\n                      value expressions: _col0 (type: int), _col2 (type: string)\n            Execution mode: llap\n            LLAP IO: no inputs\n        Map 5 \n            Map Operator Tree:\n                TableScan\n                  alias: test2\n                  Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                  Select Operator\n                    expressions: key (type: int), value (type: int), col_2 (type: string)\n                    outputColumnNames: _col0, _col1, _col2\n                    Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                    Reduce Output Operator\n                      key expressions: _col1 (type: int)\n                      sort order: +\n                      Map-reduce partition columns: _col1 (type: int)\n                      Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                      value expressions: _col0 (type: int), _col2 (type: string)\n                  Select Operator\n                    expressions: key (type: int), value (type: int), col_2 (type: string)\n                    outputColumnNames: _col0, _col1, _col2\n                    Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                    Reduce Output Operator\n                      key expressions: _col1 (type: int)\n                      sort order: +\n                      Map-reduce partition columns: _col1 (type: int)\n                      Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE\n                      value expressions: _col0 (type: int), _col2 (type: string)\n            Execution mode: llap\n            LLAP IO: no inputs\n        Reducer 2 \n            Execution mode: llap\n            Reduce Operator Tree:\n              Merge Join Operator\n                condition map:\n                     Right Outer Join0 to 1\n                keys:\n                  0 _col1 (type: int)\n                  1 _col1 (type: int)\n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                residual filter predicates: {(_col0 BETWEEN 100 AND 102 or _col3 BETWEEN 100 AND 102)}\n                Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE\n                Reduce Output Operator\n                  sort order: \n                  Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE\n                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: int), _col4 (type: int), _col5 (type: string)\n        Reducer 3 \n            Execution mode: llap\n            Reduce Operator Tree:\n              Merge Join Operator\n                condition map:\n                     Outer Join 0 to 1\n                keys:\n                  0 \n                  1 \n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11\n                residual filter predicates: {(_col1 is null or (_col10 is null and (_col7 <> _col4)))}\n                Statistics: Num rows: 36 Data size: 768 Basic stats: COMPLETE Column stats: NONE\n                File Output Operator\n                  compressed: false\n                  Statistics: Num rows: 36 Data size: 768 Basic stats: COMPLETE Column stats: NONE\n                  table:\n                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n        Reducer 4 \n            Execution mode: llap\n            Reduce Operator Tree:\n              Merge Join Operator\n                condition map:\n                     Left Outer Join0 to 1\n                keys:\n                  0 _col1 (type: int)\n                  1 _col1 (type: int)\n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                residual filter predicates: {(_col0 BETWEEN 100 AND 102 or _col3 BETWEEN 100 AND 102)}\n                Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE\n                Reduce Output Operator\n                  sort order: \n                  Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE\n                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: int), _col4 (type: int), _col5 (type: string)\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n      Processor Tree:\n        ListSink\n{code}\nFrom the explain, hive on spark loads test1, test2 twice while hive on tez only loads once. If my understanding is wrong, tell me,thanks!\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-09-11T08:56:36.014+0000","updated":"2017-09-11T08:56:36.014+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16162281","id":"16162281","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"hmm interesting, I'm curious why {{CombineEquivalentWorkResolver}} doesn't combine the scans, as far as I can tell {{Map 1}} is the same as {{Map 5}} and {{Map 4}} is the same as {{Map 7}}.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2017-09-12T00:22:32.406+0000","updated":"2017-09-12T00:22:32.406+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16162282","id":"16162282","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"Could be a bug in {{CombineEquivalentWorkResolver}}.\n\n{{SharedWorkOptimizer}} has some other interesting optimization though. According to the javadocs it can combine more than just scans, it can also combine RS and Joins, not sure if {{CombineEquivalentWorkResolver}} can do that.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2017-09-12T00:25:22.621+0000","updated":"2017-09-12T00:25:22.621+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16167441","id":"16167441","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"the reason why CombineEquivalentWorkResolver does not think Map1 is same as Map5, Map4 is same as Map7 is:\nwhen comparing Map4 and Map7\nMap4\n{code}\nTS[2]-SEL[3]-RS[13]\n{code}\nMap7 \n{code}\nTS[6]-SEL[7]-RS[9]\n{code}\n\nIt returns not equal when comparing RS\\[13\\] and RS\\[9\\] at [ExprNodeColumnDesc#isSame|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeColumnDesc.java#L181]. {code}\nif ( tabAlias != null && dest.tabAlias != null ) {\n      if ( !tabAlias.equals(dest.tabAlias) ) {\n        return false;\n      }\n    }\n{code}\n\nhere {{tabAlias}} is {{$hdt$_1}} while dest.tabAlias is {{$hdt$_3}}, actually {{$hdt$_1}} and {{$hdt$_3}} points to table {{test2}}.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-09-15T07:08:39.304+0000","updated":"2017-09-15T07:08:39.304+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16221989","id":"16221989","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"mapjoin.q\r\n{code}\r\nset hive.mapred.mode=nonstrict;\r\nset hive.explain.user=false;\r\nset hive.auto.convert.join=true;\r\nexplain\r\nselect src1.key, src1.cnt1, src2.cnt1 from\r\n(\r\n  select key, count(*) as cnt1 from \r\n  (\r\n    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key\r\n  ) subq1 group by key\r\n) src1\r\njoin\r\n(\r\n  select key, count(*) as cnt1 from \r\n  (\r\n    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key\r\n  ) subq2 group by key\r\n) src2\r\non src1.key = src2.key\r\n{code}\r\n\r\nbefore shared work optimization, the physical plan in Tez\r\n{code}\r\nTS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[11]-GBY[12]-MAPJOIN[47]-SEL[31]-FS[32]\r\nTS[3]-FIL[42]-SEL[5]-RS[7]-MAPJOIN[45]\r\nTS[14]-FIL[43]-SEL[16]-MAPJOIN[46]-GBY[24]-RS[25]-GBY[26]-RS[29]-MAPJOIN[47]\r\nTS[17]-FIL[44]-SEL[19]-RS[21]-MAPJOIN[46]\r\n{code}\r\n\r\nafter the optimization\r\n{code}\r\n\r\nTS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[11]-GBY[12]-MAPJOIN[47]-SEL[31]-FS[32]\r\n                                        -RS[25]-GBY[26]-RS[29]-MAPJOIN[47]\r\nTS[3]-FIL[42]-SEL[5]-RS[7]-MAPJOIN[45]\r\n{code}\r\n\r\nso the tez explain \r\nbefore\r\n{code}\r\nSTAGE DEPENDENCIES:\r\n  Stage-1 is a root stage\r\n  Stage-0 depends on stages: Stage-1\r\n\r\nSTAGE PLANS:\r\n  Stage: Stage-1\r\n    Tez\r\n      DagId: root_20171027044107_4977290a-6856-41c5-b0e3-24b4ec32c59e:1\r\n      Edges:\r\n        Map 1 <- Map 3 (BROADCAST_EDGE)\r\n        Map 4 <- Map 6 (BROADCAST_EDGE)\r\n        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 5 (BROADCAST_EDGE)\r\n        Reducer 5 <- Map 4 (SIMPLE_EDGE)\r\n      DagName: root_20171027044107_4977290a-6856-41c5-b0e3-24b4ec32c59e:1\r\n      Vertices:\r\n        Map 1 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: a\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Map Join Operator\r\n                        condition map:\r\n                             Inner Join 0 to 1\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n                        outputColumnNames: _col0\r\n                        input vertices:\r\n                          1 Map 3\r\n                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                        HybridGraceHashJoin: true\r\n                        Group By Operator\r\n                          aggregations: count()\r\n                          keys: _col0 (type: int)\r\n                          mode: hash\r\n                          outputColumnNames: _col0, _col1\r\n                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                          Reduce Output Operator\r\n                            key expressions: _col0 (type: int)\r\n                            sort order: +\r\n                            Map-reduce partition columns: _col0 (type: int)\r\n                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                            value expressions: _col1 (type: bigint)\r\n        Map 3 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: b\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: int)\r\n                        sort order: +\r\n                        Map-reduce partition columns: _col0 (type: int)\r\n                        Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n        Map 4 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: a\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Map Join Operator\r\n                        condition map:\r\n                             Inner Join 0 to 1\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n                        outputColumnNames: _col0\r\n                        input vertices:\r\n                          1 Map 6\r\n                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                        HybridGraceHashJoin: true\r\n                        Group By Operator\r\n                          aggregations: count()\r\n                          keys: _col0 (type: int)\r\n                          mode: hash\r\n                          outputColumnNames: _col0, _col1\r\n                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                          Reduce Output Operator\r\n                            key expressions: _col0 (type: int)\r\n                            sort order: +\r\n                            Map-reduce partition columns: _col0 (type: int)\r\n                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                            value expressions: _col1 (type: bigint)\r\n        Map 6 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: b\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: int)\r\n                        sort order: +\r\n                        Map-reduce partition columns: _col0 (type: int)\r\n                        Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n        Reducer 2 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: count(VALUE._col0)\r\n                keys: KEY._col0 (type: int)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1\r\n                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                Map Join Operator\r\n                  condition map:\r\n                       Inner Join 0 to 1\r\n                  keys:\r\n                    0 _col0 (type: int)\r\n                    1 _col0 (type: int)\r\n                  outputColumnNames: _col0, _col1, _col3\r\n                  input vertices:\r\n                    1 Reducer 5\r\n                  Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                  HybridGraceHashJoin: true\r\n                  Select Operator\r\n                    expressions: _col0 (type: int), _col3 (type: bigint), _col1 (type: bigint)\r\n                    outputColumnNames: _col0, _col1, _col2\r\n                    Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                    File Output Operator\r\n                      compressed: false\r\n                      Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                      table:\r\n                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat\r\n                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\r\n                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\r\n        Reducer 5 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: count(VALUE._col0)\r\n                keys: KEY._col0 (type: int)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1\r\n                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                Reduce Output Operator\r\n                  key expressions: _col0 (type: int)\r\n                  sort order: +\r\n                  Map-reduce partition columns: _col0 (type: int)\r\n                  Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                  value expressions: _col1 (type: bigint)\r\n\r\n  Stage: Stage-0\r\n    Fetch Operator\r\n      limit: -1\r\n      Processor Tree:\r\n        ListSink\r\n\r\n{code}\r\n\r\nAfter\r\n{code}\r\nSTAGE DEPENDENCIES:\r\n  Stage-1 is a root stage\r\n  Stage-0 depends on stages: Stage-1\r\n\r\nSTAGE PLANS:\r\n  Stage: Stage-1\r\n    Tez\r\n      DagId: root_20171027043345_8893c266-bde2-4f23-9914-0dfccc8be5ea:1\r\n      Edges:\r\n        Map 1 <- Map 4 (BROADCAST_EDGE)\r\n        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 3 (BROADCAST_EDGE)\r\n        Reducer 3 <- Map 1 (SIMPLE_EDGE)\r\n      DagName: root_20171027043345_8893c266-bde2-4f23-9914-0dfccc8be5ea:1\r\n      Vertices:\r\n        Map 1 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: a\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Map Join Operator\r\n                        condition map:\r\n                             Inner Join 0 to 1\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n                        outputColumnNames: _col0\r\n                        input vertices:\r\n                          1 Map 4\r\n                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                        HybridGraceHashJoin: true\r\n                        Group By Operator\r\n                          aggregations: count()\r\n                          keys: _col0 (type: int)\r\n                          mode: hash\r\n                          outputColumnNames: _col0, _col1\r\n                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                          Reduce Output Operator\r\n                            key expressions: _col0 (type: int)\r\n                            sort order: +\r\n                            Map-reduce partition columns: _col0 (type: int)\r\n                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                            value expressions: _col1 (type: bigint)\r\n                          Reduce Output Operator\r\n                            key expressions: _col0 (type: int)\r\n                            sort order: +\r\n                            Map-reduce partition columns: _col0 (type: int)\r\n                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                            value expressions: _col1 (type: bigint)\r\n        Map 4 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: b\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: int)\r\n                        sort order: +\r\n                        Map-reduce partition columns: _col0 (type: int)\r\n                        Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n        Reducer 2 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: count(VALUE._col0)\r\n                keys: KEY._col0 (type: int)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1\r\n                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                Map Join Operator\r\n                  condition map:\r\n                       Inner Join 0 to 1\r\n                  keys:\r\n                    0 _col0 (type: int)\r\n                    1 _col0 (type: int)\r\n                  outputColumnNames: _col0, _col1, _col3\r\n                  input vertices:\r\n                    1 Reducer 3\r\n                  Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                  HybridGraceHashJoin: true\r\n                  Select Operator\r\n                    expressions: _col0 (type: int), _col3 (type: bigint), _col1 (type: bigint)\r\n                    outputColumnNames: _col0, _col1, _col2\r\n                    Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                    File Output Operator\r\n                      compressed: false\r\n                      Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                      table:\r\n                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat\r\n                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\r\n                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\r\n        Reducer 3 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: count(VALUE._col0)\r\n                keys: KEY._col0 (type: int)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1\r\n                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                Reduce Output Operator\r\n                  key expressions: _col0 (type: int)\r\n                  sort order: +\r\n                  Map-reduce partition columns: _col0 (type: int)\r\n                  Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                  value expressions: _col1 (type: bigint)\r\n\r\n  Stage: Stage-0\r\n    Fetch Operator\r\n      limit: -1\r\n      Processor Tree:\r\n        ListSink\r\n\r\n{code}\r\n\r\nWe can see that there are only 2 Maps(Map1 and Map4) in the after explain (in before explain, there are 4 Maps).\r\nWhen i tried to change the physical plan for HOS like what HOT does. I found change of the optimization  \r\nbefore\r\n{code}\r\nSTAGE DEPENDENCIES:\r\n  Stage-3 is a root stage\r\n  Stage-2 depends on stages: Stage-3\r\n  Stage-4 depends on stages: Stage-2\r\n  Stage-1 depends on stages: Stage-4\r\n  Stage-0 depends on stages: Stage-1\r\n\r\nSTAGE PLANS:\r\n  Stage: Stage-3\r\n    Spark\r\n      DagName: root_20171027045349_0dc8e597-ffad-4da7-bbfc-c2b7533d9b58:4\r\n      Vertices:\r\n        Map 6 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: b\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Spark HashTable Sink Operator\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n            Local Work:\r\n              Map Reduce Local Work\r\n\r\n  Stage: Stage-2\r\n    Spark\r\n      Edges:\r\n        Reducer 5 <- Map 4 (GROUP, 1)\r\n      DagName: root_20171027045349_0dc8e597-ffad-4da7-bbfc-c2b7533d9b58:2\r\n      Vertices:\r\n        Map 4 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: a\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Map Join Operator\r\n                        condition map:\r\n                             Inner Join 0 to 1\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n                        outputColumnNames: _col0\r\n                        input vertices:\r\n                          1 Map 6\r\n                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                        Group By Operator\r\n                          aggregations: count()\r\n                          keys: _col0 (type: int)\r\n                          mode: hash\r\n                          outputColumnNames: _col0, _col1\r\n                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                          Reduce Output Operator\r\n                            key expressions: _col0 (type: int)\r\n                            sort order: +\r\n                            Map-reduce partition columns: _col0 (type: int)\r\n                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                            value expressions: _col1 (type: bigint)\r\n            Local Work:\r\n              Map Reduce Local Work\r\n        Reducer 5 \r\n            Local Work:\r\n              Map Reduce Local Work\r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: count(VALUE._col0)\r\n                keys: KEY._col0 (type: int)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1\r\n                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                Spark HashTable Sink Operator\r\n                  keys:\r\n                    0 _col0 (type: int)\r\n                    1 _col0 (type: int)\r\n\r\n  Stage: Stage-4\r\n    Spark\r\n      DagName: root_20171027045349_0dc8e597-ffad-4da7-bbfc-c2b7533d9b58:3\r\n      Vertices:\r\n        Map 3 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: b\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Spark HashTable Sink Operator\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n            Local Work:\r\n              Map Reduce Local Work\r\n\r\n  Stage: Stage-1\r\n    Spark\r\n      Edges:\r\n        Reducer 2 <- Map 1 (GROUP, 1)\r\n      DagName: root_20171027045349_0dc8e597-ffad-4da7-bbfc-c2b7533d9b58:1\r\n      Vertices:\r\n        Map 1 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: a\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Map Join Operator\r\n                        condition map:\r\n                             Inner Join 0 to 1\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n                        outputColumnNames: _col0\r\n                        input vertices:\r\n                          1 Map 3\r\n                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                        Group By Operator\r\n                          aggregations: count()\r\n                          keys: _col0 (type: int)\r\n                          mode: hash\r\n                          outputColumnNames: _col0, _col1\r\n                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                          Reduce Output Operator\r\n                            key expressions: _col0 (type: int)\r\n                            sort order: +\r\n                            Map-reduce partition columns: _col0 (type: int)\r\n                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                            value expressions: _col1 (type: bigint)\r\n            Local Work:\r\n              Map Reduce Local Work\r\n        Reducer 2 \r\n            Local Work:\r\n              Map Reduce Local Work\r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: count(VALUE._col0)\r\n                keys: KEY._col0 (type: int)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1\r\n                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                Map Join Operator\r\n                  condition map:\r\n                       Inner Join 0 to 1\r\n                  keys:\r\n                    0 _col0 (type: int)\r\n                    1 _col0 (type: int)\r\n                  outputColumnNames: _col0, _col1, _col3\r\n                  input vertices:\r\n                    1 Reducer 5\r\n                  Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                  Select Operator\r\n                    expressions: _col0 (type: int), _col3 (type: bigint), _col1 (type: bigint)\r\n                    outputColumnNames: _col0, _col1, _col2\r\n                    Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                    File Output Operator\r\n                      compressed: false\r\n                      Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                      table:\r\n                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat\r\n                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\r\n                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\r\n\r\n  Stage: Stage-0\r\n    Fetch Operator\r\n      limit: -1\r\n      Processor Tree:\r\n        ListSink\r\n\r\n\r\n{code}\r\n\r\nAfter\r\n{code}\r\nSTAGE DEPENDENCIES:\r\n  Stage-3 is a root stage\r\n  Stage-2 depends on stages: Stage-3\r\n  Stage-4 depends on stages: Stage-2\r\n  Stage-1 depends on stages: Stage-4\r\n  Stage-0 depends on stages: Stage-1\r\n\r\nSTAGE PLANS:\r\n  Stage: Stage-3\r\n    Spark\r\n      DagName: root_20171027045324_b7f21ae7-9bcc-4077-8577-e6c6747dfcff:3\r\n      Vertices:\r\n        Map 4 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: b\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Spark HashTable Sink Operator\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n            Local Work:\r\n              Map Reduce Local Work\r\n\r\n  Stage: Stage-2\r\n    Spark\r\n      Edges:\r\n        Reducer 3 <- Map 6 (GROUP, 1)\r\n      DagName: root_20171027045324_b7f21ae7-9bcc-4077-8577-e6c6747dfcff:2\r\n      Vertices:\r\n        Map 6 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: a\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Map Join Operator\r\n                        condition map:\r\n                             Inner Join 0 to 1\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n                        outputColumnNames: _col0\r\n                        input vertices:\r\n                          1 Map 4\r\n                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                        Group By Operator\r\n                          aggregations: count()\r\n                          keys: _col0 (type: int)\r\n                          mode: hash\r\n                          outputColumnNames: _col0, _col1\r\n                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                          Reduce Output Operator\r\n                            key expressions: _col0 (type: int)\r\n                            sort order: +\r\n                            Map-reduce partition columns: _col0 (type: int)\r\n                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                            value expressions: _col1 (type: bigint)\r\n            Local Work:\r\n              Map Reduce Local Work\r\n        Reducer 3 \r\n            Local Work:\r\n              Map Reduce Local Work\r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: count(VALUE._col0)\r\n                keys: KEY._col0 (type: int)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1\r\n                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                Spark HashTable Sink Operator\r\n                  keys:\r\n                    0 _col0 (type: int)\r\n                    1 _col0 (type: int)\r\n\r\n  Stage: Stage-4\r\n    Spark\r\n      DagName: root_20171027045324_b7f21ae7-9bcc-4077-8577-e6c6747dfcff:4\r\n      Vertices:\r\n        Map 4 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: b\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Spark HashTable Sink Operator\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n            Local Work:\r\n              Map Reduce Local Work\r\n\r\n  Stage: Stage-1\r\n    Spark\r\n      Edges:\r\n        Reducer 2 <- Map 5 (GROUP, 1)\r\n      DagName: root_20171027045324_b7f21ae7-9bcc-4077-8577-e6c6747dfcff:1\r\n      Vertices:\r\n        Map 5 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: a\r\n                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                  Filter Operator\r\n                    predicate: key is not null (type: boolean)\r\n                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                    Select Operator\r\n                      expressions: key (type: int)\r\n                      outputColumnNames: _col0\r\n                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE\r\n                      Map Join Operator\r\n                        condition map:\r\n                             Inner Join 0 to 1\r\n                        keys:\r\n                          0 _col0 (type: int)\r\n                          1 _col0 (type: int)\r\n                        outputColumnNames: _col0\r\n                        input vertices:\r\n                          1 Map 4\r\n                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                        Group By Operator\r\n                          aggregations: count()\r\n                          keys: _col0 (type: int)\r\n                          mode: hash\r\n                          outputColumnNames: _col0, _col1\r\n                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                          Reduce Output Operator\r\n                            key expressions: _col0 (type: int)\r\n                            sort order: +\r\n                            Map-reduce partition columns: _col0 (type: int)\r\n                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE\r\n                            value expressions: _col1 (type: bigint)\r\n            Local Work:\r\n              Map Reduce Local Work\r\n        Reducer 2 \r\n            Local Work:\r\n              Map Reduce Local Work\r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: count(VALUE._col0)\r\n                keys: KEY._col0 (type: int)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1\r\n                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE\r\n                Map Join Operator\r\n                  condition map:\r\n                       Inner Join 0 to 1\r\n                  keys:\r\n                    0 _col0 (type: int)\r\n                    1 _col0 (type: int)\r\n                  outputColumnNames: _col0, _col1, _col3\r\n                  input vertices:\r\n                    1 Reducer 3\r\n                  Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                  Select Operator\r\n                    expressions: _col0 (type: int), _col3 (type: bigint), _col1 (type: bigint)\r\n                    outputColumnNames: _col0, _col1, _col2\r\n                    Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                    File Output Operator\r\n                      compressed: false\r\n                      Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE\r\n                      table:\r\n                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat\r\n                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\r\n                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\r\n\r\n  Stage: Stage-0\r\n    Fetch Operator\r\n      limit: -1\r\n      Processor Tree:\r\n        ListSink\r\n\r\n{code}\r\nonly the Map about table {{b}} are merged to 1 Map, there are still two Maps about table {{a}}(Map1,Map4).\r\n\r\nThe reason causes this is because [GenSparkWork|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java#L421] will split following physical operator tree once encounting RS\r\n{code}\r\nTS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[11]-GBY[12]-MAPJOIN[47]-SEL[31]-FS[32]\r\n                                        -RS[25]-GBY[26]-RS[29]-MAPJOIN[47]\r\nTS[3]-FIL[42]-SEL[5]-RS[7]-MAPJOIN[45]\r\n{code}  to \r\n{code}\r\nMap1:TS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[11]\r\nReduce2:GBY[12]-MAPJOIN[47]-SEL[31]-FS[32]\r\nMap3:TS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[25]\r\nReduce4:GBY[26]-RS[29]\r\nMap5:TS[3]-FIL[42]-SEL[5]-RS[7]\r\n{code}\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-10-27T09:11:33.838+0000","updated":"2017-10-27T09:11:33.838+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16221992","id":"16221992","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"In tez, Map can be connected 2 Reducers while in spark, we can not do this. In above example, \r\n{code}\r\n        Map 1 <- Map 4 (BROADCAST_EDGE)\r\n        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 3 (BROADCAST_EDGE)\r\n        Reducer 3 <- Map 1 (SIMPLE_EDGE)\r\n{code}\r\nThere are 2 edges from Map1 to other vertexes( Map1->Reducer3, Map1->Reducer2)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-10-27T09:16:23.422+0000","updated":"2017-10-27T09:16:23.422+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16235054","id":"16235054","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"Now HoS does not support multiple edge between two vertex. Let's give an example to show this.\r\nTPC-DS/[query28.sql|https://github.com/kellyzly/hive-testbench/blob/hive14/sample-queries-tpcds/query28.sql].   Before scan shared optimization(HIVE-16602). the tez explain is  [scanshare.before.svg|https://issues.apache.org/jira/secure/attachment/12895148/scanshare.before.svg]. After scan shared optimization(HIVE-16602), the tez explain is [scanshare.after.svg|https://issues.apache.org/jira/secure/attachment/12895149/scanshare.after.svg]. We can see that after optimization, there is only 1 map(before there are 6 maps). But later the only map Map1 connects other 6 reducers by 6 edges. This is because tez supports mulitple edges between two vertexes(TEZ-1190). Now i am working on enabling this feature on HoS. But in HoS, it does not support \" mulitple edges between two vertexes\". So even i change the physical plan as what HoT does, it may not reduce the number of Map. [~lirui],[~xuefuz],can you help to see the problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-02T01:27:54.223+0000","updated":"2017-11-02T01:27:54.223+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16235085","id":"16235085","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~kellyzly], I think your observation is correct. Spark has certain limitations. In fact, the edge theory doesn't even apply to Spark. Spark uses RDD model. Internally Hive translates the DAG to RDD operations (transformations and actions). In the example of ( Map1->Reducer3, Map1->Reducer2), Hive on Spark actually has a plan like (map12 - > reduce2, map13 ->reduce3) with map12 = map13. This way, there will be two spark jobs. In the second job, the cached result is used instead of loading the data again. BTW, this is a multi-insert example.\r\n\r\nMultiple edges between two vertices are even less thinkable. You might be able to turn this optimization for Spark, but Spark might not be able to run it. I'm not sure if there is any case that this optimization might help Spark. My gut feeling is that this needs to be combined with Spark RDD caching or HIve's materialized view.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-02T02:16:17.264+0000","updated":"2017-11-02T02:16:17.264+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16235094","id":"16235094","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]:\r\n\r\n{quote}\r\n My gut feeling is that this needs to be combined with Spark RDD caching or Hive's materialized view.\r\n{quote}\r\n About the optimization, I found that Hive on Tez can get indeed improvement(20%+) in TPC-DS/query28,88,90 on not excellent hw or in table scan with huge data. So I want to implement it on the Hive on Spark.  \r\n I agree that we need to combine Spark RDD caching with the optimization to reduce the table scan. As you described, the multi-insert case  benefits from the Spark RDD caching because map12=map13. But more complex cases can not. Use DS/query28.sql as an example.\r\n The physical plan:\r\n {code}\r\nTS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]\r\nTS[7]-FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]\r\nTS[14]-FIL[54]-SEL[16]-GBY[17]-RS[18]-GBY[19]-RS[44]-JOIN[48]\r\nTS[21]-FIL[55]-SEL[23]-GBY[24]-RS[25]-GBY[26]-RS[45]-JOIN[48]\r\nTS[28]-FIL[56]-SEL[30]-GBY[31]-RS[32]-GBY[33]-RS[46]-JOIN[48]\r\nTS[35]-FIL[57]-SEL[37]-GBY[38]-RS[39]-GBY[40]-RS[47]-JOIN[48]\r\n{code}\r\n\r\nAfter the scan share optimization, the phyiscal plan\r\n{code}\r\nTS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]\r\n     -FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]\r\n     -FIL[54]-SEL[16]-GBY[17]-RS[18]-GBY[19]-RS[44]-JOIN[48]\r\n     -FIL[55]-SEL[23]-GBY[24]-RS[25]-GBY[26]-RS[45]-JOIN[48]\r\n     -FIL[56]-SEL[30]-GBY[31]-RS[32]-GBY[33]-RS[46]-JOIN[48]\r\n     -FIL[57]-SEL[37]-GBY[38]-RS[39]-GBY[40]-RS[47]-JOIN[48]\r\n\r\n{code}\r\n\r\nHoS will split operators trees when encounting {{RS}}.\r\n{code}\r\nMap1: TS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]\r\nMap2: TS[0]-FIL[53]-SEL[9]-GBY[10]-RS[11]\r\nMap3: TS[0]-FIL[54]-SEL[16]-GBY[17]-RS[18]\r\nMap4: TS[0]-FIL[55]-SEL[23]-GBY[24]-RS[25]\r\nMap5: TS[0] -FIL[56]-SEL[30]-GBY[31]-RS[32]\r\nMap6: TS[0]-FIL[57]-SEL[37]-GBY[38]-RS[39]\r\n{code}\r\n\r\nWe can not combine Map1,..., Map6 because the {{FIL}}(FIL\\[52\\], FIL\\[53\\],...,FIL\\[57\\]) are not same.\r\nSo what i think about can we directly extract TS from MapTask and put the TS to a single Map\r\n{code}\r\nMap0: TS[0]\r\nMap1: FIL[52]-SEL[2]-GBY[3]-RS[4]\r\nMap2: FIL[53]-SEL[9]-GBY[10]-RS[11]\r\nMap3: FIL[54]-SEL[16]-GBY[17]-RS[18]\r\nMap4: FIL[55]-SEL[23]-GBY[24]-RS[25]\r\nMap5: FIL[56]-SEL[30]-GBY[31]-RS[32]\r\nMap6: FIL[57]-SEL[37]-GBY[38]-RS[39]\r\n{code}\r\nThere is only TS\\[0\\] in the Map0 and connect Map0 to Map1,...,Map6.  Appreciate to get some suggestion from you!\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-02T02:42:19.356+0000","updated":"2017-11-02T02:42:19.356+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16235116","id":"16235116","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~kellyzly],\r\nbq. In tez, Map can be connected 2 Reducers while in spark, we can not do this.\r\nMy understanding is HoS also supports one Map connecting to multiple Reducers - that's what {{CombineEquivalentWorkResolver}} is intended for and there're some example queries in {{dynamic_rdd_cache.q}}. The problem here is HoS doesn't merge equivalent works as aggressively as HoT does. Is that right?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-02T03:27:19.701+0000","updated":"2017-11-02T03:27:19.701+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16235125","id":"16235125","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]:\r\n{quote}\r\nMy understanding is HoS also supports one Map connecting to multiple Reducers \r\n{quote}\r\nThere is only 1 RS in Map in HoS. It is true that there are cases that 1 Map is used by two Reducers in HoS. But in HoT, 2 RS are allowed in 1 Map, the different 2 RS in the 1 Map can transfer different data to 2 different Reducers. \r\n{quote}\r\nThe problem here is HoS doesn't merge equivalent works as aggressively as HoT does. \r\n{quote}\r\nyes","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-02T03:34:15.503+0000","updated":"2017-11-02T03:35:01.295+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16235131","id":"16235131","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: what i want to ask is there any possiblity to change current structure in the SparkTask in HoS\r\n{code}\r\nM->R \r\n{code}\r\nto \r\n{code}\r\nM->M->R\r\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-02T03:45:24.797+0000","updated":"2017-11-02T03:45:24.797+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16235137","id":"16235137","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kellyzly] I think M->M->R is possible. It's just that the current planner doesn't do this, but in theory it can be done. Currently the assumption is that a Map task is always followed by a Reduce task. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-02T03:54:34.889+0000","updated":"2017-11-02T03:54:34.889+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16235400","id":"16235400","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I also think that's possible in theory. But I guess it will require lots of work. E.g. we may need to modify MapOperator to accommodate the new M->M->R scheme. I'm wondering whether it'll be easier to find a way to cache equivalent MapInput. I noted the caching was disabled by HIVE-8920. [~xuefuz] do you still remember the reason why it was disabled?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-02T08:42:41.086+0000","updated":"2017-11-02T08:42:41.086+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16236087","id":"16236087","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~lirui] The reason was briefly given at https://issues.apache.org/jira/browse/HIVE-8920?focusedCommentId=14260846&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14260846. I was dealing with the IOContext initialization issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-02T16:36:54.090+0000","updated":"2017-11-02T16:36:54.090+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16237271","id":"16237271","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]:\r\n{quote}\r\nI also think that's possible in theory. But I guess it will require lots of work. E.g. we may need to modify MapOperator to accommodate the new M->M->R scheme\r\n{quote}\r\n  now i am working on changing from {{M->R}} to {{M->M->R}} schema. Not very clear about the modification on MapOperator. If you know, please say more detailed. I think at first need change {{GenSparkWork}} to split the physical operator trees once en-counting one TS has more than 1 child.  For example \r\nphysical plan\r\n{code}\r\nTS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]\r\n        -FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]\r\n\r\n{code}\r\nAs TS\\[0\\] has two children(FIL\\[52\\], FIL\\[53\\]). First split at TS\\[0\\] and bring it to Map1, then split following operator trees when en counting RS.  So the final operator tree will be\r\n{code}\r\nMap1: TS[0]\r\nMap2:FIL[52]-SEL[2]-GBY[3]-RS[4]\r\nMap3:FIL[53]-SEL[9]-GBY[10]-RS[11]\r\nReducer1:GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]\r\nReducer2:GBY[12]-RS[43]\r\n{code}\r\nThis is very initial thinking. If have suggestion, please tell me, thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-11-03T08:30:15.676+0000","updated":"2017-11-03T08:30:15.676+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16276511","id":"16276511","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"I set the flag {{hive.spark.optimize.shared.work}} to enable the SharedWorkOptimizer in Hive on Spark.  The attach explain.28.scan.share.true is the explain when enabling the flag and explain.28.scan.share.false is the explain when disabling the flag for [DS/query28.sql|https://github.com/kellyzly/hive-testbench/blob/hive14/sample-queries-tpcds/query28.sql]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-04T09:24:15.545+0000","updated":"2017-12-04T09:24:15.545+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16276526","id":"16276526","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"explain.28.scan.share.true\r\n\r\nwe can see that there is only operator (TS) in Map1, and the child of TS to the RS are belongs to another Map(Map12,Map15,Map18,Map2,Map6,Map9). So  change current {{M-R}} in 1 SparkTask to {{M-M-R}}\r\n{code}\r\nSTAGE DEPENDENCIES:\r\n  Stage-1 is a root stage\r\n  Stage-0 depends on stages: Stage-1\r\n\r\nSTAGE PLANS:\r\n  Stage: Stage-1\r\n    Spark\r\n      Edges:\r\n        Map 12 <- Map 1 (NONE, 1000)\r\n        Map 15 <- Map 1 (NONE, 1000)\r\n        Map 18 <- Map 1 (NONE, 1000)\r\n        Map 2 <- Map 1 (NONE, 1000)\r\n        Map 6 <- Map 1 (NONE, 1000)\r\n        Map 9 <- Map 1 (NONE, 1000)\r\n        Reducer 10 <- Map 9 (GROUP PARTITION-LEVEL SORT, 1)\r\n        Reducer 13 <- Map 12 (GROUP PARTITION-LEVEL SORT, 1)\r\n        Reducer 16 <- Map 15 (GROUP PARTITION-LEVEL SORT, 1)\r\n        Reducer 19 <- Map 18 (GROUP PARTITION-LEVEL SORT, 1)\r\n        Reducer 3 <- Map 2 (GROUP PARTITION-LEVEL SORT, 1)\r\n        Reducer 4 <- Reducer 10 (PARTITION-LEVEL SORT, 1), Reducer 13 (PARTITION-LEVEL SORT, 1), Reducer 16 (PARTITION-LEVEL SORT, 1), Reducer 19 (PARTITION-LEVEL SORT, 1), Reducer 3 (PARTITION-LEVEL SORT, 1), Reducer 7 (PARTITION-LEVEL SORT, 1)\r\n        Reducer 7 <- Map 6 (GROUP PARTITION-LEVEL SORT, 1)\r\n      DagName: root_20171204042631_0435ff7e-3f10-4c84-a5fc-dc5b607497ba:1\r\n      Vertices:\r\n        Map 1 \r\n            Map Operator Tree:\r\n                TableScan\r\n                  alias: store_sales\r\n                  filterExpr: ((ss_quantity BETWEEN 0 AND 5 and (ss_list_price BETWEEN 11 AND 21 or ss_coupon_amt BETWEEN 460 AND 1460 or ss_wholesale_cost BETWEEN 14 AND 34)) or (ss_quantity BETWEEN 6 AND 10 and (ss_list_price BETWEEN 91 AND 101 or ss_coupon_amt BETWEEN 1430 AND 2430 or ss_wholesale_cost BETWEEN 32 AND 52)) or (ss_quantity BETWEEN 11 AND 15 and (ss_list_price BETWEEN 66 AND 76 or ss_coupon_amt BETWEEN 920 AND 1920 or ss_wholesale_cost BETWEEN 4 AND 24)) or (ss_quantity BETWEEN 16 AND 20 and (ss_list_price BETWEEN 142 AND 152 or ss_coupon_amt BETWEEN 3054 AND 4054 or ss_wholesale_cost BETWEEN 80 AND 100)) or (ss_quantity BETWEEN 21 AND 25 and (ss_list_price BETWEEN 135 AND 145 or ss_coupon_amt BETWEEN 14180 AND 15180 or ss_wholesale_cost BETWEEN 38 AND 58)) or (ss_quantity BETWEEN 26 AND 30 and (ss_list_price BETWEEN 28 AND 38 or ss_coupon_amt BETWEEN 2513 AND 3513 or ss_wholesale_cost BETWEEN 42 AND 62))) (type: boolean)\r\n                  Statistics: Num rows: 28800991 Data size: 4751513940 Basic stats: COMPLETE Column stats: NONE\r\n            Execution mode: vectorized\r\n        Map 12 \r\n            Map Operator Tree:\r\n                Filter Operator\r\n                  predicate: (ss_quantity BETWEEN 16 AND 20 and (ss_list_price BETWEEN 142 AND 152 or ss_coupon_amt BETWEEN 3054 AND 4054 or ss_wholesale_cost BETWEEN 80 AND 100)) (type: boolean)\r\n                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                  Select Operator\r\n                    expressions: ss_list_price (type: double)\r\n                    outputColumnNames: ss_list_price\r\n                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                    Group By Operator\r\n                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)\r\n                      keys: ss_list_price (type: double)\r\n                      mode: hash\r\n                      outputColumnNames: _col0, _col1, _col2, _col3\r\n                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: double)\r\n                        sort order: +\r\n                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)\r\n        Map 15 \r\n            Map Operator Tree:\r\n                Filter Operator\r\n                  predicate: (ss_quantity BETWEEN 21 AND 25 and (ss_list_price BETWEEN 135 AND 145 or ss_coupon_amt BETWEEN 14180 AND 15180 or ss_wholesale_cost BETWEEN 38 AND 58)) (type: boolean)\r\n                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                  Select Operator\r\n                    expressions: ss_list_price (type: double)\r\n                    outputColumnNames: ss_list_price\r\n                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                    Group By Operator\r\n                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)\r\n                      keys: ss_list_price (type: double)\r\n                      mode: hash\r\n                      outputColumnNames: _col0, _col1, _col2, _col3\r\n                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: double)\r\n                        sort order: +\r\n                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)\r\n        Map 18 \r\n            Map Operator Tree:\r\n                Filter Operator\r\n                  predicate: (ss_quantity BETWEEN 26 AND 30 and (ss_list_price BETWEEN 28 AND 38 or ss_coupon_amt BETWEEN 2513 AND 3513 or ss_wholesale_cost BETWEEN 42 AND 62)) (type: boolean)\r\n                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                  Select Operator\r\n                    expressions: ss_list_price (type: double)\r\n                    outputColumnNames: ss_list_price\r\n                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                    Group By Operator\r\n                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)\r\n                      keys: ss_list_price (type: double)\r\n                      mode: hash\r\n                      outputColumnNames: _col0, _col1, _col2, _col3\r\n                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: double)\r\n                        sort order: +\r\n                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)\r\n        Map 2 \r\n            Map Operator Tree:\r\n                Filter Operator\r\n                  predicate: (ss_quantity BETWEEN 0 AND 5 and (ss_list_price BETWEEN 11 AND 21 or ss_coupon_amt BETWEEN 460 AND 1460 or ss_wholesale_cost BETWEEN 14 AND 34)) (type: boolean)\r\n                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                  Select Operator\r\n                    expressions: ss_list_price (type: double)\r\n                    outputColumnNames: ss_list_price\r\n                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                    Group By Operator\r\n                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)\r\n                      keys: ss_list_price (type: double)\r\n                      mode: hash\r\n                      outputColumnNames: _col0, _col1, _col2, _col3\r\n                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: double)\r\n                        sort order: +\r\n                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)\r\n        Map 6 \r\n            Map Operator Tree:\r\n                Filter Operator\r\n                  predicate: (ss_quantity BETWEEN 6 AND 10 and (ss_list_price BETWEEN 91 AND 101 or ss_coupon_amt BETWEEN 1430 AND 2430 or ss_wholesale_cost BETWEEN 32 AND 52)) (type: boolean)\r\n                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                  Select Operator\r\n                    expressions: ss_list_price (type: double)\r\n                    outputColumnNames: ss_list_price\r\n                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                    Group By Operator\r\n                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)\r\n                      keys: ss_list_price (type: double)\r\n                      mode: hash\r\n                      outputColumnNames: _col0, _col1, _col2, _col3\r\n                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: double)\r\n                        sort order: +\r\n                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)\r\n        Map 9 \r\n            Map Operator Tree:\r\n                Filter Operator\r\n                  predicate: (ss_quantity BETWEEN 11 AND 15 and (ss_list_price BETWEEN 66 AND 76 or ss_coupon_amt BETWEEN 920 AND 1920 or ss_wholesale_cost BETWEEN 4 AND 24)) (type: boolean)\r\n                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                  Select Operator\r\n                    expressions: ss_list_price (type: double)\r\n                    outputColumnNames: ss_list_price\r\n                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                    Group By Operator\r\n                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)\r\n                      keys: ss_list_price (type: double)\r\n                      mode: hash\r\n                      outputColumnNames: _col0, _col1, _col2, _col3\r\n                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                      Reduce Output Operator\r\n                        key expressions: _col0 (type: double)\r\n                        sort order: +\r\n                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE\r\n                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)\r\n        Reducer 10 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1, _col2\r\n                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                Reduce Output Operator\r\n                  sort order: \r\n                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)\r\n        Reducer 13 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1, _col2\r\n                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                Reduce Output Operator\r\n                  sort order: \r\n                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)\r\n        Reducer 16 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1, _col2\r\n                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                Reduce Output Operator\r\n                  sort order: \r\n                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)\r\n        Reducer 19 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1, _col2\r\n                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                Reduce Output Operator\r\n                  sort order: \r\n                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)\r\n        Reducer 3 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1, _col2\r\n                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                Reduce Output Operator\r\n                  sort order: \r\n                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)\r\n        Reducer 4 \r\n            Reduce Operator Tree:\r\n              Join Operator\r\n                condition map:\r\n                     Inner Join 0 to 1\r\n                     Inner Join 0 to 2\r\n                     Inner Join 0 to 3\r\n                     Inner Join 0 to 4\r\n                     Inner Join 0 to 5\r\n                keys:\r\n                  0 \r\n                  1 \r\n                  2 \r\n                  3 \r\n                  4 \r\n                  5 \r\n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17\r\n                Statistics: Num rows: 1 Data size: 625 Basic stats: COMPLETE Column stats: NONE\r\n                Limit\r\n                  Number of rows: 100\r\n                  Statistics: Num rows: 1 Data size: 625 Basic stats: COMPLETE Column stats: NONE\r\n                  File Output Operator\r\n                    compressed: false\r\n                    Statistics: Num rows: 1 Data size: 625 Basic stats: COMPLETE Column stats: NONE\r\n                    table:\r\n                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\r\n                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\r\n                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\r\n        Reducer 7 \r\n            Reduce Operator Tree:\r\n              Group By Operator\r\n                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)\r\n                mode: mergepartial\r\n                outputColumnNames: _col0, _col1, _col2\r\n                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                Reduce Output Operator\r\n                  sort order: \r\n                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE\r\n                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)\r\n\r\n  Stage: Stage-0\r\n    Fetch Operator\r\n      limit: 100\r\n      Processor Tree:\r\n        ListSink\r\n\r\n\r\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-04T09:40:01.225+0000","updated":"2017-12-04T09:40:01.225+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16276547","id":"16276547","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"will upload patch later and describe the problems i encountered later.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-04T09:54:25.433+0000","updated":"2017-12-04T09:54:25.433+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16278167","id":"16278167","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"Here record the problems currently I met\r\n1. I want to change the M->R to M->M->R and split the operator tree when encountering TS. I create [SparkRuleDispatcher| https://github.com/kellyzly/hive/blob/HIVE-17486.3/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkRuleDispatcher.java] to apply rules to the operator tree, the reason why i don't use DefaultRuleDispatcher is because there already a rule called [Handle Analyze Command|https://github.com/kellyzly/hive/blob/jdk9-trial/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java#L432] to split operator trees once encountering TS. Original [SparkCompiler#opRules|https://github.com/kellyzly/hive/blob/jdk9-trial/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java#L417] is a linkedHashMap which stores one key with one value. It can not deal with the case where one key with two values. So current solution is to modify SparkCompile#opRules to a Multimap and create SparkRuleDispatcher . But I am afraid once encountering TS, only 1 rule will be applied  for {{SparkRuleDispatcher#dispatch}}\r\n\r\nSparkRuleDispatcher#dispatch\r\n{code}\r\n\r\n@Override\r\n  public Object dispatch(Node nd, Stack<Node> ndStack, Object... nodeOutputs)\r\n      throws SemanticException {\r\n\r\n    // find the firing rule\r\n    // find the rule from the stack specified\r\n    Rule rule = null;\r\n    int minCost = Integer.MAX_VALUE;\r\n    for (Rule r : procRules.keySet()) {\r\n      int cost = r.cost(ndStack);\r\n      if ((cost >= 0) && (cost < minCost)) {\r\n        minCost = cost;\r\n        // Here I am afraid there is only 1 rule will be applied even there are two rules for TS\r\n        rule = r;\r\n      }\r\n    }\r\n\r\n    Collection<NodeProcessor> procSet;\r\n\r\n    if (rule == null) {\r\n      procSet = defaultProcSet;\r\n    } else {\r\n      procSet = procRules.get(rule);\r\n    }\r\n\r\n    // Do nothing in case proc is null\r\n    Object ret = null;\r\n    for (NodeProcessor proc : procSet) {\r\n      if (proc != null) {\r\n        // Call the process function\r\n        ret = proc.process(nd, ndStack, procCtx, nodeOutputs);\r\n      }\r\n    }\r\n    return ret;\r\n  }\r\n{code}\r\n\r\nI can change above code like following but don't know return the result of which rule if there are more than 1 rule for TS.\r\n{code}\r\n  @Override\r\n  public Object dispatch(Node nd, Stack<Node> ndStack, Object... nodeOutputs)\r\n      throws SemanticException {\r\n\r\n    // find the firing rule\r\n    // find the rule from the stack specified\r\n    ArrayList ruleList =new ArrayList();\r\n    int minCost = Integer.MAX_VALUE;\r\n    for (Rule r : procRules.keySet()) {\r\n      int cost = r.cost(ndStack);\r\n      if ((cost >= 0) && (cost < minCost)) {\r\n        minCost = cost;\r\n        ruleList.add(r);\r\n      }\r\n    }\r\n\r\n    Collection<NodeProcessor> procSet;\r\n\r\n    if (ruleList.size() == 0) {\r\n      procSet = defaultProcSet;\r\n    } else {\r\n      for(Rule r: ruleList) {\r\n        // Question: Here I don't know which rule I should use if there is more than 1 rule in the ruleList\r\n        procSet = procRules.get(r);\r\n      }\r\n    }\r\n\r\n    // Do nothing in case proc is null\r\n    Object ret = null;\r\n    for (NodeProcessor proc : procSet) {\r\n      if (proc != null) {\r\n        // Call the process function\r\n        ret = proc.process(nd, ndStack, procCtx, nodeOutputs);\r\n      }\r\n    }\r\n    return ret;\r\n\r\n  }\r\n}\r\n\r\n{code}\r\n[~lirui], [~xuefuz] can you give your suggestions about the problem?\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-05T07:58:59.795+0000","updated":"2017-12-05T07:58:59.795+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16281238","id":"16281238","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui], [~xuefuz]: can you spend some time to view the problem i met about  DefaultRuleDispatcher? thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-07T02:05:37.260+0000","updated":"2017-12-07T02:05:37.260+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16282964","id":"16282964","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~kellyzly], I think this thread is getting a little bit long and the problem doesn't seem trivial. Could you please create a doc that describes the problem or feature we are addressing and your proposal? That's probably easier to communicate. Thanks. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-12-08T03:12:05.731+0000","updated":"2017-12-08T03:12:05.731+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16282966","id":"16282966","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]: ok, will upload doc soon.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-08T03:14:45.979+0000","updated":"2017-12-08T03:14:45.979+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16283010","id":"16283010","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"the document includes the introduction, problems  and limitations of this optimization.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-08T03:40:02.549+0000","updated":"2017-12-08T03:40:02.549+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16283012","id":"16283012","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]: have uploaded the [design doc|https://docs.google.com/document/d/1f4f0oMhN2vKSTCtXbnd3FBYOV02H4QflX1BbkglnC30/edit?usp=sharing]. I described the problems i met in the [Problem Section|https://docs.google.com/document/d/1f4f0oMhN2vKSTCtXbnd3FBYOV02H4QflX1BbkglnC30/edit#heading=h.d0ptagvbv8k3], please help view the problem if have time, thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-08T03:42:39.674+0000","updated":"2017-12-08T03:42:39.674+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16283241","id":"16283241","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz] and [~lirui]: the second problem I found  is the modification of MapOperator after changing from M->R to M->M->R.  \r\nMapOperator  is reponsible for deserializing and  [initObjectInspector| https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java#L170]. For the second M in M->M->R,  deserializing is not necessary and initObjectInspector is necessary. Currently I am investigating how to make this work. If there is some wrong in my understanding, please tell me!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-08T09:16:48.794+0000","updated":"2017-12-08T09:16:48.794+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16285470","id":"16285470","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui] and [~xuefuz]: Please help view the [First Problem|https://docs.google.com/document/d/1f4f0oMhN2vKSTCtXbnd3FBYOV02H4QflX1BbkglnC30/edit#heading=h.d0ptagvbv8k3] i mentioned in the design doc, thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-11T03:12:41.533+0000","updated":"2017-12-11T03:12:41.533+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16285523","id":"16285523","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~kellyzly], could you provide more design and implementation details in your doc?\r\nFor the first problem, can the new feature be done in a separate pass, so that you don't need to deal with existing rules or graph walker?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-11T05:33:28.943+0000","updated":"2017-12-11T05:33:28.943+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16285550","id":"16285550","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kellyzly], Thanks for working on this. I'm not sure if we should just look at TS to determine whether to generate M-M-R. It seems that we can do so whenever an TS is connected to multiple RSs. The split point should happen at the fork. I'm not sure what's the best way to apply the optimization rules, but if you look at SparkProcessAnalyzeTable, it has an if statement to check if it's an analyze table command. If not, it doesn't do anything. Thus, you can have a super rule that covers both analyze table and the new rule you're adding.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-12-11T06:15:14.450+0000","updated":"2017-12-11T06:15:14.450+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16285551","id":"16285551","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]:\r\n{quote}\r\ncould you provide more design and implementation details in your doc?\r\n{quote}\r\nok, will add more in the design doc.\r\n{quote}\r\nFor the first problem, can the new feature be done in a separate pass, so that you don't need to deal with existing rules or graph walker?\r\n{quote}\r\nthanks for your suggestion, I will try to avoid current problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-11T06:19:28.998+0000","updated":"2017-12-11T06:19:28.998+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16285558","id":"16285558","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]\r\n{quote}  It seems that we can do so whenever an TS is connected to multiple RSs. The split point should happen at the fork. {quote}  not very understand about this. Currently the split is on the TS\r\nfor example\r\n{code}\r\nTS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]\r\n        -FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]\r\n{code}\r\n\r\n->\r\n{code}\t\t\r\nMap1: TS[0]\r\nMap2:FIL[52]-SEL[2]-GBY[3]-RS[4]\r\nMap3:FIL[53]-SEL[9]-GBY[10]-RS[11]\r\nReducer1:GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]\r\nReducer2:GBY[12]-RS[43]\r\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-11T06:29:38.903+0000","updated":"2017-12-11T06:41:19.521+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16286811","id":"16286811","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I meant, if FIL[52] and FIL[53] is the same in your example, then we should break after the filter op for M-M split. Looking forward to your complete design doc for this. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-12-12T00:01:52.328+0000","updated":"2017-12-12T00:01:52.328+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16292203","id":"16292203","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui] and [~xuefuz]:  I have updated  HIVE-17486.2.patch and designed again . A similar case like DS/query28.sql is run successfully.\r\nThis proved that current design(M-M-R) can use RDD cache to reduce the table scan.\r\nIn the latest design doc, there is a simple case.  Currently, although i have added the simple case(spark_optimize_shared_work.q), there is some exception when running qtest in my local env. Once fix the problem, will trigger Hive QA to test.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-15T08:42:40.357+0000","updated":"2017-12-15T09:15:58.918+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16292286","id":"16292286","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]: before you mentioned that the reason to disable caching for MapInput because of [IOContext initialization problem|https://issues.apache.org/jira/browse/HIVE-8920?focusedCommentId=14260846&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14260846]. And reading HIVE-9041, it shows an example to show IOContext initialization problem\r\n{code}\r\nI just found another bug regarding IOContext, when caching is turned on.\r\nTaking the sample query above as example, right now I have this result plan:\r\n\r\n   MW 1 (table0)   MW 2 (table1)   MW 3 (table0)   MW 4 (table1)\r\n      \\            /                 \\             /\r\n       \\          /                   \\           /\r\n        \\        /                     \\         /\r\n         \\      /                       \\       /\r\n           RW 1                           RW 2\r\nSuppose MapWorks are executed from left to right, also suppose we are just running with a single thread.\r\nThen, the following will happen:\r\n1. executing MW 1: since this is the first time we access table0, initialize IOContext and make input path point to table0;\r\n2. executing MW 2: since this is the first time we access table1, initialize IOContext and make input path point to table1;\r\n3. executing MW 3: since this is the second time access table0, do not initialize IOContext, and use the copy saved in step 2), which is table1.\r\n\r\nStep 3 will then fail.\r\n how to make MW 3 know that it needs to get the saved IOContext from MW 1, but not MW 2\r\n{code}\r\nIf the problem exists in the MapInput RDD cache because of IOContext is a static variable which will be stored in cache and the IOContext will be updated in different Maps. Why only disable in [MapInput rdd cache|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java#L202]? \r\nThis should be disabled in all MapTrans.  Please explain more if have time.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-15T09:58:57.741+0000","updated":"2017-12-15T09:58:57.741+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16292836","id":"16292836","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"add spark_optimize_shared_work.q.out and update HIVE-17486.3.patch. Trigger QA tests to see whether patch influences current code or not.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-15T17:16:31.841+0000","updated":"2017-12-15T17:16:31.841+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16293337","id":"16293337","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12902412/HIVE-17486.3.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/8276/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/8276/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8276/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'\n2017-12-15 22:03:28.091\n+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]\n+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'MAVEN_OPTS=-Xmx1g '\n+ MAVEN_OPTS='-Xmx1g '\n+ cd /data/hiveptest/working/\n+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-8276/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ date '+%Y-%m-%d %T.%3N'\n2017-12-15 22:03:28.094\n+ cd apache-github-source-source\n+ git fetch origin\n+ git reset --hard HEAD\nHEAD is now at f52e8b4 HIVE-18258: Vectorization: Reduce-Side GROUP BY MERGEPARTIAL with duplicate columns is broken (Matt McCline, reviewed by Teddy Choi)\n+ git clean -f -d\n+ git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n+ git reset --hard origin/master\nHEAD is now at f52e8b4 HIVE-18258: Vectorization: Reduce-Side GROUP BY MERGEPARTIAL with duplicate columns is broken (Matt McCline, reviewed by Teddy Choi)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ date '+%Y-%m-%d %T.%3N'\n2017-12-15 22:03:33.108\n+ rm -rf ../yetus\n+ mkdir ../yetus\n+ cp -R . ../yetus\n+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-8276/yetus\n+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hiveptest/working/scratch/build.patch\n+ [[ -f /data/hiveptest/working/scratch/build.patch ]]\n+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh\n+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch\nerror: a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java: does not exist in index\nerror: a/itests/src/test/resources/testconfiguration.properties: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkMapRecordHandler.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java: does not exist in index\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java: does not exist in index\nGoing to apply patch with: git apply -p1\n/data/hiveptest/working/scratch/build.patch:1448: trailing whitespace.\n        Map 1 \n/data/hiveptest/working/scratch/build.patch:1463: trailing whitespace.\n                        sort order: \n/data/hiveptest/working/scratch/build.patch:1466: trailing whitespace.\n        Map 4 \n/data/hiveptest/working/scratch/build.patch:1481: trailing whitespace.\n                        sort order: \n/data/hiveptest/working/scratch/build.patch:1484: trailing whitespace.\n        Reducer 2 \nwarning: squelched 18 whitespace errors\nwarning: 23 lines add whitespace errors.\n+ [[ maven == \\m\\a\\v\\e\\n ]]\n+ rm -rf /data/hiveptest/working/maven/org/apache/hive\n+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven\nprotoc-jar: protoc version: 250, detected platform: linux/amd64\nprotoc-jar: executing: [/tmp/protoc1701815878933627184.exe, -I/data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore, --java_out=/data/hiveptest/working/apache-github-source-source/standalone-metastore/target/generated-sources, /data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore/metastore.proto]\nANTLR Parser Generator  Version 3.5.2\nOutput file /data/hiveptest/working/apache-github-source-source/standalone-metastore/target/generated-sources/org/apache/hadoop/hive/metastore/parser/FilterParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/parser/Filter.g\norg/apache/hadoop/hive/metastore/parser/Filter.g\nlog4j:WARN No appenders could be found for logger (DataNucleus.General).\nlog4j:WARN Please initialize the log4j system properly.\nDataNucleus Enhancer (version 4.1.17) for API \"JDO\"\nDataNucleus Enhancer : Classpath\n>>  /usr/share/maven/boot/plexus-classworlds-2.x.jar\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDatabase\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFieldSchema\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MType\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTable\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MConstraint\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MOrder\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStringList\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartition\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MIndex\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRole\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRoleMap\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMasterKey\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDelegationToken\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MVersionTable\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMetastoreDBProperties\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MResourceUri\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFunction\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationLog\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationNextId\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMResourcePlan\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMPool\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMTrigger\nENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMMapping\nDataNucleus Enhancer completed with success for 35 classes. Timings : input=173 ms, enhance=222 ms, total=395 ms. Consult the log for full details\nANTLR Parser Generator  Version 3.5.2\nOutput file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveLexer.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g\norg/apache/hadoop/hive/ql/parse/HiveLexer.g\nOutput file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g\norg/apache/hadoop/hive/ql/parse/HiveParser.g\nOutput file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HintParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HintParser.g\norg/apache/hadoop/hive/ql/parse/HintParser.g\nGenerating vector expression code\nGenerating vector expression test code\n[ERROR] COMPILATION ERROR : \n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[437,7] cannot find symbol\n  symbol:   variable opRules\n  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[440,5] cannot find symbol\n  symbol:   variable opRules\n  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[443,5] cannot find symbol\n  symbol:   variable opRules\n  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[446,5] cannot find symbol\n  symbol:   variable opRules\n  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[448,5] cannot find symbol\n  symbol:   variable opRules\n  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[454,5] cannot find symbol\n  symbol:   variable opRules\n  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[483,5] cannot find symbol\n  symbol:   variable opRules\n  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[512,53] cannot find symbol\n  symbol:   variable opRules\n  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[437,7] cannot find symbol\n[ERROR] symbol:   variable opRules\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[440,5] cannot find symbol\n[ERROR] symbol:   variable opRules\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[443,5] cannot find symbol\n[ERROR] symbol:   variable opRules\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[446,5] cannot find symbol\n[ERROR] symbol:   variable opRules\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[448,5] cannot find symbol\n[ERROR] symbol:   variable opRules\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[454,5] cannot find symbol\n[ERROR] symbol:   variable opRules\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[483,5] cannot find symbol\n[ERROR] symbol:   variable opRules\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[512,53] cannot find symbol\n[ERROR] symbol:   variable opRules\n[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-exec\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12902412 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-12-15T22:06:16.735+0000","updated":"2017-12-15T22:06:16.735+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16293738","id":"16293738","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"update HIVE-17486.4.patch to fix the compilation error.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-16T09:52:59.485+0000","updated":"2017-12-16T09:52:59.485+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16293796","id":"16293796","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n|| || || || {color:brown} master Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 33s{color} | {color:blue} Maven dependency ordering for branch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 34s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 16s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 53s{color} | {color:green} master passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  5s{color} | {color:green} master passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 22s{color} | {color:blue} Maven dependency ordering for patch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 35s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |\r\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 18s{color} | {color:red} common: The patch generated 2 new + 931 unchanged - 0 fixed = 933 total (was 931) {color} |\r\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 34s{color} | {color:red} ql: The patch generated 82 new + 269 unchanged - 21 fixed = 351 total (was 290) {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 10s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 12s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black} 16m 20s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |\r\n| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |\r\n| git revision | master / f52e8b4 |\r\n| Default Java | 1.8.0_111 |\r\n| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8291/yetus/diff-checkstyle-common.txt |\r\n| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8291/yetus/diff-checkstyle-ql.txt |\r\n| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-8291/yetus/patch-asflicense-problems.txt |\r\n| modules | C: common ql itests U: . |\r\n| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8291/yetus.txt |\r\n| Powered by | Apache Yetus    http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-12-16T13:00:08.942+0000","updated":"2017-12-16T13:00:08.942+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16293809","id":"16293809","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12902486/HIVE-17486.4.patch\n\n{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 19 failed/errored test(s), 11529 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join25] (batchId=72)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=48)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=35)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucketsortoptimize_insert_2] (batchId=152)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[hybridgrace_hashjoin_2] (batchId=157)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=165)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=169)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=160)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[quotedid_smb] (batchId=157)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=160)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucketizedhiveinputformat] (batchId=178)\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[authorization_part] (batchId=93)\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_aggregator_error_1] (batchId=93)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_sortmerge_join_10] (batchId=138)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[bucketsortoptimize_insert_7] (batchId=128)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=120)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=113)\norg.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=209)\norg.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=226)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/8291/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/8291/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8291/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.YetusPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 19 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12902486 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-12-16T13:49:38.327+0000","updated":"2017-12-16T13:49:38.327+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16308547","id":"16308547","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"[~kellyzly] {quote} what i want to ask is there any possiblity to change current structure in the SparkTask in HoS {quote}\r\n\r\nWhat exactly is the advantage of introducing a M -> M edge? I know Tez does it, but I think they mainly use it to broadcast data from one set of map tasks to another (which is useful for map-joins).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2018-01-02T19:20:22.799+0000","updated":"2018-01-02T19:20:22.799+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16308995","id":"16308995","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~stakiar]:\r\nthe original purpose to change M->R to M->M->R is to let CombineEquivalentWorkResolver combine same Maps. Like\r\nlogical plan\r\n{code}\r\nTS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]\r\nTS[1] -FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]\r\n{code}\t\r\nphysical plan\r\n{code}\t\r\nMap1:TS[0]\r\nMap2:TS[1]\r\nMap3:FIL[52]-SEL[2]-GBY[3]-RS[4]\r\nMap4:FIL[53]-SEL[9]-GBY[10]-RS[11]\r\nReducer1:GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]\r\nReducer2:GBY[12]-RS[43]\r\n{code}\r\nFor {{CombineEquivalentWorkResolver}}, it will combine same Maps. In above case, Map2 will be removed because TS\\[0\\] is same as TS\\[1\\].  \r\n\r\nBut when I finished the code, I found that there is no necessary to use this way to combine TS\\[0\\] and TS\\[1\\]. {{MapInput}} is responsible for TS and I only need generate same MapInput for TS\\[0\\] and TS\\[1\\]. More detail see HIVE-17486.5.patch.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-01-03T01:39:42.796+0000","updated":"2018-01-03T01:40:11.464+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16309171","id":"16309171","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m 11s{color} | {color:red} /data/hiveptest/logs/PreCommit-HIVE-Build-8411/patches/PreCommit-HIVE-Build-8411.patch does not apply to master. Rebase required? Wrong Branch? See http://cwiki.apache.org/confluence/display/Hive/HowToContribute for help. {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8411/yetus.txt |\r\n| Powered by | Apache Yetus    http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2018-01-03T05:20:17.655+0000","updated":"2018-01-03T05:20:17.655+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13100713/comment/16309200","id":"16309200","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12904307/HIVE-17486.5.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 67 failed/errored test(s), 10834 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join25] (batchId=72)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_hook] (batchId=12)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=35)\norg.apache.hadoop.hive.cli.TestLocalSparkCliDriver.org.apache.hadoop.hive.cli.TestLocalSparkCliDriver (batchId=247)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=169)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucketsortoptimize_insert_2] (batchId=151)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[hybridgrace_hashjoin_2] (batchId=156)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=164)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=168)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=159)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=159)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=176)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=177)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=178)\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=179)\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[authorization_part] (batchId=93)\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[materialized_view_authorization_create_no_grant] (batchId=93)\norg.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_publisher_error_1] (batchId=93)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=104)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=105)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=106)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=107)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=108)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=109)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=110)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=111)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=112)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=113)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=114)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=115)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=116)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=117)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=118)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=119)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=120)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=121)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=122)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=123)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=124)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=125)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=126)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=127)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=128)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=129)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=130)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=131)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=132)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=133)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=134)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=135)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=136)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=137)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=138)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=139)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=140)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=141)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=142)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=143)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=144)\norg.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=145)\norg.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=247)\norg.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testTransactionalValidation (batchId=213)\norg.apache.hadoop.hive.ql.io.TestDruidRecordWriter.testWrite (batchId=253)\norg.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=225)\norg.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=231)\norg.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=231)\norg.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=231)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/8411/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/8411/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8411/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.YetusPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 67 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12904307 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2018-01-03T06:22:22.392+0000","updated":"2018-01-03T06:22:22.392+0000"}],"maxResults":44,"total":44,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17486/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3jt13:"}}