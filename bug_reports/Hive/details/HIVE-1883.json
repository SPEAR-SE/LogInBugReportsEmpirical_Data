{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12494680","self":"https://issues.apache.org/jira/rest/api/2/issue/12494680","key":"HIVE-1883","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2011-01-05T07:17:25.816+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Feb 17 17:20:34 UTC 2011","customfield_12310420":"42344","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-1883/watchers","watchCount":0,"isWatching":false},"created":"2011-01-05T06:07:58.486+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12314524","id":"12314524","description":"released","name":"0.6.0","archived":false,"released":true,"releaseDate":"2010-10-29"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2011-02-17T17:20:34.512+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312586","id":"12312586","name":"Query Processor","description":"Tracks issues dealing with query processing."}],"timeoriginalestimate":null,"description":"After starting hive and running queries transaction history files are getting creating in the /tmp/root folder.\nThese files we should remove periodically(not all of them but) which are too old to represent any significant information.\n\nSolution :-\nA scheduled timer task, which cleans up the log files older than the configured time.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"123479","customfield_12312823":null,"summary":"Periodic cleanup of Hive History log files.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mosikri","name":"mosikri","key":"mosikri","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mohit Sikri","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mosikri","name":"mosikri","key":"mosikri","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mohit Sikri","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Hive 0.6.0,  Hadoop 0.20.1\n\nSUSE Linux Enterprise Server 11 (i586)\nVERSION = 11\nPATCHLEVEL = 0\n","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12494680/comment/12977656","id":"12977656","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"I sent this to the dev list earlier:\n\n{quote}\nUsually it's the Ops/IT staff that ends up managing things like a production HiveServer instance, and in a UNIX shop I suspect that most of these folks are already going to be familiar with using cron and logrotate (http://linuxcommand.org/man_pages/logrotate8.html) to manage the logs produced by their other server systems. \n\nBuilding a log rotation feature into HiveServer defies this convention and will force people to learn how to configure a new log rotation system specific to HiveServer. It also requires us to write, debug, document and maintain code that isn't really necessary. I think the best approach is to take advantage of what already exists by documenting Hive's logging behavior in the Admin manual and providing a sample logrotate configuration file.\n{quote}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-01-05T07:17:25.816+0000","updated":"2011-01-05T07:17:25.816+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12494680/comment/12995934","id":"12995934","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mis","name":"mis","key":"mis","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"MIS","active":true,"timeZone":"Asia/Kolkata"},"body":"Carl is right on this. There is no need to have a 'scheduled' timer task to take care of the log files. There are enough handles already available in log4j library used by Hive to handle the log files.\nAs far as the current issue is concerned, RolllingFileAppender can be used and a max limit can be set.\nif it is wished that no data should be lost then DailyRollingFileAppender can be used and a cron job can be run to handle the a week's[or what ever the time frame chosen] log files.\n\nFurther, there is one more disadvantage in running the 'scheduled' timer task to handle log files, creates more problems than it solves. Though ScheduledThreadPoolExecutor could be an answer, but its just not worth the effort.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mis","name":"mis","key":"mis","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"MIS","active":true,"timeZone":"Asia/Kolkata"},"created":"2011-02-17T17:20:34.482+0000","updated":"2011-02-17T17:20:34.482+0000"}],"maxResults":2,"total":2,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-1883/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0lhgn:"}}