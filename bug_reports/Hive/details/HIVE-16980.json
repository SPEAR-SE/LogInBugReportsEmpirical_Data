{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13083012","self":"https://issues.apache.org/jira/rest/api/2/issue/13083012","key":"HIVE-16980","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-06-28T04:20:55.606+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Jul 03 04:11:16 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-16980/watchers","watchCount":6,"isWatching":false},"created":"2017-06-28T03:38:04.015+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-03T04:11:16.693+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"In HoSï¼Œthe join implementation is union+repartition sort. We use HashPartitioner to partition the result of union. \nSortByShuffler.java\n{code}\n    public JavaPairRDD<HiveKey, BytesWritable> shuffle(\n      JavaPairRDD<HiveKey, BytesWritable> input, int numPartitions) {\n    JavaPairRDD<HiveKey, BytesWritable> rdd;\n    if (totalOrder) {\n      if (numPartitions > 0) {\n        if (numPartitions > 1 && input.getStorageLevel() == StorageLevel.NONE()) {\n          input.persist(StorageLevel.DISK_ONLY());\n          sparkPlan.addCachedRDDId(input.id());\n        }\n        rdd = input.sortByKey(true, numPartitions);\n      } else {\n        rdd = input.sortByKey(true);\n      }\n    } else {\n      Partitioner partitioner = new HashPartitioner(numPartitions);\n      rdd = input.repartitionAndSortWithinPartitions(partitioner);\n    }\n    return rdd;\n  }\n{code}\nIn spark history server, i saw that there are 28 tasks in the repartition sort period while 21 tasks are finished less than 1s and the remaining 7 tasks spend long time to execute. Is there any way to make the data evenly assigned to every partition?","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12875205","id":"12875205","filename":"HIVE-16980_screenshot.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-30T08:42:03.367+0000","size":64696,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12875205/HIVE-16980_screenshot.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12875204","id":"12875204","filename":"query17_explain.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-30T08:42:12.778+0000","size":20271,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12875204/query17_explain.log"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"The partition of join is not divided evently in HOS","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16065908","id":"16065908","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"This is interesting. Have you checked your data for skew? In theory, hash partitioner does a pretty good job for evenly distributing the keys. Unless the keys are skewed, each partition is expected to process about the same number of rows.\n\nIt's possible to provide a custom partitioner here, but I'm not entirely sure if that's worthwhile.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-28T04:20:55.606+0000","updated":"2017-06-28T04:20:55.606+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16066150","id":"16066150","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]: thanks for comment, yes the keys maybe skewed, so in this situation , how to deal with this kind of case in a custom partitioner?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-28T08:46:07.942+0000","updated":"2017-06-28T08:46:07.942+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16066174","id":"16066174","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Skew join can mitigate the issue, on condition that skewed data only exists in one table.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-28T09:07:16.639+0000","updated":"2017-06-28T09:07:16.639+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16067695","id":"16067695","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: thanks for your suggestion, will try to use skewed join configuration to solve the problem later.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-29T04:00:41.447+0000","updated":"2017-06-29T04:00:41.447+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16069708","id":"16069708","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui] and [~xuefuz]:  attached is the screenshot of TPC-DS/query17.sql on 3TB. \nTPC-DS/query17.sql\n{code}\nselect  i_item_id\n       ,i_item_desc\n       ,s_state\n       ,count(ss_quantity) as store_sales_quantitycount\n       ,avg(ss_quantity) as store_sales_quantityave\n       ,stddev_samp(ss_quantity) as store_sales_quantitystdev\n       ,stddev_samp(ss_quantity)/avg(ss_quantity) as store_sales_quantitycov\n       ,count(sr_return_quantity) as_store_returns_quantitycount\n       ,avg(sr_return_quantity) as_store_returns_quantityave\n       ,stddev_samp(sr_return_quantity) as_store_returns_quantitystdev\n       ,stddev_samp(sr_return_quantity)/avg(sr_return_quantity) as store_returns_quantitycov\n       ,count(cs_quantity) as catalog_sales_quantitycount ,avg(cs_quantity) as catalog_sales_quantityave\n       ,stddev_samp(cs_quantity)/avg(cs_quantity) as catalog_sales_quantitystdev\n       ,stddev_samp(cs_quantity)/avg(cs_quantity) as catalog_sales_quantitycov\n from store_sales\n     ,store_returns\n     ,catalog_sales\n     ,date_dim d1\n     ,date_dim d2\n     ,date_dim d3\n     ,store\n     ,item\n where d1.d_quarter_name = '2000Q1'\n   and d1.d_date_sk = store_sales.ss_sold_date_sk\n   and item.i_item_sk = store_sales.ss_item_sk\n   and store.s_store_sk = store_sales.ss_store_sk\n   and store_sales.ss_customer_sk = store_returns.sr_customer_sk\n   and store_sales.ss_item_sk = store_returns.sr_item_sk\n   and store_sales.ss_ticket_number = store_returns.sr_ticket_number\n   and store_returns.sr_returned_date_sk = d2.d_date_sk\n   and d2.d_quarter_name in ('2000Q1','2000Q2','2000Q3')\n   and store_returns.sr_customer_sk = catalog_sales.cs_bill_customer_sk\n   and store_returns.sr_item_sk = catalog_sales.cs_item_sk\n   and catalog_sales.cs_sold_date_sk = d3.d_date_sk\n   and d3.d_quarter_name in ('2000Q1','2000Q2','2000Q3')\n group by i_item_id\n         ,i_item_desc\n         ,s_state\n order by i_item_id\n         ,i_item_desc\n         ,s_state\nlimit 100;\n{code}\nexplain is also attached. \nLet's explain the explain\n   storeï¼Œ item, d2ï¼Œd3, d1 is small table.\n   store_sales, store_returns, ctalog_sales are big table.\n   there are 7 stages in the job\n   Stage-0:  d2 union d3 union store union item  ( all these small table will be converted to map join. Here first strange thing is d1 is also small ,why d1 is in the first stage-0)\n   Stage-1\n        Reducer 2 <- Map 1 (store_sales), Map 7 (store_returns)\n        Reducer 3 <- Map 8 (catalog_sales), Reducer 2 \n        Reducer 4 <- Map 9 (d1), Reducer 3 \n        Reducer 5 <- Reducer 4 (GROUP)\n        Reducer 6 <- Reducer 5 (SORT)\nthe screenshot is about  Stage :Reducer 3 <- Map 8 (catalog_sales), Reducer 2 \" . In the history server, it shows 2178 tasks finished, Median of duration time is 4s. 75 percentile of duration is 20 min. Max of duration time 32min.  About Shuffle Read size/Records, Median of it is 0.0B/0. 75 percentile of it is 274.9MB/8695090. Max of it  is 275.3MB/8709548.  I don't understand these metrics very much but it seems that the difference between tasks are too big especially some tasks need a lot of shuffle read while others are not.  Can you help to see where is wrong?\n\n\n   ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-30T08:40:30.298+0000","updated":"2017-06-30T08:40:30.298+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16071155","id":"16071155","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~kellyzly], in the attached query plan, there's only 1 reducer for Reducer3:\n{noformat}\nReducer 3 <- Map 8 (PARTITION-LEVEL SORT, 1), Reducer 2 (PARTITION-LEVEL SORT, 1)\n{noformat}\nDo you know why we only use 1 reducer to do the join in Reducer3? Can you try forcing Hive to use more reducers in this stage? An easy way to do it is to manually set {{mapreduce.job.reduces}}.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-01T10:09:12.421+0000","updated":"2017-07-01T10:09:12.421+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16071875","id":"16071875","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: the reason why we only use 1 reducer is because there is a bug in SetSparkReducerParallelism#process in 3TB scale. We use [numberOfByteshttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java#L129] to collect the numberOfBytes of sibling of specified RS. We use Long type and it happens overflow when the data is too big. After happening this situation, the parallelism is decided by [sparkMemoryAndCores.getSecond()|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java#L184] if spark.dynamic.allocation.enabled is true, sparkMemoryAndCores.getSecond is a dymamic value which is decided by spark runtime. For example, the value of sparkMemoryAndCores.getSecond is 5 or 15 randomly. There is possibility that the value may be 1. The may problem here is the overflow of addition of Long type.  You can reproduce the overflow problem by following code\n{code}\n    public static void main(String[] args) {\n      long a1= 9223372036854775807L;\n      long a2=1022672;\n\n      long res = a1+a2;\n      System.out.println(res);  //-9223372036853753137\n\n      BigInteger b1= BigInteger.valueOf(a1);\n      BigInteger b2 = BigInteger.valueOf(a2);\n\n      BigInteger bigRes = b1.add(b2);\n\n      System.out.println(bigRes); //9223372036855798479\n\n    }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-03T03:04:25.478+0000","updated":"2017-07-03T03:04:25.478+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16071883","id":"16071883","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"have filed HIVE-17010 to trace the overflow problem, but give me more time to ensure there is no problem of not divided records evenly after several join in HOS. In my view, HiveKey is generated by ReduceSinkOperator#computeHashCode. Is there any possibility the key became skewed after several joinsï¼Ÿ For example, select * from C, (select * from A join B where key=\"1\"), the key in the result are always 1 which may be skewed.  [~lirui], do you recommend to set hive.optimize.skewjoin as true to convert a join to skewed join at runtime?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-03T03:23:01.506+0000","updated":"2017-07-03T03:23:01.506+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13083012/comment/16071895","id":"16071895","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~kellyzly], nice catch of the Long overflow!\nAs to data skew, it's usually due to some skewed keys in your data. E.g. when you join A and B on A.key = B.key, and most of keys in A are 1. Since same keys have to go to the same reducer, then you'll have one reducer handling most data from A. {{hive.optimize.skewjoin}} determines skew at runtime, and joins the skewed data with a map join. It can mitigate the problem, but skewed data still has to be shuffled to the one reducer.\nI don't think hash code can lead to skew (possible in some extreme case though, like HIVE-14797).\n\nFor your case, my suggestion is to force more reducers and verify if skew really exists. If so, enable {{hive.optimize.skewjoin}} and see if it helps.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-03T04:11:16.693+0000","updated":"2017-07-03T04:11:16.693+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-16980/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3gth3:"}}