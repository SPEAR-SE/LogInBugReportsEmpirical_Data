{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13042923","self":"https://issues.apache.org/jira/rest/api/2/issue/13042923","key":"HIVE-15912","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-02-15T04:10:53.464+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Aug 22 22:05:23 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15912/watchers","watchCount":9,"isWatching":false},"created":"2017-02-14T12:08:37.099+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12335837","id":"12335837","name":"2.2.0","archived":false,"released":true,"releaseDate":"2017-07-25"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-08-22T22:05:23.606+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12325007","id":"12325007","name":"Hive"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12323200","id":"12323200","name":"Spark","description":"Hive on Spark"}],"timeoriginalestimate":null,"description":"Hive on Spark, failed with error:\nStarting Spark Job = 12a8cb8c-ed0d-4049-ae06-8d32d13fe285\nFailed to monitor Job[ 6] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n\nHive's log:\n2017-02-14T19:03:09,147  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/14 19:03:09 INFO yarn.Client: Application report for application_1486905599813_0403 (state: ACCEPTED)\n2017-02-14T19:03:10,817  WARN [5bcf13e5-cb54-4cfe-a0d4-9a6556ab48b1 main] spark.SetSparkReducerParallelism: Failed to get spark memory/core info\njava.util.concurrent.TimeoutException\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~[netty-all-4.0.29.Final.jar:4.0.29.Final]\n        at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.getExecutorCount(RemoteHiveSparkClient.java:155) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.getExecutorCount(RemoteHiveSparkClient.java:165) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.getMemoryAndCores(SparkSessionImpl.java:77) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.process(SetSparkReducerParallelism.java:119) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:158) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.runJoinOptimizations(SparkCompiler.java:291) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.optimizeOperatorPlan(SparkCompiler.java:120) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11085) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:279) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:510) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1302) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1442) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1222) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:430) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:446) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:749) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_60]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_60]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]\n        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221) ~[hadoop-common-2.7.1.jar:?]\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136) ~[hadoop-common-2.7.1.jar:?]\n2017-02-14T19:03:10,817  INFO [5bcf13e5-cb54-4cfe-a0d4-9a6556ab48b1 main] spark.SetSparkReducerParallelism: Set parallelism for reduce sink RS[24] to: 1 (calculated)\n\n\nyarn log:\n17/02/14 19:05:36 INFO executor.Executor: Finished task 329.0 in stage 15.0 (TID 3865). 3228 bytes result sent to driver\n17/02/14 19:05:36 INFO executor.Executor: Finished task 201.0 in stage 15.0 (TID 3737). 3141 bytes result sent to driver\n17/02/14 19:05:36 INFO executor.Executor: Finished task 9.0 in stage 15.0 (TID 3545). 4027 bytes result sent to driver\n17/02/14 19:05:36 INFO executor.Executor: Finished task 457.0 in stage 15.0 (TID 3993). 3141 bytes result sent to driver\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4221\n17/02/14 19:05:36 INFO executor.Executor: Running task 687.0 in stage 15.0 (TID 4221)\n17/02/14 19:05:36 INFO executor.Executor: Finished task 265.0 in stage 15.0 (TID 3801). 3141 bytes result sent to driver\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4240\n17/02/14 19:05:36 INFO executor.Executor: Running task 692.0 in stage 15.0 (TID 4240)\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4242\n17/02/14 19:05:36 INFO executor.Executor: Running task 763.0 in stage 15.0 (TID 4242)\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4243\n17/02/14 19:05:36 INFO executor.Executor: Running task 836.0 in stage 15.0 (TID 4243)\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4244\n17/02/14 19:05:36 INFO executor.Executor: Running task 863.0 in stage 15.0 (TID 4244)\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4248\n17/02/14 19:05:36 INFO executor.Executor: Running task 979.0 in stage 15.0 (TID 4248)\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4280\n17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_687 locally\n17/02/14 19:05:36 INFO executor.Executor: Running task 990.0 in stage 15.0 (TID 4280)\n17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 687.0 in stage 15.0 (TID 4221)\n17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 73.0 in stage 15.0 (TID 3609)\n17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 763.0 in stage 15.0 (TID 4242)\n17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 979.0 in stage 15.0 (TID 4248)\n17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 990.0 in stage 15.0 (TID 4280)\n17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 863.0 in stage 15.0 (TID 4244)\n17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 836.0 in stage 15.0 (TID 4243)\n17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 692.0 in stage 15.0 (TID 4240)\n17/02/14 19:05:36 INFO storage.BlockManager: Removing RDD 24\n17/02/14 19:05:36 INFO executor.Executor: Finished task 73.0 in stage 15.0 (TID 3609). 3214 bytes result sent to driver\n17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_863 locally\n17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_692 locally\n17/02/14 19:05:36 ERROR executor.Executor: Exception in task 990.0 in stage 15.0 (TID 4280)\norg.apache.spark.TaskKilledException\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:264)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n17/02/14 19:05:36 INFO executor.Executor: Executor killed task 687.0 in stage 15.0 (TID 4221)\n17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_836 locally\n17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_979 locally\n17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_763 locally\n17/02/14 19:05:36 INFO executor.Executor: Executor killed task 863.0 in stage 15.0 (TID 4244)\n17/02/14 19:05:36 INFO executor.Executor: Executor killed task 979.0 in stage 15.0 (TID 4248)\n17/02/14 19:05:36 INFO executor.Executor: Executor killed task 836.0 in stage 15.0 (TID 4243)\n17/02/14 19:05:36 INFO executor.Executor: Executor killed task 692.0 in stage 15.0 (TID 4240)\n17/02/14 19:05:36 INFO executor.Executor: Executor killed task 763.0 in stage 15.0 (TID 4242)\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Driver from 192.168.1.1:35981 disconnected during shutdown\n17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Driver from 192.168.1.1:35981 disconnected during shutdown\n17/02/14 19:05:36 INFO memory.MemoryStore: MemoryStore cleared\n17/02/14 19:05:36 INFO storage.BlockManager: BlockManager stopped\n17/02/14 19:05:36 INFO util.ShutdownHookManager: Shutdown hook called","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Executor kill task and Failed to get spark memory/core info","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"hadoop2.7.1\nspark2.0.2\nHive2.2","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15867198","id":"15867198","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"The failure to get mem/core info is just a warning. So it's not related to executor killing tasks. You should check the AM log (if in yarn-cluster mode) to see why the driver commands a shutdown.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-02-15T04:10:53.464+0000","updated":"2017-02-15T04:10:53.464+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15867538","id":"15867538","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"body":"It's running in yarn-client mode, actually I did not see why the driver commands a shutdown from AM log, just see the driver commands a shutdown. So could the Warning be the possible cause?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-02-15T09:37:10.205+0000","updated":"2017-02-15T09:37:10.205+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15869505","id":"15869505","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"No I don't think the warning can be the cause. For yarn-client mode, you should look for the error in Hive's log.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-02-16T08:28:00.682+0000","updated":"2017-02-16T08:28:00.682+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15904417","id":"15904417","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~lirui],\nI also encountered this issue. I'm using yarn-client mode. My workload failed in 20 mins. \n\n[Settings]\nset hive.spark.job.monitor.timeout=3600s;\nset spark.network.timeout=3600s;\n\n[Hive's log]\n2017-03-09 17:22:29,771 WARN  [RPC-Handler-3]: client.SparkClientImpl (SparkClientImpl.java:rpcClosed(130)) - Client RPC channel closed unexpectedly.\n\n[App master log]\n17/03/09 17:22:30 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).\n17/03/09 17:22:30 INFO yarn.YarnAllocator: Canceling requests for 51 executor containers\n17/03/09 17:22:30 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. 10.54.5.129:48757\n17/03/09 17:22:30 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. master.titan.cluster.gao:48757\n17/03/09 17:22:30 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0\n17/03/09 17:22:30 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED\n17/03/09 17:22:30 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.\n17/03/09 17:22:30 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1488974634357_0022\n17/03/09 17:22:30 INFO util.ShutdownHookManager: Shutdown hook calle\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-10T04:26:20.464+0000","updated":"2017-03-10T04:26:54.596+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15904593","id":"15904593","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~yiyao], for yarn-client mode, you should check the hive log, which contains logs from both hive and RemoteDriver. Or you can upload the log here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-10T07:06:23.623+0000","updated":"2017-03-10T07:06:23.623+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15904644","id":"15904644","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~lirui],\nThanks for your quick response. Below is the hive log.\n\n[LOG]\n2017-03-09 17:23:29,761 WARN  [main]: impl.RemoteSparkJobStatus (RemoteSparkJobStatus.java:getSparkStageInfo(162)) - Error getting stage info\njava.util.concurrent.TimeoutException\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:160)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)\n        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:109)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1195)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:416)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:432)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:726)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n2017-03-09 17:23:29,764 ERROR [main]: status.SparkJobMonitor (RemoteSparkJobMonitor.java:startMonitor(132)) - Failed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'\njava.lang.IllegalStateException: RPC channel is closed.\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)\n        at org.apache.hive.spark.client.SparkClientImpl$ClientProtocol.run(SparkClientImpl.java:550)\n        at org.apache.hive.spark.client.SparkClientImpl.run(SparkClientImpl.java:145)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:158)\nat org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)\n        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:109)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1195)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:416)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:432)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:726)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n2017-03-09 17:23:29,766 ERROR [main]: status.SparkJobMonitor (SessionState.java:printError(997)) - Failed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'\njava.lang.IllegalStateException: RPC channel is closed.\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)\n        at org.apache.hive.spark.client.SparkClientImpl$ClientProtocol.run(SparkClientImpl.java:550)\n        at org.apache.hive.spark.client.SparkClientImpl.run(SparkClientImpl.java:145)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:158)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)\n        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:109)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1195)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)\nat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:416)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:432)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:726)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\n2017-03-09 17:23:29,778 ERROR [main]: ql.Driver (SessionState.java:printError(997)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-10T07:49:12.130+0000","updated":"2017-03-10T07:49:12.130+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15904694","id":"15904694","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~yiyao], you can see the log indicates the Rpc channel is closed unexpectedly. Therefore the error that makes the channel closed (and probably crashes the RemoteDriver) is the root cause. You should look for it in earlier part of your log.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-10T08:24:08.082+0000","updated":"2017-03-10T08:24:08.082+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15906767","id":"15906767","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~lirui], I checked the earlier log, there's only 1 timeout warning.  I have specified 1 hour as Spark network timeout threshold. I did not find any network issues in my OS log. Not sure what causes the issue.\n\n2017-03-09 17:23:29,761 WARN [main]: impl.RemoteSparkJobStatus (RemoteSparkJobStatus.java:getSparkStageInfo(162)) - Error getting stage info\njava.util.concurrent.TimeoutException\nat io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)\nat org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:160)\nat org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-13T01:19:44.261+0000","updated":"2017-03-13T01:20:08.446+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15906777","id":"15906777","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Can you see any output from the RemoteDriver in your hive.log? Such logs are printed via the redirector, so you should find them by looking for something starting with {{stdout-redir-}} or {{stderr-redir-}}.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-13T02:10:40.363+0000","updated":"2017-03-13T02:10:40.363+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15906788","id":"15906788","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"body":"I find some logs starting with 'stderr-redir-'\n\nhive.log.2017-03-06:11216:2017-03-06 02:50:14,742 WARN  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(624)) - Error in redirector thread.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-13T02:41:07.956+0000","updated":"2017-03-13T02:41:07.956+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15907035","id":"15907035","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~yiyao], is that all the redirector log you have? Usually in yarn-client mode, the redirector should output quite a lot stuff because all the RemoteDriver log goes to the redirector. You should find something like creating SparkContext, starting all the schedulers in spark, etc. Even for the specific log you posted, it should have come with an exception: https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java#L678.\nBtw, if you're using an old version of HoS, I'd suggest you try the case in a newer one.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-13T09:00:11.501+0000","updated":"2017-03-13T09:00:11.501+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15929340","id":"15929340","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui], I found the following warning always occurs before the application fail.\n\njava.util.concurrent.TimeoutException\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:160)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)\n        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:109)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:80)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-17T02:26:40.230+0000","updated":"2017-03-17T07:49:37.080+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15929542","id":"15929542","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"body":"In the client log, we also found the following warning you mentioned.\n\n2017-03-16 17:51:40,166 WARN  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(641)) - Error in redirector thread.\njava.io.IOException: Stream closed\n        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)\n        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)\n        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)\n        at java.io.InputStreamReader.read(InputStreamReader.java:184)\n        at java.io.BufferedReader.fill(BufferedReader.java:154)\n        at java.io.BufferedReader.readLine(BufferedReader.java:317)\n        at java.io.BufferedReader.readLine(BufferedReader.java:382)\n        at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:632)\n        at java.lang.Thread.run(Thread.java:745)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yiyao","name":"yiyao","key":"yiyao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yi Yao","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-17T07:40:22.772+0000","updated":"2017-03-17T07:42:37.153+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/15933961","id":"15933961","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~lirui], I am using Hive2.2 on spark2.0.2, the issue also exists. \n\n2017-03-21 03:02:30,454 Stage-5_0: 241/241 Finished     Stage-6_0: 161(+1)/162  Stage-7_0: 0/2018       Stage-8_0: 0/1009       Stage-9_0: 0/1009\nFailed to monitor Job[4] with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(java.util.concurrent.TimeoutException)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n\nin hive's log I also found the TimeoutException, WARN as well as ERROR,\n\n2017-03-21T03:02:31,466  INFO [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Closing channel due to exception in pipeline (org.apache.hive.spark.client.SparkClientImpl$ClientProtocol.handle(io.netty.channel.ChannelHandlerContext, org.apache.hive.spark.client.rpc.Rpc$MessageHeader)).\n2017-03-21T03:02:31,468  WARN [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Expected RPC header, got org.apache.spark.SparkJobInfoImpl instead.\n2017-03-21T03:02:31,468  INFO [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Closing channel due to exception in pipeline (null).\n2017-03-21T03:02:31,469  WARN [RPC-Handler-3] client.SparkClientImpl: Client RPC channel closed unexpectedly.\n2017-03-21T03:03:31,457  WARN [Thread-349] impl.RemoteSparkJobStatus: Failed to get job info.\njava.util.concurrent.TimeoutException\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~[netty-all-4.0.29.Final.jar:4.0.29.Final]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:171) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getStageIds(RemoteSparkJobStatus.java:87) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:94) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:84) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n2017-03-21T03:03:31,457 ERROR [Thread-349] status.SparkJobMonitor: Failed to monitor Job[4] with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(java.util.concurrent.TimeoutException)'\norg.apache.hadoop.hive.ql.metadata.HiveException: java.util.concurrent.TimeoutException\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:174) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getStageIds(RemoteSparkJobStatus.java:87) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:94) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:84) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\nCaused by: java.util.concurrent.TimeoutException\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~[netty-all-4.0.29.Final.jar:4.0.29.Final]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:171) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        ... 8 more\n2017-03-21T03:03:31,458 ERROR [Thread-349] SessionState: Failed to monitor Job[4] with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(java.util.concurrent.TimeoutException)'\norg.apache.hadoop.hive.ql.metadata.HiveException: java.util.concurrent.TimeoutException\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:174)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getStageIds(RemoteSparkJobStatus.java:87)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:94)\n        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:84)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:116)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79)\nCaused by: java.util.concurrent.TimeoutException\n        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:171)\n        ... 8 more\n\n2017-03-21T03:03:31,458 ERROR [Thread-349] spark.SparkTask: Failed to get Spark job information\njava.lang.IllegalStateException: RPC channel is closed.\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:149) ~[guava-14.0.1.jar:?]\n        at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hive.spark.client.SparkClientImpl$ClientProtocol.run(SparkClientImpl.java:580) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hive.spark.client.SparkClientImpl.run(SparkClientImpl.java:151) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:168) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getStageIds(RemoteSparkJobStatus.java:87) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.getSparkJobInfo(SparkTask.java:346) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:118) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]\n2017-03-21T03:03:33,233 ERROR [89254c41-8375-40ef-a364-e7b546d8ba48 main] ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n\nin driver's log, found an InterruptedException:\n\n17/03/21 03:02:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1181 on executor id: 38 hostname: hsx-node6.\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 244.0 in stage 7.0 (TID 657) in 573 ms on hsx-node4 (343/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 432.0 in stage 7.0 (TID 845) in 285 ms on hsx-node9 (344/2018)\n17/03/21 03:02:31 WARN client.RemoteDriver: Shutting down driver because RPC channel was closed.\n17/03/21 03:02:31 INFO client.RemoteDriver: Shutting down remote driver.\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Starting task 769.0 in stage 7.0 (TID 1182, hsx-node6, partition 769, PROCESS_LOCAL, 25685 bytes)\n17/03/21 03:02:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1182 on executor id: 9 hostname: hsx-node6.\n17/03/21 03:02:31 INFO scheduler.DAGScheduler: Asked to cancel job 4\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 392.0 in stage 7.0 (TID 805) in 365 ms on hsx-node10 (345/2018)\n17/03/21 03:02:31 INFO client.RemoteDriver: Failed to run job 63390316-af66-4f84-8c8b-806112ef2b94\njava.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)\n\tat org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:125)\n\tat org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:114)\n\tat scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:169)\n\tat scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:169)\n\tat scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)\n\tat scala.concurrent.Await$.ready(package.scala:169)\n\tat org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:264)\n\tat org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:277)\n\tat org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:362)\n\tat org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:323)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Starting task 770.0 in stage 7.0 (TID 1183, hsx-node6, partition 770, PROCESS_LOCAL, 25685 bytes)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 469.0 in stage 7.0 (TID 882) in 350 ms on hsx-node7 (346/2018)\n17/03/21 03:02:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1183 on executor id: 38 hostname: hsx-node6.\n17/03/21 03:02:31 INFO cluster.YarnClusterScheduler: Cancelling stage 7\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception\njava.lang.IllegalStateException: RPC channel is closed.\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:149)\n\tat org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)\n\tat org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)\n\tat org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)\n\tat org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)\n\tat org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:45)\n\tat org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)\n\tat org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)\n\tat org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)\n\tat org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:36)\n\tat org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:94)\n\tat org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)\n\tat org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1252)\n\tat org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)\n17/03/21 03:02:31 INFO server.ServerConnector: Stopped ServerConnector@10328ecb{HTTP/1.1}{0.0.0.0:0}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@282715e5{/stages/stage/kill,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6610a390{/api,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@a3e40d8{/,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@12a7530{/static,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7d6673e4{/executors/threadDump/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7603582f{/executors/threadDump,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6b8c0277{/executors/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@293aa151{/executors,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2db4db50{/environment/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@743d35fc{/environment,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@70265a43{/storage/rdd/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7207552{/storage/rdd,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5a10780b{/storage/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6a2dd645{/storage,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@574406ab{/stages/pool/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4a32fbe0{/stages/pool,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@70b816b5{/stages/stage/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1fa44e08{/stages/stage,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@25170f92{/stages/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5c6ccb81{/stages,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6af0b071{/jobs/job/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4d6efce4{/jobs/job,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3f28c543{/jobs/json,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7849dc62{/jobs,null,UNAVAILABLE}\n17/03/21 03:02:31 INFO cluster.YarnClusterScheduler: Stage 7 was cancelled\n17/03/21 03:02:31 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.4:41594\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 7.0 (TID 443) in 733 ms on hsx-node4 (347/2018)\n17/03/21 03:02:31 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (union at SparkPlan.java:70) failed in 0.744 s\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 7.0 (TID 499) in 720 ms on hsx-node4 (348/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 333.0 in stage 7.0 (TID 746) in 664 ms on hsx-node5 (349/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 425.0 in stage 7.0 (TID 838) in 400 ms on hsx-node2 (350/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 324.0 in stage 7.0 (TID 737) in 667 ms on hsx-node4 (351/2018)\n17/03/21 03:02:31 WARN spark.ExecutorAllocationManager: No stages are running, but numRunningTasks != 0\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@3241bef1,org.apache.spark.executor.TaskMetrics@21953ed8)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 440.0 in stage 7.0 (TID 853) in 387 ms on hsx-node9 (352/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 7.0 (TID 532) in 715 ms on hsx-node10 (353/2018)\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskStart(7,-1,org.apache.spark.scheduler.TaskInfo@3e8a68b)\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@4959044b,org.apache.spark.executor.TaskMetrics@76d08da7)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 408.0 in stage 7.0 (TID 821) in 420 ms on hsx-node2 (354/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 482.0 in stage 7.0 (TID 895) in 358 ms on hsx-node8 (355/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 463.0 in stage 7.0 (TID 876) in 374 ms on hsx-node8 (356/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 7.0 (TID 481) in 731 ms on hsx-node4 (357/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 447.0 in stage 7.0 (TID 860) in 382 ms on hsx-node7 (358/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 446.0 in stage 7.0 (TID 859) in 384 ms on hsx-node8 (359/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 450.0 in stage 7.0 (TID 863) in 382 ms on hsx-node7 (360/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 7.0 (TID 550) in 716 ms on hsx-node5 (361/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 421.0 in stage 7.0 (TID 834) in 408 ms on hsx-node9 (362/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 7.0 (TID 579) in 710 ms on hsx-node4 (363/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 430.0 in stage 7.0 (TID 843) in 405 ms on hsx-node6 (364/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 7.0 (TID 530) in 723 ms on hsx-node4 (365/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 473.0 in stage 7.0 (TID 886) in 372 ms on hsx-node7 (366/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 305.0 in stage 7.0 (TID 718) in 679 ms on hsx-node10 (367/2018)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 445.0 in stage 7.0 (TID 858) in 388 ms on hsx-node9 (368/2018)\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@395b13ea,org.apache.spark.executor.TaskMetrics@4b141cd3)\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@6dfb0a2c,org.apache.spark.executor.TaskMetrics@5cf4c211)\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@3dd14e26,org.apache.spark.executor.TaskMetrics@56482fcb)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 479.0 in stage 7.0 (TID 892) in 366 ms on hsx-node8 (369/2018)\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@63d7f544,org.apache.spark.executor.TaskMetrics@19c94765)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 426.0 in stage 7.0 (TID 839) in 409 ms on hsx-node2 (370/2018)\n17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@2e5e9a04,org.apache.spark.executor.TaskMetrics@39ffde73)\n17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 379.0 in stage 7.0 (TID 792) in 665 ms on hsx-node8 (371/2018)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=KaiXu","name":"KaiXu","key":"kaixu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"KaiXu","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-03-21T01:44:07.583+0000","updated":"2017-03-21T01:44:07.583+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13042923/comment/16137479","id":"16137479","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zwu.net%40gmail.com","name":"zwu.net@gmail.com","key":"zwu.net@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Wu","active":true,"timeZone":"Etc/UTC"},"body":"Got the similar issue with Spark 2.2/Hive 2.5.3 (on HortonWork).  The terrible thing is that app got hung without shutdown the context (app didn't exit by itself except by killing it with yarn command. The same app runs fine with 2.1.1 or earlier.\n\njava.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)\n\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:222)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:621)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n\tat org.apache.spark.sql.Dataset.show(Dataset.scala:644)\n\tat org.apache.spark.sql.Dataset.show(Dataset.scala:603)\n\tat org.apache.spark.sql.Dataset.show(Dataset.scala:612)\n\tat com.att.nrs.spk.MainUtil.getVoiceData(MainUtil.java:222)\n\tat com.att.nrs.spk.RedialFinderMain.main(RedialFinderMain.java:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:635)\n\n*Stderr:*\nERROR ApplicationMaster 91: Exception from Reporter thread.\norg.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt_1502715048011_224038_000002 doesn't exist in ApplicationMasterService cache.\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:445)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)\n\tat sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy18.allocate(Unknown Source)\n\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:277)\n\tat org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:265)\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:457)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zwu.net%40gmail.com","name":"zwu.net@gmail.com","key":"zwu.net@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Wu","active":true,"timeZone":"Etc/UTC"},"created":"2017-08-22T22:05:23.606+0000","updated":"2017-08-22T22:05:23.606+0000"}],"maxResults":15,"total":15,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15912/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3a20v:"}}