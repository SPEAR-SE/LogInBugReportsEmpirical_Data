{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12856215","self":"https://issues.apache.org/jira/rest/api/2/issue/12856215","key":"HIVE-11558","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-12-08T10:14:12.886+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jan 12 00:30:46 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-11558/watchers","watchCount":5,"isWatching":false},"created":"2015-08-14T16:52:31.175+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332384","id":"12332384","name":"1.2.1","archived":false,"released":true,"releaseDate":"2015-06-26"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-01-12T00:30:46.918+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12320633","id":"12320633","name":"File Formats","description":"File Formats"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12317302","id":"12317302","name":"StorageHandler"}],"timeoriginalestimate":null,"description":"When creating a Parquet table in Hive from a table in another format (in this case JSON) using CTAS, the generated parquet files are created with broken footers and cause NullPointerExceptions in both Parquet tools and Spark when reading the files directly.\n\nHere is the error from parquet tools:\n\n{code}Could not read footer: java.lang.NullPointerException{code}\n\nHere is the error from Spark reading the parquet file back:\n{code}java.lang.NullPointerException\n        at parquet.format.converter.ParquetMetadataConverter.fromParquetStatistics(ParquetMetadataConverter.java:249)\n        at parquet.format.converter.ParquetMetadataConverter.fromParquetMetadata(ParquetMetadataConverter.java:543)\n        at parquet.format.converter.ParquetMetadataConverter.readParquetMetadata(ParquetMetadataConverter.java:520)\n        at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:426)\n        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$refresh$6.apply(newParquet.scala:298)\n        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$refresh$6.apply(newParquet.scala:297)\n        at scala.collection.parallel.mutable.ParArray$Map.leaf(ParArray.scala:658)\n        at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:54)\n        at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:53)\n        at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:53)\n        at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:56)\n        at scala.collection.parallel.mutable.ParArray$Map.tryLeaf(ParArray.scala:650)\n        at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:165)\n        at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:514)\n        at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\n        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n{code}\n\nWhat's interesting is that the table works fine in Hive when selecting out of it, even when doing select * on the whole table and letting it run to the end (it's a sample data set), it's only other tools it causes problems for.\n\nAll fields are string except for the first one which is timestamp, but this is not that known issue since if I create another parquet table with 3 fields including the timestamp and two string fields using CTAS those hive generated parquet files works fine in the other tools.\n\nThe only thing I can see which appears to cause this is the other fields have lots of NULLs in them as those json fields may or may not be present.\n\nI've converted this exact same json data set to parquet using Apache Drill and also using Apache Spark SQL and both of those tools create parquet files from this data set as a straight conversion that are fine when accessed via Parquet tools or Drill or Spark or Hive (using an external Hive table definition layered over the generated parquet files).\n\nThis implies that it's Hive's generation of Parquet that is broken since both Drill and Spark can convert the dataset from JSON to Parquet without any issues on reading the files back in any of the other mentioned tools.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive generates Parquet files with broken footers, causes NullPointerException in Spark / Drill / Parquet tools","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harisekhon","name":"harisekhon","key":"harisekhon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hari Sekhon","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harisekhon","name":"harisekhon","key":"harisekhon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hari Sekhon","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"HDP 2.3","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12856215/comment/15046690","id":"15046690","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=z1lv1n4s","name":"z1lv1n4s","key":"z1lv1n4s","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Z. S.","active":true,"timeZone":"Etc/UTC"},"body":"Have the same issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=z1lv1n4s","name":"z1lv1n4s","key":"z1lv1n4s","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Z. S.","active":true,"timeZone":"Etc/UTC"},"created":"2015-12-08T10:14:12.886+0000","updated":"2015-12-08T10:14:12.886+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12856215/comment/15092521","id":"15092521","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=abalmin","name":"abalmin","key":"abalmin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Andrey Balmin","active":true,"timeZone":"America/Los_Angeles"},"body":"I got the same stack trace on CDH 5.3. The problem is fixed in CDH 5.4.\nThe problem happens if a ColumnChunk contains only nulls. The Statistics object for that ColumnChunk looks like this:\n\nstatistics = {\n max = null\n min = null\n null_count = 43927\n distinct_count = 0\n\nThus, accessing statistics.min.array() inside fromParquetStatistics() results in an NPE.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=abalmin","name":"abalmin","key":"abalmin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Andrey Balmin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-01-11T19:21:41.736+0000","updated":"2016-01-11T19:21:41.736+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12856215/comment/15093004","id":"15093004","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=abalmin","name":"abalmin","key":"abalmin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Andrey Balmin","active":true,"timeZone":"America/Los_Angeles"},"body":"after a bit more digging around, it is pretty clear that this is caused by \nhttps://issues.apache.org/jira/browse/PARQUET-161, which was fixed in Parquet 1.6, under https://issues.apache.org/jira/browse/PARQUET-136\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=abalmin","name":"abalmin","key":"abalmin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Andrey Balmin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-01-12T00:30:46.918+0000","updated":"2016-01-12T00:30:46.918+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-11558/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2ixrj:"}}