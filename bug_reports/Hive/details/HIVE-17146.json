{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13088985","self":"https://issues.apache.org/jira/rest/api/2/issue/13088985","key":"HIVE-17146","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-07-24T07:40:00.900+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Jul 31 07:08:22 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17146/watchers","watchCount":3,"isWatching":false},"created":"2017-07-21T07:56:54.929+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12335838","id":"12335838","description":"Maintenance branch for 2.1 ","name":"2.1.1","archived":false,"released":true,"releaseDate":"2016-12-08"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12340268","id":"12340268","name":"3.0.0","archived":false,"released":true,"releaseDate":"2018-05-21"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-31T07:08:22.922+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12325007","id":"12325007","name":"Hive"}],"timeoriginalestimate":null,"description":"We found a bug in the current implementation of [org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java]\n\nThe *magic number 10* for minReplication factor can cause the exception when the configuration parameter _dfs.replication_ is lower than 10. \n\nConsider these [properties configuration|https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml] on our cluster (with less than 10 nodes):\n{code}\ndfs.namenode.replication.min=1\ndfs.replication=2\ndfs.replication.max=512 (that's the default value)\n{code}\nThe current implementation counts target file replication as follows (relevant snippets of the code):\n{code}\nprivate int minReplication = 10;\n...\nint dfsMaxReplication = hconf.getInt(DFS_REPLICATION_MAX, minReplication);\n    // minReplication value should not cross the value of dfs.replication.max\nminReplication = Math.min(minReplication, dfsMaxReplication);\n...\nFileSystem fs = path.getFileSystem(htsOperator.getConfiguration());\nshort replication = fs.getDefaultReplication(path);\n...\nint numOfPartitions = replication;\nreplication = (short) Math.max(minReplication, numOfPartitions);\n//use replication value in fs.create(path, replication);\n{code}\n\n\nWith a current code the used replication value is 10 and the config value _dfs.replication_ is not used at all.\n\nThere are probably more (easy) ways to fix it:\n# Set field  {code}private int minReplication = 1 ; {code} I don't see any obvious reason for the value 10.    or\n# Init minReplication from config value _dfs.namenode.replication.min_ with a default value 1. or\n# Count replication this way: {code}replication = Math.min(numOfPartitions, dfsMaxReplication);{code} or\n# Use replication = numOfPartitions; directly\nConfig value _dfs.replication_ has a default value 3 which is supposed to be always lower than \"dfs.replication.max\", no checking is probably needed.\n\nAny suggestions which option to choose? \n\n\n\nAs a *workaround* for this issue we had to set dfs.replication.max=2, but obviously _dfs.replication_ value should NOT be ignored and the problem should be resolved.\n\n\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12879575","id":"12879575","filename":"dfs.replication-settings.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-31T06:49:31.023+0000","size":38098,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12879575/dfs.replication-settings.png"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Spark on Hive - Exception while joining tables - \"Requested replication factor of 10 exceeds maximum of x\" ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16098037","id":"16098037","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~cabot],\n{code}\nreplication = (short) Math.max(minReplication, numOfPartitions);\n{code}\n{{numOfPartitions}} is {{fs.getDefaultReplication(path)}}, and {{minReplication}} is no more than {{dfs.replication.max}}. So why it exceeds the maximum replication? And according to the [hadoop doc|https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml], {{dfs.replication}} is the default replication, not {{dfs.replication.max}} right?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-24T07:40:00.900+0000","updated":"2017-07-24T07:40:00.900+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16100502","id":"16100502","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"body":"@[~ruili] The point is that the current code is ignoring {{dfs.replication}} (if value is <10 = magic number) and the only known workaround is to set {{dfs.replication.max}} \n(which is not obvious until you spend a day with solving this problem...)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-25T18:20:49.523+0000","updated":"2017-07-25T18:21:23.195+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16102625","id":"16102625","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~cabot], the code intends to distribute the hash table to more nodes so that following tasks are more likely to get the data from local DN. In that sense, it's intended to be bigger than {{dfs.replication}}. That's why we chose the magic number 10 (not an ideal solution I agree).\nHowever, since {{minReplication = Math.min(minReplication, dfsMaxReplication)}}, I still don't understand how the replication factor exceeds {{dfs.replication.max}} (by default 512)?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-27T02:59:24.083+0000","updated":"2017-07-27T02:59:24.083+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16102991","id":"16102991","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"body":"[~lirui] The 10 exceeds {{dfs.replication}} value, not {{dfs.replication.max}}.\n\nHere is the stacktrace\n{code:java}\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Requested replication factor of 10 exceeds maximum of 3 for /tmp/hive/xxxx/5db022b0-42b8-44c1-b303-823ed6c1f133/hive_2017-07-27_10-40-52_061_7060239276893628864-1/-mr-10003/HashTable-Stage-1/MapJoin-mapfile161--.hashtable/HASHTABLESINK_218-552787044 from x.x.x.x\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:1027)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2630)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2589)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:595)\n\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:112)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:395)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1471)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1408)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n\tat com.sun.proxy.$Proxy12.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n\tat com.sun.proxy.$Proxy13.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:827)\n\tat org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.flushToFile(SparkHashTableSinkOperator.java:156)\n\tat org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.closeOp(SparkHashTableSinkOperator.java:97)\n\t... 18 more\n\n{code}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-27T09:26:03.677+0000","updated":"2017-07-27T09:26:03.677+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16103020","id":"16103020","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~cabot], that seems strange. The exception should be thrown only if the replication [exceeds|https://github.com/apache/hadoop/blob/release-2.8.0-RC3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L1125] the [maximum|https://github.com/apache/hadoop/blob/release-2.8.0-RC3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L370]. Could you provide the versions of your Hive and Hadoop?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-27T09:53:57.763+0000","updated":"2017-07-27T09:53:57.763+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16103028","id":"16103028","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"body":"[~lirui] We are using Cloudera distribution - 2.6.0-cdh5.8.0\nAccording to these sources: \nhttps://repository.cloudera.com/artifactory/cloudera-repos/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.8.0/hadoop-hdfs-2.6.0-cdh5.8.0-sources.jar\nthe code looks same as in the code you linked. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-27T10:03:19.948+0000","updated":"2017-07-27T10:04:57.089+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16103082","id":"16103082","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~cabot], is it possible the configurations used by the Hive job and HDFS are different? I can't think of other reasons why this may happen.\n[~stakiar], [~spena] could you take a look as it happens with CDH?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-27T11:31:17.957+0000","updated":"2017-07-27T11:31:17.957+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16103415","id":"16103415","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"{{Requested replication factor of 10 exceeds maximum of 3 for ...}} should only be thrown if the value of {{dfs.replication.max}} exceeds 10.\n\nHive shouldn't be setting {{dfs.replication.max}}. If you are using CM, you can check the HDFS service to see what the value for {{dfs.replication.max}} is.\n\nYou can also open a Hive session via BeeLine and run {{set dfs.replication.max;}} to see what Hive thinks the value of {{dfs.replication.max}} is.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-27T16:16:55.898+0000","updated":"2017-07-27T16:16:55.898+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16106873","id":"16106873","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"body":"Our current Cloudera- hdfs settings","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-31T06:49:54.130+0000","updated":"2017-07-31T06:49:54.130+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16106874","id":"16106874","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"body":"[~stakiar] [~ruili]\nPlease see\n!dfs.replication-settings.png|HDFS Settings!\n\nAnd the output of Beeline:\n\n{code}\nset dfs.replication.max;\n+--------------------------+--+\n|           set            |\n+--------------------------+--+\n| dfs.replication.max=512  |\n+--------------------------+--+\n{code}\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cabot","name":"cabot","key":"cabot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"George Smith","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-31T06:51:44.383+0000","updated":"2017-07-31T06:53:14.678+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13088985/comment/16106885","id":"16106885","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~cabot], that confirms the value of {{dfs.replication.max}} is different for Hive and HDFS. Can you check whether your HS2 is using different configuration files? Another possible reason is the HS2 is not restarted after {{dfs.replication.max}} has changed in CM.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-31T07:08:22.922+0000","updated":"2017-07-31T07:08:22.922+0000"}],"maxResults":11,"total":11,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17146/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3hu73:"}}