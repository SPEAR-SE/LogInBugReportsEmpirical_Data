{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13004218","self":"https://issues.apache.org/jira/rest/api/2/issue/13004218","key":"HIVE-14735","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-09-28T11:23:51.312+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri May 19 09:01:50 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-14735/watchers","watchCount":9,"isWatching":false},"created":"2016-09-12T05:39:30.931+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"9.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12488496","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12488496","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13023342","key":"HIVE-15285","self":"https://issues.apache.org/jira/rest/api/2/issue/13023342","fields":{"summary":"err info for itests mvn building is not correct","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-05-19T09:01:50.617+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312593","id":"12312593","name":"Build Infrastructure","description":"Tracks issues with build files."}],"timeoriginalestimate":null,"description":"In particular this command:\n{{curl -Sso ./../thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-1.6.0-bin-hadoop2-without-hive.tgz}}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12836920","id":"12836920","filename":"HIVE-14735.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-11-03T18:18:03.346+0000","size":3756,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12836920/HIVE-14735.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12836895","id":"12836895","filename":"HIVE-14735.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-11-03T17:08:11.000+0000","size":3756,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12836895/HIVE-14735.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12836746","id":"12836746","filename":"HIVE-14735.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-11-03T07:21:07.520+0000","size":3756,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12836746/HIVE-14735.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12835879","id":"12835879","filename":"HIVE-14735.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-10-28T20:28:21.338+0000","size":3756,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12835879/HIVE-14735.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12837309","id":"12837309","filename":"HIVE-14735.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-11-04T22:41:35.997+0000","size":3757,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12837309/HIVE-14735.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12842623","id":"12842623","filename":"HIVE-14735.3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-12-09T22:27:43.086+0000","size":81195,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12842623/HIVE-14735.3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12855000","id":"12855000","filename":"HIVE-14735.4.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-02-27T22:33:14.084+0000","size":12170,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12855000/HIVE-14735.4.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12854079","id":"12854079","filename":"HIVE-14735.4.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-02-22T22:35:00.528+0000","size":12170,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12854079/HIVE-14735.4.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12855485","id":"12855485","filename":"HIVE-14735.5.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-03-01T23:13:06.921+0000","size":12170,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12855485/HIVE-14735.5.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Build Infra: Spark artifacts download takes a long time","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15529318","id":"15529318","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"body":"It is preceeded by ../target/download.sh: line 18: md5sum: command not found","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"created":"2016-09-28T11:23:51.312+0000","updated":"2016-09-28T11:23:51.312+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15531584","id":"15531584","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I think that means you have to install md5sum.\nMaybe we should document md5sum is required for the build. [~spena], any ideas?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-29T02:33:59.986+0000","updated":"2016-09-29T02:33:59.986+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15532978","id":"15532978","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"We added a .md5sum spark file to detect if a file must be downloaded again in the next build. This saves time if you already have an exact copy of the spark assembly.\nWhere is the issue happening? On our Jenkins build?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-29T14:46:38.491+0000","updated":"2016-09-29T14:46:38.491+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15533893","id":"15533893","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"body":"\n{code}\n...\n   [exec] arget/spark\n     [exec] + [[ ! -f ./../thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz ]]\n     [exec] + local md5File=spark-1.6.0-bin-hadoop2-without-hive.tgz.md5sum\n     [exec] + curl -Sso ./../thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz.md5sum http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-1.6.0-bin-hadoop2-without-hive.tgz.md5sum\n     [exec] + cd ./../thirdparty\n     [exec] + md5sum -c spark-1.6.0-bin-hadoop2-without-hive.tgz.md5sum\n     [exec] ../target/download.sh: line 18: md5sum: command not found\n     [exec] + curl -Sso ./../thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-1.6.0-bin-hadoop2-without-hive.tgz\n     [exec] + cd -\n     [exec] + tar -zxf ./../thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz -C ./target\n     [exec] /Users/mmccline/VecDetail/itests/qtest-spark\n     [exec] + mv ./target/spark-1.6.0-bin-hadoop2-without-hive ./target/spark\n     [exec] + cp -f ./target/../../..//data/conf/spark/log4j2.properties ./target/spark/conf/\n{code}\n\nAfter the \"./target/download.sh: line 18: md5sum: command not found\" line, the download of \"+ curl -Sso ./../thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz \" takes a very long time and happens everytime.  I tried downloading a version of md5sum and that seems to make it worse -- the build went off and hung.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"created":"2016-09-29T20:02:11.977+0000","updated":"2016-09-29T20:02:11.977+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15534060","id":"15534060","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"are you running linux or mac? which linux distro if so?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-29T21:10:05.817+0000","updated":"2016-09-29T21:10:05.817+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15534076","id":"15534076","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"body":"Oh, on my Mac laptop and usually current master.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"created":"2016-09-29T21:16:10.809+0000","updated":"2016-09-29T21:16:10.809+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15535044","id":"15535044","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"body":"Looks like OSX may not have md5sum installed by default. Should we use md5 on OSX?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-30T05:02:56.991+0000","updated":"2016-09-30T05:02:56.991+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15536175","id":"15536175","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"This is how md5sum works on linux:\n{noformat}\n$ md5sum spark-1.6.0-bin-hadoop2-without-hive.tgz \n296c808fe75a09518226fc149752bf3f  spark-1.6.0-bin-hadoop2-without-hive.tgz\n{noformat}\n\nWhat is the behavior in OSX? I don't have access to it.\nIf we can get the same MD5 sum with it, then I can add that to the code to use md5sum or md5.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-30T14:44:51.010+0000","updated":"2016-09-30T14:44:51.010+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15536408","id":"15536408","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"body":"OSX has an equivalent md5 command:\n{code}\nMD5 (./itests/thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz) = b3889e892b3d290a5ef95b0047295f80\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-30T16:31:28.576+0000","updated":"2016-09-30T16:31:28.576+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15536423","id":"15536423","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"That hash looks different. I read that to get a similar output as md5sum in osx you run {{md5 -r file}}. Can you try it?\nhttps://www.garron.me/en/bits/how-to-md5sum-mac-os-x.html","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-30T16:38:16.430+0000","updated":"2016-09-30T16:38:16.430+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15536491","id":"15536491","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"body":"Still getting the same hash:\n{code}\nvgumashta:hive vgumashta$ md5 -r ./itests/thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz\nb3889e892b3d290a5ef95b0047295f80 ./itests/thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz\n\nvgumashta:hive vgumashta$ md5 ./itests/thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz\nMD5 (./itests/thirdparty/spark-1.6.0-bin-hadoop2-without-hive.tgz) = b3889e892b3d290a5ef95b0047295f80\n{code}\n\nAre you on current master?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-30T17:07:31.448+0000","updated":"2016-09-30T17:07:31.448+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15614347","id":"15614347","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"I think it would be better (and possibly more portable) to load this custom artifact by utilizing maven to download it from a maven repository..\nThis would also enable maven to cache this file in the local maven repo...which is not affected by {{git clean -dfx}}, and may reduce unneccessary network load.\n\nAny problems with this?  - i can't put this this spark artifact into maven central (at least not under org.apache.spark) - someone would be intrested in putting it there - the best would be inside the spark project? ...or it doesn't matter where it comes from as long as it works ;)\nhmm..i've just started wondering....how those cloudfront artifacts are deployed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-10-28T05:21:16.421+0000","updated":"2016-10-28T05:21:16.421+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15615854","id":"15615854","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"The TAR file is uploaded manually to a public server where we the community have read-only access. This is not very good (as the community can't help too much), but that was the only way we found to do it.\n\nI don't know how Maven would work with this file. If there is a way to keep it published in Maven, and tell maven to download it and cache it, then it would be a lot of help. This file is only used by Hive, and it is a spark assembly built without hive libraries, so I don't think it should be inside the spark project.\n\nBtw, these are not JARS, but an assembly package from Spark used to execute a minispark or something. How would we deploy this on Maven?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-10-28T16:20:57.333+0000","updated":"2016-10-28T16:20:57.333+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15616460","id":"15616460","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"[~spena] it's good to know these things...i've tryed this out - because in gradle this would be easy...i assumed maven can do it too...well, it seems it does! but it needs quite a bunch of xml to do something like this ;)\n\ni've experimented with it...and it looks like it works - i would like to submit a test ptest run to check that everything is all right - but since my own server serves this spark related maven repo, I don't really want it to go in ;)\n\ni've published a preliminary \"conceptional\" repackaging tool here:\nhttps://github.com/kgyrtkirk/hive-14735\n\n[~spena], can you take a look at it, and see if it could be a viable alternative for the current artifact delivery method (or not)...I've tryed it out locally...in the readme i've sketched my steps how I tried it out - hope it helps evaluating it!\n\nI think this will eventually work...download/unpack/etc is done by maven plugins which should be highly portable.\n\nnotes:\n\n* {{mvn clean}} clears the unpacked things - which is good\n* unpacking a new version doesn't remove the old files, just pastes the new tree on top of it...but in case someone changes distinct branches I think he will use {{mvn clean}} or a harder {{git clean -dfx}} - so it should be ok\n* there is a log4j2 properties filewhich gets copied into this unpacked directory...it can be included in the artifact...or keep it like this?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-10-28T20:28:22.159+0000","updated":"2016-10-28T20:57:45.007+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15633701","id":"15633701","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12836895/HIVE-14735.1.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1949/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1949/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1949/\n\nMessages:\n{noformat}\n**** This message was trimmed, see log for full details ****\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Iterable.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/Writable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/String.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/http/HttpStatus.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/HashMap.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/MediaType.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/Response.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar(org/codehaus/jackson/map/ObjectMapper.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Exception.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Throwable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/Serializable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Enum.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Comparable.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/core/PackagesResourceConfig.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar(com/sun/jersey/spi/container/servlet/ServletContainer.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/FileInputStream.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/commons/lang3/StringUtils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/commons/lang3/ArrayUtils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-hdfs/2.7.2/hadoop-hdfs-2.7.2.jar(org/apache/hadoop/hdfs/web/AuthFilter.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/Utils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/UserGroupInformation.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-auth/2.7.2/hadoop-auth-2.7.2.jar(org/apache/hadoop/security/authentication/client/PseudoAuthenticator.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-auth/2.7.2/hadoop-auth-2.7.2.jar(org/apache/hadoop/security/authentication/server/PseudoAuthenticationHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/GenericOptionsParser.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/rewrite/handler/RedirectPatternRule.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/rewrite/handler/RewriteHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/Handler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/Server.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/handler/HandlerList.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/FilterHolder.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/FilterMapping.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/ServletContextHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/ServletHolder.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/xml/XmlConfiguration.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/slf4j/jul-to-slf4j/1.7.10/jul-to-slf4j-1.7.10.jar(org/slf4j/bridge/SLF4JBridgeHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar(javax/servlet/http/HttpServletRequest.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceAudience$LimitedPrivate.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Unstable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/ByteArrayOutputStream.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/OutputStream.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/Closeable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/AutoCloseable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/Flushable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(javax/xml/bind/annotation/XmlRootElement.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/security/PrivilegedExceptionAction.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShimsSecure.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/ShimLoader.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShims.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShims$WebHCatJTShim.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/ToolRunner.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/InterruptedException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Boolean.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/ql/ErrorMsg.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Integer.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobStatus.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/FileNotFoundException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URISyntaxException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URI.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/fs/FileSystem.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/api/MetaException.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/Text.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Credentials.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/token/Token.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar(org/apache/thrift/TException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/InetAddress.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/UnknownHostException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/text/MessageFormat.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/regex/Matcher.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/regex/Pattern.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/DELETE.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/FormParam.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/GET.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/POST.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/PUT.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/Path.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/PathParam.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/Produces.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/QueryParam.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/Context.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/SecurityContext.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriInfo.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobProfile.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Long.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/JavaUtils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/commons-lang/commons-lang/2.6/commons-lang-2.6.jar(org/apache/commons/lang/StringUtils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/fs/FileStatus.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/wadl/config/WadlGeneratorConfig.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/wadl/config/WadlGeneratorDescription.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/server/wadl/generators/resourcedoc/WadlGeneratorResourceDocSupport.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/BufferedReader.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/InputStream.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/InputStreamReader.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/PrintWriter.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Map$Entry.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/Semaphore.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/CommandLine.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/DefaultExecutor.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteWatchdog.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/PumpStreamHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Shell.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Thread.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Runnable.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/ext/ExceptionMapper.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/ext/Provider.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/NotFoundException.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobID.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Groups.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/HashSet.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Set.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/ConcurrentHashMap.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hive/common/util/HiveVersionInfo.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Evolving.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/DataInput.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/DataOutput.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputSplit.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFramework.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/CreateMode.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/KeeperException.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs$Ids.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/OutputStreamWriter.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URLConnection.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobClient.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobConf.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/RunningJob.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/StringTokenizer.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Process.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/StringBuilder.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/NullWritable.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputFormat.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobContext.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/RecordReader.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/TaskAttemptContext.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configured.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Job.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobID.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenIdentifier.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Tool.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configurable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/ClassNotFoundException.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFrameworkFactory.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar(org/apache/curator/retry/ExponentialBackoffRetry.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Iterator.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/LinkedList.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/ExecutorService.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/Executors.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/TimeUnit.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper$Context.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URLDecoder.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Enumeration.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Properties.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriBuilder.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/LogUtils.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Class.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Annotation.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience$LimitedPrivate.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Retention.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/RetentionPolicy.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Target.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/ElementType.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/HttpMethod.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/SuppressWarnings.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Override.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(sun/misc/Contended.class)]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatException$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatDelegator$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/Server$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$3.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/LauncherDelegator$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$2.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/HDFSStorage$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonUtils$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/ZooKeeperStorage$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/LogRetriever$1.class]]\n[done in 2975 ms]\n+ [[ -d itests ]]\n+ cd itests\n+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-it-custom-serde: Error resolving project artifact: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to spark-aux-repo (http://www.rxd.hu/misc/hive-14735/): Connect to localhost:3128 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused for project org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde -> [Help 1]\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-it-custom-udfs: Error resolving project artifact: Failure to transfer org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from http://www.rxd.hu/misc/hive-14735/ was cached in the local repository, resolution will not be reattempted until the update interval of spark-aux-repo has elapsed or updates are forced. Original error: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to spark-aux-repo (http://www.rxd.hu/misc/hive-14735/): Connect to localhost:3128 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused for project org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde -> [Help 1]\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-it-util: Error resolving project artifact: Failure to transfer org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from http://www.rxd.hu/misc/hive-14735/ was cached in the local repository, resolution will not be reattempted until the update interval of spark-aux-repo has elapsed or updates are forced. Original error: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to spark-aux-repo (http://www.rxd.hu/misc/hive-14735/): Connect to localhost:3128 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused for project org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-it-custom-serde\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12836895 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-11-03T18:13:53.241+0000","updated":"2016-11-03T18:13:53.241+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15634146","id":"15634146","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12836920/HIVE-14735.1.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1953/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1953/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1953/\n\nMessages:\n{noformat}\n**** This message was trimmed, see log for full details ****\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Iterable.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/Writable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/String.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/http/HttpStatus.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/HashMap.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/MediaType.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/Response.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar(org/codehaus/jackson/map/ObjectMapper.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Exception.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Throwable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/Serializable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Enum.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Comparable.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/core/PackagesResourceConfig.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar(com/sun/jersey/spi/container/servlet/ServletContainer.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/FileInputStream.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/commons/lang3/StringUtils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/commons/lang3/ArrayUtils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-hdfs/2.7.2/hadoop-hdfs-2.7.2.jar(org/apache/hadoop/hdfs/web/AuthFilter.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/Utils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/UserGroupInformation.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-auth/2.7.2/hadoop-auth-2.7.2.jar(org/apache/hadoop/security/authentication/client/PseudoAuthenticator.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-auth/2.7.2/hadoop-auth-2.7.2.jar(org/apache/hadoop/security/authentication/server/PseudoAuthenticationHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/GenericOptionsParser.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/rewrite/handler/RedirectPatternRule.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/rewrite/handler/RewriteHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/Handler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/Server.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/server/handler/HandlerList.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/FilterHolder.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/FilterMapping.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/ServletContextHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/servlet/ServletHolder.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/eclipse/jetty/aggregate/jetty-all-server/7.6.0.v20120127/jetty-all-server-7.6.0.v20120127.jar(org/eclipse/jetty/xml/XmlConfiguration.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/slf4j/jul-to-slf4j/1.7.10/jul-to-slf4j-1.7.10.jar(org/slf4j/bridge/SLF4JBridgeHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar(javax/servlet/http/HttpServletRequest.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceAudience$LimitedPrivate.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Unstable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/ByteArrayOutputStream.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/OutputStream.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/Closeable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/AutoCloseable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/Flushable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(javax/xml/bind/annotation/XmlRootElement.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/security/PrivilegedExceptionAction.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShimsSecure.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/ShimLoader.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShims.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/shims/common/target/hive-shims-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/shims/HadoopShims$WebHCatJTShim.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/ToolRunner.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/InterruptedException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Boolean.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/ql/target/hive-exec-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/ql/ErrorMsg.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Integer.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobStatus.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/FileNotFoundException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URISyntaxException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URI.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/fs/FileSystem.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/api/MetaException.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/Text.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Credentials.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/token/Token.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar(org/apache/thrift/TException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/InetAddress.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/UnknownHostException.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/text/MessageFormat.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/regex/Matcher.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/regex/Pattern.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/DELETE.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/FormParam.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/GET.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/POST.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/PUT.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/Path.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/PathParam.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/Produces.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/QueryParam.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/Context.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/SecurityContext.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriInfo.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobProfile.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Long.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/JavaUtils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/commons-lang/commons-lang/2.6/commons-lang-2.6.jar(org/apache/commons/lang/StringUtils.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/fs/FileStatus.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/wadl/config/WadlGeneratorConfig.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/wadl/config/WadlGeneratorDescription.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/server/wadl/generators/resourcedoc/WadlGeneratorResourceDocSupport.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/BufferedReader.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/InputStream.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/InputStreamReader.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/PrintWriter.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Map$Entry.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/Semaphore.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/CommandLine.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/DefaultExecutor.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteWatchdog.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/PumpStreamHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Shell.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Thread.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Runnable.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/ext/ExceptionMapper.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/ext/Provider.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar(com/sun/jersey/api/NotFoundException.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobID.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Groups.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/HashSet.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Set.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/ConcurrentHashMap.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hive/common/util/HiveVersionInfo.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Evolving.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/DataInput.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/DataOutput.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputSplit.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFramework.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/CreateMode.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/KeeperException.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs$Ids.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/io/OutputStreamWriter.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URLConnection.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobClient.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobConf.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/RunningJob.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/StringTokenizer.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Process.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/StringBuilder.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/NullWritable.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputFormat.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobContext.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/RecordReader.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/TaskAttemptContext.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configured.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Job.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobID.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenIdentifier.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Tool.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configurable.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/ClassNotFoundException.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFrameworkFactory.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar(org/apache/curator/retry/ExponentialBackoffRetry.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Iterator.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/LinkedList.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/ExecutorService.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/Executors.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/TimeUnit.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper$Context.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URLDecoder.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Enumeration.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Properties.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriBuilder.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/LogUtils.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Class.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Annotation.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience$LimitedPrivate.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Retention.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/RetentionPolicy.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Target.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/ElementType.class)]]\n[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/HttpMethod.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/SuppressWarnings.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Override.class)]]\n[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(sun/misc/Contended.class)]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatException$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatDelegator$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/Server$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$3.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/LauncherDelegator$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$2.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/HDFSStorage$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonUtils$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/ZooKeeperStorage$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1$1.class]]\n[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/LogRetriever$1.class]]\n[done in 3514 ms]\n+ [[ -d itests ]]\n+ cd itests\n+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-it-custom-udfs: Error resolving project artifact: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to spark-aux-repo (http://www.rxd.hu/misc/hive-14735/): Connect to localhost:3128 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused for project org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde -> [Help 1]\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-it-custom-serde: Error resolving project artifact: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to spark-aux-repo (http://www.rxd.hu/misc/hive-14735/): Connect to localhost:3128 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused for project org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde -> [Help 1]\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (default) on project hive-it-util: Error resolving project artifact: Failure to transfer org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from http://www.rxd.hu/misc/hive-14735/ was cached in the local repository, resolution will not be reattempted until the update interval of spark-aux-repo has elapsed or updates are forced. Original error: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to spark-aux-repo (http://www.rxd.hu/misc/hive-14735/): Connect to localhost:3128 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused for project org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-it-custom-udfs\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12836920 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-11-03T20:31:01.160+0000","updated":"2016-11-03T20:31:01.160+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15638310","id":"15638310","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12837309/HIVE-14735.2.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 10628 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[columnstats_part_coltype] (batchId=148)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)\norg.apache.hive.spark.client.TestSparkClient.testJobSubmission (batchId=272)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1979/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1979/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1979/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 4 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12837309 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-11-05T01:41:25.437+0000","updated":"2016-11-05T01:41:25.437+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15639454","id":"15639454","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"in #2 i've changed to use https instead of http - it's still a proof of concept patch.\n\n[~spena]: It seems to me that this method works.... how do you like it? \n\n[~sseth]: I think there were some problems with the ptest executors... in an earlier build I tried to use http protocol...and maven attempted to use localhost:3128 as the http proxy - but there were some connection refused problems...the logs are wiped since that...so i can't give a link...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-11-05T11:57:29.075+0000","updated":"2016-11-05T11:57:29.075+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15644916","id":"15644916","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"Thanks [~kgyrtkirk]. The patch looks good, but I need to dig a little more, and test it. But it looks promising.\nI'll try to review it this week.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-11-07T18:11:52.709+0000","updated":"2016-11-07T18:11:52.709+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15713790","id":"15713790","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kgyrtkirk] -I'm + 1 on this-. [~spena] let us know what do you think?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-02T02:25:08.472+0000","updated":"2016-12-04T23:12:10.178+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15720770","id":"15720770","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kgyrtkirk] The build fails for me with patch v2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-04T23:12:30.562+0000","updated":"2016-12-04T23:12:30.562+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15726831","id":"15726831","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"[~kgyrtkirk] This prototype works pretty good when downloading the file. I like it.\nLet's continue working on this to use it as replacement of the shell script.\n\nA few comments:\n\n* Could you use a variable for the plugin version? You can find them on the root pom.xml\n<groupId>org.apache.maven.plugins</groupId>\n<artifactId>maven-dependency-plugin</artifactId>\n<version>2.10</version>\n\n* is the 'target/download.sh' needed now? We can keep only the log4j2.properties. The log cannot be packed into the artifact because it might be edited by other Hive developers.\n\n* How do you generate the new artifacts? We should write a README for this.\n\n* Where is the file downloaded? I see this {{${project.build.directory}/spark}}, but I don't find it. Can you use the original thirdparty directory instead?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-12-06T21:53:33.850+0000","updated":"2016-12-06T21:53:33.850+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15732552","id":"15732552","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"[~vgumashta] i'm sorry to hear that...there might be some issues because the patch is in prototype stage.\n\n[~spena] I've left some of the existing code as is for now - to aid reviewing the key parts which will change after this - i'll clean that up for the final version.\n\nI've published my \"prototype\" jar publishing mechanism on github: https://github.com/kgyrtkirk/hive-14735 - but it's currently just \"repackages\" the original artifact into a new form...it would make things easier if I would know more about how the current artifact is being built - I think it would be great to even add the spark-without-hive artifact builder to the hive repository","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-12-08T15:43:26.365+0000","updated":"2016-12-08T15:43:26.365+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15732711","id":"15732711","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"[~stakiar] [~Ferd] Do you have instructions on how to build the spark-without-hive assembly jar? Zoltan is working on a better way to download the jars, and he would like to know how the jar is built.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-12-08T16:50:47.384+0000","updated":"2016-12-08T16:50:47.384+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15732729","id":"15732729","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"You have to git clone the Spark repo and run the command:\n\n{code}\nsh ./dev/make-distribution.sh  --name hadoop2-without-hive --tgz -Phadoop-2.7 -Pyarn -Pparquet-provided -Dhadoop.version=2.7.3\n{code}\n\nThat will built the build Spark distribution, which will contain all the JAR files.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2016-12-08T16:59:25.327+0000","updated":"2016-12-08T16:59:25.327+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15733799","id":"15733799","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Yes, please use the command as [~stakiar] said.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-12-09T00:08:32.461+0000","updated":"2016-12-09T00:08:32.461+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15736556","id":"15736556","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"Thank you for the command [~stakiar], i've added it to the patch.\n\nI've uploaded #3: I hope I didn't break anything ...the ptest execution will shed light on this.\n\n[~spena] i've addressed much of your comments (however I still use fixed version for the maven plugins - i've forgot fix that)\nand also...i've missed your previous question about \"where the downloaded file is\": it's inside the local maven repository.\n\ni've changed the following:\n* added a project to repack the spark artifact under dev-support, with a readme describing the procedure\n* {{itests/thirparty}} is now a module - this way these maven \"tricks\" are isolated, other modules rely on that thirdparty have already finished - this also enabled to support even multiple spark versions - which may come handy for people who switch between branches which pull different spark version\n* it now only unpacks the spark assembly to only 1 place\n\n[~spena] what do you think about the new changes?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-12-09T22:41:39.496+0000","updated":"2016-12-09T22:41:39.496+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15742633","id":"15742633","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12842623/HIVE-14735.3.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 10795 tests executed\n*Failed tests:*\n{noformat}\nTestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=108)\n\t[groupby_grouping_id2.q,input17.q,bucketmapjoin12.q,ppd_gby_join.q,auto_join10.q,ptf_rcfile.q,vectorized_rcfile_columnar.q,vector_elt.q,ppd_join5.q,ppd_join.q,join_filters_overlap.q,join_cond_pushdown_1.q,timestamp_3.q,load_dyn_part6.q,stats_noscan_2.q]\nTestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=250)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=134)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=134)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=150)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/2542/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/2542/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2542/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 10 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12842623 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-12-12T18:05:44.726+0000","updated":"2016-12-12T18:05:44.726+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15745569","id":"15745569","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"- is skipSparkTests or skipSparkAssemblyDeploy?\n- can we use maven instead of gradle? I just want to avoid using another build tool that contributors will require to learn to do maintenance. \n- can you add on the README how to publish files manually? The current repo is not a maven repo, so the publish function won't work.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-12-13T16:33:50.734+0000","updated":"2016-12-13T16:33:50.734+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15751612","id":"15751612","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"Hello [~spena], thank you for taking a look! :)\n\n* skipSparkAssemblyDeploy - there is a single leftover setting of this variable to true - sorry for it: it was part of the previous patch version; i'll remove it...because now its not neccessary as the thirdparty project does the unpacking - it will skip even downloading/unpacking if the tests are being skipped\n\n* in its current form the publish doesnt work; because it tries to use my own private server - in its current form gradle can upload the artifacts using ssh access to any host - to make it work with another server; both of the rxd.hu references should be changed.\n\ngradle / etc topic:\n\n* the simplest would be to move this gradle project outside the project...into a custom repo; and place pointers in the readme file to it.\n* if the spark project would be willing to publish the 'spark-without-hive' artifact  as a zip into the central maven repo - that would make this whole gradle/etc thing unneccessary ; but in this case they would need to publish this new artifact for spark-2.0.0 - because hive currently uses that version - this has other \"+\" sides to. as it doesnt need an extra repository declaration.\n* I will look into alternatives...possibly using maven...or some shell scripts to achieve the same results as with gradle...\n\n[~spena] which one of the above would you prefer ?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2016-12-15T15:16:19.843+0000","updated":"2016-12-15T15:16:19.843+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15761504","id":"15761504","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"We're using Amazon S3 to store the .tgz binary for now. That's why I was wondering if we could have a way get the files to upload them manually (to a non-mvn server).\n\nI don't know if the Spark team will agree on publishing the 'spark-without-hive' as this only benefits us. Anyway, the S3 storage is temporary until we solve the spark-without-hive dependency mentioned on HIVE-14240.\n\nEither maven or shell script is good.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-12-19T15:45:33.730+0000","updated":"2016-12-19T15:45:33.730+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15798344","id":"15798344","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"thanks for the info and link to HIVE-14240 [~spena], I wasn't aware that!\n\ndeploying the repo files \"as\" a maven repository only needs the files organized in a directory tree...so it should be no problem serving them from s3 (I guess).\n\nI'll write a shell-script then...because in this case - this shouldnt stay for \"long\" - and will be replaced with official spark artifacts later - I will evaluate this option in the mean time...\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-01-04T14:09:56.635+0000","updated":"2017-01-04T14:09:56.635+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15879383","id":"15879383","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"[~spena], i've changed the logic which built the repository into a shell script.\n\nI was wondering: would it be possible (and acceptable) to upload this 'org.apache.hive.aux:spark-without-hive' artifact to repository.apache.org? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-02-22T22:35:00.536+0000","updated":"2017-02-22T22:35:00.536+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15879889","id":"15879889","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12854079/HIVE-14735.4.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/3713/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/3713/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3713/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'\n2017-02-23 05:27:14.653\n+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]\n+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'MAVEN_OPTS=-Xmx1g '\n+ MAVEN_OPTS='-Xmx1g '\n+ cd /data/hiveptest/working/\n+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-3713/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ date '+%Y-%m-%d %T.%3N'\n2017-02-23 05:27:14.655\n+ cd apache-github-source-source\n+ git fetch origin\n+ git reset --hard HEAD\nHEAD is now at 759766e HIVE-15955: make explain formatted to include opId and etc (Pengcheng Xiong, reviewed by Ashutosh Chauhan)\n+ git clean -f -d\n+ git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n+ git reset --hard origin/master\nHEAD is now at 759766e HIVE-15955: make explain formatted to include opId and etc (Pengcheng Xiong, reviewed by Ashutosh Chauhan)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ date '+%Y-%m-%d %T.%3N'\n2017-02-23 05:27:15.852\n+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hiveptest/working/scratch/build.patch\n+ [[ -f /data/hiveptest/working/scratch/build.patch ]]\n+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh\n+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch\nerror: itests/thirdparty/.gitignore: already exists in working directory\nerror: itests/thirdparty/pom.xml: already exists in working directory\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12854079 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-02-23T05:27:16.375+0000","updated":"2017-02-23T05:27:16.375+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15887322","id":"15887322","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12855000/HIVE-14735.4.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/3828/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/3828/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3828/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'\n2017-02-28 05:44:31.918\n+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]\n+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'MAVEN_OPTS=-Xmx1g '\n+ MAVEN_OPTS='-Xmx1g '\n+ cd /data/hiveptest/working/\n+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-3828/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ date '+%Y-%m-%d %T.%3N'\n2017-02-28 05:44:31.921\n+ cd apache-github-source-source\n+ git fetch origin\n+ git reset --hard HEAD\nHEAD is now at 2869eca HIVE-15958: LLAP: IPC connections are not being reused for umbilical protocol (Prasanth Jayachandran reviewed by Siddharth Seth)\n+ git clean -f -d\n+ git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n+ git reset --hard origin/master\nHEAD is now at 2869eca HIVE-15958: LLAP: IPC connections are not being reused for umbilical protocol (Prasanth Jayachandran reviewed by Siddharth Seth)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ date '+%Y-%m-%d %T.%3N'\n2017-02-28 05:44:32.822\n+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hiveptest/working/scratch/build.patch\n+ [[ -f /data/hiveptest/working/scratch/build.patch ]]\n+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh\n+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch\nerror: itests/thirdparty/.gitignore: already exists in working directory\nerror: itests/thirdparty/pom.xml: already exists in working directory\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12855000 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-02-28T05:44:33.328+0000","updated":"2017-02-28T05:44:33.328+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15891263","id":"15891263","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"it looks like the build node has a non-clean copy...this patch adds {{itests/thirdparty/pom.xml}} - I submit it again..it may have been fixed\n\n[~spena] I think it would be much better to server this artifact from {{repository.apache.org}} - it seems pretty straightforward to upload it to there - would it be okay to try? :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-03-01T23:11:53.713+0000","updated":"2017-03-01T23:11:53.713+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15891352","id":"15891352","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12855485/HIVE-14735.5.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/3879/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/3879/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3879/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'\n2017-03-02 00:14:01.641\n+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]\n+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'MAVEN_OPTS=-Xmx1g '\n+ MAVEN_OPTS='-Xmx1g '\n+ cd /data/hiveptest/working/\n+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-3879/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ date '+%Y-%m-%d %T.%3N'\n2017-03-02 00:14:01.643\n+ cd apache-github-source-source\n+ git fetch origin\n+ git reset --hard HEAD\nHEAD is now at ba8de30 HIVE-14459: TestBeeLineDriver - migration and re-enable (Peter Vary via Zoltan Haindrich reviewed by Vihang Karajgaonkar)\n+ git clean -f -d\n+ git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n+ git reset --hard origin/master\nHEAD is now at ba8de30 HIVE-14459: TestBeeLineDriver - migration and re-enable (Peter Vary via Zoltan Haindrich reviewed by Vihang Karajgaonkar)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ date '+%Y-%m-%d %T.%3N'\n2017-03-02 00:14:02.776\n+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hiveptest/working/scratch/build.patch\n+ [[ -f /data/hiveptest/working/scratch/build.patch ]]\n+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh\n+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch\nerror: itests/thirdparty/.gitignore: already exists in working directory\nerror: itests/thirdparty/pom.xml: already exists in working directory\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12855485 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-03-02T00:14:03.298+0000","updated":"2017-03-02T00:14:03.298+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15894728","id":"15894728","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"Thanks [~kgyrtkirk]. I'm gonna take a look at the patch. Having the patch on the apache repository sounds like a good idea. Let's ask [~ashutoshc]. Do you think we can publish the spark-without-hive tarball on {{repository.apache.org}} for hive building purposes?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2017-03-03T17:21:58.160+0000","updated":"2017-03-03T17:21:58.160+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15894803","id":"15894803","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"[~spena] the current patch may need some cleanup - but it should be ok overall  :)\nthe ptest server is unable to build it because the patch removes thirdparty from the .gitignore - and the previous version of this patch is still in there...\n\nAbout moving the build dependency to {{repository.apache.org}}, the current artifact name is: {{group: org.apache.hive.aux, artifactId:spark-without-hive}} - i'm open to change this to anything else...since this artifact is somewhere in the middle between hive and spark projects...I'm not sure what to choose for groupId - ideally it would be an artifact published during spark releasing...but since 2.0.0 is already out - we missed that train already :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-03-03T18:20:30.851+0000","updated":"2017-03-03T18:20:30.851+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15895056","id":"15895056","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"body":"I am not sure whether publishing an artifact of another project is a good idea. Ideally, spark project itself should publish these artifacts. At the very least we shall ask on spark list of our intention for this and see what feedback we get.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-03-03T21:28:05.809+0000","updated":"2017-03-03T21:28:05.809+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15926524","id":"15926524","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"[~kgyrtkirk] Would you mind asking the spark list if we can publish this jar on its side?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2017-03-15T16:36:14.127+0000","updated":"2017-03-15T16:36:14.127+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15932639","id":"15932639","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"[~spena] I've asked the spark developers about this: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-without-hive-assembly-for-hive-build-development-purposes-td21188.html\n\nI didn't got back a clear answer to my question...beyond a \"why do we use that\" and a reference to HIVE-15302. \nwhat should we do now?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-03-20T13:45:03.687+0000","updated":"2017-03-20T13:45:03.687+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15957471","id":"15957471","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"body":"Is there someway I can avoid downloading the file each time?  It is 135 Mb plus and is right now burning up my broadband HotSpot Gb limit.  Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmccline","name":"mmccline","key":"mmccline","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=mmccline&avatarId=36046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmccline&avatarId=36046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmccline&avatarId=36046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmccline&avatarId=36046"},"displayName":"Matt McCline","active":true,"timeZone":"America/Chicago"},"created":"2017-04-05T19:11:39.662+0000","updated":"2017-04-05T19:11:39.662+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/15957570","id":"15957570","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"body":"[~mmccline] You can disable the {{download-spark}} execution in various poms.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-04-05T20:18:38.864+0000","updated":"2017-04-05T20:18:38.864+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/16017073","id":"16017073","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"body":"I got used to using {{-DskipSparkTests}}....seems like currently disabling these tests is the best option to prevent the re-downloads","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"created":"2017-05-19T08:45:04.194+0000","updated":"2017-05-19T08:45:04.194+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13004218/comment/16017087","id":"16017087","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12855485/HIVE-14735.5.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/5344/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/5344/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5344/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'\n2017-05-19 09:01:48.936\n+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]\n+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'MAVEN_OPTS=-Xmx1g '\n+ MAVEN_OPTS='-Xmx1g '\n+ cd /data/hiveptest/working/\n+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-5344/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ date '+%Y-%m-%d %T.%3N'\n2017-05-19 09:01:48.938\n+ cd apache-github-source-source\n+ git fetch origin\n+ git reset --hard HEAD\nHEAD is now at 3be1eed HIVE-16672: Parquet vectorization doesn't work for tables with partition info (Colin Ma, reviewed by Ferdinand Xu)\n+ git clean -f -d\nRemoving ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimal.txt\nRemoving ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimalMerge.txt\nRemoving ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgMerge.txt\nRemoving ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgTimestamp.txt\nRemoving ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumTimestamp.java\n+ git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n+ git reset --hard origin/master\nHEAD is now at 3be1eed HIVE-16672: Parquet vectorization doesn't work for tables with partition info (Colin Ma, reviewed by Ferdinand Xu)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ date '+%Y-%m-%d %T.%3N'\n2017-05-19 09:01:50.111\n+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hiveptest/working/scratch/build.patch\n+ [[ -f /data/hiveptest/working/scratch/build.patch ]]\n+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh\n+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch\nerror: itests/thirdparty/.gitignore: already exists in working directory\nerror: itests/thirdparty/pom.xml: already exists in working directory\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12855485 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-05-19T09:01:50.617+0000","updated":"2017-05-19T09:01:50.617+0000"}],"maxResults":46,"total":46,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-14735/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i33i6v:"}}