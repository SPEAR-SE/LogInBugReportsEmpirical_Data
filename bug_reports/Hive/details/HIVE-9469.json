{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12770161","self":"https://issues.apache.org/jira/rest/api/2/issue/12770161","key":"HIVE-9469","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-02-11T05:54:38.121+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue May 05 19:55:22 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-9469/watchers","watchCount":5,"isWatching":false},"created":"2015-01-26T19:40:21.863+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320745","id":"12320745","description":"released","name":"0.10.0","archived":false,"released":true,"releaseDate":"2013-01-11"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-05-05T19:55:22.908+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312584","id":"12312584","name":"Metastore","description":"Tracks issue dealing with metastore."}],"timeoriginalestimate":null,"description":"Hi All,\n\nPlease review the following problem, I also posted same in the hive-user group, but didnt got any response yet. \nThis is happening quite frequently in our environment. \nSo, it would be great if somebody can see and advise. \n\nI'm using Hive Thrift Server in Production which at peak handles around 500 req/min.\nAfter certain point the Hive Thrift Server is going into the no response mode and throws \nFollowing exception \n\"org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\" \n\nAs the metastore we are using MySQL, that is being used by Thrift server. \nThe design / architecture is like this: \n\nOozie -- > Hive Action --> ELB (AWS) --> Hive Thrift ( 2 servers) --> MySQL (Master) -- > MySQL (Slave).\n\nSoftware versions: \n\n   Hive version : 0.10.0\n   Hadoop: 1.2.1\n\n\nLooks like when the load is beyond some threshold for certain operations it is having problem in responding. \nAs the hive jobs sometimes fails because of this issue, we also have a auto-restart check to see if the Thrift server is not responding, it stops / kills and restart the service. \n\nOther tuning done: \n\nThrift Server: \n\nGiven 11gb heap, and configured CMS GC algo. \n\nMySQL: \n\nTuned innodb_buffer, tmp_table and max_heap parameters.\n\nSo, can somebody please help to understand, what could be the root cause for this or somebody faced the similar issue. \n\nI found one related JIRA :https://issues.apache.org/jira/browse/HCATALOG-541\n\nBut this JIRA shows that Hive Thrift Server shows OOM error, but in my case I didnt see any OOM error in my case.\n\n\nRegards,\nManish\n\nFull Exception Stack: \n\n    at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)\n    at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)\n    at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)\n    at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:412)\n    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:399)\n    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:736)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:601)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:74)\n    at $Proxy7.getDatabase(Unknown Source)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1110)\n    at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1099)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.showTables(DDLTask.java:2206)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:334)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:138)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1336)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1122)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:935)\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:347)\n    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:706)\n    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:613)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:601)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: java.net.SocketTimeoutException: Read timed out\n    at java.net.SocketInputStream.socketRead0(Native Method)\n    at java.net.SocketInputStream.read(SocketInputStream.java:150)\n    at java.net.SocketInputStream.read(SocketInputStream.java:121)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n    at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)\n    ... 34 more\n2015-01-20 22:44:12,978 ERROR exec.Task (SessionState.java:printError(401)) - FAILED: Error in metadata: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\norg.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\n    at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1114)\n    at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1099)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.showTables(DDLTask.java:2206)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:334)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:138)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1336)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1122)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:935)\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12697899","id":"12697899","filename":"After_JMV_Profiling_Tuning.jpg","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-10T22:48:36.388+0000","size":87942,"mimeType":"image/jpeg","content":"https://issues.apache.org/jira/secure/attachment/12697899/After_JMV_Profiling_Tuning.jpg"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12697900","id":"12697900","filename":"Before_JMV_Profiling_Tuning.jpg","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-10T22:48:57.469+0000","size":81770,"mimeType":"image/jpeg","content":"https://issues.apache.org/jira/secure/attachment/12697900/Before_JMV_Profiling_Tuning.jpg"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive Thrift Server throws Socket Timeout Exception: Read time out","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"4 core cpu, 15gb memory. 2 thrift server behind load balancer","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12770161/comment/14315109","id":"14315109","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"body":"Continue on the similar thread.\nFollowing are the work, I did for the load testing and fixing some of the issues.\nIt will be great, if somebody can review this and see, if there are things which Im missing.\nAnd some time still I see the SocketTimeoutException, but ETL jobs are not failing.\n\n\nCurrently running load test with following commands / load using Hive Client APIs.\n\na.\tCreate Partition  - 10 threads \nb.\tListPartition  - 30 threads \nc.\tShow tables â€“ 100 threads\n\nLoad on the server was around 1200 Request Per Minute.\nand for this test Thirft Server + MySQL looks good.\n\nThe tuning and finding are:\n\nThrift Server\n\n1.\tJVM tuning :  (JVM profiling shows with default settings, there were too frequent Full GC happening)\n\nYoung Generation GC Algo: Parallel\n\nOld Generation GC Algo: CMS\n\nMax_Heap: 11 Gb\n\nSurvivorRatio : 6\n\n\nGraph before optimization:\n\n\n-- Attaching as separate files.\n\n\nGraph after optimization:\n\n-- Attaching as separate files.\n \n\n\n2.\tDatabase Connection Pooling: \n\nThrift Server uses DataNucleus framework for DB operations. \nAnd it uses DBCP as the connection pooling tool, the default config for DBCP is maxConnections = 10.\nChanged it to 30. \n\nAs that is the basic bottleneck to server more requests. \n\n\nDatabase: \n\n1.\tinnodb_buffer = 8gb and tmp_table_space, max_heap_space = 256 mb. \n\n\nThe other problem I unearthed was that one of the hive-table that had more 1 million rows, and in PROD the ListPartition was happening on this table, when this happened it takes a lot of time to get the response from DB as there are too many rows in the PARTITION table.\nSo, it started blocking threads in Thrift Server and keep using one of the DB Connection and eventually got into state where all the DB Connections are used and new request cannot get the DB Connection and started getting. \nThis problem was eventually making our hive queries failing and restarting. \n\nWhen solved this problem the throughput of the Thrift Server has increased a lot.  And The failure of Hive Jobs has reduced a lot. \n\nSo, please let me know if these changes and solving the ListPartion problem for big table is good or there are few other things which we should take care.\n\n\nRegards,\nManish\n\n\n---------------------------------------------------------  Following are the details of the PROD Infrastructure ---------------------------------------------------------\n\n\nLoad  = 500 req/min.\n\nException: \"org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\" \n\nAs the metastore we are using MySQL, that is being used by Thrift server. \nThe flow is like this: \n\nOozie -- > Hive Action --> ELB (AWS) --> Hive Thrift ( 2 servers) --> MySQL (Master) -- > MySQL (Slave).\n\nSoftware versions: \n\n   Hive version : 0.10.0\n   Hadoop: 1.2.1\n\n\nI found one related JIRA :https://issues.apache.org/jira/browse/HCATALOG-541\n\nBut this JIRA shows that Hive Thrift Server shows OOM error, but in my case I didnt see any OOM error in my case.\n\n\nRegards,\nManish\n\nFull Exception Stack:  ( The exception comes when the server is loaded and new requests are timing out )\n\n    at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)\n    at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)\n    at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)\n    at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:412)\n    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:399)\n    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:736)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:601)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:74)\n    at $Proxy7.getDatabase(Unknown Source)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1110)\n    at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1099)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.showTables(DDLTask.java:2206)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:334)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:138)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1336)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1122)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:935)\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:347)\n    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:706)\n    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:613)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:601)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: java.net.SocketTimeoutException: Read timed out\n    at java.net.SocketInputStream.socketRead0(Native Method)\n    at java.net.SocketInputStream.read(SocketInputStream.java:150)\n    at java.net.SocketInputStream.read(SocketInputStream.java:121)\n    at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n    at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\n    at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n    at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)\n    ... 34 more\n2015-01-20 22:44:12,978 ERROR exec.Task (SessionState.java:printError(401)) - FAILED: Error in metadata: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\norg.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out\n    at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1114)\n    at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1099)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.showTables(DDLTask.java:2206)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:334)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:138)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1336)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1122)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:935)\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-10T22:41:50.565+0000","updated":"2015-02-10T22:41:50.565+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12770161/comment/14315116","id":"14315116","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"body":"After HMS JVM Tuning","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-10T22:48:36.396+0000","updated":"2015-02-10T22:48:36.396+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12770161/comment/14315117","id":"14315117","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"body":"Before JVM Profiling Tuning","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-10T22:48:57.473+0000","updated":"2015-02-10T22:48:57.473+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12770161/comment/14315615","id":"14315615","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"body":"[~manish_malhotra] Your hive version is really old. Newer releases have fixed multiple bugs, have many new features and much lower latency. My suggestion would be to upgrade.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-11T05:54:38.121+0000","updated":"2015-02-11T05:54:38.121+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12770161/comment/14315761","id":"14315761","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~vgumashta]. \nBut does the old version had this issue, as I didnt see issue like this, apart from https://issues.apache.org/jira/browse/HCATALOG-541 where there were OOM.\n\nWe are in the process of upgrading the Hive to 12. \n\nMeanwhile the steps I have taken for better performance and to avoid this problem are:  \n\n1. Database connection pooling tuning, \nDefault is 10, made it 30 on each thrift server.\nThough the the DBCP Connection Pool ( maximum connections) config also need to be think though as that will also have implication of using MySQL resources. \n\n2. JVM GC Tuning \n\n3. keeping number of partitions in tact \n\nDo you have any other suggestion for production deployment.\n\nPlus I have another question, that is, Thrift Server uses DataNucleus framework which is Open-Source Persistence product, and internally uses JDO. \nDataNucleus doesnt support all the configs for DBCP connection pooling, \nSo, should either Thrift can use another ORM tool or provide more hooks for the DBCP support. \n\nRegards,\nManish","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-11T08:18:01.843+0000","updated":"2015-02-11T08:18:01.843+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12770161/comment/14317527","id":"14317527","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"body":"[~vgumashta]\nPlease see, based on ghe testing and results i got can i propose to add the connection pooling prop in the config.\n\nSo, does it make sense to update the data nucleus config to add thw max connection propert kn the hive-site.xml.\n\nAs what is happening, the default 10 db connection is not good for most of the prod systems.\n\nThanks.\n\nRegards,\nManish","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manish.hadoop.work%40gmail.com","name":"manish.hadoop.work@gmail.com","key":"manish.hadoop.work@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manish Malhotra","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-12T03:43:31.586+0000","updated":"2015-02-12T03:43:31.586+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12770161/comment/14529157","id":"14529157","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"body":"[~manish.hadoop.work@gmail.com] Sorry about the late response. Yes, makes sense to set the datanuclues config to allow more db connections. I would set the size based on how big is the thrift worker pool. I don't think 10 db connections are good enough for 500 worker threads.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vgumashta","name":"vgumashta","key":"vgumashta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vaibhav Gumashta","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-05T19:55:22.908+0000","updated":"2015-05-05T19:55:22.908+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-9469/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i24t7r:"}}