{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12739095","self":"https://issues.apache.org/jira/rest/api/2/issue/12739095","key":"HIVE-7980","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327352","id":"12327352","description":"Dev branch for Hive on Spark","name":"spark-branch","archived":false,"released":false}],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-09-05T17:26:55.735+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Jan 22 06:33:39 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-7980/watchers","watchCount":6,"isWatching":false},"created":"2014-09-04T10:25:43.366+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327352","id":"12327352","description":"Dev branch for Hive on Spark","name":"spark-branch","archived":false,"released":false}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=csun","name":"csun","key":"csun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"},"displayName":"Chao Sun","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-01-22T06:33:39.359+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12320408","id":"12320408","name":"HiveServer2","description":"Tracks issues related to HiveServer2"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12323200","id":"12323200","name":"Spark","description":"Hive on Spark"}],"timeoriginalestimate":null,"description":".I followed this guide(https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started). and i compiled hive from spark branch. in the next step i met the below error..\n(*i typed the hive query on beeline, i used the  simple query using \"order by\" to invoke the palleral works \n\n                   ex) select * from test where id = 1 order by id;\n)\n\n[Error list is]\n2014-09-04 02:58:08,796 ERROR spark.SparkClient (SparkClient.java:execute(158)) - Error generating Spark Plan\njava.lang.NullPointerException\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:1262)\n\tat org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:1269)\n\tat org.apache.spark.SparkContext.hadoopRDD$default$5(SparkContext.scala:537)\n\tat org.apache.spark.api.java.JavaSparkContext.hadoopRDD(JavaSparkContext.scala:318)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateRDD(SparkPlanGenerator.java:160)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate(SparkPlanGenerator.java:88)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkClient.execute(SparkClient.java:156)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.submit(SparkSessionImpl.java:52)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:77)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:72)\n2014-09-04 02:58:11,108 ERROR ql.Driver (SessionState.java:printError(696)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n2014-09-04 02:58:11,182 INFO  log.PerfLogger (PerfLogger.java:PerfLogEnd(135)) - </PERFLOG method=Driver.execute start=1409824527954 end=1409824691182 duration=163228 from=org.apache.hadoop.hive.ql.Driver>\n2014-09-04 02:58:11,223 INFO  log.PerfLogger (PerfLogger.java:PerfLogBegin(108)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\n2014-09-04 02:58:11,224 INFO  log.PerfLogger (PerfLogger.java:PerfLogEnd(135)) - </PERFLOG method=releaseLocks start=1409824691223 end=1409824691224 duration=1 from=org.apache.hadoop.hive.ql.Driver>\n2014-09-04 02:58:11,306 ERROR operation.Operation (SQLOperation.java:run(199)) - Error running hive query: \norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:284)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:146)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:69)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:196)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:508)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:722)\n2014-09-04 02:58:11,634 INFO  exec.ListSinkOperator (Operator.java:close(580)) - 47 finished. closing... \n2014-09-04 02:58:11,683 INFO  exec.ListSinkOperator (Operator.java:close(598)) - 47 Close done\n2014-09-04 02:58:12,190 INFO  log.PerfLogger (PerfLogger.java:PerfLogBegin(108)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\n2014-09-04 02:58:12,234 INFO  log.PerfLogger (PerfLogger.java:PerfLogEnd(135)) - </PERFLOG method=releaseLocks start=1409824692190 end=1409824692191 duration=1 from=org.apache.hadoop.hive.ql.Driver>\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive on spark issue..","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Test Environment is..\n\n. hive 0.14.0(spark branch version)\n. spark (http://ec2-50-18-79-139.us-west-1.compute.amazonaws.com/data/spark-assembly-1.1.0-SNAPSHOT-hadoop2.3.0.jar)\n. hadoop 2.4.0 (yarn)","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14123196","id":"14123196","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~alton.jung] Thanks for reporting the problem. I tried a similar query with spark local mode using Hive CLI and it works. Do you use beeline connecting to a HiveServer2 instance? Could you also try Hive CLI to see if you can reproduce the same problem? Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-09-05T17:26:55.735+0000","updated":"2014-09-05T17:26:55.735+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14133703","id":"14133703","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for you reply.. unluckily i got the same error with hive2 beeline and hive cli..\nI want to get a confirmation about my compiling process from you.. \nIn my case, i met the compile error(check below) when i compiled with \"mvn clean install -Dhadoop-23.version=2.4.0  -Phadoop-2,dist\". So i added \"Dmaven.test.skip=true\" to avoid testcase..is it ok to add test.skip when building the source??\n\n\n[Error logs when i compiled using maven]\n             Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.892 sec - in org.apache.hadoop.hive.metastore.TestMetastoreExpr\n\nResults :\n\nFailed tests: \n  TestOrcSerDeStats.testStringAndBinaryStatistics:211 expected:<273> but was:<165>\n  TestOrcSerDeStats.testOrcSerDeStatsList:309 expected:<430000000> but was:<250000000>\n  TestOrcSerDeStats.testOrcSerDeStatsMap:341 expected:<950000> but was:<590000>\n  TestOrcSerDeStats.testOrcSerDeStatsSimpleWithNulls:373 expected:<44500> but was:<26500>\n  TestOrcSerDeStats.testOrcSerDeStatsComplex:416 expected:<1740> but was:<1104>\n  TestOrcSerDeStats.testOrcSerDeStatsComplexOldFormat:510 expected:<1740> but was:<1104>\n\nTests in error: \n  TestOrcSerDeStats.testSerdeStatsOldFormat »  Unexpected exception, expected<ja...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"created":"2014-09-15T08:38:26.664+0000","updated":"2014-09-15T08:38:26.664+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14133716","id":"14133716","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"body":"One more thing..could you share your prebuild version of hive on spark???","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"created":"2014-09-15T08:55:54.173+0000","updated":"2014-09-15T08:55:54.173+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14133794","id":"14133794","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"body":"Sorry i got confused..\nWhen i tested with hivecli then i met the below issue.\n\n\njava.lang.NoClassDefFoundError: org/apache/spark/api/java/JavaSparkContext\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkClient.<init>(SparkClient.java:73)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkClient.getInstance(SparkClient.java:61)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.submit(SparkSessionImpl.java:51)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:76)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:72)\nFAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. org/apache/spark/api/java/JavaSparkContext\n\n\ni think i have th classpath issue..\nThanks..","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"created":"2014-09-15T11:15:54.578+0000","updated":"2014-09-15T11:15:54.578+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14133801","id":"14133801","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"body":"Mr Xuefu Zhang..\n\nSorry to make you confused,, i succeeded with hivecli after i put the spark-assembly-1.1.0-xxx to hive library..\nMaybe i got a problem with class path..\nthe guide of hive on spark tells me that -- auxpath should be used for setting the spark library and i did it like this..\nbut i failed for one week, so i finally put the spark library to hive, then it succedded...\n\n\nAnyway sorry to bother you..","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"created":"2014-09-15T11:27:09.910+0000","updated":"2014-09-15T11:27:09.910+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14136931","id":"14136931","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"body":"MR Xuefu Zhang..\n\nI have a question about supporting hiveserver2 of spark..\ni tested with hiveserver2 without spark(master, worker) actived..\n\nI submited below commands on beeline to hiveserver2..\nBut it worked well... \nI thought below commands should have failed because i deactived spark master and worker..\nwhen i changed the environment to hiveserver..it worked as i expected ( it failed since i deactived master and worker of spark)\n\n\n[command in beeline]\nset hive.execution.engine=spark;\nset spark.master=spark://localhost.localdomain:7077;\nset spark.eventLog.enabled=true;\nset spark.executor.memory=256m;\nset spark.serializer=org.apache.spark.serializer.KryoSerializer;\nselect * from test where id=1 order by id;\n\n\nBest regards..","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"created":"2014-09-17T08:11:48.645+0000","updated":"2014-09-17T08:11:48.645+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14137302","id":"14137302","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~alton.jung] Thanks for reporting the problem. I'll find a developer to look at this issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-09-17T14:18:33.910+0000","updated":"2014-09-17T14:18:33.910+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14140164","id":"14140164","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for it..\n\nI got really confused about current version(hive on spark)..\nI succeeded with query through hive cli, but when i tested it with beeline or jdbc.. I always met error...\nI wonder current version can support query with jdbc and beeline..\n\n[Error]\njava.lang.NullPointerException\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:1262)\n\tat org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:1269)\n\tat org.apache.spark.SparkContext.hadoopRDD$default$5(SparkContext.scala:537)\n\tat org.apache.spark.api.java.JavaSparkContext.hadoopRDD(JavaSparkContext.scala:318)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateRDD(SparkPlanGenerator.java:160)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate(SparkPlanGenerator.java:88)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkClient.execute(SparkClient.java:156)\n\tat org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.submit(SparkSessionImpl.java:52)\n\tat org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:76)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alton.jung","name":"alton.jung","key":"alton.jung","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"alton.jung","active":true,"timeZone":"Etc/UTC"},"created":"2014-09-19T08:15:34.546+0000","updated":"2014-09-19T08:15:34.546+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14140778","id":"14140778","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~alton.jung] For hive, you need the latest from Spark branch. For Spark, you can also have the latest in their master branch. Since both are in the development, issues can arrive. Could you describe what you are trying to do and how to reproduce your issue(s)? Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-09-19T15:53:56.805+0000","updated":"2014-09-19T15:53:56.805+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14743041","id":"14743041","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lgh1","name":"lgh1","key":"lgh1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lgh","active":true,"timeZone":"Etc/UTC"},"body":"Hello,everyone:\n      I followed this guide(https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started).\n      I also meet this problem on hadoop2.6+spark1.3+hive1.2.1.see the error log.\n  \n\n15/09/14 14:20:25 INFO exec.Utilities: No plan file found: hdfs://bird-cluster/tmp/hive/hadoop/e4c32482-eb6e-445a-bfbe-71b3bb6cafcc/hive_2015-09-14_14-20-11_084_5577536928543129148-1/-mr-10003/7bf965f8-17cb-465f-86e1-3280fccb0f5f/map.xml\n15/09/14 14:20:25 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)\njava.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:236)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n15/09/14 14:20:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1\n15/09/14 14:20:25 INFO executor.Executor: Running task 0.1 in stage 0.0 (TID 1)\n15/09/14 14:20:25 INFO rdd.HadoopRDD: Input split: Paths:/user/hive/warehouse/temp.db/test/test:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lgh1","name":"lgh1","key":"lgh1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lgh","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-14T07:09:40.540+0000","updated":"2015-09-14T07:09:40.540+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14743772","id":"14743772","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~lgh1], thanks for reporting the problem. However, the stacktrace doesn't seem matching to the code. Are you sure you're on release-1.2.1?\n\nOn the other hand, it would be great if you can provide a repro case for the error.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-14T16:22:16.938+0000","updated":"2015-09-14T16:22:16.938+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14744676","id":"14744676","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lgh1","name":"lgh1","key":"lgh1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lgh","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for your replay. I am sure that I am using hive 1.2.1.\n\nThe blow is the hive script：\nhive> set spark.home=/usr/local/spark;\nhive> set hive.execution.engine=spark;\nhive> set spark.master=yarn;\nhive> set spark.eventLog.enabled=true;\nhive> set spark.eventLog.dir=hdfs://server1:9000/directory;\nhive> set spark.serializer=org.apache.spark.serializer.KryoSerializer;\nhive> select count(*) from test;\nQuery ID = hadoop_20150915094049_9a83ffe9-63b4-4847-b7f4-0e566f9f71d9\nTotal jobs = 1\nLaunching Job 1 out of 1\nIn order to change the average load for a reducer (in bytes):\n  set hive.exec.reducers.bytes.per.reducer=<number>\nIn order to limit the maximum number of reducers:\n  set hive.exec.reducers.max=<number>\nIn order to set a constant number of reducers:\n  set mapreduce.job.reduces=<number>\nStarting Spark Job = 0368cc5b-dd5d-4c7c-b48e-636d53ed350b\n\nQuery Hive on Spark job[0] stages:\n0\n1\n\nStatus: Running (Hive on Spark job[0])\nJob Progress Format\nCurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]\n2015-09-15 09:41:20,764\tStage-0_0: 0(+1)/1\tStage-1_0: 0/1\t\n2015-09-15 09:41:22,784\tStage-0_0: 0(+1,-1)/1\tStage-1_0: 0/1\t\nStatus: Failed\nFAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n\n\n\nThen，the blow is  the worker errlog：\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/data/slot0/yarn/data/usercache/hadoop/filecache/17/spark-assembly-1.3.1-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/letv/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n15/09/15 09:41:14 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]\n15/09/15 09:41:15 INFO spark.SecurityManager: Changing view acls to: hadoop\n15/09/15 09:41:15 INFO spark.SecurityManager: Changing modify acls to: hadoop\n15/09/15 09:41:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)\n15/09/15 09:41:16 INFO slf4j.Slf4jLogger: Slf4jLogger started\n15/09/15 09:41:16 INFO Remoting: Starting remoting\n15/09/15 09:41:16 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10-140-110-157:21269]\n15/09/15 09:41:16 INFO util.Utils: Successfully started service 'driverPropsFetcher' on port 21269.\n15/09/15 09:41:16 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n15/09/15 09:41:16 INFO spark.SecurityManager: Changing view acls to: hadoop\n15/09/15 09:41:16 INFO spark.SecurityManager: Changing modify acls to: hadoop\n15/09/15 09:41:16 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)\n15/09/15 09:41:16 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n15/09/15 09:41:16 INFO slf4j.Slf4jLogger: Slf4jLogger started\n15/09/15 09:41:16 INFO Remoting: Starting remoting\n15/09/15 09:41:16 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n15/09/15 09:41:16 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10-140-110-157:15895]\n15/09/15 09:41:16 INFO util.Utils: Successfully started service 'sparkExecutor' on port 15895.\n15/09/15 09:41:16 INFO util.AkkaUtils: Connecting to MapOutputTracker: akka.tcp://sparkDriver@server1:12682/user/MapOutputTracker\n15/09/15 09:41:16 INFO util.AkkaUtils: Connecting to BlockManagerMaster: akka.tcp://sparkDriver@server1:12682/user/BlockManagerMaster\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data12/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-8e6d671f-63c6-4269-a504-03ef36ae2b0f\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data11/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-c6159085-466f-4411-bf6e-224517f92b49\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data10/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-30dc8099-6827-4a53-8fd3-c150236e61da\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data1/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-3c4a6ca3-f84f-46f7-b365-7f51a605c044\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data4/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-c3c56c5b-e368-45bb-9a50-d660301d5939\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data5/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-f19e07ea-6614-4035-92bb-e89674506c68\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data2/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-c245e542-6bd1-470c-ae8d-2a3ec389b879\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data3/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-752fd55f-e15f-43cf-af75-a726bfb498d0\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data8/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-bbeae731-7a63-455d-b453-4c1915e878dc\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data9/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-7cf05ed3-aff3-455b-8db7-c7242eda8b97\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data6/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-8cb7c93b-c15a-465d-ba0a-3be5abe5c899\n15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data7/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-1d0087cd-0c35-4f67-8bdd-331c14235290\n15/09/15 09:41:17 INFO storage.MemoryStore: MemoryStore started with capacity 1060.3 MB\n15/09/15 09:41:17 INFO util.AkkaUtils: Connecting to OutputCommitCoordinator: akka.tcp://sparkDriver@server1:12682/user/OutputCommitCoordinator\n15/09/15 09:41:17 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@server1:12682/user/CoarseGrainedScheduler\n15/09/15 09:41:17 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n15/09/15 09:41:17 INFO executor.Executor: Starting executor ID 2 on host 10-140-110-157\n15/09/15 09:41:17 INFO netty.NettyBlockTransferService: Server created on 40604\n15/09/15 09:41:17 INFO storage.BlockManagerMaster: Trying to register BlockManager\n15/09/15 09:41:17 INFO storage.BlockManagerMaster: Registered BlockManager\n15/09/15 09:41:17 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@server1:12682/user/HeartbeatReceiver\n15/09/15 09:41:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\n15/09/15 09:41:20 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n15/09/15 09:41:20 INFO executor.Executor: Fetching http://10.140.130.38:54874/jars/hive-exec-1.2.1.jar with timestamp 1442281278254\n15/09/15 09:41:20 INFO util.Utils: Fetching http://10.140.130.38:54874/jars/hive-exec-1.2.1.jar to /data/hadoop/data12/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/fetchFileTemp6606399870470341414.tmp\n15/09/15 09:41:20 INFO util.Utils: Copying /data/hadoop/data12/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/12837195511442281278254_cache to /data/slot3/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/container_1441701008982_0038_01_000003/./hive-exec-1.2.1.jar\n15/09/15 09:41:21 INFO executor.Executor: Adding file:/data/slot3/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/container_1441701008982_0038_01_000003/./hive-exec-1.2.1.jar to class loader\n15/09/15 09:41:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1\n15/09/15 09:41:21 INFO storage.MemoryStore: ensureFreeSpace(50134) called with curMem=0, maxMem=1111794647\n15/09/15 09:41:21 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 49.0 KB, free 1060.2 MB)\n15/09/15 09:41:21 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0\n15/09/15 09:41:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 299 ms\n15/09/15 09:41:21 INFO storage.MemoryStore: ensureFreeSpace(163768) called with curMem=50134, maxMem=1111794647\n15/09/15 09:41:21 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 159.9 KB, free 1060.1 MB)\n15/09/15 09:41:21 INFO rdd.HadoopRDD: Input split: Paths:/user/hive/warehouse/temp.db/test/test:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n\n15/09/15 09:41:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0\n15/09/15 09:41:22 INFO storage.MemoryStore: ensureFreeSpace(46802) called with curMem=213902, maxMem=1111794647\n15/09/15 09:41:22 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 45.7 KB, free 1060.0 MB)\n15/09/15 09:41:22 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0\n15/09/15 09:41:22 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 187 ms\n15/09/15 09:41:22 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n15/09/15 09:41:22 INFO storage.MemoryStore: ensureFreeSpace(540492) called with curMem=260704, maxMem=1111794647\n15/09/15 09:41:22 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 527.8 KB, free 1059.5 MB)\n15/09/15 09:41:22 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n15/09/15 09:41:22 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n15/09/15 09:41:22 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n15/09/15 09:41:22 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n15/09/15 09:41:22 INFO exec.Utilities: No plan file found: hdfs://bird-cluster/tmp/hive/hadoop/23ba4f2d-1f4c-42d7-af83-1cfc9411344c/hive_2015-09-15_09-40-49_573_5616367127345554376-1/-mr-10003/f96f663b-b0ad-408c-8cda-534f9a49ea2e/map.xml\n15/09/15 09:41:22 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)\njava.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:236)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nAnd blow is hive error log：\n2015-09-15 09:41:20,110 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: </PERFLOG method=SparkBuildRDDGraph start=1442281280024 end=1442281280109 duration=85 from=org.apache.hadoop.hive.ql.exec.spark.SparkPlan>\n2015-09-15 09:41:20,127 INFO  [RPC-Handler-3]: client.SparkClientImpl (SparkClientImpl.java:handle(547)) - Received spark job ID: 0 for 0368cc5b-dd5d-4c7c-b48e-636d53ed350b\n2015-09-15 09:41:20,134 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: <PERFLOG method=getSplits from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>\n2015-09-15 09:41:20,134 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: PLAN PATH = hdfs://bird-cluster/tmp/hive/hadoop/23ba4f2d-1f4c-42d7-af83-1cfc9411344c/hive_2015-09-15_09-40-49_573_5616367127345554376-1/-mr-10003/f96f663b-b0ad-408c-8cda-534f9a49ea2e/map.xml\n2015-09-15 09:41:20,135 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: ***************non-local mode***************\n2015-09-15 09:41:20,135 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: local path = hdfs://bird-cluster/tmp/hive/hadoop/23ba4f2d-1f4c-42d7-af83-1cfc9411344c/hive_2015-09-15_09-40-49_573_5616367127345554376-1/-mr-10003/f96f663b-b0ad-408c-8cda-534f9a49ea2e/map.xml\n2015-09-15 09:41:20,135 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: Open file to read in plan: hdfs://bird-cluster/tmp/hive/hadoop/23ba4f2d-1f4c-42d7-af83-1cfc9411344c/hive_2015-09-15_09-40-49_573_5616367127345554376-1/-mr-10003/f96f663b-b0ad-408c-8cda-534f9a49ea2e/map.xml\n2015-09-15 09:41:20,143 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: <PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>\n2015-09-15 09:41:20,144 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: Deserializing MapWork via kryo\n2015-09-15 09:41:20,182 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: </PERFLOG method=deserializePlan start=1442281280143 end=1442281280181 duration=38 from=org.apache.hadoop.hive.ql.exec.Utilities>\n2015-09-15 09:41:20,182 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO io.CombineHiveInputFormat: Total number of paths: 1, launching 1 threads to check non-combinable ones.\n2015-09-15 09:41:20,198 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries\n2015-09-15 09:41:20,201 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 826e7d8d3e839964dd9ed2d5f83296254b2c71d3]\n2015-09-15 09:41:20,355 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO io.CombineHiveInputFormat: CombineHiveInputSplit creating pool for hdfs://bird-cluster/user/hive/warehouse/temp.db/test; using filter path hdfs://bird-cluster/user/hive/warehouse/temp.db/test\n2015-09-15 09:41:20,379 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO input.FileInputFormat: Total input paths to process : 1\n2015-09-15 09:41:20,399 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n2015-09-15 09:41:20,400 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO io.CombineHiveInputFormat: number of splits 1\n2015-09-15 09:41:20,401 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO io.CombineHiveInputFormat: Number of all splits 1\n2015-09-15 09:41:20,401 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: </PERFLOG method=getSplits start=1442281280133 end=1442281280401 duration=268 from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>\n2015-09-15 09:41:20,417 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Registering RDD 1 (mapPartitionsToPair at MapTran.java:31)\n2015-09-15 09:41:20,420 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Got job 0 (foreachAsync at RemoteHiveSparkClient.java:257) with 1 output partitions (allowLocal=false)\n2015-09-15 09:41:20,421 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Final stage: Stage 1(foreachAsync at RemoteHiveSparkClient.java:257)\n2015-09-15 09:41:20,422 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 0)\n2015-09-15 09:41:20,429 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Missing parents: List(Stage 0)\n2015-09-15 09:41:20,440 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[1] at mapPartitionsToPair at MapTran.java:31), which has no missing parents\n2015-09-15 09:41:20,467 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.MemoryStore: ensureFreeSpace(163768) called with curMem=587318, maxMem=278302556\n2015-09-15 09:41:20,468 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 159.9 KB, free 264.7 MB)\n2015-09-15 09:41:20,490 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.MemoryStore: ensureFreeSpace(50134) called with curMem=751086, maxMem=278302556\n2015-09-15 09:41:20,490 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 49.0 KB, free 264.6 MB)\n2015-09-15 09:41:20,491 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on server1:37766 (size: 49.0 KB, free: 265.3 MB)\n2015-09-15 09:41:20,492 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0\n2015-09-15 09:41:20,493 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839\n2015-09-15 09:41:20,504 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[1] at mapPartitionsToPair at MapTran.java:31)\n2015-09-15 09:41:20,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\n2015-09-15 09:41:20,552 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10-140-110-157, RACK_LOCAL, 1515 bytes)\n2015-09-15 09:41:20,760 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=SparkSubmitToRunning start=1442281254723 end=1442281280760 duration=26037 from=org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor>\n2015-09-15 09:41:20,760 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - \nQuery Hive on Spark job[0] stages:\n2015-09-15 09:41:20,763 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 0\n2015-09-15 09:41:20,763 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 1\n2015-09-15 09:41:20,763 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - \nStatus: Running (Hive on Spark job[0])\n2015-09-15 09:41:20,763 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - Job Progress Format\nCurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]\n2015-09-15 09:41:20,764 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=SparkRunStage.0_0 from=org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor>\n2015-09-15 09:41:20,765 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 2015-09-15 09:41:20,764\tStage-0_0: 0(+1)/1\tStage-1_0: 0/1\t\n2015-09-15 09:41:21,432 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:21 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10-140-110-157:40604 (size: 49.0 KB, free: 1060.2 MB)\n2015-09-15 09:41:22,179 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:22 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10-140-110-157:40604 (size: 45.7 KB, free: 1060.2 MB)\n2015-09-15 09:41:22,612 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:22 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 10-140-110-157): java.lang.NullPointerException\n2015-09-15 09:41:22,612 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)\n2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)\n2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)\n2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)\n2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:236)\n2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)\n2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.lang.Thread.run(Thread.java:745)\n2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \n2015-09-15 09:41:22,618 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:22 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, 10-140-110-155, RACK_LOCAL, 1515 bytes)\n2015-09-15 09:41:22,784 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 2015-09-15 09:41:22,784\tStage-0_0: 0(+1,-1)/1\tStage-1_0: 0/1\t\n2015-09-15 09:41:23,436 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:23 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10-140-110-155:42408 (size: 49.0 KB, free: 1060.2 MB)\n2015-09-15 09:41:23,954 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:23 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10-140-110-155:42408 (size: 45.7 KB, free: 1060.2 MB)\n2015-09-15 09:41:24,377 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on executor 10-140-110-155: java.lang.NullPointerException (null) [duplicate 1]\n2015-09-15 09:41:24,380 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, 10-140-110-155, RACK_LOCAL, 1515 bytes)\n2015-09-15 09:41:24,426 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on executor 10-140-110-155: java.lang.NullPointerException (null) [duplicate 2]\n2015-09-15 09:41:24,428 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, 10-140-110-155, RACK_LOCAL, 1515 bytes)\n2015-09-15 09:41:24,467 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on executor 10-140-110-155: java.lang.NullPointerException (null) [duplicate 3]\n2015-09-15 09:41:24,468 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n2015-09-15 09:41:24,471 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n2015-09-15 09:41:24,475 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO cluster.YarnScheduler: Cancelling stage 0\n2015-09-15 09:41:24,477 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.DAGScheduler: Stage 0 (mapPartitionsToPair at MapTran.java:31) failed in 3.945 s\n2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO client.RemoteDriver: Failed to run job 0368cc5b-dd5d-4c7c-b48e-636d53ed350b\n2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - java.util.concurrent.ExecutionException: Exception thrown by job\n2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:311)\n2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:316)\n2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:382)\n2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:335)\n2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.lang.Thread.run(Thread.java:745)\n2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10-140-110-155): java.lang.NullPointerException\n2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)\n2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)\n2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)\n2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)\n2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:236)\n2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)\n2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat java.lang.Thread.run(Thread.java:745)\n2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \n2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - Driver stacktrace:\n2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n2015-09-15 09:41:24,511 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n2015-09-15 09:41:24,511 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n2015-09-15 09:41:24,511 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n2015-09-15 09:41:24,514 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n2015-09-15 09:41:24,514 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n2015-09-15 09:41:24,514 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n2015-09-15 09:41:24,514 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat scala.Option.foreach(Option.scala:236)\n2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n2015-09-15 09:41:24,540 INFO  [RPC-Handler-3]: client.SparkClientImpl (SparkClientImpl.java:handle(522)) - Received result for 0368cc5b-dd5d-4c7c-b48e-636d53ed350b\n2015-09-15 09:41:24,795 ERROR [main]: status.SparkJobMonitor (SessionState.java:printError(960)) - Status: Failed\n2015-09-15 09:41:24,795 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=SparkRunJob start=1442281254723 end=1442281284795 duration=30072 from=org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor>\n2015-09-15 09:41:24,817 ERROR [main]: ql.Driver (SessionState.java:printError(960)) - FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask\n2015-09-15 09:41:24,817 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=Driver.execute start=1442281249766 end=1442281284817 duration=35051 from=org.apache.hadoop.hive.ql.Driver>\n2015-09-15 09:41:24,817 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\n2015-09-15 09:41:24,817 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=releaseLocks start=1442281284817 end=1442281284817 duration=0 from=org.apache.hadoop.hive.ql.Driver>\n2015-09-15 09:41:24,819 INFO  [main]: exec.ListSinkOperator (Operator.java:close(612)) - 7 finished. closing... \n2015-09-15 09:41:24,819 INFO  [main]: exec.ListSinkOperator (Operator.java:close(634)) - 7 Close done\n2015-09-15 09:41:24,826 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\n2015-09-15 09:41:24,826 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=releaseLocks start=1442281284826 end=1442281284826 duration=0 from=org.apache.hadoop.hive.ql.Driver>\n\n\n\n\n\n\nThanks again.It seems that the spark job is already successfully running on the yarn from hive,but the explain job map.xml is lost .\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lgh1","name":"lgh1","key":"lgh1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lgh","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-15T01:48:50.462+0000","updated":"2015-09-15T01:48:50.462+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14744885","id":"14744885","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lgh1","name":"lgh1","key":"lgh1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lgh","active":true,"timeZone":"Etc/UTC"},"body":"e,aha。。\n   \n   now it works after I recompile the spark 1.3.1  using the following command\n    \n./make-distribution.sh --name \"hadoop2-without-hive\" --tgz \"-Pyarn,hadoop-provided,hadoop-2.4\" -Dhadoop.version=2.6.0 -Dyarn.version=2.6.0 -DskipTests\n   ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lgh1","name":"lgh1","key":"lgh1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lgh","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-15T06:00:30.412+0000","updated":"2015-09-15T06:00:30.412+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/14744886","id":"14744886","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lgh1","name":"lgh1","key":"lgh1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lgh","active":true,"timeZone":"Etc/UTC"},"body":"e,aha。。\n   \n   now it works after I recompile the spark 1.3.1  using the following command\n    \n./make-distribution.sh --name \"hadoop2-without-hive\" --tgz \"-Pyarn,hadoop-provided,hadoop-2.4\" -Dhadoop.version=2.6.0 -Dyarn.version=2.6.0 -DskipTests\n   ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lgh1","name":"lgh1","key":"lgh1","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lgh","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-15T06:00:38.197+0000","updated":"2015-09-15T06:00:38.197+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12739095/comment/15112005","id":"15112005","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jainsourabh2","name":"jainsourabh2","key":"jainsourabh2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sourabh Jain","active":true,"timeZone":"Asia/Kolkata"},"body":"I am facing simiar issue but when I try to build spark 1.3.3 version from the above command , I am facing below error:\n\nhttps://issues.apache.org/jira/browse/SPARK-10944\n\nPlease help I am missing anything.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jainsourabh2","name":"jainsourabh2","key":"jainsourabh2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sourabh Jain","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-22T06:33:39.359+0000","updated":"2016-01-22T06:33:39.359+0000"}],"maxResults":15,"total":15,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-7980/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1zoev:"}}