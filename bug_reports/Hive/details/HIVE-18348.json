{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13127767","self":"https://issues.apache.org/jira/rest/api/2/issue/13127767","key":"HIVE-18348","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2018-01-02T19:00:39.269+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jan 02 19:10:36 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-18348/watchers","watchCount":6,"isWatching":false},"created":"2017-12-30T19:03:14.072+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340268","id":"12340268","name":"3.0.0","archived":false,"released":true,"releaseDate":"2018-05-21"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-01-02T19:10:36.088+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312584","id":"12312584","name":"Metastore","description":"Tracks issue dealing with metastore."}],"timeoriginalestimate":null,"description":"Enabling debug logging with HikariCP, I can see that Hive creates 4 connection pools. {code:title=first connection pool creation stack trace} \"main@1\" prio=5 tid=0x1 nid=NA runnable java.lang.Thread.State: RUNNABLE at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:73) at org.datanucleus.store.rdbms.connectionpool.HikariCPConnectionPoolFactory.createConnectionPool(HikariCPConnectionPoolFactory.java:176) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117) - locked <0x102b> (a org.datanucleus.store.rdbms.ConnectionFactoryImpl) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(NativeConstructorAccessorImpl.java:-1) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330) at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203) at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162) at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(NativeConstructorAccessorImpl.java:-1) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133) at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422) - locked <0x1035> (a org.datanucleus.PersistenceNucleusContextImpl) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817) - locked <0x1036> (a org.datanucleus.api.jdo.JDOPersistenceManagerFactory) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213) - locked <0xeb8> (a java.lang.Class) at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(AccessController.java:-1) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:619) - locked <0x957> (a java.lang.Class) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:662) at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:452) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:389) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:344) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:59) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:668) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:647) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:641) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:695) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:491) - locked <0xe59> (a java.lang.Class) at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:80) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:93) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:7518) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:161) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:79) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(NativeConstructorAccessorImpl.java:-1) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.hive.metastore.utils.JavaUtils.newInstance(JavaUtils.java:84) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:95) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:148) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:119) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4100) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4152) - locked <0xe92> (a org.apache.hadoop.hive.ql.metadata.Hive) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4132) at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4394) at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:267) at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:250) at org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:407) at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:351) at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:331) at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:307) at org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:942) at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:906) at org.apache.hadoop.hive.ql.session.SessionState.applyAuthorizationPolicy(SessionState.java:1856) at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:131) at org.apache.hive.service.cli.CLIService.init(CLIService.java:115) - locked <0xe94> (a org.apache.hive.service.cli.CLIService) at org.apache.hive.service.CompositeService.init(CompositeService.java:59) - locked <0xe95> (a org.apache.hive.service.server.HiveServer2) at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:167) at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:790) at org.apache.hive.service.server.HiveServer2.access$900(HiveServer2.java:110) at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1034) at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:903) at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:234) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) {code} {code:title=second connection pool creation stack trace} \"main@1\" prio=5 tid=0x1 nid=NA runnable java.lang.Thread.State: RUNNABLE at com.zaxxer.hikari.HikariDataSource.<init>(HikariDataSource.java:73) at org.datanucleus.store.rdbms.connectionpool.HikariCPConnectionPoolFactory.createConnectionPool(HikariCPConnectionPoolFactory.java:176) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:134) - locked <0x105e> (a org.datanucleus.store.rdbms.ConnectionFactoryImpl) at org.datanucleus.store.rdbms.ConnectionFactoryImpl.createManagedConnection(ConnectionFactoryImpl.java:249) at org.datanucleus.store.connection.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:302) at org.datanucleus.store.connection.AbstractConnectionFactory.getConnection(AbstractConnectionFactory.java:84) at org.datanucleus.store.AbstractStoreManager.getConnection(AbstractStoreManager.java:378) at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:296) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(NativeConstructorAccessorImpl.java:-1) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606) at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301) at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133) at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422) - locked <0x1035> (a org.datanucleus.PersistenceNucleusContextImpl) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817) - locked <0x1036> (a org.datanucleus.api.jdo.JDOPersistenceManagerFactory) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334) at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213) - locked <0xeb8> (a java.lang.Class) at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965) at java.security.AccessController.doPrivileged(AccessController.java:-1) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:619) - locked <0x957> (a java.lang.Class) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:662) at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:452) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:389) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:344) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:59) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:668) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:647) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:641) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:695) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:491) - locked <0xe59> (a java.lang.Class) at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:80) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:93) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:7518) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:161) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:79) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(NativeConstructorAccessorImpl.java:-1) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.hive.metastore.utils.JavaUtils.newInstance(JavaUtils.java:84) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:95) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:148) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:119) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4100) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4152) - locked <0xe92> (a org.apache.hadoop.hive.ql.metadata.Hive) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4132) at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4394) at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:267) at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:250) at org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:407) at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:351) at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:331) at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:307) at org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:942) at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:906) at org.apache.hadoop.hive.ql.session.SessionState.applyAuthorizationPolicy(SessionState.java:1856) at org.apache.hive.service.cli.CLIService.applyAuthorizationConfigPolicy(CLIService.java:131) at org.apache.hive.service.cli.CLIService.init(CLIService.java:115) - locked <0xe94> (a org.apache.hive.service.cli.CLIService) at org.apache.hive.service.CompositeService.init(CompositeService.java:59) - locked <0xe95> (a org.apache.hive.service.server.HiveServer2) at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:167) at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:790) at org.apache.hive.service.server.HiveServer2.access$900(HiveServer2.java:110) at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1034) at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:903) at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:234) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) \r\n{code} \r\n2 other instances of connection pool are explicitly created (connPool and connPoolMutex) in TxnHandler https://github.com/apache/hive/blob/035eca39fc0bd7aa0d9c1809a26e000ac52978d0/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java#L256-L264 \r\n\r\nIf datanucleus.connectionPool.maxPoolSize is set to 10, then hive will create 47 (connPoolMutex creates 7 more than maxPoolSize) connections per HS2 and HMS instance. This blows up the number of connections on DB side. Ideally, we want a single shared connection pool for ObjectStore and TxnHandler so that it is easily tuneable.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340268","id":"12340268","name":"3.0.0","archived":false,"released":true,"releaseDate":"2018-05-21"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive creates 4 different connection pools to metastore of different size","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prasanth_j","name":"prasanth_j","key":"prasanth_j","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanth Jayachandran","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prasanth_j","name":"prasanth_j","key":"prasanth_j","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanth Jayachandran","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127767/comment/16308513","id":"16308513","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"body":"TxnHandler needs to use 2 pools otherwise it can deadlock.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-01-02T19:00:39.269+0000","updated":"2018-01-02T19:00:39.269+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127767/comment/16308525","id":"16308525","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prasanth_j","name":"prasanth_j","key":"prasanth_j","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanth Jayachandran","active":true,"timeZone":"America/Los_Angeles"},"body":"[~ekoifman] Does it require 2 separate pools or guaranteed connection for cleaner and other threads? How is it different from having 2 separate pools vs single big pool of 2x size?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prasanth_j","name":"prasanth_j","key":"prasanth_j","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanth Jayachandran","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-01-02T19:06:15.125+0000","updated":"2018-01-02T19:06:15.125+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127767/comment/16308532","id":"16308532","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"body":"It does require 2 pools.\r\nconnPoolMutex is used to manage locking (as in multiple metastores trying to perform the same op) via RDBMS.\r\nThis has to be done in a separate pool otherwise if you have many concurrent ops you may end up with the 1st pool being maxed out but and so the all operation get blocked because they need another connection to acquire the mutex.  HIVE-16321 has more details. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-01-02T19:10:36.088+0000","updated":"2018-01-02T19:10:36.088+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-18348/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3odvb:"}}