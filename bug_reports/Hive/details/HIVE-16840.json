{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13077867","self":"https://issues.apache.org/jira/rest/api/2/issue/13077867","key":"HIVE-16840","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2017-06-07T19:02:17.968+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Jul 03 03:30:36 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_2247174888_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2017-07-03T03:30:53.503+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-16840/watchers","watchCount":7,"isWatching":false},"created":"2017-06-07T03:17:58.659+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-03T03:30:53.543+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"We found that on 1TB data of TPC-DS, q17 of TPC-DS hanged.\n{code}\n select  i_item_id\n       ,i_item_desc\n       ,s_state\n       ,count(ss_quantity) as store_sales_quantitycount\n       ,avg(ss_quantity) as store_sales_quantityave\n       ,stddev_samp(ss_quantity) as store_sales_quantitystdev\n       ,stddev_samp(ss_quantity)/avg(ss_quantity) as store_sales_quantitycov\n       ,count(sr_return_quantity) as_store_returns_quantitycount\n       ,avg(sr_return_quantity) as_store_returns_quantityave\n       ,stddev_samp(sr_return_quantity) as_store_returns_quantitystdev\n       ,stddev_samp(sr_return_quantity)/avg(sr_return_quantity) as store_returns_quantitycov\n       ,count(cs_quantity) as catalog_sales_quantitycount ,avg(cs_quantity) as catalog_sales_quantityave\n       ,stddev_samp(cs_quantity)/avg(cs_quantity) as catalog_sales_quantitystdev\n       ,stddev_samp(cs_quantity)/avg(cs_quantity) as catalog_sales_quantitycov\n from store_sales\n     ,store_returns\n     ,catalog_sales\n     ,date_dim d1\n     ,date_dim d2\n     ,date_dim d3\n     ,store\n     ,item\n where d1.d_quarter_name = '2000Q1'\n   and d1.d_date_sk = store_sales.ss_sold_date_sk\n   and item.i_item_sk = store_sales.ss_item_sk\n   and store.s_store_sk = store_sales.ss_store_sk\n   and store_sales.ss_customer_sk = store_returns.sr_customer_sk\n   and store_sales.ss_item_sk = store_returns.sr_item_sk\n   and store_sales.ss_ticket_number = store_returns.sr_ticket_number\n   and store_returns.sr_returned_date_sk = d2.d_date_sk\n   and d2.d_quarter_name in ('2000Q1','2000Q2','2000Q3')\n   and store_returns.sr_customer_sk = catalog_sales.cs_bill_customer_sk\n   and store_returns.sr_item_sk = catalog_sales.cs_item_sk\n   and catalog_sales.cs_sold_date_sk = d3.d_date_sk\n   and d3.d_quarter_name in ('2000Q1','2000Q2','2000Q3')\n group by i_item_id\n         ,i_item_desc\n         ,s_state\n order by i_item_id\n         ,i_item_desc\n         ,s_state\nlimit 100;\n{code}\n\nthe reason why the script hanged is because we only use 1 task to implement sort.\n{code}\nSTAGE PLANS:\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 10 <- Reducer 9 (SORT, 1)\n        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 889), Map 11 (PARTITION-LEVEL SORT, 889)\n        Reducer 3 <- Map 12 (PARTITION-LEVEL SORT, 1009), Reducer 2 (PARTITION-LEVEL SORT, 1009)\n        Reducer 4 <- Map 13 (PARTITION-LEVEL SORT, 683), Reducer 3 (PARTITION-LEVEL SORT, 683)\n        Reducer 5 <- Map 14 (PARTITION-LEVEL SORT, 751), Reducer 4 (PARTITION-LEVEL SORT, 751)\n        Reducer 6 <- Map 15 (PARTITION-LEVEL SORT, 826), Reducer 5 (PARTITION-LEVEL SORT, 826)\n        Reducer 7 <- Map 16 (PARTITION-LEVEL SORT, 909), Reducer 6 (PARTITION-LEVEL SORT, 909)\n        Reducer 8 <- Map 17 (PARTITION-LEVEL SORT, 1001), Reducer 7 (PARTITION-LEVEL SORT, 1001)\n        Reducer 9 <- Reducer 8 (GROUP, 2)\n{code}\n\nThe parallelism of Reducer 9 is 1. It is a orderby limit case so we use 1 task to execute to ensure the correctness. But the performance is poor.\nthe reason why we use 1 task to implement order by limit is [here|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java#L207]","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12873750","id":"12873750","filename":"HIVE-16840.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-20T22:40:35.274+0000","size":5654,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12873750/HIVE-16840.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Investigate the performance of order by limit in HoS","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16040150","id":"16040150","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz],[~lirui],[~Ferd], [~csun]:\n Here provide 2 solutions to solve it\n1. add an extra reduce to save the result of order and a new job to finish select * from (tmp result of order) limit N.\n2. create SortByLimitShuffler like [SortByShuffle|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java], and implement order by limit N like following \n{code}\n   val composite1 = sc.parallelize(1 to 200, 10).map(p=>(1-p,p)).sortByKey().zipWithIndex().filter{case (_, idx) => idx < 5}\n\n{code}\nsortByKey+zipWithIndex+filter to implement orderByLimit. if we use this way, we may need remove limit operator from reduce tree.\nComparing option1 and option2:\nthe disadvantage of optioin1 is we create an extra job to load the result of sort to do limit . But the sort is executed in parallel. The time of saving result of sort and loading result of sort is time consuming.\nthe disadvantage of option2 is {{zipWithIndex}} [triggers|https://spark.apache.org/docs/2.0.1/api/java/org/apache/spark/rdd/RDD.html#zipWithIndex()] extra spark job to do when RDD contains more than one partitions. This will cause time consuming.\n\nAppreciate to get some suggestions from you. If you have any idea about it, please tell me.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-07T04:33:51.664+0000","updated":"2017-06-07T05:09:16.448+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16041432","id":"16041432","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kellyzly], thanks for looking into this. I'm not sure if either of your proposals works. sortByKey() only sorting keys within partition. Thus, the moment you have parallel sortByKey(), the data is not globally sorted. Along that line, I'm not sure if either your solution will give you the expected result.\n\nBoth Spark and Hive has optimizations to sample the key, partition keys by range, and then sort within each partition, which gives you a global order. You can probably take a look at those to see if they helps.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-07T19:02:17.968+0000","updated":"2017-06-07T19:02:17.968+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16041679","id":"16041679","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]: \nbq. sortByKey() only sorting keys within partition. \nI have not found this in spark document. in [SortByShuffle|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java#L51],  sort in total order is also implemented by sortByKey spark api.  If i am wrong, please tell me.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-07T21:40:33.528+0000","updated":"2017-06-07T21:40:33.528+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16041829","id":"16041829","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kellyzly], I think you're right and I was confused: sortByKey does produce global order. The reason for that you gave in the description is also accurate.\n\nLooking back at your proposals, #1 is similar to select in a subquery that outputs ordered data. It depends on SELECT following FETCH FIRST semantics. I'm not sure if this is reliable.\n\nThe proposal #2 seems more plausible except this zipWithIndex(), which could be expensive.\n\nMaybe we can something like this: First, we do parallel sort (with N partitions) but filter out rows other than first M (M is the limit), followed by another sort (with 1 partition) with limit of M. This way, one task will sort only MxN rows, which should be fast if both MxN is small. Basically we will do this:\n{code}\nval composite1 = sc.parallelize(1 to 200, 10).map(p=>(1-p,p)).sortByKey(N).filter(first M).sortByKey(1).take(M)\n{code}\nCould you please check if this is possible?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-07T22:57:03.762+0000","updated":"2017-06-08T03:52:03.768+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16042120","id":"16042120","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"In theory, RDD computation is done lazily and we're only retrieving the first 100 records. For sortByKey, the output of upstream tasks are already sorted by key. So our single reducer should be doing a multi-way merge sort and it can stop once we get 100 records. That was why I thought it's better to handle orderBy+limit with 1 reducer.\nHowever, looking at the shuffling code, it seems the computation is not performed lazily:\nhttps://github.com/apache/spark/blob/v2.0.0/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala#L105\nhttps://github.com/apache/spark/blob/v2.0.0/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala#L180\nTherefore the reducer needs to fetch all the shuffled data.\n\nI wonder whether anything can be done on Spark side. Guess we can at least do some investigation.\n\nAs to the two proposals here, I prefer #2. IMO #1 may not even work, because once the sorted result set is stored into temp files, we can't rely on a {{select * limit N}} to retrieve the top N records.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-08T03:03:33.641+0000","updated":"2017-06-08T03:03:33.641+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16043507","id":"16043507","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz] and [~lirui]:  thanks for your suggestions. will try [~xuefuz]'s suggestion.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-08T21:48:56.659+0000","updated":"2017-06-08T21:49:31.023+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16043951","id":"16043951","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]:  I found that RDD has provided [order by limit|https://stackoverflow.com/questions/32947978/how-to-sort-an-rdd-and-limit-in-spark] implementation in spark.\nAnd solve order by limit by which methods according to the driver memory\n{code}\n import java.sql.Date\ncase class Foo(name: String, createDate: java.sql.Date)\nUsing plain RDDs:\nimport org.apache.spark.rdd.RDD\nimport scala.math.Ordering\n\nval rdd: RDD[Foo] = sc\n  .parallelize(Seq(\n    (\"a\", \"2015-01-03\"), (\"b\", \"2014-11-04\"), (\"a\", \"2016-08-10\"),\n    (\"a\", \"2013-11-11\"), (\"a\", \"2015-06-19\"), (\"a\", \"2009-11-23\")))\n  .toDF(\"name\", \"createDate\")\n  .withColumn(\"createDate\", $\"createDate\".cast(\"date\"))\n  .as[Foo].rdd\n\n{code}\n1. data fits into driver memory\n1.1  fraction you want is relatively small, here [RDD#takeOrdered|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1416] is implemented by same algorithm you mentioned above\n{code}\nrdd.takeOrdered(n)(Ordering.by[Foo, Long](_.createDate.getTime))\n{code}\n1.2 fraction you want is relatively large:\n{code}\nrdd.sortBy(_.createDate.getTime).take(n)\n{code}\n\n2. data can not fit into driver memory\n{code}\nrdd\n  .sortBy(_.createDate.getTime)\n  .zipWithIndex\n  .filter{case (_, idx) => idx < n}\n  .keys\n{code}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-09T05:19:28.297+0000","updated":"2017-06-09T05:19:28.297+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16043964","id":"16043964","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kellyzly], Thanks for sharing the info. This might not fit into HoS's design. I reused your example just to demo the idea. To be more specifically, I was thinking that for a query like select * from T order by id limit 10, instead of generating a Spark plan like this:\n{code}\nSTAGE PLANS:\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 2 <- Map 1 (SORT, 1)\n{code}\nwe generate a plan like this:\n{code}\nSTAGE PLANS:\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 2 <- Map 1 (SORT, 100)\n        Reducer 3 <- Reducer 2 (SORT, 1)\n{code}\nIn {{Reducer2}} and {{Reducer3}} there is an limit operator with 10 as the limit. In essence, we introduce one additional shuffle ({{SORT 1}}) to sort, but only on 1000 rows of data.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-09T05:35:51.983+0000","updated":"2017-06-09T05:37:50.731+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16043978","id":"16043978","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]: in current code base. select * from T order by id limit 10 is \n{code}\nSTAGE PLANS:\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 2 <- Map 1 (SORT, 1)\n{code}\nLimit is in Reduce2\n\nwhat i want is adding SortByLimitShuffle to deal with orderby+limit case. the spark plan will be, limit will be removed from  Reducer2.\n{code}\nSTAGE PLANS:\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 2 <- Map 1 (SORT, 100)\n{code}\nthe reason why there is only Reducer2, not Reducer2 and Reduce3 is because we implement order by limit in SortByLimitShuffler.java.\nSortByLimitShuffler#shuffle\n{code}\n public JavaPairRDD<HiveKey, BytesWritable> shuffle(\n      JavaPairRDD<HiveKey, BytesWritable> input, int numPartitions) {\n    JavaPairRDD<HiveKey, BytesWritable> rdd;\n    // implement orderby limit by  RDD#takeOrdered(n) or RDD.sortByKey.take(n) or other ways\n    ...\n    return rdd;\n  }\n{code}\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-09T05:55:51.333+0000","updated":"2017-06-09T05:55:51.333+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16043985","id":"16043985","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kellyzly], I believe take(n) and takeOrdered(n) is driver side actions instead of transformations. They don't return a RDD instance, so you probably has no way to return an rdd from the shuffle() implementation.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-09T06:04:46.505+0000","updated":"2017-06-09T06:04:46.505+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16043998","id":"16043998","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]: maybe we can make the rdd by following:\n{code}\n// the return value of takeOrdered(N) is Array[T]\n    val composite1 = sc.parallelize(1 to 200,partitionNum).map(p=>(1-p,p)).takeOrdered(firstN)\n\n   val composite2 = SparkContext.getOrCreate.makeRDD(composite1)\n{code}\n\nMaybe this is not very good.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-09T06:13:42.476+0000","updated":"2017-06-09T06:13:42.476+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16044007","id":"16044007","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"This might not fly. SparkContext is only available in driver. In the shuffle() implementation, we can only use RDD transformations.\nYou might take a look to see if it's possible to modify the plan as I suggested above. That's more aligning with the way HoS works.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-09T06:22:42.423+0000","updated":"2017-06-09T06:22:42.423+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16044036","id":"16044036","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]:  \nbq. You might take a look to see if it's possible to modify the plan as I suggested above. That's more aligning with the way HoS works.\nwill try to modify the plan.\n1 thing i need to confirm is that we need not ensure the global sort in Map1(SORT, 100), only ensure the elements in each partition are ordered.\n{code}\nSTAGE PLANS:\n  Stage: Stage-1\n    Spark\n      Edges:\n        Reducer 2 <- Map 1 (SORT, 100)\n        Reducer 3 <- Reducer 2 (SORT, 1)\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-09T06:41:39.692+0000","updated":"2017-06-09T06:41:39.692+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16044058","id":"16044058","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Re: only ensure the elements in each partition are ordered.\n\nI think you cannot use partition-level sort in lieu of a global sort unless you use a range partitioner, in which case i guess you basically get the same global sort.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-09T07:05:54.358+0000","updated":"2017-06-09T07:05:54.358+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16046345","id":"16046345","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz]:  \nbq.I think you cannot use partition-level sort in lieu of a global sort unless you use a range partitioner, in which case i guess you basically get the same global sort.\nwhat made me confused is why the shuffle from Map1 to Reducer2 need global sort ï¼ŸIn my opinion, we only need grantee the element is sorted in each partition(M partitions) and get the first N then sort M*N globally in 1 task and finally take N.\n\nin SortByShuffle.java, we use repartitionAndSortWithinPartitions to implement the sort in each partition.\n{code}\n  @Override\n  public JavaPairRDD<HiveKey, BytesWritable> shuffle(\n      JavaPairRDD<HiveKey, BytesWritable> input, int numPartitions) {\n    JavaPairRDD<HiveKey, BytesWritable> rdd;\n    if (totalOrder) {\n      if (numPartitions > 0) {\n        if (numPartitions > 1 && input.getStorageLevel() == StorageLevel.NONE()) {\n          input.persist(StorageLevel.DISK_ONLY());\n          sparkPlan.addCachedRDDId(input.id());\n        }\n        rdd = input.sortByKey(true, numPartitions);\n      } else {\n        rdd = input.sortByKey(true);\n      }\n    } else {\n      Partitioner partitioner = new HashPartitioner(numPartitions);\n      rdd = input.repartitionAndSortWithinPartitions(partitioner);\n    }\n    return rdd;\n  }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-12T08:51:15.267+0000","updated":"2017-06-12T08:51:15.267+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16047194","id":"16047194","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~kellyzly], sorry for the confusion. I think you're right. If the final N keys will be definitely appear in the first N keys in each partition. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-12T23:08:56.014+0000","updated":"2017-06-12T23:08:56.014+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16056593","id":"16056593","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~xuefuz],[~lirui],[~Ferd],[~csun]:  attached is HIVE-16840.1.patch.\nChanges\n1.  change physical plan at SetSparkReducerParallelism#process.  If needSetSparkReucerParallelism return false,this stands for current sink maybe an order by limit case. Add newSparkSortRS(actually its type is ReduceSink), newSel(actually its type is Sel),newLimit(actually its type is Limit) before sink.\noriginal physical plan is \n{code} ...-RS-SEL-LIMIT....{code}\nnow physical plan is \n{code} ...-newSparkSortRS-newSel-newLimit-RS-LIMIT....{code}\ncurrently i add SetSparkReducerParallelism#getNumReducerForSparkSortRS, it returns 10,this set the parallelism for newSparkSortRS. i will update the function in next patch.\n2. add a property sortLimit in ReduceSinkOperator. If it is true. use partition sort  not global sort in GenSparkUtils#getEdgeProperty\n\n\nNot fully test about the patch, just test a simple qfile, but I think we can parallel, you can review, i will start fully test.\n{code}\n\nselect key,value from src order by key limit 10;\n\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-20T22:40:13.536+0000","updated":"2017-06-20T22:40:13.536+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16056852","id":"16056852","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"To clarify, the idea is to introduce an extra MR shuffle and push the limit to it right? If so, I wonder whether we should put a limit on the limited number. E.g. if the number is too large, we should skip this optimization.\nBesides, I don't think we need to add the sortLimit flag to RS. ReduceSinkDesc has a flag hasOrderBy indicating whether global order is needed. We can set that to false for the new RS and GenSparkUtils#getEdgeProperty should give us the MR shuffle.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-21T02:17:08.465+0000","updated":"2017-06-21T02:17:08.465+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16056858","id":"16056858","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: \n bq.If so, I wonder whether we should put a limit on the limited number. E.g. if the number is too large, we should skip this optimization.\nyou mean that if the limit number is too large( select * from A order by ColB limit 99, when the total records of A is 100), there is no performance improvement maybe degradation because now there is 1 extra reduce.\nbq.Besides, I don't think we need to add the sortLimit flag to RS. ReduceSinkDesc has a flag hasOrderBy indicating whether global order is needed. \nthanks for suggestion.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-21T02:23:26.650+0000","updated":"2017-06-21T02:24:00.270+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16056928","id":"16056928","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"bq. you mean that if the limit number is too large...\nYeah. But it's a little tricky to set a proper upper bound for it. How about we do something like this: if statistics is available, we can estimate the number of rows in the input of the RS. If the limit number is, say, >= 90% of the rows, we can skip the optimization. If statistics is unavailable, we run the optimization anyway.\nYou can find how we estimate num of bytes in SetSparkReducerParallelism. Guess we can estimate num of rows similarly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-21T03:34:04.567+0000","updated":"2017-06-21T03:34:04.567+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16057007","id":"16057007","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Besides, it's better to add a separate optimizer for this optimization. SetSparkReducerParallelism is only intended to set parallelism for RSes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-21T05:18:09.211+0000","updated":"2017-06-21T05:18:09.211+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16057563","id":"16057563","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Re: turning the optimization off when limit number is too big\n\nIt might seem over-engineering if we are trying to be smart. In my opinion, it's very rare that LIMIT comes with a big number compared to the total number of rows. The penalty comes only when the limit number is close to the total number of rows. Still, it's not clear how much the penalty is.\n\nConsider an extreme case: there are 1000 rows and user limits to 1000 rows. With the optimization, suppose there are 10 partitions, each sorting 100 rows within that partition. All of the sorted 100 row set  (total 1000) will be shuffled to one reducer doing global sorting. The last step may not be as costly as the global shuffle w/o the optimization because each 100 row set is already sorted. (I'm not entirely sure if Spark can take advantage of that, though.)\n\nIt might be sufficient to provide a configuration to turn this optimization off if user knows what he/she is doing. Otherwise, the optimization should be on by default.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-21T14:00:16.830+0000","updated":"2017-06-21T14:00:40.790+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16060835","id":"16060835","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~xuefuz], I doubt Spark can take much advantage of it because the reducer fetches all the data anyway. But I agree limit with large number is rare so it's OK to leave it aside at the moment.\nAnother thing I'm not sure is, hive should have already pushed down the limit to the upstream of shuffle. Looking at the RS code, it uses a TopN hash to track the top N keys in input. Ideally, each RS will only output N records. I tried some simple query to verify how this saves shuffled data.\n[~kellyzly], do you know why it's not working as expected in your case?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-23T12:48:19.217+0000","updated":"2017-06-23T12:48:19.217+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16062651","id":"16062651","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"[~lirui]: thanks for you investigation\nbq. hive should have already pushed down the limit to the upstream of shuffle. Looking at the RS code, it uses a TopN hash to track the top N keys in input. Ideally, each RS will only output N records. I tried some simple query to verify how this saves shuffled data.\nI saw the topN in ReduceSinkOperator. But let me spend some time to investigate. [~xuefuz]: can you give us some suggestion?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-26T08:04:55.523+0000","updated":"2017-06-26T08:04:55.523+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16062808","id":"16062808","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":"limit push down is in HIVE-3562.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-06-26T08:56:32.744+0000","updated":"2017-06-26T08:56:32.744+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13077867/comment/16071885","id":"16071885","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"body":" For the case \"select * from A order by key limit 10\",  limit will be pushed down to the shuffle data. Each RS will only output 10 records. The effect will be same as HIVE-16840.patch. So close the jira.  Thanks for your suggestion and time on it. The reason why TPC-DS/query17 hangs in this jira is because HIVE-17010 and others.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kellyzly","name":"kellyzly","key":"kellyzly","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liyunzhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-07-03T03:30:36.010+0000","updated":"2017-07-03T03:30:36.010+0000"}],"maxResults":26,"total":26,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-16840/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3fym7:"}}