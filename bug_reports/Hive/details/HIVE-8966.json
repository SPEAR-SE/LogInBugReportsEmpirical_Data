{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12757795","self":"https://issues.apache.org/jira/rest/api/2/issue/12757795","key":"HIVE-8966","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329278","id":"12329278","description":"Branch 1.0 release","name":"1.0.0","archived":false,"released":true,"releaseDate":"2015-02-04"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2014-11-26T20:48:20.899+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Feb 19 18:22:01 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"10002_*:*_8_*:*_3314589243_*|*_1_*:*_8_*:*_2124216170_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-01-27T19:19:39.155+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-8966/watchers","watchCount":10,"isWatching":false},"created":"2014-11-25T20:32:53.814+0000","customfield_12310192":"Don't do compaction on the current delta if it has a file in bucket pattern but not compactable","customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"7.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326450","id":"12326450","description":"released","name":"0.14.0","archived":false,"released":true,"releaseDate":"2014-11-12"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-02-19T18:22:01.737+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12320409","id":"12320409","name":"HCatalog","description":"Tracks issues related to the HCatalog"}],"timeoriginalestimate":null,"description":"hive hcatalog streaming will also create a file like bucket_n_flush_length in each delta directory. Where \"n\" is the bucket number. But the compactor.CompactorMR think this file also needs to compact. However this file of course cannot be compacted, so compactor.CompactorMR will not continue to do the compaction. \n\nDid a test, after removed the bucket_n_flush_length file, then the \"alter table partition compact\" finished successfully. If don't delete that file, nothing will be compacted. \nThis is probably a very severity bug. Both 0.13 and 0.14 have this issue","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12686124","id":"12686124","filename":"HIVE-8966.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-09T23:15:32.585+0000","size":16858,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12686124/HIVE-8966.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12688699","id":"12688699","filename":"HIVE-8966.3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-22T18:43:59.151+0000","size":19232,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12688699/HIVE-8966.3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12691437","id":"12691437","filename":"HIVE-8966.4.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-10T00:03:04.033+0000","size":84374,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12691437/HIVE-8966.4.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12692048","id":"12692048","filename":"HIVE-8966.5.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-13T21:16:46.050+0000","size":89403,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12692048/HIVE-8966.5.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12694321","id":"12694321","filename":"HIVE-8966.6.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-24T01:39:20.936+0000","size":91211,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12694321/HIVE-8966.6.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12685590","id":"12685590","filename":"HIVE-8966.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-07T05:23:59.664+0000","size":2125,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12685590/HIVE-8966.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12694674","id":"12694674","filename":"HIVE-8966-branch-1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-27T00:57:28.085+0000","size":103575,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12694674/HIVE-8966-branch-1.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Delta files created by hive hcatalog streaming cannot be compacted","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"hive","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14226794","id":"14226794","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"This flush length file should be removed when the batch is closed.  Are you closing the transaction batch on a regular basis?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-11-26T20:48:20.899+0000","updated":"2014-11-26T20:48:20.899+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14226872","id":"14226872","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Yes. Closed the transaction batch. Suggest to do either the following two updates, or do both:\n\n1. if a file is non-bucket file, don't try to compact it. So update the following code:\n   in org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java\n  Change the following code:\n\n  private void addFileToMap(Matcher matcher, Path file, boolean sawBase,\n                              Map<Integer, BucketTracker> splitToBucketMap) {\n      if (!matcher.find()) {\n        LOG.warn(\"Found a non-bucket file that we thought matched the bucket pattern! \" +\n            file.toString());\n      }\n\n   .....\n to:\n   private void addFileToMap(Matcher matcher, Path file, boolean sawBase,\n                              Map<Integer, BucketTracker> splitToBucketMap) {\n      if (!matcher.find()) {\n        LOG.warn(\"Found a non-bucket file that we thought matched the bucket pattern! \" +\n            file.toString());\n        return;\n      }\n     ....\n\n2. don't use the bucket file pattern to name to \"flush_length\" file. So update the following code:\n  in org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java\n change the following code:\n   static Path getSideFile(org.apache.tools.ant.types.Path main) {\n     return new Path(main + \"_flush_length\");\n   }\n\nto:\n static Path getSideFile(org.apache.tools.ant.types.Path main) {\n\tif (main.toString().startsWith(\"bucket_\")) {\n\t     return new Path(\"bkt\"+main.toString().substring(6)+ \"_flush_length\");\n\t}\n              else return new Path(main + \"_flush_length\");\n  }\n \nafter did the above updates and re-compiled the hive-exec.jar, the compaction works fine now\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-11-26T22:08:43.462+0000","updated":"2014-11-26T22:08:43.462+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14226890","id":"14226890","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"1 might be the right thing to do.  2 breaks backward compatibility.  Before we do that though I'd like to understand why you still see the flush length files hanging around.  In my tests I don't see this issue because the flush length file is properly cleaned up.  I want to make sure that its existence doesn't mean something else is wrong.\n\nDo you see the flush length files in all delta directories or only the most recent?  ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-11-26T22:19:10.151+0000","updated":"2014-11-26T22:19:10.151+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14226925","id":"14226925","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"That flush_length file is only in the most recent delta. By the way, for streaming loading, a transaction batch is probably always open since data keeps coming. Is it possible to do compaction in the streaming loading environment? Thanks ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-11-26T22:43:34.076+0000","updated":"2014-11-26T22:43:34.076+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14226943","id":"14226943","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Ok, that makes sense.  You're current delta has the file because it's still open and being written to.  It also explains why my tests don't see it, as they don't run long enough.  The streaming is always done by the time the compactor kicks in.  Why don't you post a patch to this JIRA with the change for 1, and I can get that committed.\n\n[~hagleitn], I'd like to put this in 0.14.1 as well as trunk if you're ok with it, since it blocks compaction for users using the streaming interface.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-11-26T22:54:56.786+0000","updated":"2014-11-26T22:54:56.786+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14227045","id":"14227045","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hagleitn","name":"hagleitn","key":"hagleitn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035"},"displayName":"Gunther Hagleitner","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 for 0.14.1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hagleitn","name":"hagleitn","key":"hagleitn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035"},"displayName":"Gunther Hagleitner","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-11-27T00:25:47.424+0000","updated":"2014-11-27T00:25:47.424+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14232306","id":"14232306","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Thanks. So now the fix is in 0.14.1?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-02T23:41:12.323+0000","updated":"2014-12-02T23:41:12.323+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14233768","id":"14233768","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"https://issues.apache.org/jira/i#browse/HIVE-8966","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-04T01:27:15.174+0000","updated":"2014-12-04T01:27:15.174+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14233775","id":"14233775","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Patch for HIVE08966","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-04T01:30:26.249+0000","updated":"2014-12-04T01:30:26.249+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14233790","id":"14233790","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"The patch is attached. Please review. Thanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-04T01:40:05.077+0000","updated":"2014-12-04T01:40:05.077+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14234769","id":"14234769","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"I think we may have to withdraw this patch for now. It looks like currently hive must not support doing compaction and loading in the same time for a partition. \nWithout this patch, if loading for a partition is not completely finished, compaction will always fail, so nothing happen. After apply this patch, compaction will go through and finish. However we may loss data! I did a test. Data could be lost if we do compaction meanwhile the loading is not finished yet. \nBut if keep the current version, it must be a limitation for hive. If streaming load to a partition for a long period, performance will be affected if cannot do compaction on it. \n\nFor completely solve this issue, my initial thinking is that the delta files with open transaction should not be compacted. Currently they must be inlcuded, and it is probably the reason for data lost. But other closed delta files should be able to compact. So we can do compaction and loading in the same time.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-04T23:11:34.973+0000","updated":"2014-12-04T23:11:34.973+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14235645","id":"14235645","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Jihong, thanks for doing the testing on this.  \n\nWe could change this to not compact the current delta file, or we could change the cleaner to not remove the delta file that was still open during compaction.  I'll try to look at this in the next couple of days.  We need to get this fixed for 0.14.1.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-05T15:40:59.915+0000","updated":"2014-12-05T15:40:59.915+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14235923","id":"14235923","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Great. I am working on that now. Will update you after finished the testing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-05T19:07:24.629+0000","updated":"2014-12-05T19:07:24.629+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14237046","id":"14237046","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"The scenario of data lost:\nAssume when start compaction there are two deltas, delta_00011_00020 and delta_00021_00030, where the transaction batch in the first one is closed, and the second one still has transaction batch open. After compaction is finished, the status in compaction_ queue  will become “ready_for_clean”. Then clean process will be triggered. Cleaner will remove all deltas if its transaction id is less than the base which just created and if there is no lock on it. In the meantime, we still load data into the second delta. When finish loading and close the transaction batch, cleaner detects no lock on that, so delete it. So the new data added after compaction will be lost. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-07T04:41:34.189+0000","updated":"2014-12-07T04:41:34.189+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14237047","id":"14237047","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Solution: \nif the last delta has any file which is in bucket file pattern, but actually is non bucket file, don’t compact this delta. When a transaction is not close, a delta will have a file like bucket_n_flash_length, which is non bucket file. Actually for any reason, if the last delta has a file with bucket file pattern but not compactable, we should ignore this delta. Since after compaction, the delta will be removed. So if the whole delta cannot be compacted, leave it as what it is. So in the above scenario, the second delta will not be compacted. And the cleaner will not remove it because it has higher transaction id than the new created compaction file(base or delta). \nThe reason we only do the above for the last delta is to consider the case that two or more transaction batches may be created and the last one is close first. Then if the last delta gets compacted, the transaction id in the base will be big, so all deltas will be removed by cleaner. So data could be lost. In this case, in the list of deltas for compaction, at least one delta has that bucket_n_flash_length file inside. Since we do not ignore it, the compaction will be auto-fail, so nothing happen, no data lost. In this case, the compaction can only be done after all transaction batches are closed. Although it is not so good, at least no data lost.\nThe patch is attached. It adds one method to test whether needs to remove the last delta from the delta list. And before process the delta list, run that method.  After applying this patch, no data is lost. We can do either major or minor compaction meanwhile keeping loading data in the same time.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-07T04:42:48.942+0000","updated":"2014-12-07T04:42:48.942+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14237048","id":"14237048","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"By the way, hive may need another cleaning process which auto removes the bucket_n_flash_length file if the connection is actually closed.  A program may not be able to close a transaction batch, due to many reasons, for example, network disconnected, server shutdown, application killed, and etc. So if the connection which creates a batch has been closed, that bucket_n_flash_length file needs to be removed. Otherwise that delta and the deltas after it can never be compacted unless we remove that file manually.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-07T04:43:23.887+0000","updated":"2014-12-07T04:43:23.887+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14237057","id":"14237057","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 no tests executed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12685584/HIVE-8966.patch\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1985/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1985/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1985/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]\n+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera\n+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera\n+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin\n+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-1985/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ svn = \\s\\v\\n ]]\n+ [[ -n '' ]]\n+ [[ -d apache-svn-trunk-source ]]\n+ [[ ! -d apache-svn-trunk-source/.svn ]]\n+ [[ ! -d apache-svn-trunk-source ]]\n+ cd apache-svn-trunk-source\n+ svn revert -R .\nReverted 'metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java'\nReverted 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java'\nReverted 'ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java'\n++ awk '{print $2}'\n++ egrep -v '^X|^Performing status on external'\n++ svn status --no-ignore\n+ rm -rf target datanucleus.log ant/target shims/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/scheduler/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target accumulo-handler/target hwi/target common/target common/src/gen common/src/java/org/apache/hadoop/hive/conf/HiveConf.java.orig contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target ql/src/test/results/clientpositive/parquet_array_of_multi_field_struct_gen_schema.q.out ql/src/test/results/clientpositive/parquet_decimal_gen_schema.q.out ql/src/test/results/clientpositive/parquet_array_of_unannotated_groups_gen_schema.q.out ql/src/test/results/clientpositive/parquet_array_of_single_field_struct_gen_schema.q.out ql/src/test/results/clientpositive/parquet_array_of_unannotated_primitives_gen_schema.q.out ql/src/test/results/clientpositive/parquet_array_of_structs_gen_schema.q.out ql/src/test/results/clientpositive/parquet_avro_array_of_primitives_gen_schema.q.out ql/src/test/results/clientpositive/parquet_thrift_array_of_single_field_struct_gen_schema.q.out ql/src/test/results/clientpositive/parquet_array_of_optional_elements_gen_schema.q.out ql/src/test/results/clientpositive/parquet_avro_array_of_single_field_struct_gen_schema.q.out ql/src/test/results/clientpositive/parquet_thrift_array_of_primitives_gen_schema.q.out ql/src/test/results/clientpositive/parquet_array_of_structs_gen_schema_ext.q.out ql/src/test/results/clientpositive/parquet_array_of_required_elements_gen_schema.q.out ql/src/test/queries/clientpositive/parquet_avro_array_of_single_field_struct_gen_schema.q ql/src/test/queries/clientpositive/parquet_array_of_single_field_struct_gen_schema.q ql/src/test/queries/clientpositive/parquet_array_of_structs_gen_schema.q ql/src/test/queries/clientpositive/parquet_array_of_multi_field_struct_gen_schema.q ql/src/test/queries/clientpositive/parquet_thrift_array_of_primitives_gen_schema.q ql/src/test/queries/clientpositive/parquet_thrift_array_of_single_field_struct_gen_schema.q ql/src/test/queries/clientpositive/parquet_array_of_required_elements_gen_schema.q ql/src/test/queries/clientpositive/parquet_array_of_unannotated_groups_gen_schema.q ql/src/test/queries/clientpositive/parquet_avro_array_of_primitives_gen_schema.q ql/src/test/queries/clientpositive/parquet_decimal_gen_schema.q ql/src/test/queries/clientpositive/parquet_array_of_structs_gen_schema_ext.q ql/src/test/queries/clientpositive/parquet_array_of_optional_elements_gen_schema.q ql/src/test/queries/clientpositive/parquet_array_of_unannotated_primitives_gen_schema.q ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java.orig ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ParquetSchemaReader.java ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ParquetToHiveSchemaConverter.java\n+ svn update\n\nFetching external item into 'hcatalog/src/test/e2e/harness'\nExternal at revision 1643648.\n\nAt revision 1643648.\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12685584 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2014-12-07T05:07:41.532+0000","updated":"2014-12-07T05:07:41.532+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14237062","id":"14237062","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Hi Alan,I have created a new patch. It works fine. The patch is pasted in that jira, also added comment about the logic. Please have a look. Thanks and have a good dayJihong\n      From: Alan Gates (JIRA) <jira@apache.org> \n To: jhliu08@yahoo.com \n Sent: Friday, December 5, 2014 7:41 AM\n Subject: [jira] [Commented] (HIVE-8966) Delta files created by hive hcatalog streaming cannot be compacted\n   \n\n    [ https://issues.apache.org/jira/browse/HIVE-8966?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14235645#comment-14235645 ] \n\nAlan Gates commented on HIVE-8966:\n----------------------------------\n\nJihong, thanks for doing the testing on this.  \n\nWe could change this to not compact the current delta file, or we could change the cleaner to not remove the delta file that was still open during compaction.  I'll try to look at this in the next couple of days.  We need to get this fixed for 0.14.1.\n\n\n\n\n--\nThis message was sent by Atlassian JIRA\n(v6.3.4#6332)\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-07T05:34:41.002+0000","updated":"2014-12-07T05:34:41.002+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14237067","id":"14237067","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Alan,\nI created a wrong patch about 1 hour ago. Before I removed it. QA automatically did the above test. Please ignore and look the current attached patch. I think it really solves the issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-07T06:04:49.406+0000","updated":"2014-12-07T06:04:49.406+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14237080","id":"14237080","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 at least one tests failed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12685590/HIVE-8966.patch\n\n{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6696 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_aggregate\norg.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1986/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1986/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1986/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 2 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12685590 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2014-12-07T06:49:05.703+0000","updated":"2014-12-07T06:49:05.703+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14237257","id":"14237257","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"I am confused about the QA test. The error looks like not related to HIVE-8966.patch. First, was this patch really included in the build? Also this patch is for 0.14.1, not for trunk.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-07T20:00:54.486+0000","updated":"2014-12-07T20:00:54.486+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14239636","id":"14239636","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Don't worry about the results from testing, those tests are flaky.  I'll review the patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-09T16:45:29.682+0000","updated":"2014-12-09T16:45:29.682+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14239750","id":"14239750","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Rather than go remove these directories from the list of deltas I think it makes more sense to change Directory.getAcidState to not include these deltas.  We obviously can't do that in all cases, as readers need to see these deltas. But we can change it to see that this is the compactor and therefore those should be excluded.  I'll post a patch with this change.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-09T17:59:50.439+0000","updated":"2014-12-09T17:59:50.439+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14240004","id":"14240004","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"I see. Basically there are two solutions. One is that when get the delta list, we don't include the current delta if it has open tranaction. So uptate the AcidUtil.getAcidState() directly. The other is what I posted here. We first get the delta list, then when do compaction, we don't compact the last one if there is open transaction. The first solution is better as long as changing getAcidState() doesn't affact other existing code, since it is a public static method. \nBy the way, we should only do that to the current delta (the delta with the largest transaction id), not to all deltas which have open transactions. If I am correct, the base file will be named based on the largest transaction id in the deltas. So if the latest delta is closed, but an early delta has an open transaction, we should not do anything. So simply let the compaction fail. Otherwise, the base will be named by the last transaction id, and all early deltas will be removed. That will cause data lost. This is my understanding, please correct me, it it is not correct. Thanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-09T20:25:27.384+0000","updated":"2014-12-09T20:25:27.384+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14240259","id":"14240259","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"A new version of the patch that moves Jihong's code into AcidUtils.getAcidState so that delta directories with flush length files are not put into the list of files to compact.  \n\n[~jihongliu], could you test this on your end to make sure it addresses your issues.  I'll also do some long running tests to see that it allows compaction while streaming is ongoing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-09T23:15:32.592+0000","updated":"2014-12-09T23:15:32.592+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14240415","id":"14240415","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"Alan, your patch looks good +1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-10T00:43:23.486+0000","updated":"2014-12-10T00:43:23.486+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14240482","id":"14240482","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 at least one tests failed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12686124/HIVE-8966.2.patch\n\n{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6704 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan\norg.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_1\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2013/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2013/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2013/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 2 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12686124 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2014-12-10T01:36:17.937+0000","updated":"2014-12-10T01:36:17.937+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14240701","id":"14240701","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Alan,\nYour idea is very good. But there is an issue here -- we should only do this \"compacting\" test for the most recent delta, not for all deltas. Following is an example for the reason:\nAssume there are two deltas:\n   1  delta_00011_00020    this delta has open transaction batch\n   2  delta_00021_00030    this delta has no open transaction batch. All closed.\n\nIn the above, the first delta has open transaction batch, the second has not. And the second delta is the most recent delta. This case is possible, especially when multiple threads write to the same partition. If we ignore the first one, then the compaction will success and create a base, like base_00030. Then cleaner will delete all the two deltas since their transaction id are less or equal to the base transaction id. Thus the data in delta 2 will be lost. This is why we should only test the most recent delta, all other deltas will be automatically in the list. Thus in this case, the compaction will be fail, since the \"flush_length\" file is there. And for this case, the compaction will be success only when all transaction batchs are closed. Although it is not perfect, at least no data lost. Since each delta file and transaction id for compaction is not saved anywhere, probably this is the only solution for now. \nIn my removeNotCompactableDeltas() method, we first sort the deltas, then only check the last one. But the name: \"removeNotCompactableDeltas\" is not good, easy makes confusion. It will be clear if named it as \"removeLastDeltaIfNotCompactable\". \nThanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-10T06:00:07.156+0000","updated":"2014-12-10T06:00:07.156+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14241394","id":"14241394","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Right, makes sense.  I need to think about whether it makes more sense to change AcidUtils.getAcidState to catch this as well or whether your approach of post processing it in the compactor makes more sense.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-10T17:11:24.598+0000","updated":"2014-12-10T17:11:24.598+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14256032","id":"14256032","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"A new version of the patch which properly handles not putting any deltas in the list once we see a delta with a flush length file.  Unfortunately, [~owen.omalley] who needs to review this is out for a couple of weeks.  [~jihongliu], please take a look at this and test it in your environment.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-22T18:43:59.159+0000","updated":"2014-12-22T18:43:59.159+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14256700","id":"14256700","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 at least one tests failed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12688699/HIVE-8966.3.patch\n\n{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6724 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_lvj_mapjoin\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2168/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2168/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2168/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 2 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12688699 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2014-12-23T07:28:53.844+0000","updated":"2014-12-23T07:28:53.844+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14264177","id":"14264177","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Did a test. Generally the new version works as expected. But for the following case, the compaction will always fail:\n\n1. due to any reason, the writer exits without closing a batch. So the \"length\" file is still there. This could happen, for example the program is killed, hive/server restarts.\n2. restart the program, so a new writer and a new batch is created and continute to write into the same partition. The data will go to a new delta.\n3. Now we manually delete that \"length\" file in the previous delta. Then do compaction, but it fails. Even we totally exit the program so that no any open batch and no any \"length\" file, the compaction will never success for this partition. \n\nHowever the current hive 14.0 will work fine for the above case.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-05T04:43:52.481+0000","updated":"2015-01-05T04:43:52.481+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14265257","id":"14265257","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"What error message does it give when it fails?  I would expect this to work.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-05T22:41:03.713+0000","updated":"2015-01-05T22:41:03.713+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14266854","id":"14266854","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"The error occur when doing the mapreduce job. Following is log in hivemetastore.log\n\n2015-01-06 16:42:22,506 INFO  [sfdmgctmn003.gid.gap.com-32]: compactor.Worker (Worker.java:run(137)) - Starting MAJOR compaction for ds_infra.event_metrics.date=2014-12-24\n2015-01-06 16:42:22,564 INFO  [sfdmgctmn003.gid.gap.com-32]: impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(285)) - Timeline service address: http://sfdmgctmn003.gid.gap.com:8188/ws/v1/timeline/\n2015-01-06 16:42:22,622 INFO  [sfdmgctmn003.gid.gap.com-32]: impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(285)) - Timeline service address: http://sfdmgctmn003.gid.gap.com:8188/ws/v1/timeline/\n2015-01-06 16:42:22,628 WARN  [sfdmgctmn003.gid.gap.com-32]: mapreduce.JobSubmitter (JobSubmitter.java:copyAndConfigureFiles(153)) - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n2015-01-06 16:42:22,753 WARN  [sfdmgctmn003.gid.gap.com-32]: split.JobSplitWriter (JobSplitWriter.java:writeOldSplits(168)) - Max block location exceeded for split: CompactorInputSplit{base: hdfs://sfdmgct/apps/hive/warehouse/ds_infra/event_metrics/date=2014-12-24/base_0035304, bucket: 1, length: 292280, deltas: [delta_0035311_0035313, delta_0035479_0035481, delta_0035491_0035493, delta_0035515_0035517, delta_0035533_0035535, delta_0035548_0035550, delta_0035563_0035565, delta_0035578_0035580, delta_0035593_0035595, delta_0035599_0035601, delta_0035656_0035658, delta_0035671_0035673, delta_0035686_0035688, delta_0035701_0035703, delta_0035716_0035718, delta_0035731_0035733, delta_0035746_0035748, delta_0035761_0035763, delta_0035776_0035778, delta_0035791_0035793, delta_0035806_0035808, delta_0035821_0035823, delta_0035830_0035832, delta_0035842_0035844, delta_0035854_0035856, delta_0035866_0035868, delta_0035878_0035880]} splitsize: 27 maxsize: 10\n2015-01-06 16:42:22,753 WARN  [sfdmgctmn003.gid.gap.com-32]: split.JobSplitWriter (JobSplitWriter.java:writeOldSplits(168)) - Max block location exceeded for split: CompactorInputSplit{base: null, bucket: 3, length: 199770, deltas: [delta_0035311_0035313, delta_0035479_0035481, delta_0035491_0035493, delta_0035515_0035517, delta_0035533_0035535, delta_0035548_0035550, delta_0035563_0035565, delta_0035578_0035580, delta_0035593_0035595, delta_0035599_0035601, delta_0035656_0035658, delta_0035671_0035673, delta_0035686_0035688, delta_0035701_0035703, delta_0035716_0035718, delta_0035731_0035733, delta_0035746_0035748, delta_0035761_0035763, delta_0035776_0035778, delta_0035791_0035793, delta_0035806_0035808, delta_0035821_0035823, delta_0035830_0035832, delta_0035842_0035844, delta_0035854_0035856, delta_0035866_0035868, delta_0035878_0035880]} splitsize: 21 maxsize: 10\n2015-01-06 16:42:22,753 WARN  [sfdmgctmn003.gid.gap.com-32]: split.JobSplitWriter (JobSplitWriter.java:writeOldSplits(168)) - Max block location exceeded for split: CompactorInputSplit{base: hdfs://sfdmgct/apps/hive/warehouse/ds_infra/event_metrics/date=2014-12-24/base_0035304, bucket: 0, length: 172391, deltas: [delta_0035311_0035313, delta_0035479_0035481, delta_0035491_0035493, delta_0035515_0035517, delta_0035533_0035535, delta_0035548_0035550, delta_0035563_0035565, delta_0035578_0035580, delta_0035593_0035595, delta_0035599_0035601, delta_0035656_0035658, delta_0035671_0035673, delta_0035686_0035688, delta_0035701_0035703, delta_0035716_0035718, delta_0035731_0035733, delta_0035746_0035748, delta_0035761_0035763, delta_0035776_0035778, delta_0035791_0035793, delta_0035806_0035808, delta_0035821_0035823, delta_0035830_0035832, delta_0035842_0035844, delta_0035854_0035856, delta_0035866_0035868, delta_0035878_0035880]} splitsize: 30 maxsize: 10\n2015-01-06 16:42:22,777 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(494)) - number of splits:4\n2015-01-06 16:42:22,793 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.JobSubmitter (JobSubmitter.java:printTokens(583)) - Submitting tokens for job: job_1419291043936_1639\n2015-01-06 16:42:23,000 INFO  [sfdmgctmn003.gid.gap.com-32]: impl.YarnClientImpl (YarnClientImpl.java:submitApplication(251)) - Submitted application application_1419291043936_1639\n2015-01-06 16:42:23,001 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:submit(1300)) - The url to track the job: http://sfdmgctmn002.gid.gap.com:8088/proxy/application_1419291043936_1639/\n2015-01-06 16:42:23,001 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:monitorAndPrintJob(1345)) - Running job: job_1419291043936_1639\n2015-01-06 16:42:30,042 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:monitorAndPrintJob(1366)) - Job job_1419291043936_1639 running in uber mode : false\n2015-01-06 16:42:30,043 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:monitorAndPrintJob(1373)) -  map 0% reduce 0%\n2015-01-06 16:42:35,066 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:printTaskEvents(1452)) - Task Id : attempt_1419291043936_1639_m_000002_0, Status : FAILED\n2015-01-06 16:42:37,078 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:monitorAndPrintJob(1373)) -  map 75% reduce 0%\n2015-01-06 16:42:41,091 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:printTaskEvents(1452)) - Task Id : attempt_1419291043936_1639_m_000002_1, Status : FAILED\n2015-01-06 16:42:45,105 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:printTaskEvents(1452)) - Task Id : attempt_1419291043936_1639_m_000002_2, Status : FAILED\n2015-01-06 16:42:52,124 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:monitorAndPrintJob(1373)) -  map 100% reduce 0%\n2015-01-06 16:42:52,130 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:monitorAndPrintJob(1386)) - Job job_1419291043936_1639 failed with state FAILED due to: Task failed task_1419291043936_1639_m_000002\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n\n2015-01-06 16:42:52,149 INFO  [sfdmgctmn003.gid.gap.com-32]: mapreduce.Job (Job.java:monitorAndPrintJob(1391)) - Counters: 32\n        File System Counters\n                FILE: Number of bytes read=0\n                FILE: Number of bytes written=668781\n                FILE: Number of read operations=0\n                FILE: Number of large read operations=0\n                FILE: Number of write operations=0\n                HDFS: Number of bytes read=840325\n                HDFS: Number of bytes written=405818\n                HDFS: Number of read operations=243\n                HDFS: Number of large read operations=0\n                HDFS: Number of write operations=3\n        Job Counters\n                Failed map tasks=4\n                Launched map tasks=7\n                Other local map tasks=3\n                Data-local map tasks=4\n                Total time spent by all maps in occupied slots (ms)=32359\n                Total time spent by all reduces in occupied slots (ms)=0\n                Total time spent by all map tasks (ms)=32359\n                Total vcore-seconds taken by all map tasks=32359\n                Total megabyte-seconds taken by all map tasks=463898624\n        Map-Reduce Framework\n                Map input records=3\n                Map output records=0\n                Input split bytes=10663\n                Spilled Records=0\n                Failed Shuffles=0\n                Merged Map outputs=0\n                GC time elapsed (ms)=153\n                CPU time spent (ms)=7680\n                Physical memory (bytes) snapshot=1065402368\n                Virtual memory (bytes) snapshot=39759937536\n                Total committed heap usage (bytes)=6714032128\n        File Input Format Counters\n                Bytes Read=0\n        File Output Format Counters\n                Bytes Written=0\n2015-01-06 16:42:52,150 ERROR [sfdmgctmn003.gid.gap.com-32]: compactor.Worker (Worker.java:run(159)) - Caught exception while trying to compact ds_infra.event_metrics.date=2014-12-24.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:184)\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:145)\n\n2015-01-06 16:42:52,150 ERROR [sfdmgctmn003.gid.gap.com-32]: txn.CompactionTxnHandler (CompactionTxnHandler.java:markCleaned(327)) - Expected to remove at least one row from completed_txn_components when marking compaction entry as clean!\n\n\n\n--------------------------------------------------------------------------------------------------------------------------------\nfollowing is mapreduce job log. Got the log from uri mentioned in the above log:  (http://sfdmgctmn002.gid.gap.com:8088/proxy/application_1419291043936_1639/)\n\n\n2015-01-06 16:42:25,991 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1419291043936_1639_000001\n2015-01-06 16:42:26,304 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-01-06 16:42:26,314 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens:\n2015-01-06 16:42:26,314 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 1639 cluster_timestamp: 1419291043936 } attemptId: 1 } keyId: -950898635)\n2015-01-06 16:42:26,842 WARN [main] org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n2015-01-06 16:42:26,919 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter set in config org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter\n2015-01-06 16:42:26,921 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter is org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter\n2015-01-06 16:42:26,951 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler\n2015-01-06 16:42:26,951 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher\n2015-01-06 16:42:26,952 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher\n2015-01-06 16:42:26,952 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher\n2015-01-06 16:42:26,952 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType for class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler\n2015-01-06 16:42:26,956 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.speculate.Speculator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher\n2015-01-06 16:42:26,956 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter\n2015-01-06 16:42:26,957 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncher$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter\n2015-01-06 16:42:26,975 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring\n2015-01-06 16:42:26,987 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring\n2015-01-06 16:42:26,999 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system is set solely by core-default.xml therefore -  ignoring\n2015-01-06 16:42:27,023 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Emitting job history data to the timeline server is not enabled\n2015-01-06 16:42:27,050 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobFinishEvent$Type for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler\n2015-01-06 16:42:27,191 WARN [main] org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-mrappmaster.properties,hadoop-metrics2.properties\n2015-01-06 16:42:27,232 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2015-01-06 16:42:27,232 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started\n2015-01-06 16:42:27,239 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1419291043936_1639 to jobTokenSecretManager\n2015-01-06 16:42:27,327 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1419291043936_1639 because: not enabled;\n2015-01-06 16:42:27,340 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1419291043936_1639 = 845156. Number of splits = 4\n2015-01-06 16:42:27,341 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1419291043936_1639 = 0\n2015-01-06 16:42:27,341 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1419291043936_1639Job Transitioned from NEW to INITED\n2015-01-06 16:42:27,341 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1419291043936_1639.\n2015-01-06 16:42:27,359 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-01-06 16:42:27,366 INFO [Socket Reader #1 for port 57524] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 57524\n2015-01-06 16:42:27,379 INFO [main] org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n2015-01-06 16:42:27,380 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2015-01-06 16:42:27,381 INFO [IPC Server listener on 57524] org.apache.hadoop.ipc.Server: IPC Server listener on 57524: starting\n2015-01-06 16:42:27,381 INFO [main] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at sfdmgctsn004.gid.gap.com/10.9.21.134:57524\n2015-01-06 16:42:27,429 INFO [main] org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n2015-01-06 16:42:27,431 INFO [main] org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.mapreduce is not defined\n2015-01-06 16:42:27,439 INFO [main] org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n2015-01-06 16:42:27,472 INFO [main] org.apache.hadoop.http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce\n2015-01-06 16:42:27,472 INFO [main] org.apache.hadoop.http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static\n2015-01-06 16:42:27,475 INFO [main] org.apache.hadoop.http.HttpServer2: adding path spec: /mapreduce/*\n2015-01-06 16:42:27,475 INFO [main] org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*\n2015-01-06 16:42:27,482 INFO [main] org.apache.hadoop.http.HttpServer2: Jetty bound to port 45674\n2015-01-06 16:42:27,482 INFO [main] org.mortbay.log: jetty-6.1.26.hwx\n2015-01-06 16:42:27,505 INFO [main] org.mortbay.log: Extract jar:file:/data/sfdmgct05/hadoop/yarn/local/filecache/12/mapreduce.tar.gz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.2.2.0.0-2041.jar!/webapps/mapreduce to /tmp/Jetty_0_0_0_0_45674_mapreduce____.ycambd/webapp\n2015-01-06 16:42:27,805 INFO [main] org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:45674\n2015-01-06 16:42:27,806 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Web app /mapreduce started at 45674\n2015-01-06 16:42:28,026 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules\n2015-01-06 16:42:28,029 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1419291043936_1639\n2015-01-06 16:42:28,030 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-01-06 16:42:28,030 INFO [Socket Reader #1 for port 33406] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 33406\n2015-01-06 16:42:28,034 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2015-01-06 16:42:28,034 INFO [IPC Server listener on 33406] org.apache.hadoop.ipc.Server: IPC Server listener on 33406: starting\n2015-01-06 16:42:28,074 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true\n2015-01-06 16:42:28,075 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3\n2015-01-06 16:42:28,075 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33\n2015-01-06 16:42:28,158 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: <memory:186368, vCores:32>\n2015-01-06 16:42:28,158 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: queue: default\n2015-01-06 16:42:28,160 INFO [main] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500\n2015-01-06 16:42:28,161 INFO [main] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\n2015-01-06 16:42:28,165 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1419291043936_1639Job Transitioned from INITED to SETUP\n2015-01-06 16:42:28,167 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP\n2015-01-06 16:42:28,168 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1419291043936_1639Job Transitioned from SETUP to RUNNING\n2015-01-06 16:42:28,182 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,186 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn003.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,188 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn001.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,191 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn005.gid.gap.com to /default/rack_06\n2015-01-06 16:42:28,194 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn002.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000000 Task Transitioned from NEW to SCHEDULED\n2015-01-06 16:42:28,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn003.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn001.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn005.gid.gap.com to /default/rack_06\n2015-01-06 16:42:28,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn002.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,198 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000001 Task Transitioned from NEW to SCHEDULED\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn003.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn001.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn005.gid.gap.com to /default/rack_06\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn002.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000002 Task Transitioned from NEW to SCHEDULED\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn001.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn005.gid.gap.com to /default/rack_06\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn002.gid.gap.com to /default/rack_05\n2015-01-06 16:42:28,199 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000003 Task Transitioned from NEW to SCHEDULED\n2015-01-06 16:42:28,200 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED\n2015-01-06 16:42:28,200 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000001_0 TaskAttempt Transitioned from NEW to UNASSIGNED\n2015-01-06 16:42:28,200 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_0 TaskAttempt Transitioned from NEW to UNASSIGNED\n2015-01-06 16:42:28,200 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000003_0 TaskAttempt Transitioned from NEW to UNASSIGNED\n2015-01-06 16:42:28,201 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: mapResourceRequest:<memory:14336, vCores:1>\n2015-01-06 16:42:28,226 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1419291043936_1639, File: hdfs://sfdmgct:8020/user/hive/.staging/job_1419291043936_1639/job_1419291043936_1639_1.jhist\n2015-01-06 16:42:29,160 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n2015-01-06 16:42:29,181 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1419291043936_1639: ask=8 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:458752, vCores:79> knownNMs=5\n2015-01-06 16:42:30,189 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 4\n2015-01-06 16:42:30,190 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1419291043936_1639_01_000002 to attempt_1419291043936_1639_m_000000_0\n2015-01-06 16:42:30,191 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1419291043936_1639_01_000003 to attempt_1419291043936_1639_m_000001_0\n2015-01-06 16:42:30,191 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1419291043936_1639_01_000004 to attempt_1419291043936_1639_m_000002_0\n2015-01-06 16:42:30,191 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1419291043936_1639_01_000005 to attempt_1419291043936_1639_m_000003_0\n2015-01-06 16:42:30,191 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:4 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:30,217 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:30,227 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://sfdmgct/user/hive/.staging/job_1419291043936_1639/job.jar\n2015-01-06 16:42:30,229 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /user/hive/.staging/job_1419291043936_1639/job.xml\n2015-01-06 16:42:30,231 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container\n2015-01-06 16:42:30,231 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1\n2015-01-06 16:42:30,231 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData\n2015-01-06 16:42:30,247 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2015-01-06 16:42:30,248 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:30,248 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000001_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2015-01-06 16:42:30,249 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:30,249 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2015-01-06 16:42:30,249 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:30,250 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000003_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2015-01-06 16:42:30,251 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1419291043936_1639_01_000002 taskAttempt attempt_1419291043936_1639_m_000000_0\n2015-01-06 16:42:30,251 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1419291043936_1639_01_000003 taskAttempt attempt_1419291043936_1639_m_000001_0\n2015-01-06 16:42:30,251 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1419291043936_1639_01_000004 taskAttempt attempt_1419291043936_1639_m_000002_0\n2015-01-06 16:42:30,251 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1419291043936_1639_01_000005 taskAttempt attempt_1419291043936_1639_m_000003_0\n2015-01-06 16:42:30,252 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1419291043936_1639_m_000002_0\n2015-01-06 16:42:30,252 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1419291043936_1639_m_000001_0\n2015-01-06 16:42:30,252 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1419291043936_1639_m_000003_0\n2015-01-06 16:42:30,252 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1419291043936_1639_m_000000_0\n2015-01-06 16:42:30,253 INFO [ContainerLauncher #2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn004.gid.gap.com:45454\n2015-01-06 16:42:30,266 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn004.gid.gap.com:45454\n2015-01-06 16:42:30,267 INFO [ContainerLauncher #3] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn004.gid.gap.com:45454\n2015-01-06 16:42:30,267 INFO [ContainerLauncher #1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn004.gid.gap.com:45454\n2015-01-06 16:42:30,294 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1419291043936_1639_m_000002_0 : 13562\n2015-01-06 16:42:30,294 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1419291043936_1639_m_000003_0 : 13562\n2015-01-06 16:42:30,294 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1419291043936_1639_m_000000_0 : 13562\n2015-01-06 16:42:30,294 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1419291043936_1639_m_000001_0 : 13562\n2015-01-06 16:42:30,295 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1419291043936_1639_m_000000_0] using containerId: [container_1419291043936_1639_01_000002 on NM: [sfdmgctsn004.gid.gap.com:45454]\n2015-01-06 16:42:30,297 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING\n2015-01-06 16:42:30,297 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1419291043936_1639_m_000003_0] using containerId: [container_1419291043936_1639_01_000005 on NM: [sfdmgctsn004.gid.gap.com:45454]\n2015-01-06 16:42:30,297 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000003_0 TaskAttempt Transitioned from ASSIGNED to RUNNING\n2015-01-06 16:42:30,297 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1419291043936_1639_m_000002_0] using containerId: [container_1419291043936_1639_01_000004 on NM: [sfdmgctsn004.gid.gap.com:45454]\n2015-01-06 16:42:30,298 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_0 TaskAttempt Transitioned from ASSIGNED to RUNNING\n2015-01-06 16:42:30,298 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1419291043936_1639_m_000001_0] using containerId: [container_1419291043936_1639_01_000003 on NM: [sfdmgctsn004.gid.gap.com:45454]\n2015-01-06 16:42:30,298 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000001_0 TaskAttempt Transitioned from ASSIGNED to RUNNING\n2015-01-06 16:42:30,298 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000000 Task Transitioned from SCHEDULED to RUNNING\n2015-01-06 16:42:30,298 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000003 Task Transitioned from SCHEDULED to RUNNING\n2015-01-06 16:42:30,299 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000002 Task Transitioned from SCHEDULED to RUNNING\n2015-01-06 16:42:30,299 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000001 Task Transitioned from SCHEDULED to RUNNING\n2015-01-06 16:42:31,194 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1419291043936_1639: ask=8 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:401408, vCores:75> knownNMs=5\n2015-01-06 16:42:33,555 INFO [Socket Reader #1 for port 33406] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1419291043936_1639 (auth:SIMPLE)\n2015-01-06 16:42:33,572 INFO [IPC Server handler 0 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1419291043936_1639_m_000002 asked for a task\n2015-01-06 16:42:33,572 INFO [IPC Server handler 0 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1419291043936_1639_m_000002 given task: attempt_1419291043936_1639_m_000000_0\n2015-01-06 16:42:33,587 INFO [Socket Reader #1 for port 33406] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1419291043936_1639 (auth:SIMPLE)\n2015-01-06 16:42:33,596 INFO [Socket Reader #1 for port 33406] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1419291043936_1639 (auth:SIMPLE)\n2015-01-06 16:42:33,601 INFO [Socket Reader #1 for port 33406] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1419291043936_1639 (auth:SIMPLE)\n2015-01-06 16:42:33,602 INFO [IPC Server handler 1 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1419291043936_1639_m_000005 asked for a task\n2015-01-06 16:42:33,602 INFO [IPC Server handler 1 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1419291043936_1639_m_000005 given task: attempt_1419291043936_1639_m_000003_0\n2015-01-06 16:42:33,607 INFO [IPC Server handler 2 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1419291043936_1639_m_000003 asked for a task\n2015-01-06 16:42:33,607 INFO [IPC Server handler 2 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1419291043936_1639_m_000003 given task: attempt_1419291043936_1639_m_000001_0\n2015-01-06 16:42:33,612 INFO [IPC Server handler 3 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1419291043936_1639_m_000004 asked for a task\n2015-01-06 16:42:33,612 INFO [IPC Server handler 3 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1419291043936_1639_m_000004 given task: attempt_1419291043936_1639_m_000002_0\n2015-01-06 16:42:35,010 INFO [IPC Server handler 4 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000002_0 is : 0.0\n2015-01-06 16:42:35,014 FATAL [IPC Server handler 5 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1419291043936_1639_m_000002_0 - exited : java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:35,014 INFO [IPC Server handler 5 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_0: Error: java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:35,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_0: Error: java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:35,016 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_0 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP\n2015-01-06 16:42:35,017 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1419291043936_1639_01_000004 taskAttempt attempt_1419291043936_1639_m_000002_0\n2015-01-06 16:42:35,017 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1419291043936_1639_m_000002_0\n2015-01-06 16:42:35,017 INFO [ContainerLauncher #4] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn004.gid.gap.com:45454\n2015-01-06 16:42:35,029 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP\n2015-01-06 16:42:35,030 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT\n2015-01-06 16:42:35,031 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_0 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED\n2015-01-06 16:42:35,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:35,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn003.gid.gap.com to /default/rack_05\n2015-01-06 16:42:35,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn001.gid.gap.com to /default/rack_05\n2015-01-06 16:42:35,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn005.gid.gap.com to /default/rack_06\n2015-01-06 16:42:35,035 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn002.gid.gap.com to /default/rack_05\n2015-01-06 16:42:35,035 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 1 failures on node sfdmgctsn004.gid.gap.com\n2015-01-06 16:42:35,036 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_1 TaskAttempt Transitioned from NEW to UNASSIGNED\n2015-01-06 16:42:35,037 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1419291043936_1639_m_000002_1 to list of failed maps\n2015-01-06 16:42:35,198 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:4 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:35,199 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1419291043936_1639: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:401408, vCores:75> knownNMs=5\n2015-01-06 16:42:35,697 INFO [IPC Server handler 4 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000001_0 is : 0.0\n2015-01-06 16:42:35,734 INFO [IPC Server handler 5 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000003_0 is : 0.0\n2015-01-06 16:42:35,833 INFO [IPC Server handler 6 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000000_0 is : 0.0\n2015-01-06 16:42:35,896 INFO [IPC Server handler 7 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000001_0 is : 1.0\n2015-01-06 16:42:35,901 INFO [IPC Server handler 8 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1419291043936_1639_m_000001_0\n2015-01-06 16:42:35,902 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000001_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP\n2015-01-06 16:42:35,903 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1419291043936_1639_01_000003 taskAttempt attempt_1419291043936_1639_m_000001_0\n2015-01-06 16:42:35,903 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1419291043936_1639_m_000001_0\n2015-01-06 16:42:35,903 INFO [ContainerLauncher #5] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn004.gid.gap.com:45454\n2015-01-06 16:42:35,911 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000001_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n2015-01-06 16:42:35,912 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1419291043936_1639_m_000001_0\n2015-01-06 16:42:35,913 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000001 Task Transitioned from RUNNING to SUCCEEDED\n2015-01-06 16:42:35,913 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 1\n2015-01-06 16:42:35,925 INFO [IPC Server handler 9 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000003_0 is : 1.0\n2015-01-06 16:42:35,927 INFO [IPC Server handler 10 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1419291043936_1639_m_000003_0\n2015-01-06 16:42:35,928 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000003_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP\n2015-01-06 16:42:35,929 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1419291043936_1639_01_000005 taskAttempt attempt_1419291043936_1639_m_000003_0\n2015-01-06 16:42:35,929 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1419291043936_1639_m_000003_0\n2015-01-06 16:42:35,929 INFO [ContainerLauncher #6] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn004.gid.gap.com:45454\n2015-01-06 16:42:35,936 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000003_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n2015-01-06 16:42:35,936 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1419291043936_1639_m_000003_0\n2015-01-06 16:42:35,936 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000003 Task Transitioned from RUNNING to SUCCEEDED\n2015-01-06 16:42:35,937 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 2\n2015-01-06 16:42:36,035 INFO [IPC Server handler 11 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000000_0 is : 1.0\n2015-01-06 16:42:36,037 INFO [IPC Server handler 12 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1419291043936_1639_m_000000_0\n2015-01-06 16:42:36,038 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000000_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP\n2015-01-06 16:42:36,039 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1419291043936_1639_01_000002 taskAttempt attempt_1419291043936_1639_m_000000_0\n2015-01-06 16:42:36,039 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1419291043936_1639_m_000000_0\n2015-01-06 16:42:36,039 INFO [ContainerLauncher #7] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn004.gid.gap.com:45454\n2015-01-06 16:42:36,045 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000000_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED\n2015-01-06 16:42:36,045 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1419291043936_1639_m_000000_0\n2015-01-06 16:42:36,045 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000000 Task Transitioned from RUNNING to SUCCEEDED\n2015-01-06 16:42:36,046 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 3\n2015-01-06 16:42:36,200 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:4 AssignedReds:0 CompletedMaps:3 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:36,207 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1419291043936_1639_01_000004\n2015-01-06 16:42:36,207 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1\n2015-01-06 16:42:36,208 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_0: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n2015-01-06 16:42:36,208 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigning container Container: [ContainerId: container_1419291043936_1639_01_000006, NodeId: sfdmgctsn003.gid.gap.com:45454, NodeHttpAddress: sfdmgctsn003.gid.gap.com:8042, Resource: <memory:14336, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.9.21.133:45454 }, ] to fast fail map\n2015-01-06 16:42:36,208 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned from earlierFailedMaps\n2015-01-06 16:42:36,208 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1419291043936_1639_01_000006 to attempt_1419291043936_1639_m_000002_1\n2015-01-06 16:42:36,208 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:4 AssignedReds:0 CompletedMaps:3 CompletedReds:0 ContAlloc:5 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:36,209 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn003.gid.gap.com to /default/rack_05\n2015-01-06 16:42:36,209 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2015-01-06 16:42:36,210 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1419291043936_1639_01_000006 taskAttempt attempt_1419291043936_1639_m_000002_1\n2015-01-06 16:42:36,210 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1419291043936_1639_m_000002_1\n2015-01-06 16:42:36,210 INFO [ContainerLauncher #8] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn003.gid.gap.com:45454\n2015-01-06 16:42:36,218 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1419291043936_1639_m_000002_1 : 13562\n2015-01-06 16:42:36,219 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1419291043936_1639_m_000002_1] using containerId: [container_1419291043936_1639_01_000006 on NM: [sfdmgctsn003.gid.gap.com:45454]\n2015-01-06 16:42:36,219 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_1 TaskAttempt Transitioned from ASSIGNED to RUNNING\n2015-01-06 16:42:37,210 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1419291043936_1639: ask=1 release= 0 newContainers=0 finishedContainers=3 resourcelimit=<memory:444416, vCores:78> knownNMs=5\n2015-01-06 16:42:37,210 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1419291043936_1639_01_000002\n2015-01-06 16:42:37,210 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1419291043936_1639_01_000003\n2015-01-06 16:42:37,210 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1419291043936_1639_01_000005\n2015-01-06 16:42:37,210 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000000_0: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n2015-01-06 16:42:37,210 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:3 CompletedReds:0 ContAlloc:5 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:37,210 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000001_0: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n2015-01-06 16:42:37,211 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000003_0: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n2015-01-06 16:42:39,096 INFO [Socket Reader #1 for port 33406] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1419291043936_1639 (auth:SIMPLE)\n2015-01-06 16:42:39,106 INFO [IPC Server handler 0 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1419291043936_1639_m_000006 asked for a task\n2015-01-06 16:42:39,106 INFO [IPC Server handler 0 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1419291043936_1639_m_000006 given task: attempt_1419291043936_1639_m_000002_1\n2015-01-06 16:42:40,344 INFO [IPC Server handler 1 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000002_1 is : 0.0\n2015-01-06 16:42:40,345 FATAL [IPC Server handler 2 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1419291043936_1639_m_000002_1 - exited : java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:40,345 INFO [IPC Server handler 2 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_1: Error: java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:40,346 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_1: Error: java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:40,346 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_1 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP\n2015-01-06 16:42:40,347 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1419291043936_1639_01_000006 taskAttempt attempt_1419291043936_1639_m_000002_1\n2015-01-06 16:42:40,347 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1419291043936_1639_m_000002_1\n2015-01-06 16:42:40,347 INFO [ContainerLauncher #9] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn003.gid.gap.com:45454\n2015-01-06 16:42:40,353 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_1 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP\n2015-01-06 16:42:40,354 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT\n2015-01-06 16:42:40,354 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_1 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED\n2015-01-06 16:42:40,354 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:40,354 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn003.gid.gap.com to /default/rack_05\n2015-01-06 16:42:40,354 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn001.gid.gap.com to /default/rack_05\n2015-01-06 16:42:40,355 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn005.gid.gap.com to /default/rack_06\n2015-01-06 16:42:40,355 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn002.gid.gap.com to /default/rack_05\n2015-01-06 16:42:40,355 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 1 failures on node sfdmgctsn003.gid.gap.com\n2015-01-06 16:42:40,355 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_2 TaskAttempt Transitioned from NEW to UNASSIGNED\n2015-01-06 16:42:40,355 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1419291043936_1639_m_000002_2 to list of failed maps\n2015-01-06 16:42:41,215 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:3 CompletedReds:0 ContAlloc:5 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:41,216 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1419291043936_1639: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:444416, vCores:78> knownNMs=5\n2015-01-06 16:42:42,218 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1419291043936_1639_01_000006\n2015-01-06 16:42:42,218 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1\n2015-01-06 16:42:42,218 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_1: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n2015-01-06 16:42:42,218 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigning container Container: [ContainerId: container_1419291043936_1639_01_000007, NodeId: sfdmgctsn003.gid.gap.com:45454, NodeHttpAddress: sfdmgctsn003.gid.gap.com:8042, Resource: <memory:14336, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.9.21.133:45454 }, ] to fast fail map\n2015-01-06 16:42:42,218 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned from earlierFailedMaps\n2015-01-06 16:42:42,218 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1419291043936_1639_01_000007 to attempt_1419291043936_1639_m_000002_2\n2015-01-06 16:42:42,218 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:3 CompletedReds:0 ContAlloc:6 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:42,218 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn003.gid.gap.com to /default/rack_05\n2015-01-06 16:42:42,219 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_2 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2015-01-06 16:42:42,219 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1419291043936_1639_01_000007 taskAttempt attempt_1419291043936_1639_m_000002_2\n2015-01-06 16:42:42,219 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1419291043936_1639_m_000002_2\n2015-01-06 16:42:42,219 INFO [ContainerLauncher #3] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn003.gid.gap.com:45454\n2015-01-06 16:42:42,226 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1419291043936_1639_m_000002_2 : 13562\n2015-01-06 16:42:42,226 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1419291043936_1639_m_000002_2] using containerId: [container_1419291043936_1639_01_000007 on NM: [sfdmgctsn003.gid.gap.com:45454]\n2015-01-06 16:42:42,227 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_2 TaskAttempt Transitioned from ASSIGNED to RUNNING\n2015-01-06 16:42:43,220 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1419291043936_1639: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:444416, vCores:78> knownNMs=5\n2015-01-06 16:42:43,270 INFO [Socket Reader #1 for port 33406] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1419291043936_1639 (auth:SIMPLE)\n2015-01-06 16:42:43,279 INFO [IPC Server handler 1 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1419291043936_1639_m_000007 asked for a task\n2015-01-06 16:42:43,279 INFO [IPC Server handler 1 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1419291043936_1639_m_000007 given task: attempt_1419291043936_1639_m_000002_2\n2015-01-06 16:42:44,498 INFO [IPC Server handler 3 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000002_2 is : 0.0\n2015-01-06 16:42:44,500 FATAL [IPC Server handler 4 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1419291043936_1639_m_000002_2 - exited : java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:44,500 INFO [IPC Server handler 4 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_2: Error: java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:44,500 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_2: Error: java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:44,501 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_2 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP\n2015-01-06 16:42:44,501 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1419291043936_1639_01_000007 taskAttempt attempt_1419291043936_1639_m_000002_2\n2015-01-06 16:42:44,501 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1419291043936_1639_m_000002_2\n2015-01-06 16:42:44,501 INFO [ContainerLauncher #1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn003.gid.gap.com:45454\n2015-01-06 16:42:44,507 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_2 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP\n2015-01-06 16:42:44,508 INFO [CommitterEvent Processor #3] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT\n2015-01-06 16:42:44,508 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_2 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED\n2015-01-06 16:42:44,508 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn004.gid.gap.com to /default/rack_05\n2015-01-06 16:42:44,508 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn003.gid.gap.com to /default/rack_05\n2015-01-06 16:42:44,508 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn001.gid.gap.com to /default/rack_05\n2015-01-06 16:42:44,508 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn005.gid.gap.com to /default/rack_06\n2015-01-06 16:42:44,508 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn002.gid.gap.com to /default/rack_05\n2015-01-06 16:42:44,509 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 2 failures on node sfdmgctsn003.gid.gap.com\n2015-01-06 16:42:44,509 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_3 TaskAttempt Transitioned from NEW to UNASSIGNED\n2015-01-06 16:42:44,509 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1419291043936_1639_m_000002_3 to list of failed maps\n2015-01-06 16:42:45,222 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:3 CompletedReds:0 ContAlloc:6 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:45,223 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1419291043936_1639: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:444416, vCores:78> knownNMs=5\n2015-01-06 16:42:46,225 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1419291043936_1639_01_000007\n2015-01-06 16:42:46,225 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1\n2015-01-06 16:42:46,226 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigning container Container: [ContainerId: container_1419291043936_1639_01_000008, NodeId: sfdmgctsn005.gid.gap.com:45454, NodeHttpAddress: sfdmgctsn005.gid.gap.com:8042, Resource: <memory:14336, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 10.9.21.135:45454 }, ] to fast fail map\n2015-01-06 16:42:46,226 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_2: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n2015-01-06 16:42:46,226 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned from earlierFailedMaps\n2015-01-06 16:42:46,226 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1419291043936_1639_01_000008 to attempt_1419291043936_1639_m_000002_3\n2015-01-06 16:42:46,226 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:3 CompletedReds:0 ContAlloc:7 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:46,226 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved sfdmgctsn005.gid.gap.com to /default/rack_06\n2015-01-06 16:42:46,227 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_3 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED\n2015-01-06 16:42:46,227 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1419291043936_1639_01_000008 taskAttempt attempt_1419291043936_1639_m_000002_3\n2015-01-06 16:42:46,227 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1419291043936_1639_m_000002_3\n2015-01-06 16:42:46,227 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn005.gid.gap.com:45454\n2015-01-06 16:42:46,235 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1419291043936_1639_m_000002_3 : 13562\n2015-01-06 16:42:46,235 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1419291043936_1639_m_000002_3] using containerId: [container_1419291043936_1639_01_000008 on NM: [sfdmgctsn005.gid.gap.com:45454]\n2015-01-06 16:42:46,235 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_3 TaskAttempt Transitioned from ASSIGNED to RUNNING\n2015-01-06 16:42:47,227 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1419291043936_1639: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:444416, vCores:78> knownNMs=5\n2015-01-06 16:42:49,199 INFO [Socket Reader #1 for port 33406] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1419291043936_1639 (auth:SIMPLE)\n2015-01-06 16:42:49,209 INFO [IPC Server handler 1 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1419291043936_1639_m_000008 asked for a task\n2015-01-06 16:42:49,209 INFO [IPC Server handler 1 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1419291043936_1639_m_000008 given task: attempt_1419291043936_1639_m_000002_3\n2015-01-06 16:42:50,432 INFO [IPC Server handler 3 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1419291043936_1639_m_000002_3 is : 0.0\n2015-01-06 16:42:50,434 FATAL [IPC Server handler 4 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1419291043936_1639_m_000002_3 - exited : java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:50,434 INFO [IPC Server handler 4 on 33406] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_3: Error: java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:50,434 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1419291043936_1639_m_000002_3: Error: java.lang.IndexOutOfBoundsException\n\tat java.nio.Buffer.checkIndex(Buffer.java:532)\n\tat java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:369)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:311)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:464)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1232)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:510)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:489)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\n2015-01-06 16:42:50,435 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_3 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP\n2015-01-06 16:42:50,435 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1419291043936_1639_01_000008 taskAttempt attempt_1419291043936_1639_m_000002_3\n2015-01-06 16:42:50,435 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1419291043936_1639_m_000002_3\n2015-01-06 16:42:50,435 INFO [ContainerLauncher #2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sfdmgctsn005.gid.gap.com:45454\n2015-01-06 16:42:50,441 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_3 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP\n2015-01-06 16:42:50,441 INFO [CommitterEvent Processor #4] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT\n2015-01-06 16:42:50,442 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1419291043936_1639_m_000002_3 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED\n2015-01-06 16:42:50,443 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1419291043936_1639_m_000002 Task Transitioned from RUNNING to FAILED\n2015-01-06 16:42:50,443 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 1 failures on node sfdmgctsn005.gid.gap.com\n2015-01-06 16:42:50,443 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 4\n2015-01-06 16:42:50,443 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Job failed as tasks failed. failedMaps:1 failedReduces:0\n2015-01-06 16:42:50,444 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1419291043936_1639Job Transitioned from RUNNING to FAIL_ABORT\n2015-01-06 16:42:50,444 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_ABORT\n2015-01-06 16:42:50,455 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1419291043936_1639Job Transitioned from FAIL_ABORT to FAILED\n2015-01-06 16:42:50,456 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: We are finishing cleanly so this is the last retry\n2015-01-06 16:42:50,456 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify RMCommunicator isAMLastRetry: true\n2015-01-06 16:42:50,456 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true\n2015-01-06 16:42:50,456 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify JHEH isAMLastRetry: true\n2015-01-06 16:42:50,456 INFO [Thread-79] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: JobHistoryEventHandler notified that forceJobCompletion is true\n2015-01-06 16:42:50,456 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Calling stop for all the services\n2015-01-06 16:42:50,456 INFO [Thread-79] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0\n2015-01-06 16:42:50,485 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://sfdmgct:8020/user/hive/.staging/job_1419291043936_1639/job_1419291043936_1639_1.jhist to hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639-1420580542797-hive-sfdmgctmn003.gid.gap.com%2D32%2Dcompactor%2Dds_infra.eve-1420580570443-3-0-FAILED-default-1420580548162.jhist_tmp\n2015-01-06 16:42:50,504 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639-1420580542797-hive-sfdmgctmn003.gid.gap.com%2D32%2Dcompactor%2Dds_infra.eve-1420580570443-3-0-FAILED-default-1420580548162.jhist_tmp\n2015-01-06 16:42:50,507 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://sfdmgct:8020/user/hive/.staging/job_1419291043936_1639/job_1419291043936_1639_1_conf.xml to hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639_conf.xml_tmp\n2015-01-06 16:42:50,524 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639_conf.xml_tmp\n2015-01-06 16:42:50,530 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639.summary_tmp to hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639.summary\n2015-01-06 16:42:50,531 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639_conf.xml_tmp to hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639_conf.xml\n2015-01-06 16:42:50,533 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639-1420580542797-hive-sfdmgctmn003.gid.gap.com%2D32%2Dcompactor%2Dds_infra.eve-1420580570443-3-0-FAILED-default-1420580548162.jhist_tmp to hdfs://sfdmgct:8020/mr-history/tmp/hive/job_1419291043936_1639-1420580542797-hive-sfdmgctmn003.gid.gap.com%2D32%2Dcompactor%2Dds_infra.eve-1420580570443-3-0-FAILED-default-1420580548162.jhist\n2015-01-06 16:42:50,533 INFO [Thread-79] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopped JobHistoryEventHandler. super.stop()\n2015-01-06 16:42:50,535 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Setting job diagnostics to Task failed task_1419291043936_1639_m_000002\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n\n2015-01-06 16:42:50,536 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: History url is http://sfdmgctmn004.gid.gap.com:19888/jobhistory/job/job_1419291043936_1639\n2015-01-06 16:42:50,539 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2015-01-06 16:42:51,540 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Final Stats: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:3 CompletedReds:0 ContAlloc:7 ContRel:0 HostLocal:4 RackLocal:0\n2015-01-06 16:42:51,541 INFO [Thread-79] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Deleting staging directory hdfs://sfdmgct /user/hive/.staging/job_1419291043936_1639\n2015-01-06 16:42:51,543 INFO [Thread-79] org.apache.hadoop.ipc.Server: Stopping server on 33406\n2015-01-06 16:42:51,544 INFO [IPC Server listener on 33406] org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 33406\n2015-01-06 16:42:51,545 INFO [TaskHeartbeatHandler PingChecker] org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler: TaskHeartbeatHandler thread interrupted\n2015-01-06 16:42:51,545 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: Stopping IPC Server Responder\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-06T22:03:49.838+0000","updated":"2015-01-06T22:03:49.838+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14270282","id":"14270282","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"The issue is that since the writer died with an unclosed batch it left the orc file in a state where it cannot be read without the length file.  So removing the length file means any reader will fail when reading it.\n\nThe proper solution is for the compactor to stop at that partition until it has determined all transactions in that file have committed or aborted.  Then it should compact it using the length file, but properly ignore the length file.  I'll work on the fix.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-09T00:32:38.591+0000","updated":"2015-01-09T00:32:38.591+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14271601","id":"14271601","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Make sense. It is so great if that solution can be implemented.Thanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-09T17:54:01.486+0000","updated":"2015-01-09T17:54:01.486+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14272112","id":"14272112","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"This patch takes a new approach.  Rather than changing AcidUtils.getAcidState (as previous 2 attempts) this patch gives a new implementation of ValidTxnList that only returns isTxnRangeValid ALL or NONE, and gives NONE if there are any open transactions <= the max transaction in the range (even if it's below the range).  This new implementation is used only by the compactor so that it's understanding of what files it should compact are different than what files a reader views as available for reading.\n\nI've also added tests to TestCompactor to test compaction during streaming and compaction after a streamer has aborted and died without cleaning up.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-10T00:03:04.041+0000","updated":"2015-01-10T00:03:04.041+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14272443","id":"14272443","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:green}Overall{color}: +1 all checks pass\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12691437/HIVE-8966.4.patch\n\n{color:green}SUCCESS:{color} +1 6764 tests passed\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2322/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2322/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2322/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12691437 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2015-01-10T11:15:22.704+0000","updated":"2015-01-10T11:15:22.704+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14274433","id":"14274433","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"[~owen.omalley] pointed out that I need to change the implementation of ValidCompactorTxnList.isTxnValid to return false for aborted transactions so that aborted records aren't carried forward in compacted files.  ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-12T23:56:47.291+0000","updated":"2015-01-12T23:56:47.291+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14275957","id":"14275957","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Yet another version of this patch, this one fixes the issue introduced by the last one that adds aborted records to compacted files.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-13T21:16:46.058+0000","updated":"2015-01-13T21:16:46.058+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14278515","id":"14278515","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 at least one tests failed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12692048/HIVE-8966.5.patch\n\n{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 7330 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2369/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2369/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2369/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 1 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12692048 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2015-01-15T10:14:37.822+0000","updated":"2015-01-15T10:14:37.822+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14284927","id":"14284927","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"This looks good, Alan. +1\n\nOne minor nit is that the class javadoc for ValidReadTxnList has \"And\" instead of the intended \"An\".\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-21T00:47:42.633+0000","updated":"2015-01-21T00:47:42.633+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14284935","id":"14284935","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"After a little more thought, I'm worried that someone will accidentally create a ValidCompactorTxnList and get confused by the different behavior. I think it would make sense to move it into the compactor package to minimize the chance that someone accidentally uses it by mistake. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-21T00:53:37.013+0000","updated":"2015-01-21T00:53:37.013+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14286267","id":"14286267","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vikram.dixit","name":"vikram.dixit","key":"vikram.dixit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vikram Dixit K","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 for a branch 1.0.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vikram.dixit","name":"vikram.dixit","key":"vikram.dixit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vikram Dixit K","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-21T21:04:30.805+0000","updated":"2015-01-21T21:04:30.805+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14290349","id":"14290349","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Final version of the patch.  Moved ValidCompactorTxnList per Owen's request.  Also made small changes to StreamingIntegrationTester to make it work properly in cases where you want it to go slowly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-24T01:39:20.944+0000","updated":"2015-01-24T01:39:20.944+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14290583","id":"14290583","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 at least one tests failed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12694321/HIVE-8966.6.patch\n\n{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 7370 tests executed\n*Failed tests:*\n{noformat}\nTestSparkCliDriver-parallel_join1.q-avro_joins.q-groupby_ppr.q-and-12-more - did not produce a TEST-*.xml file\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2506/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2506/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2506/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 1 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12694321 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2015-01-24T12:54:10.832+0000","updated":"2015-01-24T12:54:10.832+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14292593","id":"14292593","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"Looks like this was committed but I am seeing:\n\n{noformat}\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-common: Compilation failure: Compilation failure:\n[ERROR] /Users/noland/workspaces/hive-apache/hive/common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java:[23,8] org.apache.hadoop.hive.common.ValidTxnListImpl is not abstract and does not override abstract method getInvalidTransactions() in org.apache.hadoop.hive.common.ValidTxnList\n[ERROR] /Users/noland/workspaces/hive-apache/hive/common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java:[46,3] method does not override or implement a method from a supertype\n[ERROR] /Users/noland/workspaces/hive-apache/hive/common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java:[54,3] method does not override or implement a method from a supertype\n[ERROR] /Users/noland/workspaces/hive-apache/hive/common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java:[121,3] method does not override or implement a method from a supertype\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-common\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-26T23:11:51.093+0000","updated":"2015-01-26T23:11:51.093+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14292601","id":"14292601","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"I did svn add instead of svn rm on a couple of files that moved.  I'll fix it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-26T23:19:30.954+0000","updated":"2015-01-26T23:19:30.954+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14292615","id":"14292615","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"thx","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-26T23:24:42.194+0000","updated":"2015-01-26T23:24:42.194+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14292622","id":"14292622","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Fixed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-26T23:31:58.682+0000","updated":"2015-01-26T23:31:58.682+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14294010","id":"14294010","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Patch 6 checked into trunk.  Patch marked for branch 1 checked into branch 1.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-27T19:19:39.200+0000","updated":"2015-01-27T19:19:39.200+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14294911","id":"14294911","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"Any documentation needed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2015-01-28T09:17:18.679+0000","updated":"2015-01-28T09:17:18.679+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14295328","id":"14295328","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"[~leftylev] no, we just made what should have worked before work properly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-28T16:11:06.596+0000","updated":"2015-01-28T16:11:06.596+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14297958","id":"14297958","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"body":"Thanks Alan.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jihongliu","name":"jihongliu","key":"jihongliu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jihong Liu","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-30T00:31:57.915+0000","updated":"2015-01-30T00:31:57.915+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14297960","id":"14297960","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"Does this also need to be checked into branch-1.1 (formerly known as 0.15)?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2015-01-30T00:32:40.525+0000","updated":"2015-01-30T00:32:40.525+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14298910","id":"14298910","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"I confirmed that it is already in 1.1, based on the git logs.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-30T17:35:41.852+0000","updated":"2015-01-30T17:35:41.852+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14327567","id":"14327567","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"Updating release version for jiras resolved in 1.0.0 .\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-19T14:45:03.499+0000","updated":"2015-02-19T14:45:03.499+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12757795/comment/14327880","id":"14327880","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"This issue has been fixed in Apache Hive 1.0.0. If there is any issue with the fix, please open a new jira to address it.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-19T18:22:01.729+0000","updated":"2015-02-19T18:22:01.729+0000"}],"maxResults":58,"total":58,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-8966/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i22s3j:"}}