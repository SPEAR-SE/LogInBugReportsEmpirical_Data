{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13119145","self":"https://issues.apache.org/jira/rest/api/2/issue/13119145","key":"HIVE-18090","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340268","id":"12340268","name":"3.0.0","archived":false,"released":true,"releaseDate":"2018-05-21"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-11-17T18:33:48.327+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue May 22 23:13:26 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_1225044_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_422716667","customfield_12312321":null,"resolutiondate":"2017-11-22T06:03:55.967+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-18090/watchers","watchCount":3,"isWatching":false},"created":"2017-11-17T08:18:14.300+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332154","id":"12332154","description":"","name":"1.3.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12332641","id":"12332641","description":"Hive 2.0.0","name":"2.0.0","archived":false,"released":true,"releaseDate":"2016-02-15"}],"issuelinks":[{"id":"12520665","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12520665","type":{"id":"12310050","name":"Regression","inward":"is broken by","outward":"breaks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310050"},"inwardIssue":{"id":"12911397","key":"HIVE-12366","self":"https://issues.apache.org/jira/rest/api/2/issue/12911397","fields":{"summary":"Refactor Heartbeater logic for transaction","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-05-22T23:13:26.282+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12320408","id":"12320408","name":"HiveServer2","description":"Tracks issues related to HiveServer2"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12322671","id":"12322671","name":"Transactions","description":"Transaction management and ACID"}],"timeoriginalestimate":null,"description":"steps to recreate the issue. assuming two users \r\n* test\r\n* another \r\n\r\ncreate two jceks files for each user and place them on hdfs with access to that file only allowed to the user. hdfs locations with permissions \r\n{code}\r\n-rwx------   1 another another        492 2017-11-16 13:06 /user/another/another.jceks\r\n-rwx------   1 test test        489 2017-11-16 13:05 /user/test/test.jceks\r\n{code}\r\n\r\npassword used to create \r\n* /user/another/another.jceks -- another\r\n* /user/test/test.jceks -- test\r\n\r\non core-site.xml \r\n{code}\r\n    <property>\r\n        <name>hadoop.proxyuser.[superuser].hosts</name>\r\n        <value>*</value>\r\n    </property>\r\n    <property>\r\n        <name>hadoop.proxyuser.[superuser].groups</name>\r\n        <value>*</value>\r\n    </property>\r\n{code}\r\nand restart hdfs.\r\nenable ACID on HS2 (change the required properties).additional changes on  hiveserver2 configs \r\n{code}\r\n* hive.metastore.warehouse.dir=file:///tmp/hive/test-warehouse\r\n* hive.server2.enable.doAs=true\r\n* remove javax.jdo.option.ConnectionPassword property from hive-site.xml\r\n{code}\r\nstart hiveserver2\r\n\r\nconnect to the server using beeline using any user:\r\n{code}\r\ncreate table a (i int, b string);\r\ninsert into a values (0 , '0'), (1 , '1'), (2 , '2'), (3 , '3'), (4 , '4'), (5 , '5'), (6 , '6'), (7 , '7'), (8 , '8'), (9 , '9'), (10 , '10'), (11 , '11'), (12 , '12'), (13 , '13'), (14 , '14'), (15 , '15'), (16 , '16'), (17 , '17'), (18 , '18'), (19 , '19'), (20 , '20'), (21 , '21'), (22 , '22'), (23 , '23'), (24 , '24'), (25 , '25'), (26 , '26'), (27 , '27'), (28 , '28'), (29 , '29'), (30 , '30'), (31 , '31'), (32 , '32'), (33 , '33'), (34 , '34'), (35 , '35'), (36 , '36'), (37 , '37'), (38 , '38'), (39 , '39'), (40 , '40'), (41 , '41'), (42 , '42'), (43 , '43'), (44 , '44'), (45 , '45'), (46 , '46'), (47 , '47'), (48 , '48'), (49 , '49'), (50 , '50'), (51 , '51'), (52 , '52'), (53 , '53'), (54 , '54'), (55 , '55'), (56 , '56'), (57 , '57'), (58 , '58'), (59 , '59'), (60 , '60'), (61 , '61'), (62 , '62'), (63 , '63'), (64 , '64'), (65 , '65'), (66 , '66'), (67 , '67'), (68 , '68'), (69 , '69'), (70 , '70'), (71 , '71'), (72 , '72'), (73 , '73'), (74 , '74'), (75 , '75'), (76 , '76'), (77 , '77'), (78 , '78'), (79 , '79'), (80 , '80'), (81 , '81'), (82 , '82'), (83 , '83'), (84 , '84'), (85 , '85'), (86 , '86'), (87 , '87'), (88 , '88'), (89 , '89'), (90 , '90'), (91 , '91'), (92 , '92'), (93 , '93'), (94 , '94'), (95 , '95'), (96 , '96'), (97 , '97'), (98 , '98'), (99 , '99');\r\n{code}\r\n\r\nexit beeline and connect with user another \r\n{code}\r\n./beeline -u \"jdbc:hive2://localhost:10000/default?hive.strict.checks.cartesian.product=false;hive.txn.timeout=4s;hive.txn.heartbeat.threadpool.size=1;hadoop.security.credential.provider.path=jceks://hdfs/user/another/another.jceks;ssl.server.keystore.keypassword=another\" -n another\r\n\r\ncreate table another_a_acid (i int, b string) clustered by (i) into 8 buckets stored as orc tblproperties('transactional'='true');\r\n\r\ninsert overwrite table another_a_acid select a2.i, a3.b from a a1 join a a2 join a a3 on 1=1;\r\n{code}\r\n\r\nopen another beeline session with user test:\r\n{code}\r\n./beeline -u \"jdbc:hive2://localhost:10000/default?hive.strict.checks.cartesian.product=false;hive.txn.timeout=4s;hive.txn.heartbeat.threadpool.size=1;hadoop.security.credential.provider.path=jceks://hdfs/user/test/test.jceks;ssl.server.keystore.keypassword=test\" -n test\r\n\r\ncreate table a_acid (i int, b string) clustered by (i) into 8 buckets stored as orc tblproperties('transactional'='true');\r\n\r\ninsert overwrite table a_acid select a2.i, a3.b from a a1 join a a2 join a a3 on 1=1;\r\n{code}\r\n\r\nfails with exception \r\n{code}\r\n2017-11-17T12:15:52,664 DEBUG [Heartbeater-1] retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over null. Not retrying because try once and fail.\r\norg.apache.hadoop.ipc.RemoteException: Permission denied: user=test, access=EXECUTE, inode=\"/user/another/another.jceks\":another:another:drwx------\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:259)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:205)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:109)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4111)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\r\n\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554) ~[hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498) ~[hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398) ~[hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat com.sun.proxy.$Proxy30.getFileInfo(Unknown Source) ~[?:?]\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:818) ~[hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source) ~[?:?]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat com.sun.proxy.$Proxy31.getFileInfo(Unknown Source) [?:?]\r\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2165) [hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442) [hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438) [hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1438) [hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1447) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.keystoreExists(JavaKeyStoreProvider.java:65) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.<init>(AbstractJavaKeyStoreProvider.java:105) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:49) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:41) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:100) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:61) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1992) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1972) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.hive.metastore.conf.MetastoreConf.getPassword(MetastoreConf.java:1334) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:571) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:312) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:59) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:677) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:643) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:637) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:544) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source) ~[?:?]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:80) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:93) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:7516) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:169) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:77) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat sun.reflect.GeneratedConstructorAccessor148.newInstance(Unknown Source) [?:1.8.0_112]\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.8.0_112]\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423) [?:1.8.0_112]\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1445) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4051) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4103) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4083) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getMS(DbTxnManager.java:158) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.heartbeat(DbTxnManager.java:610) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.run(DbTxnManager.java:878) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_112]\r\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_112]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_112]\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_112]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]\r\n2017-11-17T12:15:52,670 ERROR [Heartbeater-1] metastore.RetryingHMSHandler: java.lang.RuntimeException: Error getting metastore password: Configuration problem with provider path.\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:577)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:312)\r\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\r\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:59)\r\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:677)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:643)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:637)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:544)\r\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:80)\r\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:93)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:7516)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:169)\r\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:77)\r\n\tat sun.reflect.GeneratedConstructorAccessor148.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1445)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4051)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4103)\r\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4083)\r\n\tat org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getMS(DbTxnManager.java:158)\r\n\tat org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.heartbeat(DbTxnManager.java:610)\r\n\tat org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.run(DbTxnManager.java:878)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.IOException: Configuration problem with provider path.\r\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:2012)\r\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1972)\r\n\tat org.apache.hadoop.hive.metastore.conf.MetastoreConf.getPassword(MetastoreConf.java:1334)\r\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:571)\r\n\t... 39 more\r\nCaused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=test, access=EXECUTE, inode=\"/user/another/another.jceks\":another:another:drwx------\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:259)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:205)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:109)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4111)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\r\n\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\r\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\r\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2167)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1438)\r\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1447)\r\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.keystoreExists(JavaKeyStoreProvider.java:65)\r\n\tat org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.<init>(AbstractJavaKeyStoreProvider.java:105)\r\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:49)\r\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:41)\r\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:100)\r\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:61)\r\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1992)\r\n\t... 42 more\r\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=test, access=EXECUTE, inode=\"/user/another/another.jceks\":another:another:drwx------\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:259)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:205)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:109)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4111)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\r\n\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\tat com.sun.proxy.$Proxy30.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:818)\r\n\tat sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\r\n\tat com.sun.proxy.$Proxy31.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2165)\r\n\t... 54 more\r\n\r\n{code}\r\n\r\nabove will only help in recreating the issue, if the _insert overwrite_ query takes longer than _hive.txn.timeout / 2 = 4 / 2 = 2seconds_","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12898154","id":"12898154","filename":"HIVE-18090.0.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-11-17T08:37:29.732+0000","size":5706,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12898154/HIVE-18090.0.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"acid heartbeat fails when metastore is connected via hadoop credential","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13119145/comment/16256655","id":"16256655","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"body":"to start a test run and see the results, [~thejas]/[~ekoifman] any suggestions as to how do have an automated test for this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-11-17T08:38:39.327+0000","updated":"2017-11-17T08:38:39.327+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13119145/comment/16257364","id":"16257364","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12898154/HIVE-18090.0.patch\n\n{color:red}ERROR:{color} -1 due to no test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 11383 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=47)\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dbtxnmgr_showlocks] (batchId=77)\norg.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[unionDistinct_1] (batchId=146)\norg.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=162)\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=102)\norg.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=223)\norg.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema.testSyntheticComplexSchema[2] (batchId=187)\norg.apache.hive.hcatalog.pig.TestSequenceFileHCatStorer.testWriteChar (batchId=187)\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/7888/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/7888/console\nTest logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7888/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 8 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12898154 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2017-11-17T18:33:48.327+0000","updated":"2017-11-17T18:33:48.327+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13119145/comment/16257735","id":"16257735","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"body":"you have \r\n{noformat}\r\ntblproperties('transactional'='tur');\r\n{noformat}\r\nI assume this is just a typo?\r\n\r\nAlso, _above will only work if the insert overwrite query takes longer than hive.txn.timeout / 2 = 4 / 2 = 2seconds_ - I assume \"work\" means \"reproduce\"?\r\n\r\nThe thread pool for heartbeating DbTxnManager.heartbeatExecutorService is static so if you are able to create 2 sessions with different users, the pool should have the tread alive with User1 when User2 issues a request, I think.\r\n\r\notherwise the patch LGTM\r\n+1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-17T23:19:10.288+0000","updated":"2017-11-17T23:19:10.288+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13119145/comment/16260895","id":"16260895","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"body":"HIVE-12366 is where shared thread pool for heartbeat was introduced","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-21T15:25:36.735+0000","updated":"2017-11-21T15:25:36.735+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13119145/comment/16261997","id":"16261997","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"body":"Thanks for the review [~ekoifman], Fixed the typos in \"Description\", Going to do a quick look at tests failures once before i commit, cant access the apache logs.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-11-22T05:46:15.361+0000","updated":"2017-11-22T05:46:15.361+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13119145/comment/16262013","id":"16262013","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"body":"Patch committed to master !","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=anishek","name":"anishek","key":"anishek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"anishek","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-11-22T06:03:56.001+0000","updated":"2017-11-22T06:03:56.001+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13119145/comment/16484912","id":"16484912","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"body":"This jira is resolved and released with Hive 3.0 If you find an issue with it, please create a new jira.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-22T23:13:26.273+0000","updated":"2018-05-22T23:13:26.273+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-18090/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3mwwn:"}}