{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13186849","self":"https://issues.apache.org/jira/rest/api/2/issue/13186849","key":"HIVE-20624","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":null,"customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sun Sep 23 01:43:30 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-20624/watchers","watchCount":2,"isWatching":false},"created":"2018-09-23T01:40:42.082+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-09-23T01:43:30.431+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12321300","id":"12321300","name":"Vectorization","description":"Vectorized query execution"}],"timeoriginalestimate":null,"description":"The reason to use Power of two is to simplify the inner loop for the hash function, but this is a significant memory issue when dealing with somewhat larger hashtables like the customer or customer_address hashtables in TPC-DS.\r\n\r\nThis doubling is pretty bad when the hashtable load-factor is 0.75 and the expected key count is 65M (for customer/customer_address)\r\n\r\n{code}\r\n    long worstCaseNeededSlots = 1L << DoubleMath.log2(numRows / hashTableLoadFactor, RoundingMode.UP);\r\n{code}\r\n\r\nThat estimate is actually matching the actual impl, but after acquiring 65M items in a single array, the rehashing will require a temporary growth to 65+128 while the rehash is in progress, all to fit exactly 65 back into it.\r\n\r\nFixing the estimate to match the implementation produced a number of regressions in query runtimes, though the part that needs fixing is the doubling implementation.\r\n\r\nThe obvious solution is to add 4M more everytime and use a modulo function or the Lemire's multiply + shift operation[1], but more on that in comments.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Vectorization: Fast Hash table should not double after certain size, instead should grow","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13186849/comment/16624893","id":"16624893","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"body":"There is one slightly complicated way to do this without reallocating the original hash probe arrays - but by doing bucketing in a slightly strange way.\r\n\r\nBeyond 4M entries in a single hash table, it is better to bucket the actual probe array out, though it is not memory efficient to pre-bucket them into n-hashtables (like what the hybrid grace did).\r\n\r\nSo instead of rehashing the entire hashtable, you can \"add a bucket\" and only move the entries which fall out of a bucket, but make that movement minimal during rehashing (i.e the entries which are removed are swapped out with the new bucket).\r\n\r\nTo be very specific, the approach is only necessary when the hashtable is oversized and not when it has less than 4M items, so after 4M items have been added, the next step is to add another smaller probe array & move everything from bucket 0 -> bucket 1 that swapped places (there's no data in bucket 1 to move backwards).\r\n\r\nThe next rehash is more complicated, because it needs to potentially swap items from bucket 0 to 1 and back, after moving new items to bucket 2 from both, which needs to make sure that enough data moves into the new bucket when rebucketing.\r\n\r\nBecause we have sequential bucketing here, there is a neat google algorithm to fall back up on (here's the code in Golang for JumpHash).\r\n\r\n{code}\r\nfunc Hash(key uint64, numBuckets int) int32 {\r\n  var b int64 = -1\r\n  var j int64\r\n  for j < int64(numBuckets) {\r\n    b = j\r\n    key = key*2862933555777941757 + 1\r\n    j = int64(float64(b+1) *\r\n      (float64(int64(1)<<31) / float64((key>>33)+1)))\r\n  }\r\n  return int32(b)\r\n}\r\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gopalv","name":"gopalv","key":"gopalv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gopal V","active":true,"timeZone":"Asia/Kolkata"},"created":"2018-09-23T01:43:30.431+0000","updated":"2018-09-23T01:43:30.431+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-20624/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3ye2v:"}}