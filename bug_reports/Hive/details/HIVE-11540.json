{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12855717","self":"https://issues.apache.org/jira/rest/api/2/issue/12855717","key":"HIVE-11540","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332154","id":"12332154","description":"","name":"1.3.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12332641","id":"12332641","description":"Hive 2.0.0","name":"2.0.0","archived":false,"released":true,"releaseDate":"2016-02-15"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2015-08-12T23:15:20.531+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Oct 26 06:47:12 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_6068131042_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_308286545","customfield_12312321":null,"resolutiondate":"2015-10-25T18:25:55.978+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-11540/watchers","watchCount":4,"isWatching":false},"created":"2015-08-12T23:12:18.445+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["TODOC1.3"],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329278","id":"12329278","description":"Branch 1.0 release","name":"1.0.0","archived":false,"released":true,"releaseDate":"2015-02-04"}],"issuelinks":[{"id":"12448612","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12448612","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"outwardIssue":{"id":"12912709","key":"HIVE-12403","self":"https://issues.apache.org/jira/rest/api/2/issue/12912709","fields":{"summary":"Too many delta files during Compaction - OOM Part Deux","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-02-16T23:51:33.924+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12322671","id":"12322671","name":"Transactions","description":"Transaction management and ACID"}],"timeoriginalestimate":null,"description":"Hello,\n\nI am streaming weblogs to Kafka and then to Flume 1.6 using a Hive sink, with an average of 20 million records a day. I have 5 compactors running at various times (30m/5m/5s), no matter what time I give, the compactors seem to run out of memory cleaning up a couple thousand delta files and ultimately falls behind compacting/cleaning delta files. Any suggestions on what I can do to improve performance? Or can Hive streaming not handle this kind of load?\n\nI used this post as reference: http://henning.kropponline.de/2015/05/19/hivesink-for-flume/\n\n{noformat}\n2015-08-12 15:05:01,197 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Direct buffer memory\n\nMax block location exceeded for split: CompactorInputSplit{base: hdfs://Dev01HWNameService/user/hive/warehouse/weblogs.db/dt=15-08-12/base_1056406, bucket: 0, length: 6493042, deltas: [delta_1056407_1056408, delta_1056409_1056410, delta_1056411_1056412, delta_1056413_1056414, delta_1056415_1056416, delta_1056417_1056418,â€¦\n, delta_1074039_1074040, delta_1074041_1074042, delta_1074043_1074044, delta_1074045_1074046, delta_1074047_1074048, delta_1074049_1074050, delta_1074051_1074052]} splitsize: 8772 maxsize: 10\n2015-08-12 15:34:25,271 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:3\n2015-08-12 15:34:25,367 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_1439397150426_0068\n2015-08-12 15:34:25,603 INFO  [upladevhwd04v.researchnow.com-18]: impl.YarnClientImpl (YarnClientImpl.java:submitApplication(274)) - Submitted application application_1439397150426_0068\n2015-08-12 15:34:25,610 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://upladevhwd02v.researchnow.com:8088/proxy/application_1439397150426_0068/\n2015-08-12 15:34:25,611 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_1439397150426_0068\n2015-08-12 15:34:30,170 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:33,756 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_1439397150426_0068 running in uber mode : false\n2015-08-12 15:34:33,757 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%\n2015-08-12 15:34:35,147 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:40,155 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:45,184 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:50,201 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:55,256 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:00,205 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:02,975 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 33% reduce 0%\n2015-08-12 15:35:02,982 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_0, Status : FAILED\n2015-08-12 15:35:03,000 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_0, Status : FAILED\n2015-08-12 15:35:04,008 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%\n2015-08-12 15:35:05,132 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:10,206 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:15,228 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:20,207 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:25,148 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:28,154 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_1, Status : FAILED\n2015-08-12 15:35:29,161 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_1, Status : FAILED\n2015-08-12 15:35:30,142 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:35,140 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:40,170 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:45,153 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:50,150 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:52,268 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_2, Status : FAILED\n2015-08-12 15:35:53,274 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_2, Status : FAILED\n2015-08-12 15:35:55,149 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:00,160 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:05,145 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:10,155 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:15,158 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:17,397 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%\n2015-08-12 15:36:18,409 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - Job job_1439397150426_0068 failed with state FAILED due to: Task failed task_1439397150426_0068_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n\n2015-08-12 15:36:18,443 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 10\n\tJob Counters \n\t\tFailed map tasks=7\n\t\tKilled map tasks=1\n\t\tLaunched map tasks=8\n\t\tOther local map tasks=6\n\t\tData-local map tasks=2\n\t\tTotal time spent by all maps in occupied slots (ms)=191960\n\t\tTotal time spent by all reduces in occupied slots (ms)=0\n\t\tTotal time spent by all map tasks (ms)=191960\n\t\tTotal vcore-seconds taken by all map tasks=191960\n\t\tTotal megabyte-seconds taken by all map tasks=884551680\n2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)\n\n2015-08-12 15:36:18,444 ERROR [upladevhwd04v.researchnow.com-18]: txn.CompactionTxnHandler (CompactionTxnHandler.java:markCleaned(327)) - Expected to remove at least one row from completed_txn_components when marking compaction entry as clean!\n^C\n{noformat}\n[ngmathew@upladevhwd04v ~]$ tail -f /var/log/hive/hivemetastore.log\n2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)\n\n\n\nSettings:\nhive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\nhive.compactor.initiator.on = true\nhive.compactor.worker.threads = 5\nTable stored as ORC\nhive.vectorized.execution.enabled = false\nhive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12768194","id":"12768194","filename":"HIVE-11540.3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-10-23T01:57:46.344+0000","size":18629,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12768194/HIVE-11540.3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12768469","id":"12768469","filename":"HIVE-11540.4.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-10-24T00:59:58.525+0000","size":18629,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12768469/HIVE-11540.4.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12768544","id":"12768544","filename":"HIVE-11540.6.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-10-24T17:34:57.116+0000","size":21555,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12768544/HIVE-11540.6.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12767962","id":"12767962","filename":"HIVE-11540.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-10-22T04:45:48.749+0000","size":18503,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12767962/HIVE-11540.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Too many delta files during Compaction - OOM","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nivinm","name":"nivinm","key":"nivinm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nivin Mathew","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nivinm","name":"nivinm","key":"nivinm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nivin Mathew","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14694397","id":"14694397","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"cc [~owen.omalley], [~ekoifman]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-12T23:15:20.531+0000","updated":"2015-08-12T23:15:20.531+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14694598","id":"14694598","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nivinm","name":"nivinm","key":"nivinm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nivin Mathew","active":true,"timeZone":"Etc/UTC"},"body":"To add, the compactor definitely is not running after 10 delta files. Possibly because of the volume of deltas coming in, but I would think right after the 11th delta, hivemetastore.log should kick off a minor compaction instead of the check interval time for minor compactions. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nivinm","name":"nivinm","key":"nivinm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nivin Mathew","active":true,"timeZone":"Etc/UTC"},"created":"2015-08-13T02:58:41.618+0000","updated":"2015-08-13T02:58:41.618+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14706691","id":"14706691","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nivinm","name":"nivinm","key":"nivinm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nivin Mathew","active":true,"timeZone":"Etc/UTC"},"body":"Update from my side. I got the streaming working after changing the flume configs for transactions. So i dont get \"too many files\" now. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nivinm","name":"nivinm","key":"nivinm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nivin Mathew","active":true,"timeZone":"Etc/UTC"},"created":"2015-08-21T12:32:17.491+0000","updated":"2015-08-21T12:32:17.491+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14968442","id":"14968442","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"body":"there are 2 things happening here:\nOOM\nand \n{noformat}\nMax block location exceeded for split: CompactorInputSplit\n{base: hdfs://Dev01HWNameService/user/hive/warehouse/weblogs.db/dt=15-08-12/base_1056406,\n bucket: 0, length: 6493042, deltas: [delta_1056407_1056408, delta_1056409_1056410, ... \ndelta_1074047_1074048, delta_1074049_1074050, delta_1074051_1074052]}\nsplitsize: 8772 maxsize: 10\n{noformat}\n\nThe latter is discussed in http://mail-archives.apache.org/mod_mbox/hadoop-user/201309.mbox/%3CCADCZBhGD5EE7d+bkTBUHvYi3Rq40JfY+KnT5Jbdv=aPJAaLBjA@mail.gmail.com%3E\nand is not in itself an error but does mean the job is running less efficiently\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-10-22T03:27:10.363+0000","updated":"2015-10-22T03:27:10.363+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14970142","id":"14970142","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12767962/HIVE-11540.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5741/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5741/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5741/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]\n+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera\n+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera\n+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin\n+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-5741/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ cd apache-github-source-source\n+ git fetch origin\nFrom https://github.com/apache/hive\n   552bfbc..51a0c03  branch-1   -> origin/branch-1\n   2fd619b..3f03d26  master     -> origin/master\n+ git reset --hard HEAD\nHEAD is now at 2fd619b HIVE-11895: CBO: Calcite Operator To Hive Operator (Calcite Return Path): fix udaf_percentile_approx_23.q (Pengcheng Xiong, reviewed by Ashutosh Chauhan)\n+ git clean -f -d\nRemoving data/files/parquet_type_promotion.txt\nRemoving ql/src/test/queries/clientpositive/parquet_type_promotion.q\nRemoving ql/src/test/results/clientpositive/parquet_type_promotion.q.out\n+ git checkout master\nAlready on 'master'\nYour branch is behind 'origin/master' by 3 commits, and can be fast-forwarded.\n+ git reset --hard origin/master\nHEAD is now at 3f03d26 HIVE-11710: Beeline embedded mode doesn't output query progress after setting any session property (Aihua via Xuefu)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ git gc\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12767962 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2015-10-22T23:59:19.333+0000","updated":"2015-10-22T23:59:19.333+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14970389","id":"14970389","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"Changes look good to me, +1 assuming you can get the patch to run through the tests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-10-23T03:58:16.975+0000","updated":"2015-10-23T03:58:16.975+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14971983","id":"14971983","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12768194/HIVE-11540.3.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5756/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5756/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5756/\n\nMessages:\n{noformat}\n**** This message was trimmed, see log for full details ****\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[68,13] error: cannot find symbol\n[ERROR] interface HBaseConnection\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[84,2] error: cannot find symbol\n[ERROR] interface HBaseConnection\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[94,2] error: cannot find symbol\n[ERROR] interface HBaseConnection\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[27,30] error: package org.apache.hadoop.hbase does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[28,30] error: package org.apache.hadoop.hbase does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[29,30] error: package org.apache.hadoop.hbase does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[30,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[31,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[32,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[33,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[34,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[35,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[36,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[37,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[38,37] error: package org.apache.hadoop.hbase.filter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[39,37] error: package org.apache.hadoop.hbase.filter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[40,37] error: package org.apache.hadoop.hbase.filter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[41,37] error: package org.apache.hadoop.hbase.filter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/PartitionKeyComparator.java:[30,37] error: package org.apache.hadoop.hbase.filter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/PartitionKeyComparator.java:[45,44] error: cannot find symbol\n[ERROR] class ByteArrayComparable\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[22,37] error: package org.apache.hadoop.hbase.client does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[749,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[810,55] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2006,10] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2025,19] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2030,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2029,19] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2034,46] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2034,19] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2039,58] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2038,19] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[68,13] error: cannot find symbol\n[ERROR] interface HBaseConnection\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[84,2] error: cannot find symbol\n[ERROR] interface HBaseConnection\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[94,2] error: cannot find symbol\n[ERROR] interface HBaseConnection\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/PartitionKeyComparator.java:[179,2] error: method does not override or implement a method from a supertype\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/PartitionKeyComparator.java:[223,2] error: method does not override or implement a method from a supertype\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[210,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[354,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[356,19] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[356,42] error: package CompareFilter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[356,64] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[358,13] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[362,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[420,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[422,19] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[422,42] error: package CompareFilter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[422,64] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[424,13] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[428,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[512,10] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[512,35] error: unexpected type\n[ERROR] \n[ERROR] E extends Object declared in class ArrayList\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[515,7] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[515,21] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[519,5] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[520,5] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[580,9] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[580,34] error: unexpected type\n[ERROR] \n[ERROR] E extends Object declared in class ArrayList\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[586,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[586,18] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[591,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[600,9] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[600,34] error: unexpected type\n[ERROR] \n[ERROR] E extends Object declared in class ArrayList\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[614,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[614,18] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[624,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[729,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[731,19] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[731,42] error: package CompareFilter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[812,13] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[818,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[867,9] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[867,34] error: unexpected type\n[ERROR] \n[ERROR] E extends Object declared in class ArrayList\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[868,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[871,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[871,18] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[876,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[916,13] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[918,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1055,9] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1055,34] error: unexpected type\n[ERROR] \n[ERROR] E extends Object declared in class ArrayList\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1070,8] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1070,22] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1082,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1104,8] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1104,22] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1111,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1127,12] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1127,26] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1136,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1161,13] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1164,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1194,15] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1196,8] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1244,9] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1244,34] error: unexpected type\n[ERROR] \n[ERROR] E extends Object declared in class ArrayList\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1245,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1249,6] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1249,18] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1253,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1286,4] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1288,19] error: cannot find symbol\n[ERROR] class HBaseReadWrite\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1288,42] error: package CompareFilter does not exist\n[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1288,64] error: cannot find symbol\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-metastore\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12768194 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2015-10-23T22:13:14.924+0000","updated":"2015-10-23T22:13:14.924+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14972412","id":"14972412","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12768469/HIVE-11540.4.patch\n\n{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 9705 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.testHasNull\norg.apache.hadoop.hive.ql.io.orc.TestJsonFileDump.testJsonDump\norg.apache.hadoop.hive.ql.txn.compactor.TestWorker2.majorNoBaseLotsOfDeltas\norg.apache.hadoop.hive.ql.txn.compactor.TestWorker2.minorNoBaseLotsOfDeltas\norg.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation\norg.apache.hive.jdbc.TestSSL.testSSLVersion\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5772/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5772/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5772/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 6 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12768469 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2015-10-24T05:35:00.735+0000","updated":"2015-10-24T05:35:00.735+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14972936","id":"14972936","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12768544/HIVE-11540.6.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 9708 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_index_bitmap3\norg.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter_partitioned\norg.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.testHasNull\norg.apache.hadoop.hive.ql.io.orc.TestJsonFileDump.testJsonDump\norg.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation\norg.apache.hive.jdbc.TestSSL.testSSLVersion\n{noformat}\n\nTest results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5782/testReport\nConsole output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5782/console\nTest logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5782/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 6 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12768544 - PreCommit-HIVE-TRUNK-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2015-10-25T00:48:05.343+0000","updated":"2015-10-25T00:48:05.343+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14972970","id":"14972970","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"Doc note:  This adds configuration parameter *hive.compactor.max.num.delta* to HiveConf.java, so it needs to be documented for release 2.0.0 in the Transactions and Compactor section of Configuration Properties.\n\n* [Configuration Properties -- Transactions and Compactor | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-TransactionsandCompactor]\n\n(And by the way, this needs a status update.  But maybe it's also going into the 1.3.0 release, in which case the TODOC2.0 label should be changed to TODOC1.3.)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2015-10-25T02:59:06.433+0000","updated":"2015-10-25T02:59:06.433+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14973361","id":"14973361","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"body":"Committed to master: https://github.com/apache/hive/commit/e3ef96f2b83ffa932dd59fc3df79dff8747309ba and branch-1 \nhttps://github.com/apache/hive/commit/e654efeb32c62fb5cd56214b823526173cb009bb\n\nThanks [~alangates] for the review","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ekoifman","name":"ekoifman","key":"ekoifman","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eugene Koifman","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-10-25T18:25:37.817+0000","updated":"2015-10-25T18:25:37.817+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12855717/comment/14973819","id":"14973819","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"Okay, changed TODOC2.0 to TODOC1.3.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2015-10-26T06:47:12.859+0000","updated":"2015-10-26T06:47:12.859+0000"}],"maxResults":12,"total":12,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-11540/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2iupr:"}}