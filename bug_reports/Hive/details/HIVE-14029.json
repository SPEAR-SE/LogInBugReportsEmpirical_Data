{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12979634","self":"https://issues.apache.org/jira/rest/api/2/issue/12979634","key":"HIVE-14029","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340269","id":"12340269","name":"2.3.0","archived":false,"released":true,"releaseDate":"2017-07-18"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2016-06-16T19:59:23.136+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Oct 18 03:42:00 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_110560100_*|*_3_*:*_1_*:*_8009142864_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_862928364","customfield_12312321":null,"resolutiondate":"2016-09-28T01:22:01.410+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-14029/watchers","watchCount":21,"isWatching":false},"created":"2016-06-16T02:11:30.456+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["Incompatible","TODOC2.2"],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"9.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12480678","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12480678","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"inwardIssue":{"id":"13005467","key":"HIVE-14777","self":"https://issues.apache.org/jira/rest/api/2/issue/13005467","fields":{"summary":"Add support of Spark-2.0.0 in Hive-2.X.X","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/5","id":"5","description":"General wishlist item.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Wish","subtask":false,"avatarId":21140}}}},{"id":"12482456","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12482456","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13010907","key":"HIVE-14919","self":"https://issues.apache.org/jira/rest/api/2/issue/13010907","fields":{"summary":"Improve the performance of Hive on Spark 2.0.0","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12481117","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12481117","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12919145","key":"SPARK-12154","self":"https://issues.apache.org/jira/rest/api/2/issue/12919145","fields":{"summary":"Upgrade to Jersey 2","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}},{"id":"12480679","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12480679","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13005420","key":"SPARK-17563","self":"https://issues.apache.org/jira/rest/api/2/issue/13005420","fields":{"summary":"Add org/apache/spark/JavaSparkListener to make Spark-2.0.0 work with Hive-2.X.X","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12481097","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12481097","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13007015","key":"HIVE-14825","self":"https://issues.apache.org/jira/rest/api/2/issue/13007015","fields":{"summary":"Figure out the minimum set of required jars for Hive on Spark after bumping up to Spark 2.0.0","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/3","id":"3","description":"A task that needs to be done.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype","name":"Task","subtask":false,"avatarId":21148}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-21T17:55:19.397+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"There are quite some new optimizations in Spark 2.0.0. We need to bump up Spark to 2.0.0 to benefit those performance improvements.\nTo update Spark version to 2.0.0, the following changes are required:\n\n* Spark API updates:\n** SparkShuffler#call return Iterator instead of Iterable\n** SparkListener -> JavaSparkListener\n** InputMetrics constructor doesn’t accept readMethod\n** Method remoteBlocksFetched and localBlocksFetched in ShuffleReadMetrics return long type instead of integer\n\n* Dependency upgrade:\n** Jackson: 2.4.2 -> 2.6.5\n** Netty version: 4.0.23.Final -> 4.0.29.Final\n** Scala binary version: 2.10 -> 2.11\n** Scala version: 2.10.4 -> 2.11.8\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12829398","id":"12829398","filename":"HIVE-14029.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-20T14:21:45.600+0000","size":33133,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12829398/HIVE-14029.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12829489","id":"12829489","filename":"HIVE-14029.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T02:57:39.564+0000","size":24839,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12829489/HIVE-14029.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12829601","id":"12829601","filename":"HIVE-14029.3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T15:44:29.168+0000","size":24245,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12829601/HIVE-14029.3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12829777","id":"12829777","filename":"HIVE-14029.4.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-22T06:44:30.810+0000","size":24491,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12829777/HIVE-14029.4.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12829975","id":"12829975","filename":"HIVE-14029.5.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-23T02:01:57.499+0000","size":24925,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12829975/HIVE-14029.5.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12830171","id":"12830171","filename":"HIVE-14029.6.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-24T04:09:22.812+0000","size":26458,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12830171/HIVE-14029.6.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12830260","id":"12830260","filename":"HIVE-14029.7.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-26T07:58:19.217+0000","size":28300,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12830260/HIVE-14029.7.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12830453","id":"12830453","filename":"HIVE-14029.8.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-27T07:20:30.627+0000","size":27927,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12830453/HIVE-14029.8.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12829100","id":"12829100","filename":"HIVE-14029.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-18T08:47:41.130+0000","size":23418,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12829100/HIVE-14029.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Update Spark version to 2.0.0","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15332903","id":"15332903","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"cc [~xuefuz]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-06-16T02:11:49.233+0000","updated":"2016-06-16T02:11:49.233+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15334568","id":"15334568","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"Maybe we'll also get newer Hadoop version in the damn tgz file so we can actually upgrade that too!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-16T19:59:23.136+0000","updated":"2016-06-16T19:59:23.136+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15335510","id":"15335510","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~spena] do you know how to update the tgz file http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-${spark.version}-bin-hadoop2-without-hive.tgz with a newly built version? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-06-17T06:23:54.648+0000","updated":"2016-06-17T06:23:54.648+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15336230","id":"15336230","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"[~Ferd] I will investigate that. However, I see that spark 2.0 is a preview release, and it is not stable yet. Should we want to upgrade to this now? I read on the website that preview releases may contain critical bugs or documentation errors, so I think we should wait until it is officially released as GA.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-06-17T14:42:41.932+0000","updated":"2016-06-17T14:42:41.932+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15338890","id":"15338890","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"OK, let us wait for GA release.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-06-20T02:01:24.624+0000","updated":"2016-06-20T02:01:24.624+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15407497","id":"15407497","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I think we can resume the work here since Spark 2.0 has released.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-08-04T09:49:05.450+0000","updated":"2016-08-04T09:49:05.450+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15407930","id":"15407930","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"Let's wait until HIVE-14240 is resolved. The current spark assembly used for all itests uses spark 1.6 so this patch won't work. Also, I've heard that spark 2.0 won't use spark assembly anymore, so we need to depend on spark maven dependencies to run the tests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-08-04T15:22:36.310+0000","updated":"2016-08-04T15:22:36.310+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15408774","id":"15408774","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Yeah it'd be great if we can get rid of that tar, or at least make it smaller - we currently package the example jar into it which shouldn't be necessary.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-08-05T02:17:32.702+0000","updated":"2016-08-05T02:17:32.702+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15500015","id":"15500015","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"This patch is used to test whether qtest is passed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-18T01:37:45.183+0000","updated":"2016-09-18T01:37:45.183+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15500035","id":"15500035","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829075/HIVE-14029.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1225/testReport\nConsole output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1225/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1225/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'\n2016-09-18 01:52:20.390\n+ [[ -n /usr/java/jdk1.8.0_25 ]]\n+ export JAVA_HOME=/usr/java/jdk1.8.0_25\n+ JAVA_HOME=/usr/java/jdk1.8.0_25\n+ export PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/jenkins-PreCommit-HIVE-Build-1225/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ date '+%Y-%m-%d %T.%3N'\n2016-09-18 01:52:20.392\n+ cd apache-github-source-source\n+ git fetch origin\nFrom https://github.com/apache/hive\n   05e2510..c90eed2  master     -> origin/master\n+ git reset --hard HEAD\nwarning: unable to access '/home/sseth/.config/git/attributes': Permission denied\nHEAD is now at 05e2510 HIVE-14767: Migrate slow MiniMr tests to faster options (Prasanth Jayachandran reviewed by Siddharth Seth)\n+ git clean -f -d\nwarning: unable to access '/home/sseth/.config/git/ignore': Permission denied\n+ git checkout master\nwarning: unable to access '/home/sseth/.config/git/attributes': Permission denied\nwarning: unable to access '/home/sseth/.config/git/ignore': Permission denied\nAlready on 'master'\nYour branch is behind 'origin/master' by 1 commit, and can be fast-forwarded.\n  (use \"git pull\" to update your local branch)\n+ git reset --hard origin/master\nwarning: unable to access '/home/sseth/.config/git/attributes': Permission denied\nHEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ date '+%Y-%m-%d %T.%3N'\n2016-09-18 01:52:22.343\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nwarning: unable to access '/home/sseth/.config/git/attributes': Permission denied\nerror: a/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java: No such file or directory\nerror: a/itests/pom.xml: No such file or directory\nerror: a/pom.xml: No such file or directory\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveBaseFunctionResultList.java: No such file or directory\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java: No such file or directory\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java: No such file or directory\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java: No such file or directory\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java: No such file or directory\nerror: a/ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java: No such file or directory\nerror: a/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestHiveKVResultCache.java: No such file or directory\nerror: a/spark-client/src/main/java/org/apache/hive/spark/client/MetricsCollection.java: No such file or directory\nerror: a/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java: No such file or directory\nerror: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/InputMetrics.java: No such file or directory\nerror: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/Metrics.java: No such file or directory\nerror: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleReadMetrics.java: No such file or directory\nerror: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java: No such file or directory\nerror: a/spark-client/src/test/java/org/apache/hive/spark/client/TestMetricsCollection.java: No such file or directory\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829075 - jenkins-PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-18T01:52:04.009+0000","updated":"2016-09-18T01:52:04.009+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15500085","id":"15500085","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829079/HIVE-14029.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1226/testReport\nConsole output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1226/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1226/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests exited with: NonZeroExitCodeException\nCommand 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'\n2016-09-18 02:31:43.131\n+ [[ -n /usr/java/jdk1.8.0_25 ]]\n+ export JAVA_HOME=/usr/java/jdk1.8.0_25\n+ JAVA_HOME=/usr/java/jdk1.8.0_25\n+ export PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '\n+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/jenkins-PreCommit-HIVE-Build-1226/source-prep.txt\n+ [[ false == \\t\\r\\u\\e ]]\n+ mkdir -p maven ivy\n+ [[ git = \\s\\v\\n ]]\n+ [[ git = \\g\\i\\t ]]\n+ [[ -z master ]]\n+ [[ -d apache-github-source-source ]]\n+ [[ ! -d apache-github-source-source/.git ]]\n+ [[ ! -d apache-github-source-source ]]\n+ date '+%Y-%m-%d %T.%3N'\n2016-09-18 02:31:43.133\n+ cd apache-github-source-source\n+ git fetch origin\n+ git reset --hard HEAD\nwarning: unable to access '/home/sseth/.config/git/attributes': Permission denied\nwarning: unable to access '/home/sseth/.config/git/attributes': Permission denied\nHEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)\n+ git clean -f -d\nwarning: unable to access '/home/sseth/.config/git/ignore': Permission denied\n+ git checkout master\nwarning: unable to access '/home/sseth/.config/git/ignore': Permission denied\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n+ git reset --hard origin/master\nHEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)\n+ git merge --ff-only origin/master\nAlready up-to-date.\n+ date '+%Y-%m-%d %T.%3N'\n2016-09-18 02:31:44.018\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nwarning: unable to access '/home/sseth/.config/git/attributes': Permission denied\nerror: patch failed: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java:76\nerror: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java: patch does not apply\nThe patch does not appear to apply with p0, p1, or p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829079 - jenkins-PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-18T02:31:25.850+0000","updated":"2016-09-18T02:31:25.850+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15500137","id":"15500137","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Rebase patch","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-18T03:14:02.244+0000","updated":"2016-09-18T03:14:02.244+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15500150","id":"15500150","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829082/HIVE-14029.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1227/testReport\nConsole output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1227/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1227/\n\nMessages:\n{noformat}\n**** This message was trimmed, see log for full details ****\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/ConcurrentHashMap.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/FileNotFoundException.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URISyntaxException.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Integer.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/IMetaStoreClient.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/api/MetaException.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/Text.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Credentials.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/token/Token.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/core/target/hive-hcatalog-core-2.2.0-SNAPSHOT.jar(org/apache/hive/hcatalog/common/HCatUtil.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar(org/apache/thrift/TException.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Groups.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/HashSet.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Set.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Date.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceAudience$Private.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/BufferedReader.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/InputStream.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/InputStreamReader.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/PrintWriter.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Map$Entry.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/Semaphore.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/CommandLine.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/DefaultExecutor.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteWatchdog.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/PumpStreamHandler.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Shell.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Thread.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Runnable.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/DataInput.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/DataOutput.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputSplit.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/OutputStreamWriter.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFramework.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/NullWritable.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/RecordReader.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/TaskAttemptContext.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/CreateMode.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/KeeperException.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs$Ids.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputFormat.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobContext.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Evolving.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URLConnection.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URLDecoder.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Enumeration.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Properties.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/StringTokenizer.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriBuilder.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/LogUtils.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Class.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/StringBuilder.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobID.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Iterator.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/LinkedList.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/ExecutorService.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/Executors.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/TimeUnit.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Process.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper$Context.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFrameworkFactory.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar(org/apache/curator/retry/ExponentialBackoffRetry.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configured.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobClient.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobConf.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Job.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenIdentifier.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Tool.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configurable.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/ClassNotFoundException.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/RunningJob.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience$LimitedPrivate.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Annotation.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/SuppressWarnings.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Retention.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/RetentionPolicy.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Target.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/ElementType.class)]]\n[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/HttpMethod.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Override.class)]]\n[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(sun/misc/Contended.class)]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$3.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatDelegator$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/LauncherDelegator$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/Server$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatException$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$2.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/LogRetriever$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/HDFSStorage$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/ZooKeeperStorage$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonUtils$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1.class]]\n[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1$1.class]]\n[done in 2216 ms]\n[WARNING] Javadoc Warnings\n[WARNING] Sep 18, 2016 3:24:28 AM com.sun.jersey.wadl.resourcedoc.ResourceDoclet start\n[WARNING] INFO: Wrote /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/resourcedoc.xml\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-webhcat ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-webhcat ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp/conf\n     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-webhcat ---\n[INFO] Compiling 9 source files to /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/test-classes\n[INFO] -------------------------------------------------------------\n[WARNING] COMPILATION WARNING : \n[INFO] -------------------------------------------------------------\n[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Recompile with -Xlint:deprecation for details.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Some input files use unchecked or unsafe operations.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Recompile with -Xlint:unchecked for details.\n[INFO] 4 warnings \n[INFO] -------------------------------------------------------------\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR : \n[INFO] -------------------------------------------------------------\n[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[67,3] method does not override or implement a method from a supertype\n[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[72,3] method does not override or implement a method from a supertype\n[INFO] 2 errors \n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Hive .............................................. SUCCESS [1.987s]\n[INFO] Hive Shims Common ................................. SUCCESS [3.928s]\n[INFO] Hive Shims 0.23 ................................... SUCCESS [2.028s]\n[INFO] Hive Shims Scheduler .............................. SUCCESS [0.789s]\n[INFO] Hive Shims ........................................ SUCCESS [0.493s]\n[INFO] Hive Storage API .................................. SUCCESS [1.208s]\n[INFO] Hive ORC .......................................... SUCCESS [4.733s]\n[INFO] Hive Common ....................................... SUCCESS [4.729s]\n[INFO] Hive Service RPC .................................. SUCCESS [3.410s]\n[INFO] Hive Serde ........................................ SUCCESS [3.448s]\n[INFO] Hive Metastore .................................... SUCCESS [19.085s]\n[INFO] Hive Ant Utilities ................................ SUCCESS [0.267s]\n[INFO] Hive Llap Common .................................. SUCCESS [2.802s]\n[INFO] Hive Llap Client .................................. SUCCESS [1.071s]\n[INFO] Hive Llap Tez ..................................... SUCCESS [1.214s]\n[INFO] Spark Remote Client ............................... SUCCESS [23.836s]\n[INFO] Hive Query Language ............................... SUCCESS [53.022s]\n[INFO] Hive Llap Server .................................. SUCCESS [3.353s]\n[INFO] Hive Service ...................................... SUCCESS [3.861s]\n[INFO] Hive Accumulo Handler ............................. SUCCESS [2.300s]\n[INFO] Hive JDBC ......................................... SUCCESS [9.583s]\n[INFO] Hive Beeline ...................................... SUCCESS [1.997s]\n[INFO] Hive CLI .......................................... SUCCESS [1.501s]\n[INFO] Hive Contrib ...................................... SUCCESS [0.808s]\n[INFO] Hive Druid Handler ................................ SUCCESS [3.230s]\n[INFO] Hive HBase Handler ................................ SUCCESS [2.421s]\n[INFO] Hive HCatalog ..................................... SUCCESS [0.202s]\n[INFO] Hive HCatalog Core ................................ SUCCESS [2.674s]\n[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.686s]\n[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.478s]\n[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.310s]\n[INFO] Hive HCatalog Webhcat ............................. FAILURE [4.897s]\n[INFO] Hive HCatalog Streaming ........................... SKIPPED\n[INFO] Hive HPL/SQL ...................................... SKIPPED\n[INFO] Hive HWI .......................................... SKIPPED\n[INFO] Hive Llap External Client ......................... SKIPPED\n[INFO] Hive Shims Aggregator ............................. SKIPPED\n[INFO] Hive TestUtils .................................... SKIPPED\n[INFO] Hive Packaging .................................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 2:51.263s\n[INFO] Finished at: Sun Sep 18 03:24:28 UTC 2016\n[INFO] Final Memory: 254M/935M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-webhcat: Compilation failure: Compilation failure:\n[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[67,3] method does not override or implement a method from a supertype\n[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[72,3] method does not override or implement a method from a supertype\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-webhcat\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829082 - jenkins-PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-18T03:24:12.204+0000","updated":"2016-09-18T03:24:12.204+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15500530","id":"15500530","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829095/HIVE-14029.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1228/testReport\nConsole output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1228/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1228/\n\nMessages:\n{noformat}\n**** This message was trimmed, see log for full details ****\n[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-it-unit ---\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-it-unit ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/main/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf\n     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---\n[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/test-classes\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-hcatalog-it-unit ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-hcatalog-it-unit ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:test-jar (default) @ hive-hcatalog-it-unit ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.pom\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Integration - Testing Utilities 2.2.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util/target\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-util ---\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-util ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---\n[INFO] Compiling 66 source files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/classes\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/test/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf\n     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-it-util ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-it-util ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Integration - Unit Tests 2.2.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-unit ---\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-unit ---\n[INFO] Executing tasks\n\nmain:\n     [exec] + /bin/pwd\n     [exec] /data/hive-ptest/working/apache-github-source-source/itests/hive-unit\n     [exec] + BASE_DIR=./target\n     [exec] + HIVE_ROOT=./target/../../../\n     [exec] + DOWNLOAD_DIR=./../thirdparty\n     [exec] + mkdir -p ./../thirdparty\n     [exec] + download 'https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0' spark\n     [exec] + url='https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0'\n     [exec] + finalName=spark\n     [exec] ++ basename 'https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0'\n     [exec] + tarName='spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0'\n     [exec] + rm -rf ./target/spark\n     [exec] + [[ ! -f ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0 ]]\n     [exec] + curl -Sso './../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0' 'https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0'\n     [exec] + tar -zxf './../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0' -C ./target\n     [exec] tar (child): ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0: Cannot open: No such file or directory\n     [exec] tar (child): Error is not recoverable: exiting now\n     [exec] tar: Child returned status 2\n     [exec] tar: Error is not recoverable: exiting now\n     [exec] + mv ./target/spark-2.0.0-preview-bin-hadoop2-without-hive ./target/spark\n     [exec] mv: cannot stat ?./target/spark-2.0.0-preview-bin-hadoop2-without-hive?: No such file or directory\n     [exec] + cp -f ./target/../../..//data/conf/spark/log4j2.properties ./target/spark/conf/\n     [exec] cp: cannot create regular file ?./target/spark/conf/?: No such file or directory\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Hive Integration - Parent ......................... SUCCESS [1.708s]\n[INFO] Hive Integration - Custom Serde ................... SUCCESS [4.071s]\n[INFO] Hive Integration - Custom udfs .................... SUCCESS [0.714s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-util  SUCCESS [0.696s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-udf1  SUCCESS [0.711s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-udf2  SUCCESS [0.623s]\n[INFO] Hive Integration - Custom UDFs - udf-vectorized-badexample  SUCCESS [0.894s]\n[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [4.463s]\n[INFO] Hive Integration - Testing Utilities .............. SUCCESS [4.189s]\n[INFO] Hive Integration - Unit Tests ..................... FAILURE [1.870s]\n[INFO] Hive Integration - Test Serde ..................... SKIPPED\n[INFO] Hive Integration - QFile Tests .................... SKIPPED\n[INFO] Hive Integration - QFile Accumulo Tests ........... SKIPPED\n[INFO] JMH benchmark: Hive ............................... SKIPPED\n[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED\n[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED\n[INFO] Hive Integration - QFile Spark Tests .............. SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 20.754s\n[INFO] Finished at: Sun Sep 18 08:04:50 UTC 2016\n[INFO] Final Memory: 92M/697M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (download-spark) on project hive-it-unit: An Ant BuildException has occured: exec returned: 1\n[ERROR] around Ant part ...<exec failonerror=\"true\" dir=\"/data/hive-ptest/working/apache-github-source-source/itests/hive-unit\" executable=\"bash\">... @ 4:122 in /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target/antrun/build-main.xml\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-it-unit\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829095 - jenkins-PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-18T08:04:31.390+0000","updated":"2016-09-18T08:04:31.390+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15500571","id":"15500571","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829099/HIVE-14029.patch\n\n{color:red}ERROR:{color} -1 due to build exiting with an error\n\nTest results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1229/testReport\nConsole output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1229/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1229/\n\nMessages:\n{noformat}\n**** This message was trimmed, see log for full details ****\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-hcatalog-it-unit ---\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-it-unit ---\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-it-unit ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/main/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf\n     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---\n[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/test-classes\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-hcatalog-it-unit ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-hcatalog-it-unit ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:test-jar (default) @ hive-hcatalog-it-unit ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.pom\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Integration - Testing Utilities 2.2.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util/target\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-util ---\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-util ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---\n[INFO] Executing tasks\n\nmain:\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---\n[INFO] Compiling 66 source files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/classes\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Some input files use or override a deprecated API.\n[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/test/resources\n[INFO] Copying 3 resources\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---\n[INFO] Executing tasks\n\nmain:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/warehouse\n    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf\n     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf\n[INFO] Executed tasks\n[INFO] \n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---\n[INFO] No sources to compile\n[INFO] \n[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-it-util ---\n[INFO] Tests are skipped.\n[INFO] \n[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-it-util ---\n[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar\n[INFO] \n[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---\n[INFO] \n[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.jar\n[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.pom\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Hive Integration - Unit Tests 2.2.0-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target\n[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])\n[INFO] \n[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-unit ---\n[INFO] \n[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-unit ---\n[INFO] Executing tasks\n\nmain:\n     [exec] + /bin/pwd\n     [exec] + BASE_DIR=./target\n     [exec] + HIVE_ROOT=./target/../../../\n     [exec] + DOWNLOAD_DIR=./../thirdparty\n     [exec] + mkdir -p ./../thirdparty\n     [exec] /data/hive-ptest/working/apache-github-source-source/itests/hive-unit\n     [exec] + download http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz spark\n     [exec] + url=http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz\n     [exec] + finalName=spark\n     [exec] ++ basename http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz\n     [exec] + tarName=spark-2.0.0-bin-hadoop2-without-hive.tgz\n     [exec] + rm -rf ./target/spark\n     [exec] + [[ ! -f ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz ]]\n     [exec] + curl -Sso ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz\n     [exec] + tar -zxf ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz -C ./target\n     [exec] + mv ./target/spark-2.0.0-preview-bin-hadoop2-without-hive ./target/spark\n     [exec] mv: cannot stat ?./target/spark-2.0.0-preview-bin-hadoop2-without-hive?: No such file or directory\n     [exec] + cp -f ./target/../../..//data/conf/spark/log4j2.properties ./target/spark/conf/\n     [exec] cp: cannot create regular file ?./target/spark/conf/?: No such file or directory\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Hive Integration - Parent ......................... SUCCESS [1.573s]\n[INFO] Hive Integration - Custom Serde ................... SUCCESS [3.578s]\n[INFO] Hive Integration - Custom udfs .................... SUCCESS [0.591s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-util  SUCCESS [1.068s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-udf1  SUCCESS [0.834s]\n[INFO] Hive Integration - Custom UDFs - udf-classloader-udf2  SUCCESS [0.644s]\n[INFO] Hive Integration - Custom UDFs - udf-vectorized-badexample  SUCCESS [0.615s]\n[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [4.321s]\n[INFO] Hive Integration - Testing Utilities .............. SUCCESS [4.851s]\n[INFO] Hive Integration - Unit Tests ..................... FAILURE [9.328s]\n[INFO] Hive Integration - Test Serde ..................... SKIPPED\n[INFO] Hive Integration - QFile Tests .................... SKIPPED\n[INFO] Hive Integration - QFile Accumulo Tests ........... SKIPPED\n[INFO] JMH benchmark: Hive ............................... SKIPPED\n[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED\n[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED\n[INFO] Hive Integration - QFile Spark Tests .............. SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 28.170s\n[INFO] Finished at: Sun Sep 18 08:34:08 UTC 2016\n[INFO] Final Memory: 90M/757M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (download-spark) on project hive-it-unit: An Ant BuildException has occured: exec returned: 1\n[ERROR] around Ant part ...<exec failonerror=\"true\" dir=\"/data/hive-ptest/working/apache-github-source-source/itests/hive-unit\" executable=\"bash\">... @ 4:122 in /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target/antrun/build-main.xml\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :hive-it-unit\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829099 - jenkins-PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-18T08:33:50.494+0000","updated":"2016-09-18T08:33:50.494+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15500712","id":"15500712","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829100/HIVE-14029.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10497 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\norg.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles\norg.apache.hive.spark.client.TestSparkClient.testCounters\norg.apache.hive.spark.client.TestSparkClient.testMetricsCollection\norg.apache.hive.spark.client.TestSparkClient.testRemoteClient\norg.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob\n{noformat}\n\nTest results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1230/testReport\nConsole output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1230/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1230/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 9 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829100 - jenkins-PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-18T10:09:42.441+0000","updated":"2016-09-18T10:09:42.441+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15505709","id":"15505709","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Fix some dependencies issues","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-20T06:04:38.577+0000","updated":"2016-09-20T06:04:38.577+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506147","id":"15506147","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829333/HIVE-14029.1.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10498 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1237/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1237/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1237/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 7 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829333 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-20T09:25:19.716+0000","updated":"2016-09-20T09:25:19.716+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506455","id":"15506455","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aihuaxu","name":"aihuaxu","key":"aihuaxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aihua Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Do we need to finish HIVE-14240 to get this unblocked? [~spena]  It doesn't look like to me.\n\n[~Ferd]  There are several baselines which got updated by moving {{\"BASIC_STATS\":\"true\"}}. Do you know what causes it? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aihuaxu","name":"aihuaxu","key":"aihuaxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aihua Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-20T13:05:33.585+0000","updated":"2016-09-20T13:05:33.585+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506647","id":"15506647","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~Ferd], could you also explain why we need to upgrade the dependencies, and why DataReadMethod is removed from InputMetrics?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-20T14:08:56.361+0000","updated":"2016-09-20T14:08:56.361+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506658","id":"15506658","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~aihuaxu], HIVE-14240 is trying to replacing current tgz file. This patch can bypass it by using a tmp file. It doesn't block HIVE-14240. \nI am not quite sure why the order changed. It looks strange that the context is the same expect the displaying order.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-20T14:12:11.407+0000","updated":"2016-09-20T14:12:11.407+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506676","id":"15506676","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~lirui], some APIs are changed in Spark side and the updates for dependencies are required since Spark use newer version which will lead inconsistent errors for HoS.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-20T14:14:58.359+0000","updated":"2016-09-20T14:14:58.359+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506697","id":"15506697","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"https://builds.apache.org/job/PreCommit-HIVE-Build/1239 was failed. Retest the patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-20T14:21:45.604+0000","updated":"2016-09-20T14:21:45.604+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506716","id":"15506716","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"[~Ferd] I think we should try to fix HIVE-14240 first to avoid dependencies issues when Spark and Hive are running in the same machine. I talked with the Spark team a few times, and they think this assembly tar.gz will cause issues due to other Hive libraries Spark depends, such as Hive 1.2 metastore and Hive 1.2 serde.\n\nWould you like to start working on HIVE-14240? You can ask [~stakiar] if you're interested.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-20T14:25:18.689+0000","updated":"2016-09-20T14:25:18.689+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506738","id":"15506738","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~spena], I am not quite sure why assembly tar.gz  will cause issues for Hive since it's included in Hive itest only. Could you explain a little bit more?  BTW, I will take a look at how to remove it from itest.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-20T14:32:27.415+0000","updated":"2016-09-20T14:32:27.415+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506772","id":"15506772","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"Sure. [~stakiar] Let me know if the below statements are correct, and feel free to correct me.\n\n- Spark2 uses a fork of Hive 1.2 due to issues with Apache Hive. They called this project {{spark-hive}}. Spark only uses Hive 1.2 metastore/serde/udf jars form this forked project.\n  They download this from https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.10 \n\n- Spark2 assembly without hive will be built without any of the above dependencies.\n\n- Hive2 itests will use Spark2 assembly to run Hive2 tests. This means Hive2 might not test Spark2 correctly due to the lack of Hive 1.2 libraries in it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-20T14:43:27.467+0000","updated":"2016-09-20T14:43:27.467+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506833","id":"15506833","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"bq. some APIs are changed in Spark side\nIs there any other way we can track the read method? If not, guess we can just remove the class from Hive side.\nbq. Hive2 itests will use Spark2 assembly to run Hive2 tests. This means Hive2 might not test Spark2 correctly due to the lack of Hive 1.2 libraries in it.\nI'm not sure what problem spark has without hive libraries. We have been requiring that spark is built without hive. Otherwise we'll have different hive libraries in our classpath which causes conflicts.\nI don't think HIVE-14240 blocks this one. Actually HIVE-14240 should be implemented for Spark 2.0 right?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-20T15:09:20.150+0000","updated":"2016-09-20T15:09:20.150+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506839","id":"15506839","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"My understanding is spark needs hive libraries only for SparkSQL, which is not needed for HoS.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-20T15:12:29.623+0000","updated":"2016-09-20T15:12:29.623+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15506942","id":"15506942","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829398/HIVE-14029.1.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 23 failed/errored test(s), 10556 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucket4]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucket5]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[disable_merge_for_bucketing]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[list_bucket_dml_10]\norg.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[reduce_deduplicate]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[bucket4]\norg.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[disable_merge_for_bucketing]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse\norg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\norg.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles\norg.apache.hive.spark.client.TestSparkClient.testCounters\norg.apache.hive.spark.client.TestSparkClient.testErrorJob\norg.apache.hive.spark.client.TestSparkClient.testJobSubmission\norg.apache.hive.spark.client.TestSparkClient.testMetricsCollection\norg.apache.hive.spark.client.TestSparkClient.testRemoteClient\norg.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob\norg.apache.hive.spark.client.TestSparkClient.testSyncRpc\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1242/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1242/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1242/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 23 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829398 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-20T15:53:50.414+0000","updated":"2016-09-20T15:53:50.414+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15507641","id":"15507641","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"[~Ferd] how was the http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz built?\n\nI don't think HIVE-14240 is a blocker for this assuming the tar-ball was built in a supported way, but I'm trying to contact some Spark committers to see if they have any input.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2016-09-20T20:03:24.574+0000","updated":"2016-09-20T20:03:24.574+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15508316","id":"15508316","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~stakiar], the tgz was built via the following commands:\n{code}\nsh ./dev/make-distribution.sh  --name hadoop2-without-hive --tgz -Phadoop-2.7 -Pyarn -Pparquet-provided -Dhadoop.version=2.7.3\n{code}\n[~dapengsun], can you confirm it please?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T01:03:41.327+0000","updated":"2016-09-21T01:16:05.914+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15508350","id":"15508350","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~spena], I think we should move it forwards since HIVE-14240 needs further discussions and it doesn't block this ticket. We can upload the tgz into a stable location to upgrade the Spark version and once we fixed HIVE-14240, we can easily remove this tgz. [~lirui] [~stakiar] [~aihuaxu] any thoughts?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T01:22:15.160+0000","updated":"2016-09-21T01:22:15.160+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15508353","id":"15508353","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dapengsun","name":"dapengsun","key":"dapengsun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dapeng Sun","active":true,"timeZone":"Asia/Shanghai"},"body":"[~Ferd]\nYes, I used this command","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dapengsun","name":"dapengsun","key":"dapengsun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dapeng Sun","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-21T01:23:45.280+0000","updated":"2016-09-21T01:24:45.861+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15508437","id":"15508437","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I agree to move this forward. HIVE-14240 can be done in parallel, if it doesn't depend on this one :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-21T02:10:10.575+0000","updated":"2016-09-21T02:10:10.575+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15508636","id":"15508636","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~lirui] \nbq. Is there any other way we can track the read method? If not, guess we can just remove the class from Hive side.\n\nI will investigate this in a separate JIRA. Thank you for pointing this out.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T03:48:37.121+0000","updated":"2016-09-21T03:48:37.121+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15508990","id":"15508990","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829489/HIVE-14029.2.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10556 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1249/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1249/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1249/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 6 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829489 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-21T07:01:06.709+0000","updated":"2016-09-21T07:01:06.709+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15509101","id":"15509101","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi folks, the failed cases are not related. [~spena], do you have comments for the patch? I am going to commit it if no further comments.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T07:46:47.466+0000","updated":"2016-09-21T07:46:47.466+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15509231","id":"15509231","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"+1 to the latest patch.\n[~Ferd], is there anything needs to be updated in our getting started [wiki|https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started]? I think at least we need to update how to integrate spark since we no longer have the assembly jar.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-21T08:40:45.552+0000","updated":"2016-09-21T08:40:45.552+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15509847","id":"15509847","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~lirui], WIKI needs to be updated because for Spark 2.0.0 or above, there is no assembly jar built. Before updating the wiki, I need to verify it locally.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T13:02:06.749+0000","updated":"2016-09-21T13:02:06.749+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15509966","id":"15509966","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"OK. AFAIK, HoS only needs spark-core. So we can try adding spark-core and all its dependencies to hive's classpath.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-21T13:50:03.337+0000","updated":"2016-09-21T13:50:03.337+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15510135","id":"15510135","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"Thanks [~Ferd]. I uploaded the spark 2.0 assembly jar to the stable location. Could you upload a new patch without the URL spark assembly change? To test that it will work.\n\nI will review the patch today.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-21T14:37:14.915+0000","updated":"2016-09-21T14:37:14.915+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15510245","id":"15510245","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Do you mean the tgz file? What's the new address for it or the same name as before? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T15:14:08.243+0000","updated":"2016-09-21T15:14:08.243+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15510257","id":"15510257","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"It is the same as before.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-21T15:18:18.633+0000","updated":"2016-09-21T15:18:18.633+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15510337","id":"15510337","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Attached as HIVE-14029.3.patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-21T15:45:41.848+0000","updated":"2016-09-21T15:45:41.848+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15510503","id":"15510503","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829601/HIVE-14029.3.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10556 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1254/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1254/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1254/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 6 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829601 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-21T16:54:34.583+0000","updated":"2016-09-21T16:54:34.583+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15510656","id":"15510656","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"[~Ferd] overall this looks good to me. It would be nice if you could update the description to list out the high-level changes that needed to be made to Hive to add support for Spark 2.0.0. For example, dependency updates, which APIs changed (change from Iterable to Iterator, InputMetrics constructor change).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2016-09-21T17:51:38.201+0000","updated":"2016-09-21T17:51:38.201+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15511038","id":"15511038","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"[~Ferd] The patch looks good. +1\n\nI just found a variable that is not used anymore.\n{noformat}\nMetricsCollection.java\n - Should we remove 'DataReadMethod readMethod = null'? is not used anymore.\n{noformat}\n\n[~xuefuz] Do you think this patch is ready to go to start supporting spark 2.0?\n[~Ferd] Have we run any other tests in an environment with spark 2.0 and hive 2.1? I think we should do that if you haven't yet before commit the patch. Just to confirm we don't have issues with the classpath.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-21T20:16:15.904+0000","updated":"2016-09-21T20:16:15.904+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15511533","id":"15511533","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi guys, thanks for working/reviewing this. The patch looks good. I understand that there is a pending discussion about removing spark tarball from the test. However, in this long thread there seems a confusion of this with the spark's assembly jar which is part of spark build as of 1.6. [~Ferd], do we have a clear picture of that for 2.0? If there is any change, we do want to update the doc. For instance, I used to get the assembly.jar from spark build and copy it to hive's /lib directory and I'm ready to run Hive on Spark.\n\nSorry I'm a little behind Spark 2.0. I will try to figure it out on my end as well.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-21T23:36:36.899+0000","updated":"2016-09-21T23:36:36.899+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15511577","id":"15511577","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I made a build of spark 2.0 and indeed spark-assembly.jar is missing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-22T00:00:03.153+0000","updated":"2016-09-22T00:00:03.153+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15511610","id":"15511610","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~xuefuz], Spark assembly was removed since Spark 2.0.0. They don't provide an assembly jar considering some dependency conflicts. I find some comments in the root pom file for Spark. To support 2.0.0, we have to copy all Spark related jars under the hive/lib AFAIK.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-22T00:16:48.440+0000","updated":"2016-09-22T00:16:48.440+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15511614","id":"15511614","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~spena], do we need to support in Hive 2.1? I do some smoke tests in current upstream and Spark 2.0 and it passed if you set SPARK_HOME correctly and copy all lib jars of Spark into hive/lib folder. This needed to be updated in Hive On Spark WIKI.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-22T00:19:25.156+0000","updated":"2016-09-22T00:19:25.156+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15511682","id":"15511682","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~Ferd], the classpath is just for HS2/CLI, so I don't think we need all the spark jars. Please find a minimum set of required jars. You can start with spark-core.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-22T00:54:16.712+0000","updated":"2016-09-22T00:54:16.712+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15512191","id":"15512191","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Thanks [~stakiar] for your review. Description is updated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-22T05:15:29.677+0000","updated":"2016-09-22T05:15:29.677+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15512315","id":"15512315","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~lirui], \nbq. I don't think we need all the spark jars.\n\nAgree. It's not required for all Spark jars. From Hive pom.xml file, we see that it depends only on Spark_core. You can find the dependency for Spark_core. All of them should be included into HIVE_CLASSPATH. Do we really need to filter those jars? It isn't very user friendly because users have to find them one by one in Spark jars and add it to HIVE classpath. I think we can simple add the whole folder to HIVE classpath when running Hive on Spark. Any thoughts?\n\n{code}\ncore]# mvn dependency:tree\n[INFO] Scanning for projects...\n[INFO]                                                                         \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Spark Project Core 2.0.0\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-dependency-plugin:2.10:tree (default-cli) @ spark-core_2.11 ---\n[INFO] org.apache.spark:spark-core_2.11:jar:2.0.0\n[INFO] +- org.apache.avro:avro-mapred:jar:hadoop2:1.7.7:compile\n[INFO] |  +- org.apache.avro:avro-ipc:jar:1.7.7:compile\n[INFO] |  |  \\- org.apache.avro:avro:jar:1.7.7:compile\n[INFO] |  +- org.apache.avro:avro-ipc:jar:tests:1.7.7:test\n[INFO] |  +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile\n[INFO] |  \\- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile\n[INFO] +- com.google.guava:guava:jar:14.0.1:provided\n[INFO] +- com.twitter:chill_2.11:jar:0.8.0:compile\n[INFO] |  \\- com.esotericsoftware:kryo-shaded:jar:3.0.3:compile\n[INFO] |     \\- com.esotericsoftware:minlog:jar:1.3.0:compile\n[INFO] +- com.twitter:chill-java:jar:0.8.0:compile\n[INFO] +- org.apache.xbean:xbean-asm5-shaded:jar:4.4:compile\n[INFO] +- org.apache.hadoop:hadoop-client:jar:2.2.0:compile\n[INFO] |  +- org.apache.hadoop:hadoop-common:jar:2.2.0:compile\n[INFO] |  |  +- commons-cli:commons-cli:jar:1.2:compile\n[INFO] |  |  +- xmlenc:xmlenc:jar:0.52:compile\n[INFO] |  |  +- commons-io:commons-io:jar:2.4:compile\n[INFO] |  |  +- commons-lang:commons-lang:jar:2.6:compile\n[INFO] |  |  +- commons-configuration:commons-configuration:jar:1.6:compile\n[INFO] |  |  |  +- commons-digester:commons-digester:jar:1.8:compile\n[INFO] |  |  |  |  \\- commons-beanutils:commons-beanutils:jar:1.7.0:compile\n[INFO] |  |  |  \\- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile\n[INFO] |  |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile\n[INFO] |  |  +- org.apache.hadoop:hadoop-auth:jar:2.2.0:compile\n[INFO] |  |  \\- org.apache.commons:commons-compress:jar:1.4.1:compile\n[INFO] |  |     \\- org.tukaani:xz:jar:1.0:compile\n[INFO] |  +- org.apache.hadoop:hadoop-hdfs:jar:2.2.0:compile\n[INFO] |  |  \\- org.mortbay.jetty:jetty-util:jar:6.1.26:compile\n[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.2.0:compile\n[INFO] |  |  +- org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.2.0:compile\n[INFO] |  |  |  +- org.apache.hadoop:hadoop-yarn-client:jar:2.2.0:compile\n[INFO] |  |  |  |  \\- com.google.inject:guice:jar:3.0:compile\n[INFO] |  |  |  |     +- javax.inject:javax.inject:jar:1:compile\n[INFO] |  |  |  |     \\- aopalliance:aopalliance:jar:1.0:compile\n[INFO] |  |  |  \\- org.apache.hadoop:hadoop-yarn-server-common:jar:2.2.0:compile\n[INFO] |  |  \\- org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.2.0:compile\n[INFO] |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.2.0:compile\n[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.2.0:compile\n[INFO] |  |  \\- org.apache.hadoop:hadoop-yarn-common:jar:2.2.0:compile\n[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.2.0:compile\n[INFO] |  \\- org.apache.hadoop:hadoop-annotations:jar:2.2.0:compile\n[INFO] +- org.apache.spark:spark-launcher_2.11:jar:2.0.0:compile\n[INFO] +- org.apache.spark:spark-network-common_2.11:jar:2.0.0:compile\n[INFO] +- org.apache.spark:spark-network-shuffle_2.11:jar:2.0.0:compile\n[INFO] |  +- org.fusesource.leveldbjni:leveldbjni-all:jar:1.8:compile\n[INFO] |  \\- com.fasterxml.jackson.core:jackson-annotations:jar:2.6.5:compile\n[INFO] +- org.apache.spark:spark-unsafe_2.11:jar:2.0.0:compile\n[INFO] +- net.java.dev.jets3t:jets3t:jar:0.7.1:compile\n[INFO] |  +- commons-codec:commons-codec:jar:1.10:compile\n[INFO] |  \\- commons-httpclient:commons-httpclient:jar:3.1:compile\n[INFO] +- org.apache.curator:curator-recipes:jar:2.4.0:compile\n[INFO] |  +- org.apache.curator:curator-framework:jar:2.4.0:compile\n[INFO] |  |  \\- org.apache.curator:curator-client:jar:2.4.0:compile\n[INFO] |  \\- org.apache.zookeeper:zookeeper:jar:3.4.5:compile\n[INFO] +- org.eclipse.jetty:jetty-plus:jar:9.2.16.v20160414:compile\n[INFO] |  +- org.eclipse.jetty:jetty-webapp:jar:9.2.16.v20160414:compile\n[INFO] |  |  \\- org.eclipse.jetty:jetty-xml:jar:9.2.16.v20160414:compile\n[INFO] |  \\- org.eclipse.jetty:jetty-jndi:jar:9.2.16.v20160414:compile\n[INFO] +- org.eclipse.jetty:jetty-security:jar:9.2.16.v20160414:compile\n[INFO] +- org.eclipse.jetty:jetty-util:jar:9.2.16.v20160414:compile\n[INFO] +- org.eclipse.jetty:jetty-server:jar:9.2.16.v20160414:compile\n[INFO] |  \\- org.eclipse.jetty:jetty-io:jar:9.2.16.v20160414:compile\n[INFO] +- org.eclipse.jetty:jetty-http:jar:9.2.16.v20160414:compile\n[INFO] +- org.eclipse.jetty:jetty-continuation:jar:9.2.16.v20160414:compile\n[INFO] +- org.eclipse.jetty:jetty-servlet:jar:9.2.16.v20160414:compile\n[INFO] +- org.eclipse.jetty:jetty-servlets:jar:9.2.16.v20160414:compile\n[INFO] +- javax.servlet:javax.servlet-api:jar:3.1.0:compile\n[INFO] +- org.apache.commons:commons-lang3:jar:3.3.2:compile\n[INFO] +- org.apache.commons:commons-math3:jar:3.4.1:compile\n[INFO] +- com.google.code.findbugs:jsr305:jar:1.3.9:compile\n[INFO] +- org.slf4j:slf4j-api:jar:1.7.16:compile\n[INFO] +- org.slf4j:jul-to-slf4j:jar:1.7.16:compile\n[INFO] +- org.slf4j:jcl-over-slf4j:jar:1.7.16:compile\n[INFO] +- log4j:log4j:jar:1.2.17:compile\n[INFO] +- org.slf4j:slf4j-log4j12:jar:1.7.16:compile\n[INFO] +- com.ning:compress-lzf:jar:1.0.3:compile\n[INFO] +- org.xerial.snappy:snappy-java:jar:1.1.2.4:compile\n[INFO] +- net.jpountz.lz4:lz4:jar:1.3.0:compile\n[INFO] +- org.roaringbitmap:RoaringBitmap:jar:0.5.11:compile\n[INFO] +- commons-net:commons-net:jar:2.2:compile\n[INFO] +- org.scala-lang:scala-library:jar:2.11.8:compile\n[INFO] +- org.json4s:json4s-jackson_2.11:jar:3.2.11:compile\n[INFO] |  \\- org.json4s:json4s-core_2.11:jar:3.2.11:compile\n[INFO] |     +- org.json4s:json4s-ast_2.11:jar:3.2.11:compile\n[INFO] |     +- com.thoughtworks.paranamer:paranamer:jar:2.6:compile\n[INFO] |     \\- org.scala-lang:scalap:jar:2.11.8:compile\n[INFO] |        \\- org.scala-lang:scala-compiler:jar:2.11.8:compile\n[INFO] |           \\- org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.0.4:compile\n[INFO] +- org.glassfish.jersey.core:jersey-client:jar:2.22.2:compile\n[INFO] |  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile\n[INFO] |  +- org.glassfish.hk2:hk2-api:jar:2.4.0-b34:compile\n[INFO] |  |  +- org.glassfish.hk2:hk2-utils:jar:2.4.0-b34:compile\n[INFO] |  |  \\- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34:compile\n[INFO] |  +- org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34:compile\n[INFO] |  \\- org.glassfish.hk2:hk2-locator:jar:2.4.0-b34:compile\n[INFO] +- org.glassfish.jersey.core:jersey-common:jar:2.22.2:compile\n[INFO] |  +- javax.annotation:javax.annotation-api:jar:1.2:compile\n[INFO] |  +- org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.22.2:compile\n[INFO] |  \\- org.glassfish.hk2:osgi-resource-locator:jar:1.0.1:compile\n[INFO] +- org.glassfish.jersey.core:jersey-server:jar:2.22.2:compile\n[INFO] |  +- org.glassfish.jersey.media:jersey-media-jaxb:jar:2.22.2:compile\n[INFO] |  \\- javax.validation:validation-api:jar:1.1.0.Final:compile\n[INFO] +- org.glassfish.jersey.containers:jersey-container-servlet:jar:2.22.2:compile\n[INFO] +- org.glassfish.jersey.containers:jersey-container-servlet-core:jar:2.22.2:compile\n[INFO] +- org.apache.mesos:mesos:jar:shaded-protobuf:0.21.1:compile\n[INFO] +- io.netty:netty-all:jar:4.0.29.Final:compile\n[INFO] +- io.netty:netty:jar:3.8.0.Final:compile\n[INFO] +- com.clearspring.analytics:stream:jar:2.7.0:compile\n[INFO] +- io.dropwizard.metrics:metrics-core:jar:3.1.2:compile\n[INFO] +- io.dropwizard.metrics:metrics-jvm:jar:3.1.2:compile\n[INFO] +- io.dropwizard.metrics:metrics-json:jar:3.1.2:compile\n[INFO] +- io.dropwizard.metrics:metrics-graphite:jar:3.1.2:compile\n[INFO] +- com.fasterxml.jackson.core:jackson-databind:jar:2.6.5:compile\n[INFO] |  \\- com.fasterxml.jackson.core:jackson-core:jar:2.6.5:compile\n[INFO] +- com.fasterxml.jackson.module:jackson-module-scala_2.11:jar:2.6.5:compile\n[INFO] |  +- org.scala-lang:scala-reflect:jar:2.11.8:compile\n[INFO] |  \\- com.fasterxml.jackson.module:jackson-module-paranamer:jar:2.6.5:compile\n[INFO] +- org.apache.derby:derby:jar:10.11.1.1:test\n[INFO] +- org.apache.ivy:ivy:jar:2.4.0:compile\n[INFO] +- oro:oro:jar:2.0.8:compile\n[INFO] +- org.seleniumhq.selenium:selenium-java:jar:2.52.0:test\n[INFO] |  +- org.seleniumhq.selenium:selenium-chrome-driver:jar:2.52.0:test\n[INFO] |  |  \\- org.seleniumhq.selenium:selenium-remote-driver:jar:2.52.0:test\n[INFO] |  |     +- cglib:cglib-nodep:jar:2.1_3:test\n[INFO] |  |     +- com.google.code.gson:gson:jar:2.3.1:test\n[INFO] |  |     \\- org.seleniumhq.selenium:selenium-api:jar:2.52.0:test\n[INFO] |  +- org.seleniumhq.selenium:selenium-edge-driver:jar:2.52.0:test\n[INFO] |  |  \\- org.apache.commons:commons-exec:jar:1.3:test\n[INFO] |  +- org.seleniumhq.selenium:selenium-firefox-driver:jar:2.52.0:test\n[INFO] |  +- org.seleniumhq.selenium:selenium-ie-driver:jar:2.52.0:test\n[INFO] |  |  +- net.java.dev.jna:jna:jar:4.1.0:test\n[INFO] |  |  \\- net.java.dev.jna:jna-platform:jar:4.1.0:test\n[INFO] |  +- org.seleniumhq.selenium:selenium-safari-driver:jar:2.52.0:test\n[INFO] |  +- org.seleniumhq.selenium:selenium-support:jar:2.52.0:test\n[INFO] |  +- org.webbitserver:webbit:jar:0.4.14:test\n[INFO] |  \\- org.seleniumhq.selenium:selenium-leg-rc:jar:2.52.0:test\n[INFO] +- org.seleniumhq.selenium:selenium-htmlunit-driver:jar:2.52.0:test\n[INFO] |  +- net.sourceforge.htmlunit:htmlunit:jar:2.18:test\n[INFO] |  |  +- xalan:xalan:jar:2.7.2:test\n[INFO] |  |  |  \\- xalan:serializer:jar:2.7.2:test\n[INFO] |  |  +- org.apache.httpcomponents:httpmime:jar:4.5.2:test\n[INFO] |  |  +- net.sourceforge.htmlunit:htmlunit-core-js:jar:2.17:test\n[INFO] |  |  +- xerces:xercesImpl:jar:2.11.0:test\n[INFO] |  |  +- net.sourceforge.nekohtml:nekohtml:jar:1.9.22:test\n[INFO] |  |  +- net.sourceforge.cssparser:cssparser:jar:0.9.16:test\n[INFO] |  |  |  \\- org.w3c.css:sac:jar:1.3:test\n[INFO] |  |  +- commons-logging:commons-logging:jar:1.2:test\n[INFO] |  |  \\- org.eclipse.jetty.websocket:websocket-client:jar:9.2.12.v20150709:test\n[INFO] |  |     \\- org.eclipse.jetty.websocket:websocket-common:jar:9.2.12.v20150709:test\n[INFO] |  |        \\- org.eclipse.jetty.websocket:websocket-api:jar:9.2.12.v20150709:test\n[INFO] |  +- commons-collections:commons-collections:jar:3.2.2:compile\n[INFO] |  \\- org.apache.httpcomponents:httpclient:jar:4.5.2:test\n[INFO] |     \\- org.apache.httpcomponents:httpcore:jar:4.4.4:test\n[INFO] +- xml-apis:xml-apis:jar:1.4.01:test\n[INFO] +- org.hamcrest:hamcrest-core:jar:1.3:test\n[INFO] +- org.hamcrest:hamcrest-library:jar:1.3:test\n[INFO] +- org.mockito:mockito-core:jar:1.10.19:test\n[INFO] |  \\- org.objenesis:objenesis:jar:2.1:compile\n[INFO] +- org.scalacheck:scalacheck_2.11:jar:1.12.5:test\n[INFO] |  \\- org.scala-sbt:test-interface:jar:1.0:test\n[INFO] +- org.apache.curator:curator-test:jar:2.4.0:test\n[INFO] |  +- org.javassist:javassist:jar:3.15.0-GA:compile\n[INFO] |  \\- org.apache.commons:commons-math:jar:2.2:compile\n[INFO] +- net.razorvine:pyrolite:jar:4.9:compile\n[INFO] +- net.sf.py4j:py4j:jar:0.10.1:compile\n[INFO] +- org.apache.spark:spark-tags_2.11:jar:2.0.0:compile\n[INFO] +- org.apache.commons:commons-crypto:jar:1.0.0:compile\n[INFO] +- org.spark-project.spark:unused:jar:1.0.0:compile\n[INFO] +- org.scalatest:scalatest_2.11:jar:2.2.6:test\n[INFO] |  \\- org.scala-lang.modules:scala-xml_2.11:jar:1.0.2:compile\n[INFO] +- junit:junit:jar:4.12:test\n[INFO] \\- com.novocode:junit-interface:jar:0.11:test\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 2.475 s\n[INFO] Finished at: 2016-09-22T06:34:12+08:00\n[INFO] Final Memory: 23M/963M\n[INFO] ------------------------------------------------------------------------\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-22T06:13:59.011+0000","updated":"2016-09-22T06:13:59.011+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15512366","id":"15512366","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"GitHub user winningsix opened a pull request:\n\n    https://github.com/apache/hive/pull/103\n\n    HIVE-14029: Update Spark version to 2.0.0\n\n    Changes include:\n    * Spark API updates:\n    \n    1. SparkShuffler#call return Iterator instead of Iterable\n    2. SparkListener -> JavaSparkListener\n    3. InputMetrics constructor doesn’t accept readMethod\n    4. Method remoteBlocksFetched and localBlocksFetched in ShuffleReadMetrics return long type instead of integer\n    \n    * Dependency upgrade:\n    \n    1. Jackson: 2.4.2 -> 2.6.5\n    2. Netty version: 4.0.23.Final -> 4.0.29.Final\n    3. Scala binary version: 2.10 -> 2.11\n    4. Scala version: 2.10.4 -> 2.11.8\n    \n    Test done by smoke tests in a cluster and integration test in Jenkins\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/winningsix/hive HIVE-14029\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/hive/pull/103.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #103\n    \n----\ncommit 965e57295a83b06db61b22f3fda0bb19e47c248a\nAuthor: Ferdinand Xu <cheng.a.xu@intel.com>\nDate:   2016-09-17T19:10:04Z\n\n    HIVE-14029: Update Spark version to 2.0.0\n\n----\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2016-09-22T06:39:56.763+0000","updated":"2016-09-22T06:39:56.763+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15512372","id":"15512372","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Update patch addressing [~spena]'s comments.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-22T06:44:30.815+0000","updated":"2016-09-22T06:44:30.815+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15512499","id":"15512499","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~Ferd], My thought was we should be able to tell the user what are actually needed (i.e. the minimum set of required jars) for HoS to work. Users can decide whether they want to add just the required jars, or all the jars under spark's dir for convenience. This is just something good to have and doesn't block this ticket - we used to add the whole assembly anyway.\nBesides, I think not all the dependencies of spark-core are needed because some of them should be already in Hive's classpath, e.g. hadoop, commons, etc.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-22T07:47:40.585+0000","updated":"2016-09-22T07:47:40.585+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15512678","id":"15512678","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"When I tried the patch locally, I got a compile error:\n{noformat}\norg.apache.hive.hcatalog.templeton.mock.MockUriInfo is not abstract and does not override abstract method relativize(java.net.URI) in javax.ws.rs.core.UriInfo\n{noformat}\nDoes anybody have the same issue?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-22T09:00:46.172+0000","updated":"2016-09-22T09:00:46.172+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15512929","id":"15512929","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829777/HIVE-14029.4.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10555 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\norg.apache.hive.spark.client.TestSparkClient.testJobSubmission\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1269/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1269/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1269/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 7 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829777 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-22T10:46:40.226+0000","updated":"2016-09-22T10:46:40.226+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15513291","id":"15513291","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"I have the same issue. You need to update it with following changes:\n{code}\n--- a/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java\n+++ b/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java\n@@ -64,6 +64,14 @@ public UriBuilder getBaseUriBuilder() {\n     return null;\n   }\n \n+  public URI resolve(URI uri) {\n+    return null;\n+  }\n+\n+  public URI relativize(URI uri) {\n+    return null;\n+  }\n+\n   @Override\n   public List<String> getMatchedURIs() {\n     // TODO Auto-generated method stub\n\n{code}\n\nI am not quite sure why this happens which can be reproduced in Jenkins. Possibly related to JDK version.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-22T13:22:01.872+0000","updated":"2016-09-22T13:22:56.256+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15513430","id":"15513430","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"Sorry Fer, I meant 2.2 :P. I got confused with numbers.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-22T14:26:00.341+0000","updated":"2016-09-22T14:26:00.341+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15513473","id":"15513473","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Seems we have two {{javax.ws.rs.core.UriInfo}} interfaces from two jars: javax.ws.rs-api and jersey-core. Before the patch, we only have one from jersey-core. Maybe there's some conflicts in the dependency upgrade. We need to fix it because it breaks build.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-22T14:41:01.240+0000","updated":"2016-09-22T14:41:01.240+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15513528","id":"15513528","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 on identifying the minimum set.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-22T15:08:45.705+0000","updated":"2016-09-22T15:08:45.705+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15513534","id":"15513534","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"Which JDK you're using? Jenkins is using JDK8","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-22T15:12:51.704+0000","updated":"2016-09-22T15:12:51.704+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15513539","id":"15513539","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I'm using:\n{noformat}\njava version \"1.8.0_91\"\nJava(TM) SE Runtime Environment (build 1.8.0_91-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-22T15:14:59.651+0000","updated":"2016-09-22T15:14:59.651+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15515104","id":"15515104","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~spena], it's weird why Jenkins can build it successfully. Hi [~lirui] I exclude the {code}javax.ws.rs{code} imported by spark-core in 5th patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-23T02:01:57.503+0000","updated":"2016-09-23T02:01:57.503+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15515131","id":"15515131","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~lirui], [~xuefuz], HIVE-14825 was created addressing this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-23T02:17:59.112+0000","updated":"2016-09-23T02:17:59.112+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15515964","id":"15515964","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"The offending jar comes as a dependency of jersey-client:\n{noformat}\n[INFO] |  +- org.glassfish.jersey.core:jersey-client:jar:2.22.2:compile\n[INFO] |  |  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile\n[INFO] |  |  +- org.glassfish.hk2:hk2-api:jar:2.4.0-b34:compile\n[INFO] |  |  |  +- org.glassfish.hk2:hk2-utils:jar:2.4.0-b34:compile\n[INFO] |  |  |  \\- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34:compile\n[INFO] |  |  +- org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34:compile\n[INFO] |  |  \\- org.glassfish.hk2:hk2-locator:jar:2.4.0-b34:compile\n[INFO] |  |     \\- org.javassist:javassist:jar:3.18.1-GA:compile\n{noformat}\nI think it's related to SPARK-12154. Spark updated to Jersey 2 and replaced com.sun.jersey with org.glassfish.jersey. Good news is seems we don't pack the jersey stuff in hive-exec. But not sure if this only affects the compile.\n[~xuefuz] what do you think about this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-23T09:39:43.685+0000","updated":"2016-09-23T09:39:43.685+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15516797","id":"15516797","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12829975/HIVE-14029.5.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10559 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testDelegationTokenSharedStore\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1287/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1287/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1287/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 7 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12829975 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-23T15:45:06.158+0000","updated":"2016-09-23T15:45:06.158+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15516900","id":"15516900","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~lirui], Thanks for the investigation. I'm wondering if that dependency can be excluded in Hive's build? While the latest patch builds, it changes Hive's existing dependency, which might cause some problem.\n\nAlso, we are upgrading the following libraries. I'm not sure If it's absolutely necessary. From my build alone, it seems not. [~Ferd], any thoughts?\n{quote}\n** Jackson: 2.4.2 -> 2.6.5\n** Netty version: 4.0.23.Final -> 4.0.29.Final\n{quote}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-23T16:27:52.503+0000","updated":"2016-09-23T16:27:52.503+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15518237","id":"15518237","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~xuefuz] These two dependencies (Jackson and Netty) are not required in build. It's required for the runtime. If you try to run some HoS job, it will fail to create Spark client since API changes in these two library. You can see failed queries above for the reference.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-24T03:04:31.477+0000","updated":"2016-09-24T03:10:40.723+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15518326","id":"15518326","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Let's see whether it breaks qtest after removing org.glassfish.jersey related dependencies from Spark_core","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-24T04:09:22.817+0000","updated":"2016-09-24T04:09:22.817+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15518578","id":"15518578","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12830171/HIVE-14029.6.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 16 failed/errored test(s), 10629 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse\norg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\norg.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles\norg.apache.hive.spark.client.TestSparkClient.testCounters\norg.apache.hive.spark.client.TestSparkClient.testErrorJob\norg.apache.hive.spark.client.TestSparkClient.testJobSubmission\norg.apache.hive.spark.client.TestSparkClient.testMetricsCollection\norg.apache.hive.spark.client.TestSparkClient.testRemoteClient\norg.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob\norg.apache.hive.spark.client.TestSparkClient.testSyncRpc\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1299/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1299/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1299/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 16 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12830171 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-24T07:08:13.294+0000","updated":"2016-09-24T07:08:13.294+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15518591","id":"15518591","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":" [~xuefuz] [~lirui], Removing the dependency about org.glassfish.jersey will fail QTest with following error. I think we should use and commit the 5th patch instead.  Any thoughts about it? \n\n{code}\n2016-09-23T23:48:02,527  INFO [Driver] util.log: Logging initialized @3685ms\nException in thread \"Driver\" java.lang.NoClassDefFoundError: org/glassfish/jersey/servlet/ServletContainer\n\tat org.apache.spark.status.api.v1.ApiRootResource$.getServletHandler(ApiRootResource.scala:193)\n\tat org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:75)\n\tat org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:81)\n\tat org.apache.spark.ui.SparkUI$.create(SparkUI.scala:215)\n\tat org.apache.spark.ui.SparkUI$.createLiveUI(SparkUI.scala:157)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:443)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:157)\n\tat org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:516)\n\tat org.apache.hive.spark.client.SparkClientImpl$2.run(SparkClientImpl.java:228)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: org.glassfish.jersey.servlet.ServletContainer\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:360)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 11 more\n{code}\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-24T07:18:11.572+0000","updated":"2016-09-24T09:42:46.083+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15519134","id":"15519134","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"[~Ferd], you mean unit test will fail right? I don't see failed spark qtest in last QA report. For qtest/runtime, we use spark-submit to submit the app, so spark should add all its dependencies to classpath. One problem I can think of is if we don't exclude the glassfish jersey, will hive pull two versions of jersey into its classpath, i.e. the lib dir? If so, that can cause problem for hive's functionalities that depend on jersey.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-24T14:40:25.214+0000","updated":"2016-09-24T14:40:25.214+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15521852","id":"15521852","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Hi [~lirui]\n bq.  you mean unit test will fail right?\nIt's my fault, it's failing some unit tests. HMM, the failed cases are caused by lack of those jars. To fix them, include Glassfish related jars in *test only*. Attached is the new version addressing above.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-26T02:41:17.886+0000","updated":"2016-09-26T02:41:17.886+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15521956","id":"15521956","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12830239/HIVE-14029.7.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10629 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse\norg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1300/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1300/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1300/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 8 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12830239 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-26T03:54:22.217+0000","updated":"2016-09-26T03:54:22.217+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15522284","id":"15522284","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12830248/HIVE-14029.7.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10629 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\norg.apache.hive.spark.client.rpc.TestRpc.testClientTimeout\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1301/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1301/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1301/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 7 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12830248 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-26T07:04:26.235+0000","updated":"2016-09-26T07:04:26.235+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15522527","id":"15522527","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12830260/HIVE-14029.7.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10629 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse\norg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1303/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1303/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1303/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 8 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12830260 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-26T09:09:09.137+0000","updated":"2016-09-26T09:09:09.137+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15525265","id":"15525265","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12830442/HIVE-14029.8.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10640 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\norg.apache.hive.spark.client.TestSparkClient.testJobSubmission\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1309/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1309/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1309/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 7 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12830442 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-27T06:45:49.306+0000","updated":"2016-09-27T06:45:49.306+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15525416","id":"15525416","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"I tried the 5th patch locally. The jersey 2 stuff won't be pulled to lib, or the hive-exec jar. I also tried to identify the minimum set. I managed to run some simple queries (spark on yarn) with only {{scala-library, spark-core, spark-network-common, spark-network-shuffle}}. However, if we want to support local mode, we need more jars added to hive's lib, including the jersey 2. Then we may have conflict problem. Other than that, I think the 5th patch is enough for us (although I think we should exclude jersey 2 instead of just javax.ws.rs).\n\nIf we still want to go the way as the 6th, 7th patches, maybe we can look at how we handle the guava conflict in the pom of spark-client and do something similar.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-27T07:59:25.863+0000","updated":"2016-09-27T07:59:25.863+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15525460","id":"15525460","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"[~lirui], thank you for your investigation. Can you please update HIVE-14825 about the minimum jar set?\nbq. If we still want to go the way as the 6th, 7th patches, maybe we can look at how we handle the guava conflict in the pom of spark-client and do something similar.\n\nWe can do it in a separate ticket about Guava conflict.  For the 7th patch, there's still one failed test case \"org.apache.hive.spark.client.TestSparkClient.testJobSubmission\" which I can't reproduce locally. Let's wait for another HIVE QA report to see whether it's reproducible.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-27T08:19:46.842+0000","updated":"2016-09-27T08:19:46.842+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15525573","id":"15525573","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Yeah I'll update HIVE-14825 once I have identified the minimum set for different modes.\nbq. We can do it in a separate ticket about Guava conflict.\nI mean we used to have guava conflict (HIVE-7387), which is similar to this one: spark uses a newer version while hive/hadoop stick to the old one. At the end, spark shaded guava in the assembly to solve the issue (SPARK-2848). You can refer to the pom of spark-client about how to explicitly add the guava jars to run the unit tests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-27T09:16:05.863+0000","updated":"2016-09-27T09:16:05.863+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15525706","id":"15525706","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12830453/HIVE-14029.8.patch\n\n{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.\n\n{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10640 tests executed\n*Failed tests:*\n{noformat}\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]\norg.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]\norg.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]\norg.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts\norg.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching\n{noformat}\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1311/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1311/console\nTest logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1311/\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.TestCheckPhase\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\nTests exited with: TestsFailedException: 6 tests failed\n{noformat}\n\nThis message is automatically generated.\n\nATTACHMENT ID: 12830453 - PreCommit-HIVE-Build","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2016-09-27T10:26:46.180+0000","updated":"2016-09-27T10:26:46.180+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15526164","id":"15526164","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"The latest patch passed all tests. [~xuefuz] [~lirui], do you have any further comments? I'd like to commit it if you have no further comments about the latest patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-27T13:44:13.710+0000","updated":"2016-09-27T13:44:13.710+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15526180","id":"15526180","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Thank you for providing this information. I will try to investigate it in Spark side. Considering Spark 2.0.0 is already released, if there is some work to do in Spark side, we may have to wait for next release.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-27T13:47:52.894+0000","updated":"2016-09-27T13:47:52.894+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15528012","id":"15528012","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Committed to the master. [~aihuaxu] [~szehon] [~xuefuz] [~lirui] [~spena] [~stakiar] Thank you for the reviews.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-28T01:22:01.655+0000","updated":"2016-09-28T01:22:01.655+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15528017","id":"15528017","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user winningsix closed the pull request at:\n\n    https://github.com/apache/hive/pull/103\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2016-09-28T01:24:14.138+0000","updated":"2016-09-28T01:24:14.138+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15528484","id":"15528484","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"Should this be documented in the wiki?\n\n* [Hive on Spark: Getting Started | https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2016-09-28T05:39:15.931+0000","updated":"2016-09-28T05:39:15.931+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15528490","id":"15528490","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Thanks [~leftylev] for the remind. It should be updated in WIKI. Shall we add a new section for Hive on Spark 2.0?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-28T05:44:24.752+0000","updated":"2016-09-28T05:44:24.752+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15528537","id":"15528537","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Since different versions of Spark are not binary compatible, it'd be good if we can document the min/max supported Spark version for each release of Hive, e.g. the minimum supported Spark version is 2.0.0 for Hive 2.2.0. Should have done this in previous upgrades :(","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-09-28T06:01:36.080+0000","updated":"2016-09-28T06:01:36.080+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15528563","id":"15528563","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"Okay, I added a TODOC2.2 label.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2016-09-28T06:14:41.708+0000","updated":"2016-09-28T06:14:41.708+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15529838","id":"15529838","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"[~lirui] Oh, Sounds like a big compatibility change for Hive 2.x series. \n[~xuefuz] Do you know how we handle these breaking changes on Hive versions? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-09-28T14:35:07.779+0000","updated":"2016-09-28T14:35:07.779+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15531017","id":"15531017","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"[~Ferd] can you also add an \"Incompatible Change\" Flag to this JIRA.\n\nI'm guessing this should go into the Hive 2.2.0 release since its an incompatible change, and I agree we should document this all on the wiki. I don't know much about Spark 2, but will Hive-on-Spark2 be able to run against a Spark1 cluster, or vice versa?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2016-09-28T22:04:49.667+0000","updated":"2016-09-28T22:04:49.667+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15531389","id":"15531389","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"bq. I'm guessing this should go into the Hive 2.2.0 release since its an incompatible change\n\nAgree.\n\nbq. will Hive-on-Spark2 be able to run against a Spark1 cluster, or vice versa?\n\nAFAIK, it will not able to run against Spark1 cluster for the dependency conflicts. If we want to support different Spark cluster, we may need a shim loader for Spark in Spark client. Do we have a strong requirement for that?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2016-09-29T00:41:46.121+0000","updated":"2016-09-29T00:41:46.121+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15542999","id":"15542999","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"body":"We probably don't need a shim loader right now. I don't know of any requirements to have one, so we should be good for now. If users starting hitting upgrade issues then it may be something to consider in the future.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stakiar","name":"stakiar","key":"stakiar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sahil Takiar","active":true,"timeZone":"Etc/UTC"},"created":"2016-10-03T18:01:03.664+0000","updated":"2016-10-03T18:01:03.664+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15544407","id":"15544407","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"[~Ferd] and [~lirui], yes we should add a section on Spark versions that are compatible with different Hive releases, and include as much information as possible.\n\n* [Hive on Spark:  Getting Started | https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started]\n\nI was about to add such a section at the beginning of the doc (before Spark Installation) but hesitated because I don't know what version(s) can be used with the installation instructions.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2016-10-04T05:33:13.501+0000","updated":"2016-10-04T05:33:13.501+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15583248","id":"15583248","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"[~xuefuz] [~Ferd] [~lirui] Back to compatibility discussion, I think we should continue keeping Spark 1.x compatibility on Hive 2.x series (as we did on Hive 1.x with Hadoop 1.x/2.x). If there are users using Spark 1.x, then they won't be able to upgrade to Hive 2.2, and they do not necessary need to upgrade to Spark 2.0 as it is still a new release, and not many people upgrade to a 2.0 version immediately.\n\nWhat do you thing about this guys? is it important to keep compatibility on Hive 2.x until we release Hive 3.0 in the future?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-10-17T19:51:23.190+0000","updated":"2016-10-17T19:51:23.190+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15583356","id":"15583356","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~spena], Keeping b/c is a good thing in general. Before we take the effort (which seems a lot) to do it, I think we should clearly understand and define what b/c is in this case. Spark is rapidly releasing w/o much b/c in mind. So far, Hive on Spark has once depended on Spark 1.2, 1.3, 1.4, 1.5, and 1.6. I'm not sure what versions of Spark Hive has been released with, but one thing is clear, Spark isn't b/c between these releases. Before Spark community has a good sense of keeping b/c in their APIs, it's going to be very hard and burdensome for Hive to maintain support for different Spark releases, not to mention the library dependency issues we have had.\n\nI'm okay to start thinking of a shim layer to support multiple versions of Spark, but it sounds daunting to me due to the dynamics of Spark project.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-10-17T20:31:45.913+0000","updated":"2016-10-17T20:31:45.913+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15583393","id":"15583393","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"body":"Interesting, so even between Spark 1.x versions, Hive wasn't compatible at all with them? This is going to be a lot of work as you said. If Spark 2.1 isn't compatible with Spark 2.0 for instance, then we will have a shim layer with minor changes per Spark version to keep compatibility.\n\n[~xuefuz] Were there users in the community complaining about Spark 1.x incompatibilities with Hive in the past? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=spena","name":"spena","key":"spena","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergio Peña","active":true,"timeZone":"America/Chicago"},"created":"2016-10-17T20:41:26.753+0000","updated":"2016-10-17T20:41:26.753+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15583487","id":"15583487","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Spark claims API compatibility within a major release, but it doesn't seem so based on our experience. \nhttps://issues.apache.org/jira/browse/HIVE-9726\nhttps://issues.apache.org/jira/browse/HIVE-10999\nhttps://issues.apache.org/jira/browse/HIVE-11473\nhttps://issues.apache.org/jira/browse/HIVE-12828\nIn two of the four upgrades, there are incompatibility API changes.\n\nSpark is still a young project, so people may have lower expectation on this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xuefuz","name":"xuefuz","key":"xuefuz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Xuefu Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-10-17T21:12:07.687+0000","updated":"2016-10-17T21:12:07.687+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979634/comment/15584325","id":"15584325","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"body":"Hmm even with a shim layer, it's difficult to support different Spark versions if b/c is not maintained between minor releases of Spark.\nI'm wondering if the Spark used by Hive can be considered as some kind of embedded binaries that exclusively used for HoS. On Hive side, we just need to set spark.home pointing to this Spark. User's other Spark applications, e.g. SparkSQL, streaming, can still run against the current Spark they have in the cluster. Will this make it easier for the upgrade?\nI think we also need to be more careful to upgrade Spark in the future, if the upgrade is breaking compatibility. For such upgrade, we need to firstly make sure there's no obvious regression in functionality and performance.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lirui","name":"lirui","key":"lirui","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rui Li","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-10-18T03:42:00.427+0000","updated":"2016-10-18T03:42:00.427+0000"}],"maxResults":102,"total":102,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-14029/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2zj7b:"}}