{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13191563","self":"https://issues.apache.org/jira/rest/api/2/issue/13191563","key":"HIVE-20747","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":null,"customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"2018-10-15 11:54:32.802","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-20747/watchers","watchCount":3,"isWatching":false},"created":"2018-10-15T11:54:32.802+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-10-15T11:54:32.802+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12323200","id":"12323200","name":"Spark","description":"Hive on Spark"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12317406","id":"12317406","name":"Tests"}],"timeoriginalestimate":null,"description":"reproduce:\r\n{code}\r\ntime mvn install -Pitests -pl itests/qtest-spark/ -Dtest=TestMiniSparkOnYarnCliDriver#testCliDriver[spark_explainuser_1]  -am \r\n{code}\r\n\r\nI think the actual error is misleading...the real exception in hive.log is:\r\n{code}\r\n2018-10-15T04:44:39,102 ERROR [5bad7b56-dbbe-4868-8006-0aeecf9eb6c3 main] status.SparkJobMonitor: Spark job[1] failed\r\njava.util.concurrent.ExecutionException: Exception thrown by job\r\n        at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:337) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:342) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:404) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n        at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:365) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_181]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_181]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_181]\r\n        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_181]\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, savara.lan, executor 1): java.lang.NoSuchMethodError: com.esotericsoftware.kryo.io.Output.writeVarInt(IZ)I\r\n        at org.apache.hive.spark.HiveKryoRegistrator$HiveKeySerializer.write(HiveKryoRegistrator.java:44)\r\n        at org.apache.hive.spark.HiveKryoRegistrator$HiveKeySerializer.write(HiveKryoRegistrator.java:41)\r\n        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568)\r\n        at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:241)\r\n        at org.apache.spark.serializer.SerializationStream.writeKey(Serializer.scala:132)\r\n        at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:240)\r\n        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library-2.11.8.jar:?]\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) ~[scala-library-2.11.8.jar:?]\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at scala.Option.foreach(Option.scala:257) ~[scala-library-2.11.8.jar:?]\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\nCaused by: java.lang.NoSuchMethodError: com.esotericsoftware.kryo.io.Output.writeVarInt(IZ)I\r\n        at org.apache.hive.spark.HiveKryoRegistrator$HiveKeySerializer.write(HiveKryoRegistrator.java:44) ~[?:?]\r\n        at org.apache.hive.spark.HiveKryoRegistrator$HiveKeySerializer.write(HiveKryoRegistrator.java:41) ~[?:?]\r\n        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568) ~[kryo-2.21.jar:?]\r\n        at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:241) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.serializer.SerializationStream.writeKey(Serializer.scala:132) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:240) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) ~[spark-core_2.11-2.3.0.jar:2.3.0]\r\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Running a single spark tests alone (spark_explainuser_1) results in NoSuchMethodError","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kgyrtkirk","name":"kgyrtkirk","key":"kgyrtkirk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kgyrtkirk&avatarId=32755","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kgyrtkirk&avatarId=32755","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kgyrtkirk&avatarId=32755","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kgyrtkirk&avatarId=32755"},"displayName":"Zoltan Haindrich","active":true,"timeZone":"Europe/Budapest"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[],"maxResults":0,"total":0,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-20747/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3z6zb:"}}