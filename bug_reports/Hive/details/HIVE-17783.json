{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13108780","self":"https://issues.apache.org/jira/rest/api/2/issue/13108780","key":"HIVE-17783","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-10-12T21:07:49.089+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Oct 31 08:53:27 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17783/watchers","watchCount":5,"isWatching":false},"created":"2017-10-12T03:06:38.217+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12335837","id":"12335837","name":"2.2.0","archived":false,"released":true,"releaseDate":"2017-07-25"}],"issuelinks":[{"id":"12517222","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12517222","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12822410","key":"HIVE-10403","self":"https://issues.apache.org/jira/rest/api/2/issue/12822410","fields":{"summary":"Add n-way join support for Hybrid Grace Hash Join","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-10-31T08:53:27.646+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"Most configurations are using default value. And the benchmark is to test enabling against disabling hybrid grace hash join using TPC-DS queries at 3TB data scales. Many queries related to N-way join has performance degradation over three times test. Detailed result  is attached.\r\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12891616","id":"12891616","filename":"Hybrid_Grace_Hash_Join.xlsx","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2017-10-12T03:07:27.829+0000","size":14877,"mimeType":"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet","content":"https://issues.apache.org/jira/secure/attachment/12891616/Hybrid_Grace_Hash_Join.xlsx"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12891870","id":"12891870","filename":"screenshot-1.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2017-10-13T01:43:52.188+0000","size":7540,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12891870/screenshot-1.png"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hybrid Grace Hash Join has performance degradation for N-way join using Hive on Tez","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"8*Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz\r\n1 master + 7 workers\r\nTPC-DS at 3TB data scales\r\nHive version : 2.2.0","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108780/comment/16201380","id":"16201380","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"CC [~wzheng] [~sershe] ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2017-10-12T03:10:29.650+0000","updated":"2017-10-12T03:10:29.650+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108780/comment/16202623","id":"16202623","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"Hmm, we observed this with LLAP (which has advantage of table sharing in non-hybrid case), but not with containers to the best of my knowledge. It's possible it's slower even w/o accounting for sharing.\r\nIt's also possible that mapjoin table size was configured to be too small so there was too much needless spilling...\r\n[~gopalv] any input?\r\nWe could disable it by default if it's hard to tune. Then for the cases where it can shine people can turn it on and tune the sizing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-10-12T21:07:49.089+0000","updated":"2017-10-12T21:07:49.089+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108780/comment/16202933","id":"16202933","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"body":"I think we can disable it by default  \r\nThe main motivation was actually avoiding OOMs as far as I understand. I don't thin anyone is working on perf improvements right now.\r\ncc [~gopalv]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sershe","name":"sershe","key":"sershe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sergey Shelukhin","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-10-13T01:52:21.163+0000","updated":"2017-10-13T01:52:21.163+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108780/comment/16202938","id":"16202938","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Thanks [~sershe] for your input. I think the degradation in my test should cause by the unnecessary spilling. I take a look at the longest tasks for both enable/disable which processing the same number of records and have a roughly estimation for each phase with log.\r\n!screenshot-1.png!\r\n\r\nExtra reprocessing takes longer time and in disable case those data actually is not spilled into the disk. And from the following logs we can see that the spilled row numbers are actually very small (e.g. partition 0: 0 row, partition 3: 1 row) while the estimated memory is relative high (e.g. partition 0: 65636, partition 3: 589924). This is because the estimated memory is obtained from WBS from BytesBytesMultiHashMap. But in disabled case, there is limited overhead of creating BytesBytesMultiHashMap which maintains only one such hash map. I think we need to figure out a way to have better estimation of memory to avoid unnecessary spill caused by memory. Any thoughts or suggestions about this point?\r\n\r\n{noformat}\r\n2017-10-13 09:14:43,666 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Spilling hash partition 0 (Rows: 0, Mem size: 65636): /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-0-1870407313239959464.tmp\r\n2017-10-13 09:14:43,666 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Memory usage before spilling: 1050880\r\n2017-10-13 09:14:43,666 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Memory usage after spilling: 985244\r\n2017-10-13 09:14:43,667 [INFO] [pool-29-thread-1] |common.FileUtils|: Local directories not specified; created a tmp file: /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-3-8788724383233259683.tmp\r\n2017-10-13 09:14:43,667 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Trying to spill hash partition 3 ...\r\n2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Spilling hash partition 3 (Rows: 1, Mem size: 589924): /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-3-8788724383233259683.tmp\r\n2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Memory usage before spilling: 1509532\r\n2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Memory usage after spilling: 919608\r\n2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |common.FileUtils|: Local directories not specified; created a tmp file: /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-15-1832304146451074287.tmp\r\n2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Trying to spill hash partition 15 ...\r\n2017-10-13 09:14:43,676 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Spilling hash partition 15 (Rows: 2, Mem size: 589924): /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-15-1832304146451074287.tmp\r\n{noformat}\r\n\r\n\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2017-10-13T02:04:30.837+0000","updated":"2017-10-13T02:04:30.837+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108780/comment/16202985","id":"16202985","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"> It's possible it's slower even w/o accounting for sharing. \r\nCould you please expand a little bit and explain in more detail?\r\n\r\n> The main motivation was actually avoiding OOMs as far as I understand.\r\nAgree, this feature can make map join more general when hash table can not fit into the memory.\r\n\r\n> I don't thin anyone is working on perf improvements right now.\r\nLogically it should have some performance benefits over the non hybrid grace hash join since it isn't required to scan the big table again during the reprocessing phase when hash table can not fit into the memory.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2017-10-13T02:24:36.779+0000","updated":"2017-10-13T02:29:59.263+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108780/comment/16225678","id":"16225678","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wzheng","name":"wzheng","key":"wzheng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wei Zheng","active":true,"timeZone":"America/Los_Angeles"},"body":"[~Ferd] Sorry for the late reply. Yes the spilling part is the bottleneck and there's no easy way to get around it. In your case for the n-way joins, the optimizer stats estimation may not be accurate which makes the situation worse. Anyway, the ultimate way to solve this problem is to have a reliable memory manager which can provide memory usage/quota at any moment. Right now we're following a conservative approach, which is to use a soft (possibly inaccurate) memory limit. That way we can avoid unnecessary spilling if there is enough memory for loading the hashtable.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wzheng","name":"wzheng","key":"wzheng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wei Zheng","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-10-30T20:31:12.873+0000","updated":"2017-10-30T20:31:12.873+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108780/comment/16226472","id":"16226472","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Thanks [~wei.zheng] for your reply.\r\n>  In your case for the n-way joins, the optimizer stats estimation may not be accurate which makes the situation worse.\r\n\r\nAFAIK, the row size estimated should be the same with non-hybrid grace hash join case. It's strange why the spill happens in hybrid grace hash join case.\r\nAnother observation is o rows of data for one partition is occupying about 65636 bytes memory.\r\n\r\n> Anyway, the ultimate way to solve this problem is to have a reliable memory manager which can provide memory usage/quota at any moment. Right now we're following a conservative approach, which is to use a soft (possibly inaccurate) memory limit. That way we can avoid unnecessary spilling if there is enough memory for loading the hashtable.\r\n\r\nInterest. Any ticket addressing this part of work?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Ferd","name":"Ferd","key":"ferd","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ferd&avatarId=21543","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ferd&avatarId=21543","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ferd&avatarId=21543","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ferd&avatarId=21543"},"displayName":"Ferdinand Xu","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2017-10-31T08:53:27.646+0000","updated":"2017-10-31T08:53:27.646+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-17783/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3l5zb:"}}