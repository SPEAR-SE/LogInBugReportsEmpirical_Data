{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13020309","self":"https://issues.apache.org/jira/rest/api/2/issue/13020309","key":"HIVE-15189","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/10004","id":"10004","description":"Not A Bug","name":"Not A Bug"},"customfield_12312322":null,"customfield_12310220":"2016-11-14T05:43:24.835+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Nov 14 05:43:24 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_113818651_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2016-11-14T05:43:59.326+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15189/watchers","watchCount":2,"isWatching":false},"created":"2016-11-12T22:07:02.510+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332384","id":"12332384","name":"1.2.1","archived":false,"released":true,"releaseDate":"2015-06-26"}],"issuelinks":[{"id":"12486402","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12486402","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"outwardIssue":{"id":"12996273","key":"SPARK-16996","self":"https://issues.apache.org/jira/rest/api/2/issue/12996273","fields":{"summary":"Hive ACID delta files not seen","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-11-14T05:44:01.044+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12322671","id":"12322671","name":"Transactions","description":"Transaction management and ACID"}],"timeoriginalestimate":null,"description":"Hi,\nWhen one creates a new ACID table and inserts data into it using INSERT INTO, Hive does not write a 'base' file : it only creates a delta. That may lead to two issues (at least):\n# when you try to read it, you might get  a 'serious problem' like that :\n{noformat}\njava.lang.RuntimeException: serious problem\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)\n        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n        at scala.Option.getOrElse(Option.scala:120)\n        at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n        at scala.Option.getOrElse(Option.scala:120)\n        at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n        at scala.Option.getOrElse(Option.scala:120)\n        at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n        at scala.Option.getOrElse(Option.scala:120)\n        at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n        at org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:166)\n        at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n        at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:635)\n        at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:64)\n        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:311)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:226)\n        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)\n        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)\n        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)\n        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)\n        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: delta_0000000_0000000 does not start with base_\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:998)\n        ... 44 more\n{noformat}\nI do not always get this error, but when I get it, I need to drop/recreate table.\n# Spark-sql does not see data as long as there is no base file. So as long as compaction has not occurred, no data can be read using Spar-SQL. See https://issues.apache.org/jira/browse/SPARK-16996\n\nI know Hive always creates a base file when ou INSERT OVERWRITE, but you cannot always use OVERWRITE instead of INTO : in my use case, I use a statement that writes data into partitions that already exist and partitions that do not exist yet.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"No base file for ACID table","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bbonnet","name":"bbonnet","key":"bbonnet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Benjamin BONNET","active":true,"timeZone":"Europe/Paris"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bbonnet","name":"bbonnet","key":"bbonnet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Benjamin BONNET","active":true,"timeZone":"Europe/Paris"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"HDP 2.4, HDP2.5","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13020309/comment/15662845","id":"15662845","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"This is by design.  Work needs to be done in Spark to make it so that it understands the delta files (including which ones to read).  Note also that it's possible with ACID tables to have multiple base files simultaneously (since it works by keeping multiple versions of the data and using the metadata to determine which version a given reader should see), and I assume this would mess up spark readers as well.\n\nYou can work around the situation by forcing a major compaction before you read the data by doing \"alter table compact\" (see https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterTable/PartitionCompact ),","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-11-14T05:43:24.835+0000","updated":"2016-11-14T05:43:24.835+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-15189/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3699j:"}}