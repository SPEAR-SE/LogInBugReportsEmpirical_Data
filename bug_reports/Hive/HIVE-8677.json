{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hagleitn&avatarId=16035",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hagleitn&avatarId=16035",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hagleitn&avatarId=16035",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=hagleitn&avatarId=16035"
            },
            "displayName": "Gunther Hagleitner",
            "key": "hagleitn",
            "name": "hagleitn",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=hagleitn",
            "timeZone": "America/Los_Angeles"
        },
        "components": [],
        "created": "2014-10-30T21:51:04.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmokhtar&avatarId=21863",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmokhtar&avatarId=21863",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmokhtar&avatarId=21863",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=mmokhtar&avatarId=21863"
            },
            "displayName": "Mostafa Mokhtar",
            "key": "mmokhtar",
            "name": "mmokhtar",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=mmokhtar",
            "timeZone": "America/Los_Angeles"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2014-10-31T06:15:34.575+0000",
        "customfield_12310222": "10002_*:*_1_*:*_47590979_*|*_1_*:*_1_*:*_30294413_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "1.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i21sfj:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Nov 13 19:42:20 UTC 2014",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "TPC-DS Q51 fails with the exception below \n{code}\n, TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n{code}\n\nQuery\n{code}\nset hive.cbo.enable=true;\nset hive.stats.fetch.column.stats=true;\nset hive.exec.dynamic.partition.mode=nonstrict;\nset hive.tez.auto.reducer.parallelism=true;\nset hive.tez.exec.print.summary=true;\nset hive.auto.convert.join.noconditionaltask.size=1280000000;\nset hive.exec.reducers.bytes.per.reducer=100000000;\nset hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;\nset hive.support.concurrency=false;\n \nWITH web_v1 as (\nselect\n  ws_item_sk item_sk, d_date, sum(ws_sales_price),\n  sum(sum(ws_sales_price))\n      over (partition by ws_item_sk order by d_date rows between unbounded preceding and current row) cume_sales\nfrom web_sales\n    ,date_dim\nwhere ws_sold_date_sk=d_date_sk\n  and d_month_seq between 1193 and 1193+11\n  and ws_item_sk is not NULL\ngroup by ws_item_sk, d_date),\nstore_v1 as (\nselect\n  ss_item_sk item_sk, d_date, sum(ss_sales_price),\n  sum(sum(ss_sales_price))\n      over (partition by ss_item_sk order by d_date rows between unbounded preceding and current row) cume_sales\nfrom store_sales\n    ,date_dim\nwhere ss_sold_date_sk=d_date_sk\n  and d_month_seq between 1193 and 1193+11\n  and ss_item_sk is not NULL\ngroup by ss_item_sk, d_date)\n select  *\nfrom (select item_sk\n     ,d_date\n     ,web_sales\n     ,store_sales\n     ,max(web_sales)\n         over (partition by item_sk order by d_date rows between unbounded preceding and current row) web_cumulative\n     ,max(store_sales)\n         over (partition by item_sk order by d_date rows between unbounded preceding and current row) store_cumulative\n     from (select case when web.item_sk is not null then web.item_sk else store.item_sk end item_sk\n                 ,case when web.d_date is not null then web.d_date else store.d_date end d_date\n                 ,web.cume_sales web_sales\n                 ,store.cume_sales store_sales\n           from web_v1 web full outer join store_v1 store on (web.item_sk = store.item_sk\n                                                          and web.d_date = store.d_date)\n          )x )y\nwhere web_cumulative > store_cumulative\norder by item_sk\n        ,d_date\nlimit 100\n{code}\n\nPlan\n{code}\nOK\nSTAGE DEPENDENCIES:\n  Stage-1 is a root stage\n  Stage-0 depends on stages: Stage-1\n\nSTAGE PLANS:\n  Stage: Stage-1\n    Tez\n      Edges:\n        Map 2 <- Map 8 (BROADCAST_EDGE)\n        Map 9 <- Map 1 (BROADCAST_EDGE)\n        Reducer 10 <- Map 9 (SIMPLE_EDGE)\n        Reducer 11 <- Reducer 10 (SIMPLE_EDGE)\n        Reducer 3 <- Map 2 (SIMPLE_EDGE)\n        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)\n        Reducer 5 <- Reducer 11 (SIMPLE_EDGE), Reducer 4 (SIMPLE_EDGE)\n        Reducer 6 <- Reducer 5 (SIMPLE_EDGE)\n        Reducer 7 <- Reducer 6 (SIMPLE_EDGE)\n      DagName: mmokhtar_20141030010808_11af3ba0-8b28-4a33-9f4d-73618503e272:1\n      Vertices:\n        Map 1 \n            Map Operator Tree:\n                TableScan\n                  alias: date_dim\n                  filterExpr: (d_date_sk is not null and d_month_seq BETWEEN 1193 AND 1204) (type: boolean)\n                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: (d_date_sk is not null and d_month_seq BETWEEN 1193 AND 1204) (type: boolean)\n                    Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                    Reduce Output Operator\n                      key expressions: d_date_sk (type: int)\n                      sort order: +\n                      Map-reduce partition columns: d_date_sk (type: int)\n                      Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                      value expressions: d_date (type: string), d_month_seq (type: int)\n                    Select Operator\n                      expressions: d_date_sk (type: int)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                      Group By Operator\n                        keys: _col0 (type: int)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                        Dynamic Partitioning Event Operator\n                          Target Input: store_sales\n                          Partition key expr: ss_sold_date_sk\n                          Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                          Target column: ss_sold_date_sk\n                          Target Vertex: Map 9\n            Execution mode: vectorized\n        Map 2 \n            Map Operator Tree:\n                TableScan\n                  alias: web_sales\n                  filterExpr: ws_item_sk is not null (type: boolean)\n                  Statistics: Num rows: 21594638446 Data size: 2850189889652 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: ws_item_sk is not null (type: boolean)\n                    Statistics: Num rows: 21594638446 Data size: 259124859072 Basic stats: COMPLETE Column stats: COMPLETE\n                    Map Join Operator\n                      condition map:\n                           Inner Join 0 to 1\n                      condition expressions:\n                        0 {ws_item_sk} {ws_sales_price} {ws_sold_date_sk}\n                        1 {d_date_sk} {d_date} {d_month_seq}\n                      keys:\n                        0 ws_sold_date_sk (type: int)\n                        1 d_date_sk (type: int)\n                      outputColumnNames: _col2, _col20, _col33, _col37, _col39, _col40\n                      input vertices:\n                        1 Map 8\n                      Statistics: Num rows: 24145061366 Data size: 2752536995724 Basic stats: COMPLETE Column stats: COMPLETE\n                      Filter Operator\n                        predicate: (((_col33 = _col37) and _col40 BETWEEN 1193 AND 1204) and _col2 is not null) (type: boolean)\n                        Statistics: Num rows: 6036265341 Data size: 688134248874 Basic stats: COMPLETE Column stats: COMPLETE\n                        Select Operator\n                          expressions: _col2 (type: int), _col39 (type: string), _col20 (type: float)\n                          outputColumnNames: _col2, _col39, _col20\n                          Statistics: Num rows: 6036265341 Data size: 688134248874 Basic stats: COMPLETE Column stats: COMPLETE\n                          Group By Operator\n                            aggregations: sum(_col20)\n                            keys: _col2 (type: int), _col39 (type: string)\n                            mode: hash\n                            outputColumnNames: _col0, _col1, _col2\n                            Statistics: Num rows: 6036265341 Data size: 639844126146 Basic stats: COMPLETE Column stats: COMPLETE\n                            Reduce Output Operator\n                              key expressions: _col0 (type: int), _col1 (type: string)\n                              sort order: ++\n                              Map-reduce partition columns: _col0 (type: int), _col1 (type: string)\n                              Statistics: Num rows: 6036265341 Data size: 639844126146 Basic stats: COMPLETE Column stats: COMPLETE\n                              value expressions: _col2 (type: double)\n            Execution mode: vectorized\n        Map 8 \n            Map Operator Tree:\n                TableScan\n                  alias: date_dim\n                  filterExpr: (d_date_sk is not null and d_month_seq BETWEEN 1193 AND 1204) (type: boolean)\n                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: (d_date_sk is not null and d_month_seq BETWEEN 1193 AND 1204) (type: boolean)\n                    Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                    Reduce Output Operator\n                      key expressions: d_date_sk (type: int)\n                      sort order: +\n                      Map-reduce partition columns: d_date_sk (type: int)\n                      Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                      value expressions: d_date (type: string), d_month_seq (type: int)\n                    Select Operator\n                      expressions: d_date_sk (type: int)\n                      outputColumnNames: _col0\n                      Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                      Group By Operator\n                        keys: _col0 (type: int)\n                        mode: hash\n                        outputColumnNames: _col0\n                        Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                        Dynamic Partitioning Event Operator\n                          Target Input: web_sales\n                          Partition key expr: ws_sold_date_sk\n                          Statistics: Num rows: 36524 Data size: 3725448 Basic stats: COMPLETE Column stats: COMPLETE\n                          Target column: ws_sold_date_sk\n                          Target Vertex: Map 2\n            Execution mode: vectorized\n        Map 9 \n            Map Operator Tree:\n                TableScan\n                  alias: store_sales\n                  filterExpr: ss_item_sk is not null (type: boolean)\n                  Statistics: Num rows: 82510879939 Data size: 6873789738208 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: ss_item_sk is not null (type: boolean)\n                    Statistics: Num rows: 82510879939 Data size: 982359338028 Basic stats: COMPLETE Column stats: COMPLETE\n                    Map Join Operator\n                      condition map:\n                           Inner Join 0 to 1\n                      condition expressions:\n                        0 {ss_item_sk} {ss_sales_price} {ss_sold_date_sk}\n                        1 {d_date_sk} {d_date} {d_month_seq}\n                      keys:\n                        0 ss_sold_date_sk (type: int)\n                        1 d_date_sk (type: int)\n                      outputColumnNames: _col1, _col12, _col22, _col26, _col28, _col29\n                      input vertices:\n                        1 Map 1\n                      Statistics: Num rows: 92255782124 Data size: 10517159162136 Basic stats: COMPLETE Column stats: COMPLETE\n                      Filter Operator\n                        predicate: (((_col22 = _col26) and _col29 BETWEEN 1193 AND 1204) and _col1 is not null) (type: boolean)\n                        Statistics: Num rows: 23063945531 Data size: 2629289790534 Basic stats: COMPLETE Column stats: COMPLETE\n                        Select Operator\n                          expressions: _col1 (type: int), _col28 (type: string), _col12 (type: float)\n                          outputColumnNames: _col1, _col28, _col12\n                          Statistics: Num rows: 23063945531 Data size: 2629289790534 Basic stats: COMPLETE Column stats: COMPLETE\n                          Group By Operator\n                            aggregations: sum(_col12)\n                            keys: _col1 (type: int), _col28 (type: string)\n                            mode: hash\n                            outputColumnNames: _col0, _col1, _col2\n                            Statistics: Num rows: 23063945531 Data size: 2444778226286 Basic stats: COMPLETE Column stats: COMPLETE\n                            Reduce Output Operator\n                              key expressions: _col0 (type: int), _col1 (type: string)\n                              sort order: ++\n                              Map-reduce partition columns: _col0 (type: int), _col1 (type: string)\n                              Statistics: Num rows: 23063945531 Data size: 2444778226286 Basic stats: COMPLETE Column stats: COMPLETE\n                              value expressions: _col2 (type: double)\n            Execution mode: vectorized\n        Reducer 10 \n            Reduce Operator Tree:\n              Group By Operator\n                aggregations: sum(VALUE._col0)\n                keys: KEY._col0 (type: int), KEY._col1 (type: string)\n                mode: mergepartial\n                outputColumnNames: _col0, _col1, _col2\n                Statistics: Num rows: 23063945531 Data size: 2537034008410 Basic stats: COMPLETE Column stats: COMPLETE\n                Reduce Output Operator\n                  key expressions: _col0 (type: int), _col1 (type: string)\n                  sort order: ++\n                  Map-reduce partition columns: _col0 (type: int)\n                  Statistics: Num rows: 23063945531 Data size: 2537034008410 Basic stats: COMPLETE Column stats: COMPLETE\n                  value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: double)\n            Execution mode: vectorized\n        Reducer 11 \n            Reduce Operator Tree:\n              Extract\n                Statistics: Num rows: 23063945531 Data size: 2537034008410 Basic stats: COMPLETE Column stats: COMPLETE\n                PTF Operator\n                  Statistics: Num rows: 23063945531 Data size: 2537034008410 Basic stats: COMPLETE Column stats: COMPLETE\n                  Select Operator\n                    expressions: _col0 (type: int), _col1 (type: string), _wcol0 (type: double)\n                    outputColumnNames: _col0, _col1, _col3\n                    Statistics: Num rows: 23063945531 Data size: 184511564248 Basic stats: COMPLETE Column stats: COMPLETE\n                    Reduce Output Operator\n                      key expressions: _col0 (type: int), _col1 (type: string)\n                      sort order: ++\n                      Map-reduce partition columns: _col0 (type: int), _col1 (type: string)\n                      Statistics: Num rows: 23063945531 Data size: 184511564248 Basic stats: COMPLETE Column stats: COMPLETE\n                      value expressions: _col3 (type: double)\n        Reducer 3 \n            Reduce Operator Tree:\n              Group By Operator\n                aggregations: sum(VALUE._col0)\n                keys: KEY._col0 (type: int), KEY._col1 (type: string)\n                mode: mergepartial\n                outputColumnNames: _col0, _col1, _col2\n                Statistics: Num rows: 6036265341 Data size: 663989187510 Basic stats: COMPLETE Column stats: COMPLETE\n                Reduce Output Operator\n                  key expressions: _col0 (type: int), _col1 (type: string)\n                  sort order: ++\n                  Map-reduce partition columns: _col0 (type: int)\n                  Statistics: Num rows: 6036265341 Data size: 663989187510 Basic stats: COMPLETE Column stats: COMPLETE\n                  value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: double)\n            Execution mode: vectorized\n        Reducer 4 \n            Reduce Operator Tree:\n              Extract\n                Statistics: Num rows: 6036265341 Data size: 663989187510 Basic stats: COMPLETE Column stats: COMPLETE\n                PTF Operator\n                  Statistics: Num rows: 6036265341 Data size: 663989187510 Basic stats: COMPLETE Column stats: COMPLETE\n                  Select Operator\n                    expressions: _col0 (type: int), _col1 (type: string), _wcol0 (type: double)\n                    outputColumnNames: _col0, _col1, _col3\n                    Statistics: Num rows: 6036265341 Data size: 48290122728 Basic stats: COMPLETE Column stats: COMPLETE\n                    Reduce Output Operator\n                      key expressions: _col0 (type: int), _col1 (type: string)\n                      sort order: ++\n                      Map-reduce partition columns: _col0 (type: int), _col1 (type: string)\n                      Statistics: Num rows: 6036265341 Data size: 48290122728 Basic stats: COMPLETE Column stats: COMPLETE\n                      value expressions: _col3 (type: double)\n        Reducer 5 \n            Reduce Operator Tree:\n              Merge Join Operator\n                condition map:\n                     Outer Join 0 to 1\n                condition expressions:\n                  0 {KEY.reducesinkkey0} {KEY.reducesinkkey1} {VALUE._col1}\n                  1 {KEY.reducesinkkey0} {KEY.reducesinkkey1} {VALUE._col1}\n                outputColumnNames: _col0, _col1, _col3, _col4, _col5, _col7\n                Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                Select Operator\n                  expressions: CASE WHEN (_col0 is not null) THEN (_col0) ELSE (_col4) END (type: int), CASE WHEN (_col1 is not null) THEN (_col1) ELSE (_col5) END (type: string), _col3 (type: double), _col7 (type: double)\n                  outputColumnNames: _col0, _col1, _col2, _col3\n                  Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                  Reduce Output Operator\n                    key expressions: _col0 (type: int), _col1 (type: string)\n                    sort order: ++\n                    Map-reduce partition columns: _col0 (type: int)\n                    Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                    value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: double), _col3 (type: double)\n        Reducer 6 \n            Reduce Operator Tree:\n              Extract\n                Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                PTF Operator\n                  Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE\n                  Filter Operator\n                    predicate: (_wcol0 > _wcol1) (type: boolean)\n                    Statistics: Num rows: 3074457345618258602 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                    Select Operator\n                      expressions: _col0 (type: int), _col1 (type: string), _col2 (type: double), _col3 (type: double), _wcol0 (type: double), _wcol1 (type: double)\n                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                      Statistics: Num rows: 3074457345618258602 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                      Reduce Output Operator\n                        key expressions: _col0 (type: int), _col1 (type: string)\n                        sort order: ++\n                        Statistics: Num rows: 3074457345618258602 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                        TopN Hash Memory Usage: 0.04\n                        value expressions: _col2 (type: double), _col3 (type: double), _col4 (type: double), _col5 (type: double)\n        Reducer 7 \n            Reduce Operator Tree:\n              Select Operator\n                expressions: KEY.reducesinkkey0 (type: int), KEY.reducesinkkey1 (type: string), VALUE._col0 (type: double), VALUE._col1 (type: double), VALUE._col2 (type: double), VALUE._col3 (type: double)\n                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                Statistics: Num rows: 3074457345618258602 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                Limit\n                  Number of rows: 100\n                  Statistics: Num rows: 100 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                  File Output Operator\n                    compressed: false\n                    Statistics: Num rows: 100 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                    table:\n                        input format: org.apache.hadoop.mapred.TextInputFormat\n                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n            Execution mode: vectorized\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: 100\n      Processor Tree:\n        ListSink\n{code}\n\nThe full exception \n{code}\nStatus: Failed\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Status: Failed\nVertex failed, vertexName=Reducer 11, vertexId=vertex_1414029100044_0733_1_05, diagnostics=[Task failed, taskId=task_1414029100044_0733_1_05_000027, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:262)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)\n\t... 13 more\nCaused by: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.tez.runtime.library.common.ValuesIterator$1$1.next(ValuesIterator.java:142)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processKeyValues(ReduceRecordSource.java:288)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)\n\t... 15 more\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum Error:  CurrentOffset=166741, offset=4, off=0, dataLength=167232, origLen=495, len=491, length=167236, checksumSize=4\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.doRead(IFileInputStream.java:254)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.read(IFileInputStream.java:184)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.close(IFileInputStream.java:131)\n\tat org.apache.hadoop.io.compress.DecompressorStream.close(DecompressorStream.java:205)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFile$Reader.close(IFile.java:784)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.closeReader(TezMerger.java:332)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.close(TezMerger.java:338)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.adjustPriorityQueue(TezMerger.java:489)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.next(TezMerger.java:503)\n\tat org.apache.tez.runtime.library.common.shuffle.orderedgrouped.MergeManager$RawKVIteratorReader.readRawKey(MergeManager.java:765)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.readRawKey(TezMerger.java:319)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.adjustPriorityQueue(TezMerger.java:481)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.next(TezMerger.java:503)\n\tat org.apache.tez.runtime.library.common.ValuesIterator.readNextKey(ValuesIterator.java:181)\n\tat org.apache.tez.runtime.library.common.ValuesIterator.access$300(ValuesIterator.java:47)\n\tat org.apache.tez.runtime.library.common.ValuesIterator$1$1.next(ValuesIterator.java:140)\n\t... 17 more\n], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1414029100044_0733_1_05 [Reducer 11] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex failed, vertexName=Reducer 11, vertexId=vertex_1414029100044_0733_1_05, diagnostics=[Task failed, taskId=task_1414029100044_0733_1_05_000027, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:262)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:168)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:163)\n\t... 13 more\nCaused by: java.lang.RuntimeException: problem advancing post rec#334499\n\tat org.apache.tez.runtime.library.common.ValuesIterator$1$1.next(ValuesIterator.java:142)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processKeyValues(ReduceRecordSource.java:288)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:252)\n\t... 15 more\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum Error:  CurrentOffset=166741, offset=4, off=0, dataLength=167232, origLen=495, len=491, length=167236, checksumSize=4\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.doRead(IFileInputStream.java:254)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.read(IFileInputStream.java:184)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFileInputStream.close(IFileInputStream.java:131)\n\tat org.apache.hadoop.io.compress.DecompressorStream.close(DecompressorStream.java:205)\n\tat org.apache.tez.runtime.library.common.sort.impl.IFile$Reader.close(IFile.java:784)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.closeReader(TezMerger.java:332)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.close(TezMerger.java:338)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.adjustPriorityQueue(TezMerger.java:489)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.next(TezMerger.java:503)\n\tat org.apache.tez.runtime.library.common.shuffle.orderedgrouped.MergeManager$RawKVIteratorReader.readRawKey(MergeManager.java:765)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$Segment.readRawKey(TezMerger.java:319)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.adjustPriorityQueue(TezMerger.java:481)\n\tat org.apache.tez.runtime.library.common.sort.impl.TezMerger$MergeQueue.next(TezMerger.java:503)\n\tat org.apache.tez.runtime.library.common.ValuesIterator.readNextKey(ValuesIterator.java:181)\n\tat org.apache.tez.runtime.library.common.ValuesIterator.access$300(ValuesIterator.java:47)\n\tat org.apache.tez.runtime.library.common.ValuesIterator$1$1.next(ValuesIterator.java:140)\n\t... 17 more\n], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: Reduce operator initialization failed\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)\n\t... 13 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: : init not supported\n\tat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEvaluator.init(GenericUDAFStreamingEvaluator.java:70)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.setupWdwFnEvaluator(PTFDeserializer.java:209)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializeWindowing(PTFDeserializer.java:130)\n\tat org.apache.hadoop.hive.ql.plan.PTFDeserializer.initializePTFChain(PTFDeserializer.java:94)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.reconstructQueryDef(PTFOperator.java:144)\n\tat org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:74)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)\n\tat org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)\n\t... 14 more\n]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1414029100044_0733_1_05 [Reducer 11] killed/failed due to:null]\nVertex killed, vertexName=Reducer 7, vertexId=vertex_1414029100044_0733_1_10, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_10 [Reducer 7] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex killed, vertexName=Reducer 7, vertexId=vertex_1414029100044_0733_1_10, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_10 [Reducer 7] killed/failed due to:null]\nVertex killed, vertexName=Reducer 6, vertexId=vertex_1414029100044_0733_1_09, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_09 [Reducer 6] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex killed, vertexName=Reducer 6, vertexId=vertex_1414029100044_0733_1_09, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_09 [Reducer 6] killed/failed due to:null]\nVertex killed, vertexName=Reducer 4, vertexId=vertex_1414029100044_0733_1_07, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_07 [Reducer 4] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex killed, vertexName=Reducer 4, vertexId=vertex_1414029100044_0733_1_07, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_07 [Reducer 4] killed/failed due to:null]\nVertex killed, vertexName=Reducer 5, vertexId=vertex_1414029100044_0733_1_08, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_08 [Reducer 5] killed/failed due to:null]\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: Vertex killed, vertexName=Reducer 5, vertexId=vertex_1414029100044_0733_1_08, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1414029100044_0733_1_08 [Reducer 5] killed/failed due to:null]\nDAG failed due to vertex failure. failedVertices:1 killedVertices:4\n14/10/30 01:19:19 [main]: ERROR tez.TezJobMonitor: DAG failed due to vertex failure. failedVertices:1 killedVertices:4\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask\n14/10/30 01:19:19 [main]: ERROR ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask\n{code}\n",
        "duedate": null,
        "environment": null,
        "fixVersions": [{
            "archived": false,
            "description": "released",
            "id": "12326450",
            "name": "0.14.0",
            "releaseDate": "2014-11-12",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12326450"
        }],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
            "id": "2",
            "name": "Critical",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/2"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935"
            },
            "id": "12310843",
            "key": "HIVE",
            "name": "Hive",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310843"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=mmokhtar&avatarId=21863",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=mmokhtar&avatarId=21863",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=mmokhtar&avatarId=21863",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=mmokhtar&avatarId=21863"
            },
            "displayName": "Mostafa Mokhtar",
            "key": "mmokhtar",
            "name": "mmokhtar",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=mmokhtar",
            "timeZone": "America/Los_Angeles"
        },
        "resolution": {
            "description": "A fix for this issue is checked into the tree and tested.",
            "id": "1",
            "name": "Fixed",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1"
        },
        "resolutiondate": "2014-10-31T19:29:09.000+0000",
        "status": {
            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
            "id": "6",
            "name": "Closed",
            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "TPC-DS Q51 : fails with \"init not supported\" exception in GenericUDAFStreamingEvaluator.init",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2014-11-13T19:42:20.000+0000",
        "versions": [{
            "archived": false,
            "description": "released",
            "id": "12326450",
            "name": "0.14.0",
            "releaseDate": "2014-11-12",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12326450"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HIVE-8677/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HIVE-8677/watchers",
            "watchCount": 4
        },
        "workratio": -1
    },
    "id": "12751801",
    "key": "HIVE-8677",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12751801"
}