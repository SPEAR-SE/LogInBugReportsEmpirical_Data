{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [],
        "created": "2019-01-04T04:53:42.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Lê Văn Thanh",
            "key": "thanhlv93",
            "name": "thanhlv93",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=thanhlv93",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": null,
        "customfield_12310222": null,
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|u00i9s:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "2019-01-04 04:53:42.0",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "Hello , \r\n\r\nI have a table with 10GB data and 4GB free RAM , I tried to select data from this table to other table with ORC format but I got an error about memory limit ( I'm running the query SELECT in console ) . \r\n\r\nDetail bug : \r\n\r\n\r\n\r\n\r\nCaused by: java.util.concurrent.ExecutionException: Exception thrown by job\r\n at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:272)\r\n at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:277)\r\n at org.apache.hadoop.hive.ql.exec.spark.status.impl.LocalSparkJobStatus.getError(LocalSparkJobStatus.java:171)\r\n at org.apache.hadoop.hive.ql.exec.spark.SparkTask.getSparkJobInfo(SparkTask.java:369)\r\n at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:118)\r\n at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)\r\n at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\r\n at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2182)\r\n at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1838)\r\n at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1525)\r\n at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1236)\r\n at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1231)\r\n at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)\r\n ... 11 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 0.0 failed 1 times, most recent failure: Lost task 6.0 in stage 0.0 (TID 7, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n at java.util.Arrays.copyOfRange(Arrays.java:3664)\r\n at java.lang.String.<init>(String.java:207)\r\n at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567)\r\n at java.nio.CharBuffer.toString(CharBuffer.java:1241)\r\n at org.apache.hadoop.io.Text.decode(Text.java:412)\r\n at org.apache.hadoop.io.Text.decode(Text.java:389)\r\n at org.apache.hadoop.io.Text.toString(Text.java:280)\r\n at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveJavaObject(WritableStringObjectInspector.java:46)\r\n at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getString(PrimitiveObjectInspectorUtils.java:891)\r\n at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$StringConverter.convert(PrimitiveObjectInspectorConverter.java:508)\r\n at org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.evaluate(GenericUDFToUnixTimeStamp.java:127)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\r\n at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(GenericUDFBaseNumeric.java:128)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)\r\n at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)\r\n at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)\r\n at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)\r\n at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148)\r\n at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547)\r\n at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\r\n at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\r\n at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\r\n at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\r\n at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\r\n at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\r\n at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\r\n\r\nDriver stacktrace:\r\n at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n at scala.Option.foreach(Option.scala:257)\r\n at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n at java.util.Arrays.copyOfRange(Arrays.java:3664)\r\n at java.lang.String.<init>(String.java:207)\r\n at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567)\r\n at java.nio.CharBuffer.toString(CharBuffer.java:1241)\r\n at org.apache.hadoop.io.Text.decode(Text.java:412)\r\n at org.apache.hadoop.io.Text.decode(Text.java:389)\r\n at org.apache.hadoop.io.Text.toString(Text.java:280)\r\n at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveJavaObject(WritableStringObjectInspector.java:46)\r\n at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getString(PrimitiveObjectInspectorUtils.java:891)\r\n at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$StringConverter.convert(PrimitiveObjectInspectorConverter.java:508)\r\n at org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.evaluate(GenericUDFToUnixTimeStamp.java:127)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)\r\n at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.evaluate(GenericUDFBaseNumeric.java:128)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)\r\n at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)\r\n at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)\r\n at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)\r\n at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)\r\n at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148)\r\n at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547)\r\n at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)\r\n at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)\r\n at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)\r\n at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)\r\n at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\r\n at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\r\n at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)\r\n\r\n\r\n\r\n\r\n\r\nHow to setup Apache Spark to use local hard disk when data does not fit in RAM in local mode?",
        "duedate": null,
        "environment": "Ubuntu 16.04\r\nHive 2.3.0\r\nSpark 2.0.0",
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
            "id": "2",
            "name": "Critical",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/2"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935"
            },
            "id": "12310843",
            "key": "HIVE",
            "name": "Hive",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310843"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Lê Văn Thanh",
            "key": "thanhlv93",
            "name": "thanhlv93",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=thanhlv93",
            "timeZone": "Etc/UTC"
        },
        "resolution": null,
        "resolutiondate": null,
        "status": {
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "id": "1",
            "name": "Open",
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "statusCategory": {
                "colorName": "blue-gray",
                "id": 2,
                "key": "new",
                "name": "To Do",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2"
            }
        },
        "subtasks": [],
        "summary": "gc overhead limit exceeded ",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2019-01-04T08:42:17.000+0000",
        "versions": [],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HIVE-21084/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HIVE-21084/watchers",
            "watchCount": 1
        },
        "workratio": -1
    },
    "id": "13207610",
    "key": "HIVE-21084",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13207610"
}