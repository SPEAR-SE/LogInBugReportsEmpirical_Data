[As per the "Alter" query , two DDL tasks will be created. 
When the first task execution is failed, the "taskCleanup" is internally stopping the VM. 


{code}


if (running.size() != 0) {
		taskCleanup();
}

public void taskCleanup()
{
	    // The currently existing Shutdown hooks will be automatically called, 
	    // killing the map-reduce processes. 
	    // The non MR processes will be killed as well.
	    System.exit(9);
}

{code}

Here, the System.exit(9) is used to kill all the "map-reduce" and "non-MR" process. 
Instead of killing all the MR and non-MR process , we can kill only the current running processes 

The update code will be ,. 

{code}
public void taskCleanup(Map<TaskResult, TaskRunner> running) {
	
	    // get all the remaining running process , 
	    Set < Entry < TaskResult, TaskRunner >> entrySet = running.entrySet ();
	    for(Entry <TaskResult, TaskRunner> entry : entrySet)
	    {
	        TaskRunner value = entry.getValue ();
		// Interrupt. 
	        value.interrupt ();
	    }
	}
{code}

This will internally kills the task ( which need not required to be exeuted ).  , Yongqiang, can you take a look ?
I dont remember the exact usecase, but remember some discussions around it, Bharath, i am not sure about your use case. Ignoring an error maybe not a good idea, especially this 'set location' command, because it may cause the following queries give wrong results.

And i saw there is a .q file added to negativequeries, you need to also include a .q.out file using 'ant test -Dtestcase=TestNegativeCliDriver -Dqfile=xxx.q -Doverwrite=true', I don't know about the solution, but I think the System.exit is also the cause of HIVE-1867, which has been intermittently preventing ant test from passing., Note that the System.exit was introduced in HIVE-549; maybe worth revisiting some of the discussion at the end of the comments there regarding refactoring to use JobControl.
, Based on the thread  HIVE-549 ,  there was a comment to cleanup the running jobs if one of them have failed. 

	* Shall we immediately stop all other running jobs if one of them have failed?
		+ console.printError(errorMessage);
		+ taskCleanup(runnable); 

This comment was implemented by calling System.exit(9). This makes the JVM to exit. 

Rather than relying on the ShutdownHook , we could directly invoke that functionality in the taskCleanUp

As per my proposed solution , in taskCleanup. 

                 1)  Sending an interrupt to runningJobs "TaskRunner" so that the task associated will be interrupted. 
                 2)  And  if the task is "MapRed" task, then the task cleanup should post the "kill url" from "runningJobKillURIs" hashmap in ExecDriver

Comments?  

Thanks, Calling System.exit is definitely a problem since for HIVE-549, this causes JUnit to treat the JVM as crashed.  It seems like just returning 9 should be good enough, since all the same System.exit processing (thread-killing and shutdown hook execution) will take place as soon as the main thread returns; the only difference is that JUnit gets a chance to clean up as the stack unwinds.

interrupt isn't really guaranteed to do much more, e.g. threads can catch and ignore InterruptedException, so I don't think we should bother adding it.

If there's a reason we need to leave the explicit System.exit call there, then maybe we need a conf to tell it not to do that during tests.
,  Here i have few doubts. Just returning 9 will solve the problem of exiting jvm, but all the tasks (Mapred task and Non mapred task) that are running  will still continue to run if main thread is not returning (In case of HiveServer), there by wasting the cluster resources.  So the problem here is not only exiting the JVM but also the running tasks. Any suggestions on this., Good point about Hive server.  As a short-term solution to flaky tests, adding a config param that is only enabled for tests should still work fine, but I agree that it would be best to address the bigger issues.]