[I think it's because we renamed the file when uploading to HDFS. I initially did this to handle the situation when user adds different files with same file name (from different paths). Looked into how spark distributes files to executors and seems this is not supported. So it doesn't make sense to support this on hive side.

Lifeng, could you please try the patch to see if it fixes your issue? Thanks., [~lirui], could you please check HIVE-12045 has the same root cause? Thanks., I don't think it's the root cause for HIVE-12045. The UDF classes should be loaded properly as long as the jar is added to classpath. It doesn't matter if we rename the jar file., Patch v2 also makes custom script work for local mode., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12768298/HIVE-12229.2-spark.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 6450 tests executed
*Failed tests:*
{noformat}
TestContribNegativeCliDriver - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.initializationError
org.apache.hadoop.hive.cli.TestHBaseCliDriver.initializationError
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_inner_join
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.initializationError
org.apache.hadoop.hive.cli.TestMinimrCliDriver.initializationError
org.apache.hadoop.hive.cli.TestNegativeCliDriver.initializationError
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.initializationError
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/972/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/972/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-972/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12768298 - PreCommit-HIVE-SPARK-Build, Run tests again., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12769189/HIVE-12229.2-spark.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 9462 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_inner_join
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join2
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/982/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/982/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-982/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12769189 - PreCommit-HIVE-SPARK-Build, Latest failures are not related., Hi [~lirui], thanks for fixing this. Two minor questions:

1. detecting Spark local mode, instead of equals() in {{ sparkConf.get("spark.master").equals("local")}}, should we use startWith(), to cover cases such as local[2] as well as local-cluster?

2. If user adds a file which exists remote, should we overwrite instead of throwing an exception? Maybe the user just wants to replace the file added previously.

What's your thought?, Thanks for your inputs, Xuefu.

1. Good catch. I'll make it work with local[*]. But we don't have to worry about local-cluster mode, because executors run in separate JVMs in local-cluster mode so the child process will inherit the proper working directory.
2. OK. Whether the new file can be added successfully on executor is up to spark, i.e. configured by {{spark.files.overwrite}}. Let's just overwrite on our side., Address Xuefu's comments.

Since we just overwrite existing remote files, on the driver side, we need a way to tell if the remote file is updated. We can't simply download the remote jars due to HIVE-10671. Therefore, I add a timestamp to the added jar. The driver only needs to download the jar if it doesn't exist locally or has a newer timestamp., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12769691/HIVE-12229.3-spark.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 17 failed/errored test(s), 9478 tests executed
*Failed tests:*
{noformat}
TestCliDriver-authorization_cli_nonsql.q-skewjoinopt19.q-tez_self_join.q-and-12-more - did not produce a TEST-*.xml file
TestCliDriver-load_dyn_part5.q-notable_alias3.q-type_conversions_1.q-and-12-more - did not produce a TEST-*.xml file
TestCliDriver-ptf_seqfile.q-constprog_dpp.q-udaf_number_format.q-and-12-more - did not produce a TEST-*.xml file
TestCliDriver-smb_mapjoin_4.q-udf_asin.q-udf_to_unix_timestamp.q-and-12-more - did not produce a TEST-*.xml file
TestCliDriver-udf_bitwise_and.q-bucketcontext_4.q-orc_ends_with_nulls.q-and-12-more - did not produce a TEST-*.xml file
TestCliDriver-udf_current_user.q-join44.q-input41.q-and-12-more - did not produce a TEST-*.xml file
TestCliDriver-union2.q-exchange_partition2.q-udf_initcap.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-constprog_dpp.q-dynamic_partition_pruning.q-vectorization_10.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-mapjoin_decimal.q-vectorized_distinct_gby.q-union_fast_stats.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-vectorization_13.q-auto_sortmerge_join_13.q-tez_bmj_schema_evolution.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-vectorization_16.q-vector_decimal_round.q-orc_merge6.q-and-12-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-bucket_num_reducers.q-table_nonprintable.q-scriptfile1.q-and-1-more - did not produce a TEST-*.xml file
TestSparkCliDriver-groupby6_map.q-join13.q-join_reorder3.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-ppd_join4.q-skewjoinopt3.q-union27.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-table_access_keys_stats.q-bucketsortoptimize_insert_4.q-join_rc.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.hwi.TestHWISessionManager.testHiveDriver
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/987/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/987/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-987/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 17 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12769691 - PreCommit-HIVE-SPARK-Build, Run several failed tests locally but cannot reproduce. Try again., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12769708/HIVE-12229.3-spark.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 9635 tests executed
*Failed tests:*
{noformat}
TestSparkCliDriver-bucketmapjoin12.q-avro_decimal_native.q-udf_percentile.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.initializationError
org.apache.hadoop.hive.hwi.TestHWISessionManager.testHiveDriver
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/988/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/988/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-988/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12769708 - PreCommit-HIVE-SPARK-Build, I still can't reproduce the failures. Seems we still have some issue of the test framework?, Hi [~szehon]/[~spena], could you please take a look to see if there is some problem with the env? At one point, it went away, but now it seems it has resurfaced. Thanks., [~xuefuz] It is a different error caused by the MiniTez cluster. Below is the error exception that was thrown:

{noformat}
java.io.IOException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.shims.Hadoop23Shims$MiniTezShim.createAndLaunchLlapDaemon(Hadoop23Shims.java:447)
	at org.apache.hadoop.hive.shims.Hadoop23Shims$MiniTezShim.<init>(Hadoop23Shims.java:402)
	at org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniTezCluster(Hadoop23Shims.java:379)
	at org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniTezCluster(Hadoop23Shims.java:116)
	at org.apache.hadoop.hive.ql.QTestUtil.<init>(QTestUtil.java:450)
	at org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.<clinit>(TestMiniLlapCliDriver.java:54)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.internal.runners.SuiteMethod.testFromSuiteMethod(SuiteMethod.java:35)
{noformat}, Thanks, [~spena]. Is the following the problem a consequence of that, or else?
{noformat}
TestSparkCliDriver-bucketmapjoin12.q-avro_decimal_native.q-udf_percentile.q-and-12-more - did not produce a TEST-*.xml file
{noformat}, This is a strange issue..   seems the tests are not run because they were in the wrong directory (the base directory).

[http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-988/succeeded/TestSparkCliDriver-bucketmapjoin12.q-avro_decimal_native.q-udf_percentile.q-and-12-more/TestSparkCliDriver-bucketmapjoin12.q-avro_decimal_native.q-udf_percentile.q-and-12-more.txt|http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-988/succeeded/TestSparkCliDriver-bucketmapjoin12.q-avro_decimal_native.q-udf_percentile.q-and-12-more/TestSparkCliDriver-bucketmapjoin12.q-avro_decimal_native.q-udf_percentile.q-and-12-more.txt]

There is some code to find the directory with TestSparkCliDriver.java that seems to be broken in this case.  I dont really understand this code.. need to look into it.

[https://github.com/apache/hive/blob/master/testutils/ptest2/src/main/resources/batch-exec.vm#L64|https://github.com/apache/hive/blob/master/testutils/ptest2/src/main/resources/batch-exec.vm#L64], Continuing the investigation and findings in HIVE-12230 to not pollute this JIRA.  There I am giving another try with a config fix., typo: HIVE-12330, Seems to work for now.. see HIVE-12330 for latest test result., Thanks Szehon for taking care of the tests!
Just upload the same patch to have another run., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12770529/HIVE-12229.3-spark.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 9650 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.initializationError
org.apache.hadoop.hive.hwi.TestHWISessionManager.testHiveDriver
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/992/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/992/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-992/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12770529 - PreCommit-HIVE-SPARK-Build, +1 for the patch. I have a followup question though. Based on your previous comment, what happens if spark.files.overwrite is false? Will user see an error? User should be able to set this property to true at Hive (say, beeline), right?, If spark.files.overwrite is false and the user overwrites an already added jar (different in file contents), then spark will throw exception on this.
Yes user can set this at hive just as other spark properties. However, if the user already launches an RSC and later changes spark.files.overwrite, then we'll launch another RSC because the SparkConf has changed. And accordingly we'll launch new executors for which no jar has been added. In such a case, I think it's uncertain which jar will be eventually added/used on the executor. Therefore it's better if the user set spark.files.overwrite in advance and doesn't change it throughout the session., When the new executors are launched, shouldn't the added jars, which is available in the context be added to the classpath? On Hive side, we should be able to update the context in a deterministic manner, right?, I'll use some examples to explain my point. Let's consider the following case:

1. User adds A.jar from hive and runs some query.
2. User adds(updates) a different A.jar and run some query but the query fails because spark.files.overwrite is false.
3. User sets spark.files.overwrite to true and run that failed query again.
But at this point, we'll create a new RSC as well as new executors. And these new executors have no jars added by now. Then we add the two A.jar to RSC and the second added A.jar will overwrite the first, and eventually get distributed to the executors. But IMO the order of the two A.jar is not deterministic because added jars are retrieved from {{Utilities.getResourceFiles}} and added by an async RPC job.

Now let's consider another case:

1. User sets spark.files.overwrite to true, adds A.jar and runs some queries.
2. User adds another A.jar and runs another query.
At this point, the former A.jar is already added so we won't add it again and just add the new A.jar to RSC. RSC ships the new A.jar to executors and successfully overwrites the existing A.jar. Then the query will use the updated jar, which is the expected behavior., Then the real question is: when the new RSC comes up, should we make all jars that have been added so far in the user session available to the new executors, including new jars that overwriting the previous ones?

It seems to be a usability issue if a user adds a jar, and then change some configuration (possibly irrelevant to add jar) that ends up with a new RSC, which results the new executors having no idea of the added jar?, Since we don't know which jar is newer on hive side, we simply send all the added jars to the new RSC. And as I said, which jar is really used is not deterministic. If we want to solve this, we need a way to remove jars that have been overwritten, i.e. when user adds another A.jar, the previous one should be removed from the session (no longer returned by {{Utilities.getResourceFiles}}).

Regarding your second question, everything will be fine if user doesn't add multiple jars with the same name (i.e. overwriting older jars). All the already added jars will be shipped to the executors., Okay. Thanks for all the explanation. I guess the same problem can happen to MR as well. That problem can be handled in a separate JIRA if ever., OK. I'll commit this shortly., Committed to spark branch.
Thanks Xuefu for the review, and Szehon and Sergio for fixing the tests!]