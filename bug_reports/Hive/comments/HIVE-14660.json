[GitHub user bonnetb opened a pull request:

    https://github.com/apache/hive/pull/100

    HIVE-14660 : ArrayIndexOutOfBounds on delete

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/bonnetb/hive HIVE-14660

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hive/pull/100.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #100
    
----
commit 21f333f0483249949dd97a6960c169b6dd255491
Author: Benjamin BONNET <benjamin.bonnet@m4x.org>
Date:   2016-08-27T20:20:15Z

    HIVE-14660 : ArrayIndexOutOfBounds on delete

----
, [~ekoifman]  ran across this at random, not sure if you had seen it, does this problem still exist in Hive 2.X?, [~cartershanklin] I've not seen this before.  ACID writes require number of reducers (writers) to be equal the number of buckets.  So we should detect "set mapred.reduce.tasks=1;" type of config and raise a meaningful error.  I don't think there are any other options.

cc [~alangates], [~ekoifman] : actually, we encountered that bug without setting mapred.reduce.tasks=1. But I managed to reproduce it on a sandbox only by forcing the FileSinkOperator to deal with more than one bucket, forcing the number of reducers to 1.

On the platform where we encountered the issue (with default mapred settings), we have a work-around setting mapred.reduce.tasks to number of buckets., [~bbonnet]
bq. we have a work-around setting mapred.reduce.tasks to number of buckets.
that is the correct solution.  This should happen automatically - it's a serious bug if it does not.  Can you describe more precisely how you are ending up in this situation?  (Your config settings, query to repro this, relevant DDL.), Here are the properties we set before creating the table and before inserting/deleting rows:
{code}
set hive.support.concurrency=true;
set hive.enforce.bucketing=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.compactor.initiator.on=true;
set hive.compactor.worker.threads=1;
{code}

Apart from those properties, settings are "standard" (we use a HDP 2.3 cluster). 
mapred.reduce.tasks is not set

We have (by default): 
mapreduce.job.reduces=-1
mapreduce.reduce.speculative=true

Table has 36 columns, is clustered into 4 buckets (on a single column), ORC formatted, transactional and partitioned by year/month. It has about half billion rows.

The genuine query that fails is a kind of :
{code}DELETE FROM table WHERE string_operations_on_some_columns IN ( select_from_another_table );
{code}

Concerning the mapred.reduce.tasks setting, I talk about a work-around (not a solution) since reading the FileSinkOperator, one sees it has be designed to operate on multiple buckets. In my opinion, the only mistake was the use of an array instead of a map, or a circular array (if you are guaranteed the way buckets are dealtwith  is circular for the latter, which I could not assume when I wrote the patch)., Not having a writer for every bucket will cause incorrect results.  Any bucket that doesn't get a writer won't get a delta file written that covers that bucket, and thus any records in that bucket that should be deleted will not be.  So changing the FileSinkOperator to not assume there's a writer for each bucket is not the right approach.

We need to figure out how it is that we're getting to the write phase without the proper number of writers., Actually, every bucket gets a writer : even though you have only one reducer, every bucket is covered - by the same writer. And indeed, writers are designed to cover several buckets (see that comment "// Find the bucket id, and switch buckets if need to" in FileSeekOperator source code).
In my opinion, there is just a small bug in the way the bucket switch is implemented.
Regards
, Github user bonnetb closed the pull request at:

    https://github.com/apache/hive/pull/100
, GitHub user bonnetb opened a pull request:

    https://github.com/apache/hive/pull/299

    HIVE-14660 : ArrayIndexOutOfBounds on delete

    See https://issues.apache.org/jira/browse/HIVE-14660

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/bonnetb/hive HIVE-14660

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hive/pull/299.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #299
    
----
commit 323f4bfa92835921780c057082b440bf54a7f5c8
Author: Benjamin BONNET <benjamin.bonnet@...>
Date:   2016-08-27T20:20:15Z

    HIVE-14660 : ArrayIndexOutOfBounds on delete

----
]