[Looking at the code where the exception occurs, It seems like the array unpackedPatch could be empty after SerializationUtils.readInts(), but there's no checking when accessing the array.

   // unpack the patch blob
    long[] unpackedPatch = new long[pl];
    SerializationUtils.readInts(unpackedPatch, 0, pl, pw + pgw, input);

    // apply the patch directly when decoding the packed data
    int patchIdx = 0;
    long currGap = 0;
    long currPatch = 0;
    currGap = unpackedPatch[patchIdx] >>> pw;


, Or pl could be 0, My findings show that there is a problem in run length encoding.
You can reproduce the problem by doing the following steps:
1. Create the table:
{code:sql}
CREATE TABLE test_orc_format(
  site STRING,
  a DOUBLE,
  b BIGINT,
  c BIGINT,
  d BIGINT,
  e DOUBLE,
  f DOUBLE,
  g DOUBLE,
  h DOUBLE,
  i DOUBLE,
  j DOUBLE,
  k BIGINT,
  l BIGINT,
  m BIGINT,
  n BIGINT,
  o BIGINT,
  p BIGINT,
  q ARRAY<DOUBLE>,
  r ARRAY<DOUBLE>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ','
STORED AS ORC
;
{code}
2. Load the data from attached file.
{code:sql}
load data local inpath 'test_data' overwrite into test_orc_format;
{code}
3. Use one of the following queries:
{code:sql}
select * from test_orc_format;
select o from test_orc_format;
{code}

Note, the attached file was created by hive during a job execution and not crafted by hands, it might be wrongly encoded as well. Also, note that the query that does calculation for column "o" cannot give negative results., [~ericchu30] and [~Aleksei] thanks for looking into this issue and your findings. I tried to reproduce the issue with the test data that is being provided. I think the attached test data is a corrupted orc file and so I was not able to reproduce the issue. But I recently faced a similar issue, ArrayIndexOutOfBoundsException while writing data using the new RunLengthIntegerReaderV2. I suspect that this issue might be related to writing of data RLEv2. I posted a patch for it here https://issues.apache.org/jira/browse/HIVE-5991. Will it be possible to apply this patch and see if this issue happens again?, Also running into same problem. We have setup a new test cluster (Hortonworks 2.0) where we move some test data (roughly 6 000 000 000 rows) and try to move into a more optimized format (orc with buckets in a partitioned table).

Not sure if this will help you but here is the stacktrace. Do you want some more information?

{noformat}
INSERT OVERWRITE TABLE hellotestdataend2 PARTITION(dt) SELECT * FROM event.hellotestdataend2 SORT BY dt, someid, msts, userid
{noformat}

{noformat}
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":10017,"reducesinkkey1":1384880695473,"reducesinkkey2":1242212467},"value":{"_col0":1384880695473,"_col1":"10017","_col2":1242212467,"_col3":"ASPV6f3f82454d0df89704c74f7f0884fa15","_col4":1384880654,"_col5":19,"_col6":15,"_col7":21700,"_col8":0,"_col9":2552,"_col10":100,"_col11":1384880624,"_col12":0,"_col13":"2013-11-19"}}
at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:258)
... 7 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 3
at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.preparePatchedBlob(RunLengthIntegerWriterV2.java:593)
at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.determineEncoding(RunLengthIntegerWriterV2.java:541)
at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerWriterV2.write(RunLengthIntegerWriterV2.java:746)
at org.apache.hadoop.hive.ql.io.orc.WriterImpl$IntegerTreeWriter.write(WriterImpl.java:744)
at org.apache.hadoop.hive.ql.io.orc.WriterImpl$StructTreeWriter.write(WriterImpl.java:1320)
at org.apache.hadoop.hive.ql.io.orc.WriterImpl.addRow(WriterImpl.java:1849)
at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.write(OrcOutputFormat.java:75)
at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:638)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:501)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:842)
at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:88)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:501)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:842)
at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)
at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:501)
at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:249)
... 7 more
{noformat}, [~jakeri] This issue with new RLEv2 encoding should be addressed by HIVE-5991. You can try HIVE-5991 patch or the alternative workaround would be to use hive.exec.orc.write.format=“0.11”. This will use the old RLE encoding. , If we have already started using RLE 0.11 for some partitions of the table and want to switch to RLEv2, would we need to rewrite existing partitions in RLEv2? If not, would we need to set hive.exec.orc.write.format=0.11 when we query partitions with the old RLE, and set hive.exec.orc.write.format=null when we query partitions with RLEv2?, bq. If we have already started using RLE 0.11 for some partitions of the table and want to switch to RLEv2, would we need to rewrite existing partitions in RLEv2?

Yes. The partitions that used RLE 0.11 earlier needs to be rewritten to take advantage of RLEv2. In fact both RLE and RLEv2 partitions can co-exist. The advantage of using RLEv2 is that it has a better compression in most of the cases wrt old RLE because of improved run lengths and bit packing.

bq. If not, would we need to set hive.exec.orc.write.format=0.11 when we query partitions with the old RLE, and set hive.exec.orc.write.format=null when we query partitions with RLEv2?

No.  reading the ORC files or while querying, there is no need to set hive.exec.orc.write.format config. The config is required only during writing the data. While reading ORC reader will read the version of RLE encoding used from the file footer., FYI

Setting set hive.exec.orc.write.format=0.11 (without quotes) resolved my error., The wiki does not document hive.exec.orc.write.format, nor any other ORC configuration parameters.

I found these in HiveConf.java with an "orc" search: 

* hive.exec.orc.memory.pool
* hive.exec.orc.write.format
* hive.exec.orc.default.stripe.size

Also, hive.default.fileformat can take the value ORC.  Are there any others?, I just verified with the attached test data. This bug is solved by HIVE-6382., Closing this issue as I was able to verify HIVE-6382 solves the issue for the test data provided. Feel free to reopen the issue if this issue is not solved., Just for the record, ORC parameters are now documented in the wiki:

* [Configuration Properties -- ORC File Format | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-ORCFileFormat]]