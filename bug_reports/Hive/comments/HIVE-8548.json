[BTW, you can still use the client for local mode. It just means the "remote" context and executors will be on the same machine (but still on a different process, which is still a gain).

Might actually be better, since it will mean tests still go through the remote interface., Agreed. However, I think local SparkContext is still helpful for easier debugging. , [~xuefuz], if we set spark.master as local, hive users connect to HiveServer2 which use local spark context to submit job with a seperate session for each user, we may still hit into multi spark context issue. so HiveServer2 could only use remote spark context, and CLI may use either local spark context or remote spark context, which we can add a parameter to configure it and set local spark context as default, what do you think about it?, Hi [~chengxiang li], I think nobody is going to deploy HS2 in production with local mode and HS2 embedded mode (embedded in Beeline) should behave like Hive CLI. Thus, I think it might be better to keep them consistent. Based on this, I think "local" should be the default whether it's Hive CLI or HS2, and they actually share the same code path (w.r.t. spark integration). In addition, "local" should refer to local spark context in both cases. As to the concurrentcy problem, we just need some proper documentation. Remote spark context should be used when {{spark.master != local}}. I think his approach makes the implemention simpler with seemingly better usability. We can revist this at a later phase., Hi [~xuefuz], I agree with your point, and with a extra addup. As you mentioned, user should use remote spark context in HS2 production environment, and local spark context is mainly used for debugging. Finally we should enable automatic test for remote spark context, which should run with "spark.master=local" as it would be to overstaffing to enable a standalone/yarn/mesos cluster for hive test. So besides the default option you mentioned before, we may add a parameter to control spark context choice more flexible., Hi [~chengxiang li], as to automatical testing of remote spart context, we will be switch out unit test from "local" to "local-cluster", hoping that will cover it. For now, it seems that we don't need an extra configuration for that purpose., HI [~xuefuz] - Regarding unit testing remote-spark context with local-cluster mode, we will need to use either yarn or mesos as the cluster manager. Is that going to be our test setup? 

The reason is that currently, if spark.master=local, it implies a *spark-standalone cluster* which only supports *client* deploy mode, and not *local-cluster* deployment mode., Suhas, I'm not sure I understand your question.

First, you can't use yarn in unit tests since Spark does not publish the classes needed to run against yarn in any artifact that you can depend on.

Second, the choice of "client" vs. "cluster" should make no difference when running unit tests. A "local" master is not Spark standalone; it's Spark running in a single JVM, with no cluster manager. A "local-cluster" master is standalone mode, similar to running a MiniYARNCluster, and should support both client and cluster mode., [~ssatish], Spark's local-cluster is a a test cluster consistenting several processes on a single host, each serveing a master/worker node. It's similar to Spark standalone cluster, and has nothing to do with either yarn or mesos. local-cluster is used for Spark's unit tests in Spark project., Thanks for clarifying [~xuefuz] and [~vanzin]. I had some misconceptions about the naming conventions. ]