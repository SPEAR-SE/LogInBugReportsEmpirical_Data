[[~ndimiduk] would you agree that HBase input formats are not stateless?

[~sushanth] you've done work with the formats. Your thoughts?, Yes, it looks like our input formats are stateful. We're set up to specify a config object and that's closed over to provide custom record readers, based on which mapred API is being implemented., FYI, this may be changing with HBase 1.0 as we're reworking the way connection management is handled. I'm behind on my reviews, so I don't know if this assumption will change., From a strict M/R standpoint:

Traditional M/R guarantees state information availability to InputFormats through JobConf, and through serialized InputSplits. Any expectation past that by the InputFormat is not guaranteed to work. Practically, though, M/R does not standardize a setInput equivalent call, and thus, InputFormats wind up implementing their own methodologies. It is not unheard of for them to maintain state.

In practice also, though, we absolutely need to have a standardization, to be able to access it from Hive/HCatalog. HCatalog took a route where it said that InputFormats, as currently defined are not well-specified enough to be able to do all the setup needed to be effectively stateless, and so, relegated that notion upwards, (in earlier versions of HCatalog to something called StorageDriver, but as of HCat 0.3, replaced with Hive's StorageHandler) to StorageHandlers, HCat's primary storage abstraction. While the InputFormat is itself considered stateless, a StorageHandler is considered stateful, and HCat does the following:

a) Instantiate the appropriate StorageHandler using ReflectionUtils.newInstance   (will call setConf if available, and usually is)
b) Call configureInputJobProperties() on that StorageHandler to set it up for input, and it modifies a map of key value properties (jobProperties) that HCat ensures that it will put into JobConf/Job before calling any methods on the relevant InputFormat.
c) Call .getInputFormatClass,  that class eventually gets instantiated at run time with ReflectionUtils.newInstance(inputFormatClass, Job). Now, this allows the InputFormat to set up the Job (which already had the above map of kvps inserted into it) any which way it wants, without itself being stateful.
d) Call .getInputSplits, again passing in the relevant JobConf as the state-carrier into it, and the InputSplits themselves being a serializable state carrier on the outbound.
e) Call .createRecordReader on that InputFormat, again, the InputFormat itself can(and is) stateless, but gets passed in an InputSplit(has state) and a TaskAttemptContext (has state, with the above jobProperties map), Thank you. I am thinking until the HBase input format is "fixed" we should either remove this cache entirely or not cache HBase input formats. Which do you prefer?, +cc [~elserj] for an accumulo perspective on this.

My personal preference is that in getInputFormatFromCache, if we detect that the class is a hbase input format, then we return new instance and not populate the cache. I did something similar recently in HIVE-8704 for the output side for PTOF-based outputformats., Thanks for looping me in, [~sushanth].

As far as I can recall, Accumulo's InputFormat classes are stateless, relying on the state to be provided through the JobConf/InputSplits as you described. I know we have some "annoyances" where multiple calls to the InputFormat which alter the JobConf are not idempotent (they typically throw an error if things are re-set). I work around most of that pain in the StorageHandler impl.

Nothing is coming to mind that would be fundamentally broken if we get a re-used instance of the input format. HTH test/evaluate this too., bq. re-used instance of the input format.

Re-used is different than "thread safe". Is the Accumulo support thread-safe?, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12681162/HIVE-8808.patch

{color:red}ERROR:{color} -1 due to 15 failed/errored test(s), 6686 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_histogram_numeric
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_external_table_ppd
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_external_table_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_map_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_map_queries_prefix
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_binary_storage_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_joins
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_ppd_join
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_ppd_key_range
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_pushdown
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_queries
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_single_sourced_multi_insert
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_timestamp
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_ppd_key_ranges
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1768/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1768/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1768/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 15 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12681162 - PreCommit-HIVE-TRUNK-Build, 

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12681651/HIVE-8808.patch

{color:green}SUCCESS:{color} +1 6688 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1800/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1800/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1800/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12681651 - PreCommit-HIVE-TRUNK-Build, [~sushanth] can you take a peek at this one?, +1, Looks good to me., Committed to trunk. Thanks, Brock!, Bumped into this problem with another input format that wrapped an hbase input format.  The wrapper did not have hbase in its name.

I've gotta ask, though:  What's the purpose of caching a stateless object?

If M/R supports calling setConf as part of the instantiation system, that seems intended to initialize state.

 

Other examples of stateful input formats broken by the caching:

[https://github.com/cloudera/kudu/blob/master/java/kudu-mapreduce/src/main/java/org/apache/kudu/mapreduce/KuduTableInputFormat.java]

[https://github.com/twitter/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/mapred/input/DeprecatedInputFormatWrapper.java]

[https://github.com/hanborq/hadoop/blob/master/src/mapred/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java]

 

 ]