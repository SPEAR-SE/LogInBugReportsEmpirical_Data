[below is the stacktrace  that stream be cached in the URLClassloader when load TaskCounter.properties in hadoop-mapreduce-client-core.jar

Thread [Thread-31585] (Suspended (breakpoint at line 239 in URLClassLoader))	
	owns: WeakHashMap<K,V>  (id=2282)	
	owns: Class<T> (org.apache.hadoop.mapreduce.util.ResourceBundles) (id=1456)	
	owns: Counters  (id=2283)	
	URLClassLoader.getResourceAsStream(String) line: 239	
	ResourceBundle$Control$1.run() line: 2601 [local variables unavailable]	
	ResourceBundle$Control$1.run() line: 2586	
	AccessController.doPrivileged(PrivilegedExceptionAction<T>) line: not available [native method]	
	ResourceBundle$Control.newBundle(String, Locale, String, ClassLoader, boolean) line: 2585	
	ResourceBundle.loadBundle(CacheKey, List<String>, Control, boolean) line: 1436	
	ResourceBundle.findBundle(CacheKey, List<Locale>, List<String>, int, Control, ResourceBundle) line: 1400	
	ResourceBundle.findBundle(CacheKey, List<Locale>, List<String>, int, Control, ResourceBundle) line: 1354	
	ResourceBundle.findBundle(CacheKey, List<Locale>, List<String>, int, Control, ResourceBundle) line: 1354	
	ResourceBundle.getBundleImpl(String, Locale, ClassLoader, ResourceBundle$Control) line: 1296	
	ResourceBundle.getBundle(String, Locale, ClassLoader) line: 1028	
	ResourceBundles.getBundle(String) line: 37	
	ResourceBundles.getValue(String, String, String, T) line: 56	
	ResourceBundles.getCounterGroupName(String, String) line: 77	
	Counters$GroupFactory(CounterGroupFactory<C,G>).newGroup(String, Limits) line: 94	
	Counters(AbstractCounters<C,G>).getGroup(String) line: 226	
	Counters.getGroup(String) line: 113	
	Counters.findCounter(String, String) line: 479	
	HadoopJobExecHelper.progress(HadoopJobExecHelper$ExecDriverTaskHandle) line: 347	
	HadoopJobExecHelper.progress(RunningJob, JobClient, HiveTxnManager) line: 548	
	MapRedTask(ExecDriver).execute(DriverContext) line: 435	
	MapRedTask.execute(DriverContext) line: 159	
	MapRedTask(Task<T>).executeTask() line: 153	
	TaskRunner.runSequential() line: 85	
	TaskRunner.run() line: 72


below is the stacktrace that steam is closed:
java.util.zip.InflaterInputStream.getStack(InflaterInputStream.java:309)
java.util.zip.InflaterInputStream.close(InflaterInputStream.java:247)
java.util.zip.ZipFile$ZipFileInflaterInputStream.close(ZipFile.java:398)
java.util.zip.ZipFile.close(ZipFile.java:585)
sun.net.www.protocol.jar.URLJarFile.close(URLJarFile.java:167)
java.net.URLClassLoader.close(URLClassLoader.java:294)
org.apache.hadoop.hive.common.JavaUtils.closeClassLoader(JavaUtils.java:101)
org.apache.hadoop.hive.common.JavaUtils.closeClassLoadersTo(JavaUtils.java:79)
org.apache.hadoop.hive.ql.session.SessionState.close(SessionState.java:942)
org.apache.hive.service.cli.session.HiveSessionImpl.close(HiveSessionImpl.java:562)


so here the root cause is that:  add jar will lead to new classloader created, and it will cache the JarFile ,  but JarFile in jvm only has one instance(see JarURLConnection.setUseCache), so when other thread is read from the same JarFile, once hive session closed will lead to classloader close the stream., The issue is related to Java Bugs:
http://bugs.java.com/view_bug.do?bug_id=7087947
http://bugs.java.com/bugdatabase/view_bug.do?bug_id=7194301, I thought this was more of a multi-threading concurrency related issue where one hive job is trying to access a JAR which has been closed by another hive job which completed execution and was using the same session level jar. And hence HS2 directly sends a KILL signal to an MR job which was submitted and is IN PROGRESS. So, 
1. Can we not solve the problem by maintaining ClassLoader counts and checking the counts before closing the loaders in JavaUtils class' closeClassLoader method?
2. Can we change settings related to JarURLConnection.setUseCache while class loading?

As a temporary solution for this issue, we stopped using "add jar" for concurrent hive jobs. Instead added the auxiliary jars in Hive's classpath at the time of HS2 service start. This ensures that the class is loaded only once and is not closed unless HS2 is shutdown and hence it is available to all sessions and jobs.


, Adding another scenario which can lead so similar errors.

* add hive config to hive-site.xml -- > hive.downloaded.resources.dir = /tmp/download 
* start hiverserver2
* put a jar with a hive udf class on hdfs location /tmp/someudf.jar . Assume hive udf class is 'org.someorg.hive.udf.TextUDF'
* on beeline start a session as user 1 ==>   add jar hdfs://tmp/someudf.jar; create temporary function userOneFun as 'org.someorg.hive.udf.TextUDF';
* on beeline start another session as user 2 ==> add jar hdfs://tmp/someudf.jar;
* close session of user 1
* on session of user 2 ==>  create temporary function userTwoFun as 'org.someorg.hive.udf.TextUDF';  ==> throws error with ClassNotFound.

It doesnt throw any error relating to the stream closed but seems to be because of same JarUrlConnection for different class loaders and then one of them is closed. Since the underlying location is same due to setting hive.download.resources.dir . The default value for this config sets is correctly by additionally doing {code}File.separator + "${hive.session.id}_resources" {code}. It would be great to do this programmatically in SessionState Constructor to create the correct resourceDownloader. 



, https://issues.apache.org/jira/browse/HADOOP-12404
this post might be a decent solution, Yes disabling jar cache seems to be a viable option., https://issues.apache.org/jira/browse/HADOOP-13809 is a similar case.

I believe they are all related to https://bugs.openjdk.java.net/browse/JDK-6947916, which hasn't been released.

I am able to recreate it with oracle jdk 1.8.0_131.]