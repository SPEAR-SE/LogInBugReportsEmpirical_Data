[[~alton.jung] Thanks for reporting the problem. I tried a similar query with spark local mode using Hive CLI and it works. Do you use beeline connecting to a HiveServer2 instance? Could you also try Hive CLI to see if you can reproduce the same problem? Thanks., Thanks for you reply.. unluckily i got the same error with hive2 beeline and hive cli..
I want to get a confirmation about my compiling process from you.. 
In my case, i met the compile error(check below) when i compiled with "mvn clean install -Dhadoop-23.version=2.4.0  -Phadoop-2,dist". So i added "Dmaven.test.skip=true" to avoid testcase..is it ok to add test.skip when building the source??


[Error logs when i compiled using maven]
             Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.892 sec - in org.apache.hadoop.hive.metastore.TestMetastoreExpr

Results :

Failed tests: 
  TestOrcSerDeStats.testStringAndBinaryStatistics:211 expected:<273> but was:<165>
  TestOrcSerDeStats.testOrcSerDeStatsList:309 expected:<430000000> but was:<250000000>
  TestOrcSerDeStats.testOrcSerDeStatsMap:341 expected:<950000> but was:<590000>
  TestOrcSerDeStats.testOrcSerDeStatsSimpleWithNulls:373 expected:<44500> but was:<26500>
  TestOrcSerDeStats.testOrcSerDeStatsComplex:416 expected:<1740> but was:<1104>
  TestOrcSerDeStats.testOrcSerDeStatsComplexOldFormat:510 expected:<1740> but was:<1104>

Tests in error: 
  TestOrcSerDeStats.testSerdeStatsOldFormat »  Unexpected exception, expected<ja..., One more thing..could you share your prebuild version of hive on spark???, Sorry i got confused..
When i tested with hivecli then i met the below issue.


java.lang.NoClassDefFoundError: org/apache/spark/api/java/JavaSparkContext
	at org.apache.hadoop.hive.ql.exec.spark.SparkClient.<init>(SparkClient.java:73)
	at org.apache.hadoop.hive.ql.exec.spark.SparkClient.getInstance(SparkClient.java:61)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.submit(SparkSessionImpl.java:51)
	at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:76)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:72)
FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. org/apache/spark/api/java/JavaSparkContext


i think i have th classpath issue..
Thanks.., Mr Xuefu Zhang..

Sorry to make you confused,, i succeeded with hivecli after i put the spark-assembly-1.1.0-xxx to hive library..
Maybe i got a problem with class path..
the guide of hive on spark tells me that -- auxpath should be used for setting the spark library and i did it like this..
but i failed for one week, so i finally put the spark library to hive, then it succedded...


Anyway sorry to bother you.., MR Xuefu Zhang..

I have a question about supporting hiveserver2 of spark..
i tested with hiveserver2 without spark(master, worker) actived..

I submited below commands on beeline to hiveserver2..
But it worked well... 
I thought below commands should have failed because i deactived spark master and worker..
when i changed the environment to hiveserver..it worked as i expected ( it failed since i deactived master and worker of spark)


[command in beeline]
set hive.execution.engine=spark;
set spark.master=spark://localhost.localdomain:7077;
set spark.eventLog.enabled=true;
set spark.executor.memory=256m;
set spark.serializer=org.apache.spark.serializer.KryoSerializer;
select * from test where id=1 order by id;


Best regards.., [~alton.jung] Thanks for reporting the problem. I'll find a developer to look at this issue., Thanks for it..

I got really confused about current version(hive on spark)..
I succeeded with query through hive cli, but when i tested it with beeline or jdbc.. I always met error...
I wonder current version can support query with jdbc and beeline..

[Error]
java.lang.NullPointerException
	at org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:1262)
	at org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:1269)
	at org.apache.spark.SparkContext.hadoopRDD$default$5(SparkContext.scala:537)
	at org.apache.spark.api.java.JavaSparkContext.hadoopRDD(JavaSparkContext.scala:318)
	at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateRDD(SparkPlanGenerator.java:160)
	at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate(SparkPlanGenerator.java:88)
	at org.apache.hadoop.hive.ql.exec.spark.SparkClient.execute(SparkClient.java:156)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.submit(SparkSessionImpl.java:52)
	at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:76)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85), [~alton.jung] For hive, you need the latest from Spark branch. For Spark, you can also have the latest in their master branch. Since both are in the development, issues can arrive. Could you describe what you are trying to do and how to reproduce your issue(s)? Thanks., Hello,everyone:
      I followed this guide(https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started).
      I also meet this problem on hadoop2.6+spark1.3+hive1.2.1.see the error log.
  

15/09/14 14:20:25 INFO exec.Utilities: No plan file found: hdfs://bird-cluster/tmp/hive/hadoop/e4c32482-eb6e-445a-bfbe-71b3bb6cafcc/hive_2015-09-14_14-20-11_084_5577536928543129148-1/-mr-10003/7bf965f8-17cb-465f-86e1-3280fccb0f5f/map.xml
15/09/14 14:20:25 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:236)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/09/14 14:20:25 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
15/09/14 14:20:25 INFO executor.Executor: Running task 0.1 in stage 0.0 (TID 1)
15/09/14 14:20:25 INFO rdd.HadoopRDD: Input split: Paths:/user/hive/warehouse/temp.db/test/test:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat
, [~lgh1], thanks for reporting the problem. However, the stacktrace doesn't seem matching to the code. Are you sure you're on release-1.2.1?

On the other hand, it would be great if you can provide a repro case for the error., Thanks for your replay. I am sure that I am using hive 1.2.1.

The blow is the hive script：
hive> set spark.home=/usr/local/spark;
hive> set hive.execution.engine=spark;
hive> set spark.master=yarn;
hive> set spark.eventLog.enabled=true;
hive> set spark.eventLog.dir=hdfs://server1:9000/directory;
hive> set spark.serializer=org.apache.spark.serializer.KryoSerializer;
hive> select count(*) from test;
Query ID = hadoop_20150915094049_9a83ffe9-63b4-4847-b7f4-0e566f9f71d9
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = 0368cc5b-dd5d-4c7c-b48e-636d53ed350b

Query Hive on Spark job[0] stages:
0
1

Status: Running (Hive on Spark job[0])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
2015-09-15 09:41:20,764	Stage-0_0: 0(+1)/1	Stage-1_0: 0/1	
2015-09-15 09:41:22,784	Stage-0_0: 0(+1,-1)/1	Stage-1_0: 0/1	
Status: Failed
FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask



Then，the blow is  the worker errlog：
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/data/slot0/yarn/data/usercache/hadoop/filecache/17/spark-assembly-1.3.1-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/letv/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/09/15 09:41:14 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
15/09/15 09:41:15 INFO spark.SecurityManager: Changing view acls to: hadoop
15/09/15 09:41:15 INFO spark.SecurityManager: Changing modify acls to: hadoop
15/09/15 09:41:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
15/09/15 09:41:16 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/09/15 09:41:16 INFO Remoting: Starting remoting
15/09/15 09:41:16 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@10-140-110-157:21269]
15/09/15 09:41:16 INFO util.Utils: Successfully started service 'driverPropsFetcher' on port 21269.
15/09/15 09:41:16 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/09/15 09:41:16 INFO spark.SecurityManager: Changing view acls to: hadoop
15/09/15 09:41:16 INFO spark.SecurityManager: Changing modify acls to: hadoop
15/09/15 09:41:16 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
15/09/15 09:41:16 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/09/15 09:41:16 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/09/15 09:41:16 INFO Remoting: Starting remoting
15/09/15 09:41:16 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/09/15 09:41:16 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutor@10-140-110-157:15895]
15/09/15 09:41:16 INFO util.Utils: Successfully started service 'sparkExecutor' on port 15895.
15/09/15 09:41:16 INFO util.AkkaUtils: Connecting to MapOutputTracker: akka.tcp://sparkDriver@server1:12682/user/MapOutputTracker
15/09/15 09:41:16 INFO util.AkkaUtils: Connecting to BlockManagerMaster: akka.tcp://sparkDriver@server1:12682/user/BlockManagerMaster
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data12/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-8e6d671f-63c6-4269-a504-03ef36ae2b0f
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data11/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-c6159085-466f-4411-bf6e-224517f92b49
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data10/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-30dc8099-6827-4a53-8fd3-c150236e61da
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data1/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-3c4a6ca3-f84f-46f7-b365-7f51a605c044
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data4/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-c3c56c5b-e368-45bb-9a50-d660301d5939
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data5/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-f19e07ea-6614-4035-92bb-e89674506c68
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data2/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-c245e542-6bd1-470c-ae8d-2a3ec389b879
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data3/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-752fd55f-e15f-43cf-af75-a726bfb498d0
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data8/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-bbeae731-7a63-455d-b453-4c1915e878dc
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data9/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-7cf05ed3-aff3-455b-8db7-c7242eda8b97
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data6/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-8cb7c93b-c15a-465d-ba0a-3be5abe5c899
15/09/15 09:41:16 INFO storage.DiskBlockManager: Created local directory at /data/hadoop/data7/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/blockmgr-1d0087cd-0c35-4f67-8bdd-331c14235290
15/09/15 09:41:17 INFO storage.MemoryStore: MemoryStore started with capacity 1060.3 MB
15/09/15 09:41:17 INFO util.AkkaUtils: Connecting to OutputCommitCoordinator: akka.tcp://sparkDriver@server1:12682/user/OutputCommitCoordinator
15/09/15 09:41:17 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://sparkDriver@server1:12682/user/CoarseGrainedScheduler
15/09/15 09:41:17 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
15/09/15 09:41:17 INFO executor.Executor: Starting executor ID 2 on host 10-140-110-157
15/09/15 09:41:17 INFO netty.NettyBlockTransferService: Server created on 40604
15/09/15 09:41:17 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/09/15 09:41:17 INFO storage.BlockManagerMaster: Registered BlockManager
15/09/15 09:41:17 INFO util.AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@server1:12682/user/HeartbeatReceiver
15/09/15 09:41:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
15/09/15 09:41:20 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/09/15 09:41:20 INFO executor.Executor: Fetching http://10.140.130.38:54874/jars/hive-exec-1.2.1.jar with timestamp 1442281278254
15/09/15 09:41:20 INFO util.Utils: Fetching http://10.140.130.38:54874/jars/hive-exec-1.2.1.jar to /data/hadoop/data12/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/fetchFileTemp6606399870470341414.tmp
15/09/15 09:41:20 INFO util.Utils: Copying /data/hadoop/data12/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/12837195511442281278254_cache to /data/slot3/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/container_1441701008982_0038_01_000003/./hive-exec-1.2.1.jar
15/09/15 09:41:21 INFO executor.Executor: Adding file:/data/slot3/yarn/data/usercache/hadoop/appcache/application_1441701008982_0038/container_1441701008982_0038_01_000003/./hive-exec-1.2.1.jar to class loader
15/09/15 09:41:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
15/09/15 09:41:21 INFO storage.MemoryStore: ensureFreeSpace(50134) called with curMem=0, maxMem=1111794647
15/09/15 09:41:21 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 49.0 KB, free 1060.2 MB)
15/09/15 09:41:21 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
15/09/15 09:41:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 299 ms
15/09/15 09:41:21 INFO storage.MemoryStore: ensureFreeSpace(163768) called with curMem=50134, maxMem=1111794647
15/09/15 09:41:21 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 159.9 KB, free 1060.1 MB)
15/09/15 09:41:21 INFO rdd.HadoopRDD: Input split: Paths:/user/hive/warehouse/temp.db/test/test:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat

15/09/15 09:41:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
15/09/15 09:41:22 INFO storage.MemoryStore: ensureFreeSpace(46802) called with curMem=213902, maxMem=1111794647
15/09/15 09:41:22 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 45.7 KB, free 1060.0 MB)
15/09/15 09:41:22 INFO storage.BlockManagerMaster: Updated info of block broadcast_0_piece0
15/09/15 09:41:22 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 187 ms
15/09/15 09:41:22 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/09/15 09:41:22 INFO storage.MemoryStore: ensureFreeSpace(540492) called with curMem=260704, maxMem=1111794647
15/09/15 09:41:22 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 527.8 KB, free 1059.5 MB)
15/09/15 09:41:22 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/09/15 09:41:22 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/09/15 09:41:22 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/09/15 09:41:22 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/09/15 09:41:22 INFO exec.Utilities: No plan file found: hdfs://bird-cluster/tmp/hive/hadoop/23ba4f2d-1f4c-42d7-af83-1cfc9411344c/hive_2015-09-15_09-40-49_573_5616367127345554376-1/-mr-10003/f96f663b-b0ad-408c-8cda-534f9a49ea2e/map.xml
15/09/15 09:41:22 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:236)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

And blow is hive error log：
2015-09-15 09:41:20,110 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: </PERFLOG method=SparkBuildRDDGraph start=1442281280024 end=1442281280109 duration=85 from=org.apache.hadoop.hive.ql.exec.spark.SparkPlan>
2015-09-15 09:41:20,127 INFO  [RPC-Handler-3]: client.SparkClientImpl (SparkClientImpl.java:handle(547)) - Received spark job ID: 0 for 0368cc5b-dd5d-4c7c-b48e-636d53ed350b
2015-09-15 09:41:20,134 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: <PERFLOG method=getSplits from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>
2015-09-15 09:41:20,134 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: PLAN PATH = hdfs://bird-cluster/tmp/hive/hadoop/23ba4f2d-1f4c-42d7-af83-1cfc9411344c/hive_2015-09-15_09-40-49_573_5616367127345554376-1/-mr-10003/f96f663b-b0ad-408c-8cda-534f9a49ea2e/map.xml
2015-09-15 09:41:20,135 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: ***************non-local mode***************
2015-09-15 09:41:20,135 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: local path = hdfs://bird-cluster/tmp/hive/hadoop/23ba4f2d-1f4c-42d7-af83-1cfc9411344c/hive_2015-09-15_09-40-49_573_5616367127345554376-1/-mr-10003/f96f663b-b0ad-408c-8cda-534f9a49ea2e/map.xml
2015-09-15 09:41:20,135 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: Open file to read in plan: hdfs://bird-cluster/tmp/hive/hadoop/23ba4f2d-1f4c-42d7-af83-1cfc9411344c/hive_2015-09-15_09-40-49_573_5616367127345554376-1/-mr-10003/f96f663b-b0ad-408c-8cda-534f9a49ea2e/map.xml
2015-09-15 09:41:20,143 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: <PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-09-15 09:41:20,144 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO exec.Utilities: Deserializing MapWork via kryo
2015-09-15 09:41:20,182 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: </PERFLOG method=deserializePlan start=1442281280143 end=1442281280181 duration=38 from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-09-15 09:41:20,182 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO io.CombineHiveInputFormat: Total number of paths: 1, launching 1 threads to check non-combinable ones.
2015-09-15 09:41:20,198 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries
2015-09-15 09:41:20,201 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 826e7d8d3e839964dd9ed2d5f83296254b2c71d3]
2015-09-15 09:41:20,355 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO io.CombineHiveInputFormat: CombineHiveInputSplit creating pool for hdfs://bird-cluster/user/hive/warehouse/temp.db/test; using filter path hdfs://bird-cluster/user/hive/warehouse/temp.db/test
2015-09-15 09:41:20,379 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO input.FileInputFormat: Total input paths to process : 1
2015-09-15 09:41:20,399 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2015-09-15 09:41:20,400 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO io.CombineHiveInputFormat: number of splits 1
2015-09-15 09:41:20,401 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO io.CombineHiveInputFormat: Number of all splits 1
2015-09-15 09:41:20,401 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO log.PerfLogger: </PERFLOG method=getSplits start=1442281280133 end=1442281280401 duration=268 from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>
2015-09-15 09:41:20,417 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Registering RDD 1 (mapPartitionsToPair at MapTran.java:31)
2015-09-15 09:41:20,420 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Got job 0 (foreachAsync at RemoteHiveSparkClient.java:257) with 1 output partitions (allowLocal=false)
2015-09-15 09:41:20,421 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Final stage: Stage 1(foreachAsync at RemoteHiveSparkClient.java:257)
2015-09-15 09:41:20,422 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 0)
2015-09-15 09:41:20,429 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Missing parents: List(Stage 0)
2015-09-15 09:41:20,440 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[1] at mapPartitionsToPair at MapTran.java:31), which has no missing parents
2015-09-15 09:41:20,467 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.MemoryStore: ensureFreeSpace(163768) called with curMem=587318, maxMem=278302556
2015-09-15 09:41:20,468 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 159.9 KB, free 264.7 MB)
2015-09-15 09:41:20,490 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.MemoryStore: ensureFreeSpace(50134) called with curMem=751086, maxMem=278302556
2015-09-15 09:41:20,490 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 49.0 KB, free 264.6 MB)
2015-09-15 09:41:20,491 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on server1:37766 (size: 49.0 KB, free: 265.3 MB)
2015-09-15 09:41:20,492 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO storage.BlockManagerMaster: Updated info of block broadcast_1_piece0
2015-09-15 09:41:20,493 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839
2015-09-15 09:41:20,504 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[1] at mapPartitionsToPair at MapTran.java:31)
2015-09-15 09:41:20,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks
2015-09-15 09:41:20,552 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10-140-110-157, RACK_LOCAL, 1515 bytes)
2015-09-15 09:41:20,760 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=SparkSubmitToRunning start=1442281254723 end=1442281280760 duration=26037 from=org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor>
2015-09-15 09:41:20,760 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 
Query Hive on Spark job[0] stages:
2015-09-15 09:41:20,763 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 0
2015-09-15 09:41:20,763 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 1
2015-09-15 09:41:20,763 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 
Status: Running (Hive on Spark job[0])
2015-09-15 09:41:20,763 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
2015-09-15 09:41:20,764 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=SparkRunStage.0_0 from=org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor>
2015-09-15 09:41:20,765 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 2015-09-15 09:41:20,764	Stage-0_0: 0(+1)/1	Stage-1_0: 0/1	
2015-09-15 09:41:21,432 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:21 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10-140-110-157:40604 (size: 49.0 KB, free: 1060.2 MB)
2015-09-15 09:41:22,179 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:22 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10-140-110-157:40604 (size: 45.7 KB, free: 1060.2 MB)
2015-09-15 09:41:22,612 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:22 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 10-140-110-157): java.lang.NullPointerException
2015-09-15 09:41:22,612 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)
2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)
2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)
2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)
2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:236)
2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)
2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
2015-09-15 09:41:22,613 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
2015-09-15 09:41:22,614 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.Task.run(Task.scala:64)
2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.lang.Thread.run(Thread.java:745)
2015-09-15 09:41:22,615 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 
2015-09-15 09:41:22,618 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:22 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, 10-140-110-155, RACK_LOCAL, 1515 bytes)
2015-09-15 09:41:22,784 INFO  [main]: status.SparkJobMonitor (SessionState.java:printInfo(951)) - 2015-09-15 09:41:22,784	Stage-0_0: 0(+1,-1)/1	Stage-1_0: 0/1	
2015-09-15 09:41:23,436 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:23 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10-140-110-155:42408 (size: 49.0 KB, free: 1060.2 MB)
2015-09-15 09:41:23,954 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:23 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10-140-110-155:42408 (size: 45.7 KB, free: 1060.2 MB)
2015-09-15 09:41:24,377 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on executor 10-140-110-155: java.lang.NullPointerException (null) [duplicate 1]
2015-09-15 09:41:24,380 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, 10-140-110-155, RACK_LOCAL, 1515 bytes)
2015-09-15 09:41:24,426 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on executor 10-140-110-155: java.lang.NullPointerException (null) [duplicate 2]
2015-09-15 09:41:24,428 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, 10-140-110-155, RACK_LOCAL, 1515 bytes)
2015-09-15 09:41:24,467 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on executor 10-140-110-155: java.lang.NullPointerException (null) [duplicate 3]
2015-09-15 09:41:24,468 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
2015-09-15 09:41:24,471 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-09-15 09:41:24,475 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO cluster.YarnScheduler: Cancelling stage 0
2015-09-15 09:41:24,477 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO scheduler.DAGScheduler: Stage 0 (mapPartitionsToPair at MapTran.java:31) failed in 3.945 s
2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/09/15 09:41:24 INFO client.RemoteDriver: Failed to run job 0368cc5b-dd5d-4c7c-b48e-636d53ed350b
2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - java.util.concurrent.ExecutionException: Exception thrown by job
2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:311)
2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:316)
2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:382)
2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:335)
2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
2015-09-15 09:41:24,506 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.lang.Thread.run(Thread.java:745)
2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 10-140-110-155): java.lang.NullPointerException
2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)
2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)
2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)
2015-09-15 09:41:24,507 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)
2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:236)
2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:212)
2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-09-15 09:41:24,508 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.Task.run(Task.scala:64)
2015-09-15 09:41:24,509 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at java.lang.Thread.run(Thread.java:745)
2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 
2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - Driver stacktrace:
2015-09-15 09:41:24,510 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
2015-09-15 09:41:24,511 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
2015-09-15 09:41:24,511 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
2015-09-15 09:41:24,511 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
2015-09-15 09:41:24,514 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
2015-09-15 09:41:24,514 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
2015-09-15 09:41:24,514 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
2015-09-15 09:41:24,514 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at scala.Option.foreach(Option.scala:236)
2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
2015-09-15 09:41:24,515 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2015-09-15 09:41:24,540 INFO  [RPC-Handler-3]: client.SparkClientImpl (SparkClientImpl.java:handle(522)) - Received result for 0368cc5b-dd5d-4c7c-b48e-636d53ed350b
2015-09-15 09:41:24,795 ERROR [main]: status.SparkJobMonitor (SessionState.java:printError(960)) - Status: Failed
2015-09-15 09:41:24,795 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=SparkRunJob start=1442281254723 end=1442281284795 duration=30072 from=org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor>
2015-09-15 09:41:24,817 ERROR [main]: ql.Driver (SessionState.java:printError(960)) - FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
2015-09-15 09:41:24,817 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=Driver.execute start=1442281249766 end=1442281284817 duration=35051 from=org.apache.hadoop.hive.ql.Driver>
2015-09-15 09:41:24,817 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
2015-09-15 09:41:24,817 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=releaseLocks start=1442281284817 end=1442281284817 duration=0 from=org.apache.hadoop.hive.ql.Driver>
2015-09-15 09:41:24,819 INFO  [main]: exec.ListSinkOperator (Operator.java:close(612)) - 7 finished. closing... 
2015-09-15 09:41:24,819 INFO  [main]: exec.ListSinkOperator (Operator.java:close(634)) - 7 Close done
2015-09-15 09:41:24,826 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
2015-09-15 09:41:24,826 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=releaseLocks start=1442281284826 end=1442281284826 duration=0 from=org.apache.hadoop.hive.ql.Driver>






Thanks again.It seems that the spark job is already successfully running on the yarn from hive,but the explain job map.xml is lost .


, e,aha。。
   
   now it works after I recompile the spark 1.3.1  using the following command
    
./make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.4" -Dhadoop.version=2.6.0 -Dyarn.version=2.6.0 -DskipTests
   , e,aha。。
   
   now it works after I recompile the spark 1.3.1  using the following command
    
./make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.4" -Dhadoop.version=2.6.0 -Dyarn.version=2.6.0 -DskipTests
   , I am facing simiar issue but when I try to build spark 1.3.3 version from the above command , I am facing below error:

https://issues.apache.org/jira/browse/SPARK-10944

Please help I am missing anything.]