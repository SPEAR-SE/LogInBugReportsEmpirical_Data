[What is retaining them as per MAT ?
, I am still analyzing the heap, but appears like they are all stashed away in HashMap, perhaps in a threadlocal. I do not have the allocation stack for these objects so I cannot tell what part of the code creates these instances.

Just running a simple query iteratively via beeline where it connects + disconnects every iteration, I observe the leak. Not sure if it is the same workload as in the environment where the heap dump was generated from.

I am currently running some tests with a change to remove it from the threadlocal at
https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L811
and
https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L483

, [~thejas] I have a better understanding of what is causing this issue. It appears that FileSystem.Cache (hadoop APIs) is retaining the instances of Configuration in its cache. 
Anytime we call a FileSystem.get(conf), like so
https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L1685

the conf object becomes the part of the key for the map entry. Its meant to improve performance so we dont have to re-create these FileSystem objects, but doesnt appear that Hive's use of these APIs is using the cache efficiently. 
There are other areas in the code that contribute, like Path.getFileSystem() under the covers could add to this cache.
https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java#L104

Caching can be turned off entirely by using 
fs.%s.impl.disable.cache=true where %s is the caching scheme (ex: hdfs or s3) which might make this problem go away but has a performance overhead. (I havent measured it though).

Unfortunately, there is no means to selectively turn off the caching on a per call basis. So we have to fix this in the hive code. fs.close() would remove the entry from the cache. But we cannot call it every time we use this API, as it would be the same as disabling the cache entirely. So its easy choice to add fs.close() here
https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L1685

But for the other code in Warehouse, we need more data around the cache hits and misses. I am working on instrumenting the FileSystem code to provide this info.

Alternate thought, (I am not sure how feasible it is though), since the FileSystem code does not appear to be using the properties within this Configuration object itself, it may be safe to use a static instance of HiveConf on most calls to FileSystem, like mkdirs(), get() etc. This way we use the cache efficiently too. However, I am not sure if there will be session specific properties that get used across all calls to the FileSystem APIs. 

Thoughts? Thanks in advance., This FileSystem.CACHE leak is an issue that shows up in several places across projects. Its not a cache in the traditional sense, that it doesn't have an eviction policy.
The peculiar thing about this cache is that the underlying Map has a key that contains UGI object, and the key comparison uses object equality (== operator) instead of .equals method for comparison. This is apparently by design for some security aspects.

So you have one entry in the cache for every UGI object. 

See the final patch in HIVE-3098 for example. The usual way to solve this is to call FileSystem.closeAllForUGI once the UGI use is over.
Other related leak jiras - HIVE-4501, HIVE-9234 .

You might want to check if this is happening in latest trunk as well. There might be other patches that have fixed the problem.



, perhaps in the HiveMetaStore.shutdown() we clear the cache for the current UGI. Make sense?  Could you please review the patch when you have a chance ?
I have had the customer disable the FileSystem caching by adding {{fs.hdfs.impl.disable.cache=true}} to the HMS configuration, the re-run the workloads. The same site that had 66000+ Configuration instances in their heapdump now has 80 instances and none of them are in Cache. So it is clear that the FileSystem.CACHE is the problem. 
Thanks, Regarding the patch
1. How do you make sure that files created by this ugi are not in use in other parts ? We need do the closing only after we are sure that the ugi object is no longer going to be used. 
2. I am not sure if this would fix the leak. As you can see we have patches that deal with the closing when UGI object is no longer used.
Are you able to reproduce this in your environment ? If not, you might want to add some debugging around code that adds entries in the cache, and see if the closing of files generated from those places is happening.
You might also want to see if the user is some some plugins that might be creating new UGI objects.

, Oops just posted the patch to RB (https://reviews.apache.org/r/47918/) at the same time as this comment.
1) Isnt the shutdown() called when a HMS request is fulfilled and the executor thread is being released back to the pool? So any new calls would potentially have a new UGI and a new instance of HiveConf. Also, calling closeAll() just removes the cached element. At worst, the FileSystem object is re-cached on a miss.
2) The other fixes are to address a similar issue on the HS2 side where using the FileSystem APIs causes the Cache to grow. This issue is on the HMS side.
Regarding reproducing this locally, yes and no. I ran 100's of iterations of beeline executing a script that create a table and then drops it while randomly toggling the value of a hive conf property. For 300 iterations, I have gotten it to retain 60 instances which is not quite the same success as the customer is having. I think because of my test being run as a single user. Re-running the test with this fix, I have 8 instances retained but none in this particular cache.
I have run with debug around this code and during the drop table command, I can see an element being added to the cache. I am also waiting for logs from this customer who is running with some instrumentation + fix. I can confirm that from those logs too.

Alternatively, in checkTrashPurgeCombination() we could add a close() to this FileSystem. In my testcase, this has been the primary reason for the retained instances.
{code}
          HadoopShims.HdfsEncryptionShim shim =
            ShimLoader.getHadoopShims().createHdfsEncryptionShim(FileSystem.get(hiveConf), hiveConf);
{code}

Thoughts? Thanks, HIVE-3098 fixes it for metastore . 
This fix can be dangerous for the embedded metastore use case.

bq. I think because of my test being run as a single user. 
Single user shouldn't matter as the cache is based on the UGI object as I mentioned earlier. Testing using hive-cli might be better that would ensure creation on new metastore connection each time as well.

I assume you haven't seen this in other user environments. I suspect there is something unique about their environment that would be triggering this. You might want to check if their are using any specific plugins.
Is this with kerberos enabled ?, [~ngangam] Just try to understand the issue. FileSystem will cache the key (containing conf). But seems it doesn't make copies of conf but uses the reference? Are we seeing the conf is held by cache object? , [~thejas] Thanks for continued discussion on this. However, I am not entirely sure I understand your comment about it being dangerous for scenarios with embedded metastore. Could you please elaborate and help me understand? 
I will lay out what I found so far so that we are all on the same page. Helps answer [~aihuaxu] questions too.
1) In a HMS heap dump from a customer, there were 66000 instances of Configuration class stored in a HashMap (FileSystem.Cache.CACHE). These instances along with the set of its retained objects accounted for 95% of the total heap. Thats huge.
2) FileSystem.Cache use a map that uses a composite key with (UGI + Configuration) objects. Because of the way, these keys are compared or because Configuration objects used in Hive are different (because of session params and what not), or a combination of both, we have a lot of cache misses which result in new entries being added to this cache.
3) This Cache is storing FileSystem objects (an abstraction to the underlying DFS), which, I am told by Hadoop engineer, are a bit expensive to instantiate.  
4) HMS uses APIs like, FileSystem.get() (in HiveMetaStore) and Path.getFileSystem()(In Warehouse.java) that cause the cache to grow. 
5) I have had the customer disable this cache entirely using a hadoop property, and they have neither seen any issues functionally nor observed any severe performance regression while the HMS heap size remained <350MB compared to 10GB with the cache enabled.

Since disabling the cache entirely causes no functional regression, wouldn't adding a call to delete elements from this cache would be fine too, functionally? Thank you, Unfortunately, my test environment got auto-destroyed. I will set it up again to re-run with hive CLI instead. I wasnt using kerberos the first time. Did you want me to use kerberos this time? Thanks, OK. So it's not the objects of Conf themselves occupy the memory, but the instances of the conf (too many of them) as I read from above and HIVE-3098?, The retained size for each Configuration instance is roughly 100KB. ~66k instances x ~100KB ~= 6+GB. Thats what we are trying to address.

[~thejas] We have seen several issues around HMS stability (sudden death crashes) in the past, a good % of them were OOMs, some connections leaks, some incorrectly sized thread pools etc. We have asked customers to increase the max memory for the HMS process. Some customers running with 40GB memory for HMS and that too at times is not enough. 16GB to 24GB is quite common amongst other customers too. 
In the past, we have not received heap dumps from the process prior to the crash. So we did not get a chance to analyze it., bq. Since disabling the cache entirely causes no functional regression, wouldn't adding a call to delete elements from this cache would be fine too, functionally
No, that method is closeAllForUGI, not deleteAllForUGIFromCache, ie it closes the FileSystem instances. In case of metastore being used in embedded mode (-hiveconf hive.metastore.uris=' ') , if there are references to the FileSystem object in other parts of the code, they will be suddenly closed instances.

[~aihuaxu] The FileSystem.CACHE keeps the references to the FileSystem objects around even if most of them aren't being used and GC will not free them as a result.

The changes in HIVE-3098 should be addressing this, we need to understand why it wouldn't do that before adding yet another call to closeAllForUGI.
We haven't seen this OOM in metastore with hive 1.2.0 users, except for an issue when ACID was enabled, which [~wzheng] worked on. So one thing to analyze is if any plugins are creating new UGI object and that is causing this OOM. 

[~daijy] has some experience with dealing with this type of leak. He can also advise on this.
, [~ngangam], what I used to do before to diagnose is to use a patched hadoop client libraries to catch the stack of every invocation of FileSystem.get, and understand exactly where the leak coming from. I don't want to blindly remove it in shutdown, plus, UGI object might already get lost at that time and you might not able to remove it.

Here is how I patch Hadoop:
{code}
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
@@ -20,6 +20,8 @@
 import java.io.Closeable;
 import java.io.FileNotFoundException;
 import java.io.IOException;
+import java.io.StringWriter;
+import java.io.PrintWriter;
 import java.lang.ref.WeakReference;
 import java.net.URI;
 import java.net.URISyntaxException;
@@ -2699,6 +2701,10 @@ private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOEx
         }
         fs.key = key;
         map.put(key, fs);
+        StringWriter sw = new StringWriter();
+        new Throwable("").printStackTrace(new PrintWriter(sw));
+        LOG.info("calling context for getInternal:" + sw.toString());
+        LOG.info("# of maps:" + map.size());
         if (conf.getBoolean("fs.automatic.close", true)) {
           toAutoClose.add(key);
         }
@@ -2752,6 +2758,7 @@ synchronized void closeAll(boolean onlyAutomatic) throws IOException {
       if (!exceptions.isEmpty()) {
         throw MultipleIOException.createIOException(exceptions);
       }
+      LOG.info("map size after closeAll:" + map.size());
     }

     private class ClientFinalizer implements Runnable {
@@ -2789,6 +2796,7 @@ synchronized void closeAll(UserGroupInformation ugi) throws IOException {
       if (!exceptions.isEmpty()) {
         throw MultipleIOException.createIOException(exceptions);
       }
+      LOG.info("map size after closeAll:" + map.size());
     }

     /** FileSystem.Cache.Key */
{code}

Here is how to instruct Hive to use this jar:
1. Put attached hadoop-common.jar into $HIVE_HOME/lib
2. export HADOOP_USER_CLASSPATH_FIRST=true
3. Make sure fs cache is enabled
4. Restart hivemetastore
5. Collect hivemetastore.log, Thanks [~daijy] 
I have been running with some added instrumentation in the HMS code to figure out the cache sizes before and after. But your idea seems better, seeking info from the hadoop's end.
There are 3 general areas that seem to be adding objects to the cache.
1) The compactor.Initiator and CompactorThread create about ~420k objects. These seem to be addressed in HIVE-13151. This environment is not running with this fix.
2) The Warehouse.getFs() and Warehouse.getFileStatusesForLocation() are invoked about ~900k times, but not all calls result in new object in the cache.
3) A small % of the calls are from drop_table_core. 

I will try to see other areas that use these FS apis that could be adding to this cache.

Thejas, the fix from HIVE-3098 no longer exists in the codebase. It has been replaced by the fix in HIVE-8228 (simliar intent). The root cause could very well be the initiator thread. I will check their configuration to affirm this and use HIVE-13151 if needed. Thanks, [~thejas] After disabling the compactor.Initiator and the Compactor threads, (because this customer is not using the fix from HIVE-13151), there appear to be no more leaks. 
However, there are still about 400 instances of Configuration objects in memory (about 80MB of retained objects, 12% in this case), about 11 of them from static initializers in *Writable classes and the remaining of them stashed in thread locals, 1 per thread. So HMS roughly has 390 threads, each has 1 instance of Configuration set in its threadlocals. These references should be re-set when the thread gets re-assigned but they would be retained until this occurs. Would it make sense to do this cleanup sooner. Something like this
{code}
         try {
           ms.shutdown();
         } finally {
           threadLocalConf.remove();
           threadLocalMS.remove();
{code}
As always, thank you for your input in advance. , Have a new heap dump from a customer that has been running for an extended period of time, with the following properties removed.
{code}
<property><name>hive.compactor.initiator.on</name><value>true</value></property>
<property><name>hive.compactor.worker.threads</name><value>1</value></property>
{code}

The leak from the above compactor thread was later addressed in HIVE-13151 that the customer was not running with. So I had them remove these properties.

HMS has been stable, no OOM crashes yet. There are about 214 instances of Configuration occupying 66MB of heap space, roughly 18% of the total active heap. Each of this instance was from an individual thread holding on to the object in the threadlocals. I think we can save this unnecessary reference and do better by cleaning up the threadlocal before the thread is returning to the pool.

I have uploaded a new patch that does just that. Let me know of any thoughts. Thanks, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12815813/HIVE-13749.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10287 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_subquery_multiinsert
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_complex_all
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vector_complex_join
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.testDelayedLocalityNodeCommErrorImmediateAllocation
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/350/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/350/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-350/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12815813 - PreCommit-HIVE-MASTER-Build, The test failures above do not appear to be related to the patch. So +1 for me., +1. The change makes sense to me., +1 looks good.
Thanks for chasing this down!, Pushed to master. Thanks Naveen for the contribution.]