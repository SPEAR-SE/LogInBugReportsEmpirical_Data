[Avro file containing the sample data. To reproduce the issue, just create a Hive table from this file and issue a 
{code}
CREATE TABLE broken_parquet_table STORED AS PARQUET
AS SELECT * FROM the_avro_table;

SELECT * FROM broken_parquet_table;
{code}, Hi [~Akryus]

What is the command you used to create the avro table from the file you attached?, Here is how you can create the sample Avro table:
{code}
CREATE EXTERNAL TABLE my_test_table(`avreau_col_1` map<string,string>) 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS  
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'   
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' 
LOCATION '/user/fterrazzoni/my_test_table' 
TBLPROPERTIES("avro.schema.url"="hdfs://localhost:9000/user/fterrazzoni/path/to/avro.schema") ;
{code}

Where : 
- /user/fterrazzoni/my_test_table/ is a directory containing the file I provided (map_null_val.avro).
- /user/fterrazzoni/path/to/avro.schema is a text file containing the Avro schema corresponding to the data, which is:
  {code}
  {"type":"record","name":"dku_record_0","namespace":"com.dataiku.dss","doc":"","fields":[{"name":"avreau_col_1","type":["null",{"type":"map","values":["null","string"]}],"default":null}]}
  {code}

Now, you can simply try :
{code} SELECT * from my_test_table; {code}
... and observe that the result is correct.

However, if you copy that into a Parquet table:
{code} 
CREATE TABLE test_parquet STORED AS PARQUET AS SELECT * FROM my_test_table;
SELECT * FROM test_parquet;
{code}
... the output is corrupted., I cannot deserialize the avro schema on the current hive version 0.14., Attach patch that supports map null values on parquet., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12678229/HIVE-8359.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6609 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_acid
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1561/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1561/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1561/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12678229 - PreCommit-HIVE-TRUNK-Build, I submitted the patch to code review.
https://reviews.apache.org/r/27404/, This is the same issue as HIVE-6994. I made a fix also to handle map and array. We can also write nested complex type., [~mickaellcr] - [~spena] and [~rdblue] have done a ton of review work on this issue. Thus I am inclined to commit this one. If we do, will there be any work in HIVE-6994 or are they complete duplicates?, Thanks, Brock. De-duplicating this and HIVE-6994 is on my list of to-do items for today. I really like the tests that Sergio has put together for this, but I still need to see what Mickael has done that isn't done by Sergio's patch. I think it would be good if we all looked over both and then discussed what is needed for both of them., I'll take a look at the code. , Yes, I will take a look too and it's totally fine for me to take this patch !, +1 for me.

The patch works fine for map containing null values, but it doesn't solve the HIVE-6994. So it's not duplicate. Can I remove the duplicate link ?, Linking to the review, which I think has a newer patch., [~mickaellcr], it looks HIVE-6994 fixes the read and write handling of Arrays to ensure that null values are written and read correctly. Is that a good summary?

This issue fixes writing null values in maps and arrays. So I think there is still some duplication between the two where arrays are written. Given that [~spena]'s work includes unit tests for the write side, would you mind solving the duplication by working on the write side on this issue?, Ok , I will work on this patch., I only did two minor changes on :

* ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
* ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java

And added a qtest (same as HIVE-6994). I used the patch available on the review link., Hi [~mickaellcr],

I believe that [~spena] is still working this one based on the Review Board discussion here: https://reviews.apache.org/r/27404/

Sorry for the crossed wires!

Brock, 

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12681943/HIVE-8359.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1822/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1822/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1822/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-1822/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target shims/scheduler/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target accumulo-handler/target hwi/target common/target common/src/gen contrib/target service/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update
U    ql/src/test/results/clientnegative/udf_assert_true.q.out
U    ql/src/test/results/clientnegative/udf_assert_true2.q.out
U    ql/src/test/results/clientpositive/lateral_view_noalias.q.out
U    ql/src/test/results/clientpositive/lateral_view.q.out
U    ql/src/test/results/clientpositive/udtf_stack.q.out
U    ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcCtx.java
U    ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java

Fetching external item into 'hcatalog/src/test/e2e/harness'
Updated external to revision 1640231.

Updated to revision 1640231.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12681943 - PreCommit-HIVE-TRUNK-Build, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12681988/HIVE-8359.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6657 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1824/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1824/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1824/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12681988 - PreCommit-HIVE-TRUNK-Build, [~mickaellcr],

I was confused earlier... my mistake! The patch you attached to this JIRA is actually for HIVE-6994. Can you upload it there?, [~brocknoland], normally I picked the patch that [~rdblue] told me about (the review on the Review Board), but maybe not the last version.

[~rdblue] wanted me to update this patch to handle the HIVE-6994 instead of having two patches that will have the same behavior/code. And I like the way [~spena] wrote the solution (better than mine in my opinion).

[~spena], basically I modified the WritableGroupConverter to clean the 'current value'. If you don't do that, you will never have a null value inside an array, but the previous one.
{code}
diff --git ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
index 582a5df..052b36d 100644
--- ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
+++ ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ArrayWritableGroupConverter.java
@@ -54,6 +54,7 @@ public void start() {
     if (isMap) {
       mapPairContainer = new Writable[2];
     }
+    currentValue = null;
   }
 
   @Override
{code}

And the second part was to add "Null" values from the ParquetHiveSerDe (values that I was skipping before for no valid reason).

{code}
diff --git ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
index b689336..4b36767 100644
--- ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
+++ ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java
@@ -202,13 +202,11 @@ private ArrayWritable createArray(final Object obj, final ListObjectInspector in
     if (sourceArray != null) {
       for (final Object curObj : sourceArray) {
-        final Writable newObj = createObject(curObj, subInspector);
-        if (newObj != null) {
-          array.add(newObj);
-        }
+        array.add(createObject(curObj, subInspector));
       }
     }
     if (array.size() > 0) {
-      final ArrayWritable subArray = new ArrayWritable(array.get(0).getClass(),
+      final ArrayWritable subArray = new ArrayWritable(Writable.class,
           array.toArray(new Writable[array.size()]));
       return new ArrayWritable(Writable.class, new Writable[] {subArray});
     } else {
{code}

And the qtest was just to be sure to handle empty array, null array, array with null, and the same for map.

{code}
+++ data/files/parquet_array_null_element.txt
@@ -0,0 +1,3 @@
+1|,7|CARRELAGE,MOQUETTE|key11:value11,key12:value12,key13:value13
+2|,|CAILLEBOTIS,|
+3|,42,||key11:value11,key12:,key13:
{code}


If you want to integrate them into your patch, feel free to do it, else I might want to duplicate your patch (:p) and add this fix., Thanks [~mickaellcr].

Sorry for the confusion. I did not see you uploaded another patch here. 
I just added two extra lines to the patch you uploaded. I will integrate your fixes there, and upload the patch again., Attach new patch that integrates Mickael Lacour HIVE-6994 fix., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12682178/HIVE-8359.5.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6659 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1835/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1835/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1835/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12682178 - PreCommit-HIVE-TRUNK-Build, I think with [~mickaellcr]'s addition, this is ready to go in.

Good catch in the SerDe code, I didn't realize that the nulls were stripped at that point as well. I'm a little confused about why we're translating the ArrayWritable again though: isn't this properly constructed by the Converter code? Why can't we just pass the ArrayWritable that was created already? It seems like we're doing a lot of unnecessary work here that we might be able to remove (in future patches). Ideally, we would detect that the structure matches what is expected by the following Hive code and pass it along., bq. this is ready to go in.....It seems like we're doing a lot of unnecessary work here that we might be able to remove (in future patches). 

It sounds like this change can be done in a future patch and you are +1 on committing this change?, Yes. +1 on this change and we'll clean up the rest in a future patch and after I confirm what I think is happening., +1, Thank you so much Sergio, Ryan and Mickael!! I have committed this contribution to trunk!]