[Hi [~KaiXu], the failure is just a warning and it shouldn't make the query fail:
{code}
  @Override
  public String getAppID() {
    Future<String> getAppID = sparkClient.run(new GetAppIDJob());
    try {
      return getAppID.get(sparkClientTimeoutInSeconds, TimeUnit.SECONDS);
    } catch (Exception e) {
      LOG.warn("Failed to get APP ID.", e);
      return null;
    }
  }
{code}
Please check your log again to find the real cause of the failure. Btw this has nothing to do with Spark and you don't need to log a Spark JIRA., Thanks Rui Li for the information, the JIRA was first loged on Spark by mistake I have changed it to Hive., Seems the application is ACCEPTED but didn't reach RUNNING. YARN log may be helpful to find out why the app hasn't launched., Hi [~lirui], above container log is the yarn application log, the middle has been cut off for they're repeated "Failed to connect to driver at 192.168.1.1:43656, retrying"., From nodemanager log, can only see container transitioned from LOCALIZED to RUNNING, then failed with exitCode=10.

2017-02-13 05:04:00,536 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://hsx-node1:8020/user/root/.sparkStaging/application_1486905599813_0046/__spark_libs__6842484649003444330.zip(->/mnt/disk6/yarn/nm/usercache/root/filecache/94/__spark_libs__6842484649003444330.zip) transitioned from DOWNLOADING to LOCALIZED
2017-02-13 05:04:00,641 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://hsx-node1:8020/user/root/.sparkStaging/application_1486905599813_0046/__spark_conf__.zip(->/mnt/disk7/yarn/nm/usercache/root/filecache/95/__spark_conf__.zip) transitioned from DOWNLOADING to LOCALIZED
2017-02-13 05:04:00,641 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1486905599813_0046_01_000001 transitioned from LOCALIZING to LOCALIZED
2017-02-13 05:04:00,661 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1486905599813_0046_01_000001 transitioned from LOCALIZED to RUNNING
2017-02-13 05:04:00,661 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread
2017-02-13 05:04:00,717 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /mnt/disk2/yarn/nm/usercache/root/appcache/application_1486905599813_0046/container_1486905599813_0046_01_000001/default_container_executor.sh]
2017-02-13 05:04:03,304 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1486905599813_0047_000001 (auth:SIMPLE)

2017-02-13 05:05:42,694 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1486905599813_0046_01_000001 is : 10
2017-02-13 05:05:42,695 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exception from container-launch with container ID: container_1486905599813_0046_01_000001 and exit code: 10
ExitCodeException exitCode=10:
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)
        at org.apache.hadoop.util.Shell.run(Shell.java:456)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2017-02-13 05:05:42,699 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: Exception from container-launch.
2017-02-13 05:05:42,699 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: Container id: container_1486905599813_0046_01_000001
2017-02-13 05:05:42,699 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: Exit code: 10, this issue occurred again yesterday, to be notable, this issue occurred when dynamic allocation is as default(disabled)., In the AM log, the "failure to connect to driver" happened around 5:05. According to Hive log, the query failed around 4:11 and the remote diver was killed. Does this mean the app runs after the job has failed on Hive side?, it seems like that, Hive side timeout after hive.spark.job.monitor.timeout, after that time yarn still tried to run the query but failed., The symptom is quite similar to that of HIVE-12650. Is this a busy cluster?]