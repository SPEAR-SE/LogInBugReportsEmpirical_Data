[vector_join30.q on TestMiniLlapCliDriver also hangs
{code}
2016-12-19T16:46:52,027 DEBUG [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@676f0a60] BlockStateChange: BLOCK* neededReplications = 0 pendingReplications = 0
2016-12-19T16:46:54,567  INFO [cfb99845-b121-4700-a883-ef8f2a33b4dd main] SessionState: Map 1: 1/1	Map 4: 0(+1,-1)/1	Reducer 2: 0/1	Reducer 3: 0/1	Reducer 5: 0/1
2016-12-19T16:46:55,030 DEBUG [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@676f0a60] BlockStateChange: BLOCK* neededReplications = 0 pendingReplications = 0
2016-12-19T16:46:57,628  INFO [cfb99845-b121-4700-a883-ef8f2a33b4dd main] SessionState: Map 1: 1/1	Map 4: 0(+1,-1)/1	Reducer 2: 0/1	Reducer 3: 0/1	Reducer 5: 0/1
2016-12-19T16:46:58,035 DEBUG [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@676f0a60] BlockStateChange: BLOCK* neededReplications = 0 pendingReplications = 0
{code}, [~prasanth_j], ping? , This test does not hang for me on a Linux box on recent master. On mac, minillaplocal tests fail entirely. , [~pxiong] I see similar errors on my local machine (mac) when I run encrypted q tests. Did you figure out how to get around this problem?, I've seen this issue a few months ago...I blame my machine for it - since it happend when it had heavy load on it :D
..but today it's back again, on a machine which has plenty of resources...and without load.

I'm running these tests: {{-Dqfile=schema_evol_text_nonvec_part_all_primitive.q,schema_evol_text_vec_part_all_primitive.q,schema_evol_text_vecrow_part_all_primitive.q}}
but I'm afraid the test case itself doesn't matter.
, the same issue happened again for me...I've looked into it a bit more...and it seems like there is some issues with the nodemanagers...they report that the local dirs are bad

resourcemanager ui shows this info:
{code}
NodeHealthReport 	4/4 local-dirs are bad: /home/kirk/hw/asf-hive/itests/qtest/target/hive/hive-localDir-nm-1_0,/home/kirk/hw/asf-hive/itests/qtest/target/hive/hive-localDir-nm-1_2,/home/kirk/hw/asf-hive/itests/qtest/target/hive/hive-localDir-nm-1_1,/home/kirk/hw/asf-hive/itests/qtest/target/hive/hive-localDir-nm-1_3; 4/4 log-dirs are bad: /home/kirk/hw/asf-hive/itests/qtest/target/hive/hive-logDir-nm-1_1,/home/kirk/hw/asf-hive/itests/qtest/target/hive/hive-logDir-nm-1_0,/home/kirk/hw/asf-hive/itests/qtest/target/hive/hive-logDir-nm-1_3,/home/kirk/hw/asf-hive/itests/qtest/target/hive/hive-logDir-nm-1_2 
{code}

nodemanagers are in an unworkable state...and because of this the tez AM stucks in initializing state
the resourcemanager ui seems to be not available...and I've not found any other usefull info...

I've switched to a dfferent cli  driver which didn't get stuck..., okay...after setting yarn to debug and going thru the logs; I found the reason: nodemanagers are in-operable in case disk usage is over 90%

https://stackoverflow.com/questions/29131449/why-does-hadoop-report-unhealthy-node-local-dirs-and-log-dirs-are-bad

it seems like the llap mini driver doesn't react to "my" extra yarn settings...so I've moved away some files..from my work disk - and it now works

...someone might disable this feature by setting {{yarn.nodemanager.disk-health-checker.enable}} to {{false}}...

]