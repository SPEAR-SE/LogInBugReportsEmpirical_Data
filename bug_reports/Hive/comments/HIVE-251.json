[more information about this error based on Venky's tests.

For the case where the job succeeds even though the script fails, the following logs appear:
--
2009-02-02 17:49:26,251 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: StreamThread ErrorProcessor done
*2009-02-02 17:49:26,251 ERROR org.apache.hadoop.hive.ql.exec.ScriptOperator: Script failed with code 1*
2009-02-02 17:49:26,251 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: StreamThread OutputProcessor done
2009-02-02 17:49:26,251 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: DESERIALIZE_ERRORS:0
2009-02-02 17:49:26,252 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: SERIALIZE_ERRORS:0
2009-02-02 17:49:26,296 INFO org.apache.hadoop.mapred.TaskRunner: Task 'task_200901301729_7391_m_000000_0' done.
--

For the case where the job fails:
--
2009-02-02 17:59:03,398 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: StreamThread ErrorProcessor done
*2009-02-02 17:59:03,398 ERROR org.apache.hadoop.hive.ql.exec.ScriptOperator: Error in writing to script: Broken pipe*
2009-02-02 17:59:03,398 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: StreamThread OutputProcessor done
2009-02-02 17:59:03,399 INFO org.apache.hadoop.hive.ql.exec.MapOperator: DESERIALIZE_ERRORS:0
2009-02-02 17:59:03,399 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: SERIALIZE_ERRORS:0
2009-02-02 17:59:03,399 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: DESERIALIZE_ERRORS:0
2009-02-02 17:59:03,429 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.lang.RuntimeException: java.io.IOException: Broken pipe
--

Looks like exceptions thrown in ScriptOperator.close are being ignored., Marking this as a blocker for 0.3 as there is no suitable workaround., I believe this has to do with cases where the (streaming) script reads in all of its input from stdin and _then_ fails. Scripts that fail before reading in all their input are correctly marked as having failed. , Fixed the issue.

The problem was the Operator.close was catching and ignoring HiveExceptions.

Also in FileSinkOperator we through HiveExceptions that need to be ignored, e.g. while copying the transient _tmp files. This is thrown from copyUtils in hadoop.

Further, the input20.q test was actually failing but no failure was reported due to masking by this bug. We cannot support unix pipes yet. So as a workaround I have moved them to within a unix script.
, submitting patch., why are we ignoring the exception in FileSinkOperator?, copyUtils in hadoop throws IOException if the copying file disappears after the list status is done on the directory. We hit that with tmp_ files., +1

looks good -- All IOExceptions in FileSinkOperator are beign ignored, but that is the same as current behavior, uploading a new patch as there was a non determinism in an existing test. Please review this again., +1

looks good. 

Before committing, can you remove the extra commented line in the reduce script, and add a comment explaining what it is doing, committed. Also made the changes suggested by Namit.]