[hash() uses ObjectInspectorUtils.hashCode(), which has special case handling for Text so that the hashcode matches String.hashCode(). This special case handling should have been done for char/varchar as well.
Note that fixing this will break existing behavior of hash() for char/varchar., [~ashutoshc] would it be suitable to make this change?, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12675330/HIVE-8488.1.patch

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 6561 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_boolean
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_timestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_varchar
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key2
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_custom_key3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_cbo_correctness
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1307/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1307/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1307/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12675330
 - PreCommit-HIVE-TRUNK-Build, Test failure are because the test queries use sum(hash()), and of course hash() behavior has changed in this patch. Fix would be to simply update the diffs, but just goes to highlight that this is breaking existing hash() behavior for char/varchar., One thing to keep in mind is effect of this on join/group-by etc. e.g.,
{code}
create table t1 (a string);
create table t2 (b char(500));
create table t3 (c varchar(500));
select * from t1 join t2 on t1.a = t2.b join t3 on t2.b = t3.c;
{code}
Now, columns should match and generate rows. You may want to check with mysql and/or postgres behavior for this. If this is what we want, you may want to add testcase for this.
{code}
select a , count (a) from t1 group by a; 
alter table t1 change column a a char(500);
select a , count (a) from t1 group by a;
alter table t1 change column a a varchar(500);
select a , count (a) from t1 group by a;
{code}
All of this should return same result. You may want to add tests for these as well.

Of course, if column length is > 500, than answers should be different but that would be expected.
, Can {{CLUSTERED BY}} be saved in this process? That is intimately tied to the hash() impl., Let me re-emphasize that comment - hash code controls bucket map-joins and SMB joins today.

We need to fix this so that old inserted data which has {{CLUSTERED BY varchar_column}} can be queried against a newly inserted partition with the same schema.

Otherwise people will have to re-insert all existing data for JOINs to keep working correctly., Ok, that's not good.  Will have to find a way for this not to break existing tables clustered on char/varchar columns if we do this change.]