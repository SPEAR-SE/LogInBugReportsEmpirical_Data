[cc [~alangates], This is very similar to what [~vihangk1] is working on in HIVE-17580.  It probably makes sense to combine these two JIRAs or mark this one as a duplicate of that one., Hmm, that actually seems opposite of that JIRA if I read that right.
What I want to do is remove the need for every user of metastore to know about SERDESUSINGMETASTOREFORSCHEMA and getFieldsFromDeserializer; so that every metastore API acts like the the get_fields... (and get_schema by extension) one in that respect; rather than removing the dependency. Perhaps Deserializer can be moved into common or proxied?, Hi [~sershe] I am still looking at the AvroSerDe (and SerDes/ObjectInspectors/TypeInfos in general), so may be I don't understand the big picture correctly but here are my thoughts:

I am not sure if changing all the metastore APIs to fallback on getting the schema from Deserializer is the way forward. This will create strong dependency with the Hive source code and make the metastore separation work largely irrelevant (in the sense, you won't be able to use standalone metastore with having hive jars in the classpath). I like the idea of looking if we can move the Deserializer and friends to some common project (storage-api?) or metastore instead. But I think Alan had investigated that early on in his work and it was not trivial. SerDes, TypeInfo and ObjectInspector are all intertwined such that we cannot move one out without the others if I understand it right.

I am not sure how it makes sense from design perspective for metastore to serve something which it doesn't know in the first place and has to go an external source to fetch that information. Its not really a metastore if it doesn't store metadata isn't it. Do you know what was the motivation of using this way to get the fields information from external sources (urls)., Hmm, the problem from the clean separation perspective does exist, but the opposite problem from the practical (and also good design) perspective is that it's difficult to use Hive tables correctly from metastore (which is a major use case) if Hive and non-Hive code uses different approach to establish the schema.
I think, as was done with ORC split and vectorization, there may need to be some dependency (via storage-api would actually make sense), or some classes might need to be moved to metastore with Hive depending on them (e.g. SerDe stuff), I looked into this a bit more and followed the history of the changes to SerDes related to this. Initially, I thought of move Serializer, Deserializer and AbstractSerde classes to storage-api. This turned out to be pretty straight-forward with no backward compatibility implications since the package name still remains the same of the moved classes.

However, this may not solve the problem entirely because it still means that standalone Metastore JVM will need these jars in its classpath to instantiate and get the schema from Deserializer in the runtime. SerDe implementations are spread all over the code and I am afraid that bringing one jar will bring in the rest of the world in terms of dependencies. This is probably not an issue in embedded mode of metastore though because metastore resides in the HS2 process and will have access to all the hive jars anyways, but in case of remote standalone metastore it doesn't make sense to add all these jars in the class path in the runtime.

I also was a bit confused by this [line of code here | https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java#L980 ] in {{Table.java}} where it says that any SerDe which is a subclass of AbstractSerDe should store the fields information in metastore. While {{AbstractSerDe}} itself returns {{false}} in {{shouldStoreFieldsInMetastore}} which is contradictory.

Based on what I have looked so far there is no easy way out for this and HIVE-17580 to solve it consistently for all the use-cases without breaking backwards compatibility. I propose we make the following changes:

1.  Change {{AbstractSerDe:shouldStoreFieldsInMetastore}} to return {{true}} 
It still behaves as if its true based on what we see in Table.java above and claim that all the SerDes implementations which extend from AbstractSerDe will store schema in metastore unless explicitly overridden to return false. This should cover all the SerDes in Hive source code since HIVE-15167 moved them to subclass from AbstractSerDe instead of directly implementing interfaces.
2. We move the Serializer, Deserializer and AbstractSerDe classes to storage-api.
This enables metastore to consume them without having to create a compile time dependency on hive.
3. We claim that if there are users who implement directly from the Serializer/Deserializer interfaces and still want metastore to store schema for them should make sure that their jar can be added into the classpath of the standalone metastore and metastore will use the existing mechanism to load and deserialize from the Serde class.
4. Add the check in {{HiveMetaStoreUtils.getFieldsFromDeserializer}} to throw exception before trying to use deserializer to get the schema if the implementation of {{shouldStoreFieldsInMetastore}} returns false. I don't think metastore can ever be 100% sure if SerDes declares that fields are not supposed to be stored in metastore.

[~sershe] and [~alangates] What do you guys think about these suggestions?, [~sershe] are you suggesting that all calls to the metastore should rely on parsing the schema from the SerDe rather than looking up the column list in the metadata?  I would not be in favor that.  That's going to slow down the metastore access times and make the code much more complicated.  If you are concerned about correctness, it is better to call the SerDe during data write time and confirm that the columns written match with the columns specified in the metadata (idea credit to [~owen.omalley]).

[~vihangk1]  I propose a couple of modifications to your proposal:

Item 2, we move Serializer, Deserializer, AbstractSerDe [and I suspect TypeInfo and ObjectInspector will have to come too] to a *new* module in storage-api.  This avoids the need for ORC and any other storage format to pick it up.  I agree that serde implementations should not become part of the storage-api because they are still undergoing lots of development, and that will make the release cycle harder in Hive.  Serializer et al APIs are not changing much and thus moving them to the storage-api will have a minimal cost for Hive.

I also propose we add a new item 5:  Inside Hive, we work to move all of the SerDe implementations from exec to serde module.  We do not change what packages the classes are in, just move them into the existing serde module.  This will result in a single module that the metastore (and anyone else who wants to use Hive serdes) can use without having to pick up all of Hive.  The standalone metastore still shouldn't directly depend on this serde module (that would make a mess of our release process) but users could easily pull it in at runtime.  , Thanks [~alangates] for the response. I have some questions regarding your suggestions:

bq. [and I suspect TypeInfo and ObjectInspector will have to come too] to a new module in storage-api. This avoids the need for ORC and any other storage format to pick it up. 
I will try bringing in TypeInfo and ObjectInspector too. What are the specific advantages of doing that? Also, I didn't quite understand by "avoids the need for ORC and any other storage format to pick it up". Can you please elaborate?

bq. This will result in a single module that the metastore (and anyone else who wants to use Hive serdes) can use without having to pick up all of Hive.
This assumes that SerDes implementations do not bring along other dependencies like hive-common etc. I am not sure yet but I think it is very likely that these SerDes will have more dependencies, so it may not be just adding hive-serde.jar to the standalone-metastore classpath. I already see hive-serde depends on hive-common, hive-service-rpc and hive-shims so not sure if we will be able to create a standalone serde jar for metastore., Hmm... I was writing the below, when I realized something we might be missing. So if this is resolved, the below applies, otherwise none of the above or below suggestions work as far as I can tell.
In order to store the derived schema in metastore, wouldn't we need the serde jar to be present in the first place? To ask it for the schema. Otherwise if we allow users to specify both columns and external schema, we are outsourcing even the initial correctness, which seems wrong.
I think it's reasonable to expect that if a SerDe is used, it should be available to the user (and metastore). I don't think having extra jars is a problem... the user will anyway have to have all the jars to actually query the table with the SerDe, right?

==== The below (without jars).

My main concern is about ensuring that the schema stored in metastore is synced with the actual schema by the serde. These can get out of sync from both sides; Hive columns can be added and altered despite the serde being present that is responsible for the schema (I filed a jira somewhere to block the modification like this) - these modifications will be visible to the users (because of the metastore APIs); for most serde-s however they won't reflect on the schema that Hive will actually use, so that is confusing.
Some serdes also support schema in external files that we have no control over, and other such mechanisms could exist.
Verifying schema at use time solves the problem for Hive, however not for other users of the metastore, which is kind of the point - Hive already ignores metastore columns for these tables, going instead to the SerDe, so the mismatch is not a problem for it. 
And adding such checks in metastore would mean needing access to jars, at which point we might as well return the correct schema.
How about this... 
1) We can remove the logic that avoids storing schema in metastore entirely, and always store the schema, like before.
2) Metastore will try to get SerDe class on reads, and if available, will return the schema from SerDe, or do a check as suggested above.
3) We could add a compat flag (like the one added for MM tables that fails getTable/etc calls for them unless the client explicitly claims to support MM tables, or disables compat checks)  that will break everyone trying to access such tables when the jars are absent (so the client is required to be aware of the potential discrepancy) unless they set a config flag to disable checks (so they know they might hit some rare issues), or actually implement the equivalent of get-from-deserializer.
, bq. We can remove the logic that avoids storing schema in metastore entirely, and always store the schema, like before.
No, -1.  For the reasons I gave above.  I'm fine with working on ways at write and alter time to make sure things are in sync.  I am not ok with complicating the read path., But the point is that the things are not in sync because they can be changed without Hive being aware. The only calls that will be slowed down will be getTable/etc., that require schema, and only for tables using custom serdes. Esp. if the serde classes themselves are cached, the slowdown would be trivial. , bq. I will try bringing in TypeInfo and ObjectInspector too. What are the specific advantages of doing that? 
I think you'll forced to by the interdependencies of the interfaces.  If you are not, then fine, we don't have to move them.

bq. Also, I didn't quite understand by "avoids the need for ORC and any other storage format to pick it up". Can you please elaborate?
ORC today depends on the storage-api.  It works hard to keep down the number of its dependencies in order to minimize its jar size.  So I suspect you'll get pushback from the ORC community on adding Serializer et al to the storage-api.  By making serde interfaces a separate module in storage-api we can address this concern from ORC.

bq. This assumes that SerDes implementations do not bring along other dependencies like hive-common etc. I am not sure yet but I think it is very likely that these SerDes will have more dependencies, so it may not be just adding hive-serde.jar to the standalone-metastore classpath. I already see hive-serde depends on hive-common, hive-service-rpc and hive-shims so not sure if we will be able to create a standalone serde jar for metastore.
Fair point, though even if we could get them to only pull in common, shims, and serdes that would be a big improvement over needing the exec jar.

, I'm also -1 to the metastore using the Serdes to recreate the table schema. The Avro serde is particularly bad in this regard because it can use an external file to store the schema. Thus, the schema of the table can change without notifying the metastore. That is pretty broken. Does anyone know what the original goal of 
that capability was?

I think the long term goal should be to make "load data" should determine if the type is self-describing and invoke an interface to determine the types of the loaded data.

For managed tables, the metastore needs to know the types of the tables. The goal should be to remove the functions that allow users to update the data directly without going through Hive. The metastore needs to know the types and have relevant statistics. That is the only way the optimizer has a chance of figuring out the proper plan., [~alangates] Thanks for watching out for adding dependencies to storage-api. Adding JSON and Avro as recursive dependencies for storage-api would be really painful. Minimizing the size of storage-api also means that fewer changes cross the hive to storage-api artifacts., Let me summarize on the high level.
Metastore not creating the schema for such SerDes is the current state after HIVE-11985
However, that means that most metastore APIs return bogus fields for such tables (only get_schema/get_fields return correct fields - by calling the deserializer inside metastore).
So that means that everyone who wants to use metastore for such tables needs to know about these shennanigans (in particular the internal Hive SerDe list from HiveConf, and the fromDeserializer stuff). And also, metastore compile-time depends on SerDe interface and runtime-depends on SerDe jars.
We can resolve this either by either:
# removing the SerDe dependency, and either
## screwing everyone who wants to read Hive tables without intricate understanding of SerDe/Hive internals. I know for sure that it will break Presto, but I suspect it will actually break everyone trying to use metastore at this time :) And I'm not even sure how non-Java users can support this.
## forcing the table creation and updates to externally recreate the schema for the benefit of the readers. This is not as bad as messing with readers, cause those tables are mostly created by Hive, but still bad (if external users do create the tables) and also doesn't solve the external schema case.
# keeping the SerDe dependency
## recreating the schema. The old metastore approach before HIVE-11985 that nobody seems to like.
## changing get_table/etc. APIs to return the correct schema from SerDe (with in-memory caching for most cases, based on internal config?). 

The last one IMO is the right solution.
At compile time, the main dependency that metastore would need is "Deserializer" interface, not individual SerDes, so it's a reasonable addition to storage-api (or a new module).
At runtime, I think it's reasonable to expect the user to deploy jars with metastore if they want to use the table, since they'd likely need the same jars anyway to read from the table using the SerDe  (although it does present some inconvenience to non-Java readers). Also, if jars are not available we can output an error; and we can optionally add a compat flag for users that are aware of Hive internals and can override the jar requirement.
, bq. However, that means that most metastore APIs return bogus fields for such tables (only get_schema/get_fields return correct fields - by calling the deserializer inside metastore).

Can you please give an example? If the other APIs should really be using Deserializer till now, I am surprised that nobody is hit with this issue till now. If there is a way to reproduce this perhaps it will make it easier to understand.

bq. screwing everyone who wants to read Hive tables without intricate understanding of SerDe/Hive internals. I know for sure that it will break Presto, but I suspect it will actually break everyone trying to use metastore at this time  And I'm not even sure how non-Java users can support this.

Again, if this was such a fundamental problem I don't know why anyone has not seen this till now since only two APIs currently get the schema from serde while rest just query from the database.

bq. forcing the table creation and updates to externally recreate the schema for the benefit of the readers. This is not as bad as messing with readers, cause those tables are mostly created by Hive, but still bad (if external users do create the tables) and also doesn't solve the external schema case.

Just to clarify are you saying that anybody who is reading this table (Hive/Impala/Presto etc) will have to recreate the schema using Deserializer if the table schema is not stored in metastore? Isn't that happening right now anyways? I looked at the describe table implementation in Hive and it gets the schema from deserializer. The {{getTable}} API by default does not retrieve the storageDescriptor and columnDescriptor currently.



, The reason people never hit it before is that before HIVE-11985, which people are only starting to use, metastore would duplicate the schema from deserializer into the metastore columns (2.1 in my previous comment). So, in most cases (unless either the user or the serde messed with it), the schema returned would actually be the real schema.
I actually filed this JIRA based on a case where someone was using a Hive table from Presto, and in addition to the problems introduced by HIVE-11985 for that scenario (presto would not get the correct type; but in this case it worked anyway because it was a text-based serde so it was anyway always reading string), also managed to modify the columns manually so they were out of sync with the SerDe, bq. metastore would duplicate the schema from deserializer into the metastore columns (2.1 in my previous comment). So, in most cases (unless either the user or the serde messed with it), the schema returned would actually be the real schema.

Sounds like before HIVE-11985 all the APIs would have returned consistent schema. Why did we change that in HIVE-11985? (traced it to [this comment | https://issues.apache.org/jira/browse/HIVE-11985?focusedCommentId=14949665&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14949665] by [~xuefuz] on that JIRA

bq. "If we spend time on this, I'd rather solve the problem in the generic way, regardless the serde type and db type. The obvious inconsistency I see here is that we store for avro the schema if it's less than 2000 while storing a constant string for anything over that. If we determine that it's not necessary to store it for avro, don't store it at all. Or if we can solve the length problem for all serdes, then that's probably the the right way to go."

Based on what I understand so far (please forgive me if I am repeating the obvious) the inconsistency in the returned schema is only in the cases of tables where the schema should be derived from deserializer because in HIVE-11985 we decided not to store the such schemas in metastore. And the reason why we don't store these schemas in metastore in the first places is due to the 4000 character limit. The patch for HIVE-12274 changed the column type to CLOB. Shouldn't the original problem which is causing all this not exist anymore?, [~sershe] Do you have any further thoughts or comments? If my understanding stated above is correct, is it reasonable to revert HIVE-11985 (fully or partially) instead of adding Deserializer usage on all the HMS APIs a better approach?, Initial motivation for HIVE-11985 was the field length, which is not a problem anymore (at least some fields were converted to text). However, as per the above another reason not to store is the potential inconsistency.
There are 2 further concerns:
* [~owen.omalley] appears to have -1d the store-recreated-schema approach above; and [~alangates] also doesn't like it.
* In order to recreate the schema, you need the SerDe :) Otherwise how would you know the schema from the SerDe? Unless you are willing to rely on user who creates the table to re-create it for you and send you the correct schema (which you won't be able to validate either, without the SerDe). Which is even more brittle than just re-creating it.]