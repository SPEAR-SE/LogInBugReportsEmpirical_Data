[[~ekoifman], The insert overwrite problem happens for table insert or static partition insert, it works fine for dynamic partition insert. So make the code change similar to what is in dynamic partition. , Need code review., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12737007/HIVE-10880.1.patch

{color:red}ERROR:{color} -1 due to 20 failed/errored test(s), 8991 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_delete
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_delete_own_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_update_own_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_delete_where_no_match
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_histogram_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_where_no_match
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_delete_where_no_match
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_update_where_no_match
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_sort_1_23
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_sort_skew_1_23
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_nullsafe
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4147/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4147/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4147/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 20 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12737007 - PreCommit-HIVE-TRUNK-Build, Attach second patch to fix the test failures. , 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12737388/HIVE-10880.2.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 8993 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_autogen_colalias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_nondeterministic
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_2
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4164/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4164/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4164/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12737388 - PreCommit-HIVE-TRUNK-Build, All these failures are not related, their ages are 11 or more. , [~xuefuz], [~szehon], [~ctang.ma], [~csun], could you review the code? Thanks, The implement of method private static String replaceTaskId(String taskId, int bucketNum) looks not right. For the code is in the source for a while, I am not very confident about that. 
Attached the patch 3 fixes that issue too. If the tests pass, should use patch3, otherwise keep patch2. , 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12737564/HIVE-10880.3.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 8999 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_autogen_colalias
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_nondeterministic
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_2
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.testTempTable
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4178/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4178/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4178/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12737564 - PreCommit-HIVE-TRUNK-Build, The failures are not related. 
Following two testes failed age more than 10. 
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx_cbo_2
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.testTempTable

org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_autogen_colalias failed in build 4179(the build after this build) too.

For the spark failure, I tested locally, all pass. And my code change only affect when hive.enforce.bucketing is true, the spark test never set this value. So it is not related. 

-------------------------------------------------------
 T E S T S
-------------------------------------------------------

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 72.272 sec - in org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

Could anyone review the code? Thanks, [~ychena], thanks for working on this. Looking at the patch, I wasn't confident that I know the root cause of the problem and how your patch addresses it. From the problem description, I originally thought that it's a problem of setting the right number of reducers. However, your patch seems not approaching in that direction. Instead, it appears that your patch "adds" the missing buckets by creating empty files. I'm not sure if this fixes the root cause. In general, the rows should be relatively evenly distributed in different buckets, and so missing or empty bucket files should be rare rather than normal.

Could you please share your thoughts on this?, [~xuefuz], when I debug the issue, I noticed that right number of reducer is used. I also noticed that dynamic partition insert works fine because it adds the missing files. I think we should treat static partition and ordinary table the same way, so I fixed the issue by adding the missing buckets. Following is the code for dynamic partition part:

{noformat}
        taskIDToFile = removeTempOrDuplicateFiles(items, fs);
        // if the table is bucketed and enforce bucketing, we should check and generate all buckets
        if (dpCtx.getNumBuckets() > 0 && taskIDToFile != null) {
          // refresh the file list
          items = fs.listStatus(parts[i].getPath());
          // get the missing buckets and generate empty buckets
          String taskID1 = taskIDToFile.keySet().iterator().next();
          Path bucketPath = taskIDToFile.values().iterator().next().getPath();
          for (int j = 0; j < dpCtx.getNumBuckets(); ++j) {
            String taskID2 = replaceTaskId(taskID1, j);
            if (!taskIDToFile.containsKey(taskID2)) {
              // create empty bucket, file name should be derived from taskID2
              String path2 = replaceTaskIdFromFilename(bucketPath.toUri().getPath().toString(), j);
              result.add(path2);
            }
          }
        }

{noformat}, Adding the missing files seems file for inserting into regular tables/partitions. However, I don't quite follow why in your given example all data rows will go to one bucket while the other bucket is empty. Is this expected? I'm just wondering if there is something else going on., [~xuefuz], I agree with you, there are something more serious than the missing files. I think the bucket algorithm is broken. I just tried to insert overwrite from a very big table, all the data goes to one bucket too. Seems the hash map no longer working. I will try to figure out why. , It is hadoop issue. Hive uses hadoop-core-1.2.1.jar, it has a bug. I did a test, I rename a 2.6 version hadoop-core jar to hadoop-core-1.2.1.jar and run HIVE 1.2, the data distribute to all the buckets. 
But I can not use hadoop-core 2.6 to build hive, it will make Hive Shims Common(may be more) fail. So we need lower version to support hadoop-1 (shims 0.20s) 
[~xuefuz], you said that you read something regarding hadoop bug on localjob which does not respect number of reducers. Could you find the jira number? Thanks
For now I think my fix is a good workaround for the issue. The buckets are not balanced but they are fully functional. 
How do you think? 
, hadoop-core.jar is for Hadoop-1. For Hadoop-2, you don't need hadoop-core.jar as far as I know. You can just build your Hive using -Phadoop-2.

The above mentioned problem about the number of reducers appeared in several jiras. Unfortunately I can only find https://issues.apache.org/jira/browse/HIVE-6588. There is one recently also about bucketing but I cannot find it any more.

Anyway, I don't quite understand the problem. To make your query work with Hadoop-1, you might need to disable local execution. The fix here might be useful in case of empty bucket, but I'm not sure how users have survived w/o it., My build uses -Phadoop-2, the error is:
{noformat}
INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.693s
[INFO] Finished at: Tue Jun 09 17:00:24 EDT 2015
[INFO] Final Memory: 26M/310M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project hive-shims-common: Could not resolve dependencies for project org.apache.hive.shims:hive-shims-common:jar:1.2.0: Could not find artifact org.apache.hadoop:hadoop-core:jar:2.6.0 in datanucleus (http://www.datanucleus.org/downloads/maven2) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-shims-common

{noformat}
And in run time, it does call into functions in hadoop-core.jar. All my test in hadoop-2 env. The problem is that without my fix, hive after insert overwrite bucketed table and partition in local mode, the table can not used to do bucketmapjoin.sortedmerge because of missing files (always 1 vs. bucket number)., The patch is fixing following issue:
In local mode and when enforce.bucketing is true, for bucket table, insert overwrite to table or static partition, bucket number is not respected. 

Because only dynamic partition works fine, this fix uses the same idea as how to handle the dynamic partition scenario.

Attach patch 4 after rebase. 



, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12748527/HIVE-10880.4.patch

{color:red}ERROR:{color} -1 due to 15 failed/errored test(s), 9320 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_convert_enum_to_string
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_dynamic_rdd_cache
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_bigdata
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_having
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_insert_into2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_nullgroup2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_join5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_sample5
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_timestamp_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_timestamp_lazy
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_transform_ppr2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_11
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_19
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json
org.apache.hive.jdbc.TestSSL.testSSLConnectionWithProperty
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4813/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4813/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4813/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 15 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12748527 - PreCommit-HIVE-TRUNK-Build, Ran the spark tests on my local machine, they passed. Re-Attach the patch. , 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12748670/HIVE-10880.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 9323 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4826/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4826/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4826/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12748670 - PreCommit-HIVE-TRUNK-Build, The failure is not related(it is known issue):
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit
Error Message
Table/View 'TXNS' already exists in Schema 'APP'.
[~xuefuz], could you review the change?  Thanks

The patch is fixing following issue:
In local mode and when enforce.bucketing is true, for bucket table, insert overwrite to table or static partition, bucket number is not respected.
Because only dynamic partition works fine, this fix uses the same idea as how to handle the dynamic partition scenario.

It seems that HIVE-11360 has similar issue. 


, Okay. I will take a look shortly., +1, Committed to master/branch-1. Thanks, Yongzhi., Thanks [~xuefuz] for reviewing the code.]