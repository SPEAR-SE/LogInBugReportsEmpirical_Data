[* Modified TextRecordReader to return a line when the size has reached a threshold, even if the newline hasn't been encoutered.

Test Plan:

err.sh is a bash script that continuously writes a string to stderr. Using err.sh, the error can be reproduced by running in hive:

{code}
set mapred.child.java.opts=-Xmx8m;
add file err.sh;
SELECT TRANSFORM(*) USING 'err.sh' FROM src;
{code}

The job will fail quickly with a memory error (set mapred.child.java.opts=-Xmx8m reduces available memory so that the job fails quickly).

With the patch, the job should run continuously.
, This also means that we are limiting the row length for the data.

Can we add a conf variable into TextRecord and set the default to maybe 10MB?
In this way, nobody will notice the difference unless the row is bigger than 10MB, which is rare enough but is still smaller than typical memory size.

{code}
set hive.text.record.reader.max.length=10485760;
{code}

We can treat this one as an internal variable (by not adding it to hive-default.xml), following hadoop internal variable convention., Right, missed how ScriptOperator.outThread depends on TextRecordReader

* Changed limit to 10MB
* Added var to conf

Would it be worthwhile to allow the user to specify a different RecordReader for the error stream? Then we could use a size-limited subclass of TextRecordReader for only the error stream., Yes, it is possible to do so via the configuration variable:

hive.script.recordreader.

Infact, there is another reader which we support: TypedBytesRecordReader.

The new paramter that you are adding should not be specific to TextRecordReader, but should be general to any recordreader.

Maybe, it should be named:

set hive.record.reader.max.length=10485760;


and all recordreaders should be modified to take care of that., I don't think we can easily trim off long records in binary streams. It may not be possible sometimes, depending on the format.
Text format is special because we just need to find the next newline.

Let's get this in first. We can add that to TypedBytesRecordReader if such a need comes up later.
, Unfortunately the function readLine(Text, long, long) is not available in hadoop 0.17.

http://hadoop.apache.org/common/docs/r0.17.2/api/org/apache/hadoop/mapred/LineRecordReader.LineReader.html

We can either:
1. Add this to shims;
2. Wait after we branched off hive 0.5.  We might want to drop the support of hadoop 0.17 starting from Hive 0.6.
]