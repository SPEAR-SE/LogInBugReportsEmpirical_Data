[I cannot reproduce this, it was either fixed or was an environmental problem. If you have more information please re-open and share., Brock,
I've also been seeing this same symptom several times over the past month, but with RCFile and not ORC.
I also cannot reliably reproduce, but it's happening.
See also http://mail-archives.apache.org/mod_mbox/hive-user/201306.mbox/%3CCANsfGRkR0jy5W3eY3z8aWTWpypHGu5YeDicybVJBnwR_o_52Vg@mail.gmail.com%3E which seems like the same symptom.
Hive 0.10 in CDH4.3.1, Observed for text files also. There were as many as 8 reducers running concurrently., We ran into this issue in our cluster.
The error message is like this
{code}
org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://***:8020/tmp/hive-svcckppi/hive_2014-06-16_20-24-09_584_6615934756634587679/_task_tmp.-ext-10002/_tmp.000000_3 to: hdfs://***:8020/tmp/hive-svcckppi/hive_2014-06-16_20-24-09_584_6615934756634587679/_tmp.-ext-10002/000000_3
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commit(FileSinkOperator.java:197)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.access$300(FileSinkOperator.java:108)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:867)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:588)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:597)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:597)
        at org.apache.hadoop.hive.ql.exec.ExecReducer.close(ExecReducer.java:309)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:470)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:407)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:158)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1478)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:153)
{code}

The log of NameNode shows
{code}
2014-06-16 20:43:38,582 WARN org.apache.hadoop.hdfs.StateChange: DIR* FSDirectory.unprotectedRenameTo: failed to rename /tmp/hive-svcckppi/hive_2014-06-16_20-24-09_584_6615934756634587679/_task_tmp.-ext-10002/_tmp.000000_3 to /tmp/hive-svcckppi/hive_2014-06-16_20-24-09_584_6615934756634587679/_tmp.-ext-10002/000000_3 because destination's parent does not exist
{code}, I went through the code of FileSink operator
The code is like this.
{code}
         if ((bDynParts || isSkewedStoredAsSubDirectories)
              && !fs.exists(finalPaths[idx].getParent())) {
            fs.mkdirs(finalPaths[idx].getParent());
          }
{code}

I am wondering why we should check bDynParts and isSkewedStoredAsSubDirectories. In the code, the output is move to finalPath no matter what the values of bDynParts and isSkewedStoredAsSubDirectories are. Since the date move is not avoidable, why not change the code to the following to make sure the path exists before moving the file.
{code}
         if (!fs.exists(finalPaths[idx].getParent())) {
            fs.mkdirs(finalPaths[idx].getParent());
          }
{code}, I'm also seeing exact same issue with hive-0.11. The exception message is attached below

It looks like that the bug started happening when we started using the command in hive "create table as"

org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://ares-nn.vip.ebay.com:8020/tmp/abc/hive_2014-07-02_22-28-32_991_2129131884187129074/_task_tmp.-ext-10001/_tmp.000000_0 to: hdfs://ares-nn.vip.ebay.com:8020/tmp/abc/hive_2014-07-02_22-28-32_991_2129131884187129074/_tmp.-ext-10001/000000_0
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commit(FileSinkOperator.java:197)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.access$300(FileSinkOperator.java:108)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:867)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:588)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:597)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:597)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:194)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:365)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
org.apache.hadoop.hive.ql.metadata.HiveException: Unable to rename output from: hdfs://ares-nn.vip.ebay.com:8020/tmp/abc/hive_2014-07-02_22-28-32_991_2129131884187129074/_task_tmp.-ext-10001/_tmp.000000_0 to: hdfs://ares-nn.vip.ebay.com:8020/tmp/abc/hive_2014-07-02_22-28-32_991_2129131884187129074/_tmp.-ext-10001/000000_0
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.commit(FileSinkOperator.java:197)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths.access$300(FileSinkOperator.java:108)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:867)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:588)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:597)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:597)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:194)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:365)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.mapred.Child.main(Child.java:249), We are seeing this issue as well., The patch by [~GWong] seems to work for us., George Wong or somebody can help to go through patch available procedure., Hi [~arif.iqbal] and [~ccondit], would you mind to share the failure HQL statement with us? It is very helpful to find the root cause.

Thanks., I'm seeing this error, too, with Hive 0.13, but it's because another process deleted the {{_tmp.-ext-10001}} folder.
So it's not really a bug from my perspective. 
To find the guilty process deleting the folder, have a look at hdfs audits file to figure out who deleted the folder., 
I am seeing the same issue and maybe this sheds some light. Using Hive version 1.0.0 and below (haven't tried later versions) on EMR, this error happens consistently. 

The table uses its own storage handler but when I use "create external table" to create the table, somehow Hive thinks the table is native, which means that it runs commit on the FS paths causing the error (see stack trace above). Removing external fixed the issue. Maybe overwrite in the above case had a similar result?]