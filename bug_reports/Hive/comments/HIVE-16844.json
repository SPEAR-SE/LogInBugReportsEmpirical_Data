[

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12871856/HIVE-16844.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10820 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[materialized_view_create_rewrite] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query78] (batchId=232)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=216)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5568/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5568/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5568/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12871856 - PreCommit-HIVE-Build, +1
, Committed to master. Thanks Sunitha!, [~sbeeram]

Looks like this patch has broken the build. 
Below tests from TestReplicationScenariosAcrossInstances are failing ever since this patch has gone into master.
{quote}
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=216)
{quote}

I'll fix it for TestReplicationScenariosAcrossInstances along with my HIVE-16785 patch.
Could you please check if this change broken any other tests?

cc [~thejas], [~anishek], [~sushanth], [~sankarh] I am tracking the failures via  HIVE-16908 - the failures were in TestHcatClient. Thanks for taking care of the TestReplicationScenariosAcrossInstances test. Does that test create multiple instances of metastore? , [~sbeeram] Yes, it does create multiple metastore instances with different configurations. I tried to keep the configurations same and looks it is working now locally. Will submit the patch and see if the error is gone in pre-commit build., [~sankarh] I am running into some issue fixing the unit tests and wondering if you have some input. I tried using an approach similar to what you did to fix the failures in TestReplicationScenariosAcrossInstances : ie, use the same configuration, but a different source and destination db name. However, the serialize and deserialize methods encode/decode the db and table names. I was able to work around them somewhat for: 
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (by resetting the target dbname via HCatTable interface)
and 
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (by doing a string replace of dbName on the partition-spec string).

But for org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema, I have hit a block; I can't update the dbname through the HCatAddPartitionDesc APIs nor HCatPartition APIs. I could add methods to either of these to update the dbname (HCatTable allows that), but I am beginning to wonder if this is right approach. 

Running multiple instances of Metastore within the same JVM is probably error prone as there could be other static variables in classes that might have unintended sharing, similar to how it has been an issue with this tests that this change broke. Are we better off handling these tests via integration tests and not unit tests? The other option might be to mock out the db completely.

Let me know if you have further input on this. Thanks!, [~sbeeram]
Yes, you are right about running multiple instances of Metastore within same JVM. It is error prone. 
TestReplicationScenariosAcrossInstances worked fine by keeping the configurations same. But, it beats the purpose of having separate WarehouseInstances for primary and replica warehouses. With this fix, both primary and replica seems to share the same metastore dbs and also share common paths for replication dumps which is not great. Now, I'm trying to see if it is possible to instantiate metastore only once for the entire test suit.

Another approach would be to mock the ObjectStore class and keep only relevant methods used by our tests. We may override ObjectStore through the config hive.metastore.rawstore.impl. , Sorry to resurrect this discussion. I was pondering over the solution on HIVE-16908, and wondered whether the solution here is complete. Here's the code to {{ObjectStore::setConf()}}:

{code:java|title=ObjectStore.java}
  @Override
  @SuppressWarnings("nls")
  public void setConf(Configuration conf) {
    // Although an instance of ObjectStore is accessed by one thread, there may
    // be many threads with ObjectStore instances. So the static variables
    // pmf and prop need to be protected with locks.
    pmfPropLock.lock();
    try {
      isInitialized = false;
      hiveConf = conf;
      configureSSL(conf);
      Properties propsFromConf = getDataSourceProps(conf);
      boolean propsChanged = !propsFromConf.equals(prop);

      if (propsChanged) {
        if (pmf != null){
          clearOutPmfClassLoaderCache(pmf);
          // close the underlying connection pool to avoid leaks
          pmf.close();
        }
        pmf = null;
        prop = null;
      }
    ...
  }
{code}

Note that {{pmfPropLock}} is locked before {{pmf.close()}} is called. But this is also the only place where {{pmfPropLock}} is used. So, if another thread is in the middle of accessing {{pmf}}, it is possible that the instance is messed up for that thread.
Before this code change, resetting {{pmf}} would not affect any threads with an outstanding reference., Thanks [~mithun] for the review.

I am still getting familiar with the Hive code base, so do pardon my ignorance (and the length of this comment)! Later in this comment I have more notes on the connection leak itself, but essentially you are right - we are pulling the plug on the connections while another thread is possibly using it.
Also, on closer inspection of the code/comments I came across [this|https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java#L8344]:
{quote}
   * The NucleusContext cache gets freed up only on calling a close on it.
   * We're not closing NucleusContext since it does a bunch of other things which we don't want.
{quote}

It looks like a decision was made earlier to not close...

I am not sure yet if there is a better way to handle it, however, there are a few questions that pop up:
- When the pmf reference is cleared, it seems like the assumption is that the earlier connection pool is GC'ed - is that right? It looks like thats not occurring in our case. We are using BoneCP\(!\), but I do recollect trying HikariCP as well and the problem was reproducible. However, I did not probe further to see if Hikari offers idle connection cleanup etc. My unvalidated assumption is that we have min connections in the pool as 1 and with no idle connection reclamation, we have active references thats preventing GC...
- Another way of looking at the issue, albeit contrived, is that the connection being used by a thread can become invalid if the remote end closes it for whatever reason and the application should deal with it gracefully - and the connection pool getting closed is a degenerate instance of this. So, if we have code similar to what we have in RetryingHMSHandler, it would handle the state invalidation gracefully, right?
- Another issue with not closing the pool is that we now have parts of code using different JDO config while the other thread(s) could continue to use older config indefinitely. Thats perhaps not desirable either, right?

Early last week I had spent some time looking through the code a bit more, as up until then I wasn't sure what execution paths actually led to the leak. I believe one possibility is this:
a) Most calls to the Metastore go through RetryingHMSHandler that has the logic to retry and reload the configuration in case of errors. So, this code path sees new configuration if one is available soon after a DB error.
b) Calls going through DBTokenStore, access RawStore directly and hence not only miss out on the retrying logic, but also continue to use old configuration through a HiveConf object instantiated at startup.

It looks like in our case, with frequent calls to getDelegationToken, we end up with cases where (a) and (b) interleave, resulting in connection pool objects getting leaked.

I am just beginning to catch up on the implementation of HCatClientHMSImpl/HCatUtil and will need to look through a bit more through that code on the JDO configuration that will ultimately get used etc.

I do appreciate any comments/pointers you have regarding this. Also, let me know if you think its better to revert the change until this is sorted out.

, [~mithun] Do you have further input on this?, [~sbeeram], my apologies for the radio silence.

bq. ... let me know if you think its better to revert the change until this is sorted out.
Your argument regarding connection-leaks is (and was) a compelling one. Keeping the {{PersistenceManagementFactory}} open in lieu of addressing the concurrency issue would not be right. As such, perhaps it's best to keep this commit for the moment.

bq. ... we are pulling the plug on the connections while another thread is possibly using it.
I'll examine the code more closely to see if retries from other threads will go through. This code is nasty. :/

bq. I am just beginning to catch up on the implementation of HCatClientHMSImpl/HCatUtil and will need to look through a bit more through that code on the JDO configuration that will ultimately get used etc.
This isn't all that much to that side of the code. It functions purely as an HMSClient. I'm not sure that it has bearing on the JDO-config (unless I've missed something again).
, Hive 3.0.0 has been released so closing this jira.]