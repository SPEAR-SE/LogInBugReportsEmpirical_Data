[The problem here is that the raw data encapsulated by HCatRecord and HCatSchema are out of synch, which was one of my worries back in HCATALOG-425 : https://issues.apache.org/jira/browse/HCATALOG-425?focusedCommentId=13439652&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13439652

Basically, the raw data contained in the smallint/tinyint columns are raw shorts and bytes, and we try to read it as an Int. In the case of rcfile, the underlying raw data is also stored as an IntWritable in the cases of smallint and tinyint, but not so in the case of orc. This leads to the following kind of calls in the rcfile case, and in the orc case:

RCFILE:
{noformat}
13/05/11 02:56:10 INFO mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe with properties {transient_lastDdlTime=1368266162, serialization.null.format=\N, columns=ti,si,i,bi,f,d,b, serialization.format=1, columns.types=int,int,int,bigint,float,double,boolean}
==> org.apache.hadoop.hive.serde2.lazy.LazyInteger:-3
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector:int
==> org.apache.hadoop.hive.serde2.lazy.LazyInteger:9001
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector:int
==> org.apache.hadoop.hive.serde2.lazy.LazyInteger:86400
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector:int
==> org.apache.hadoop.hive.serde2.lazy.LazyLong:4294967297
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector:bigint
==> org.apache.hadoop.hive.serde2.lazy.LazyFloat:34.532
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector:float
==> org.apache.hadoop.hive.serde2.lazy.LazyDouble:2.184239842983489E15
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector:double
==> org.apache.hadoop.hive.serde2.lazy.LazyBoolean:true
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector:boolean
==> org.apache.hadoop.hive.serde2.lazy.LazyInteger:0
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector:int
==> org.apache.hadoop.hive.serde2.lazy.LazyInteger:0
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector:int
==> org.apache.hadoop.hive.serde2.lazy.LazyInteger:0
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector:int
==> org.apache.hadoop.hive.serde2.lazy.LazyLong:0
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyLongObjectInspector:bigint
==> org.apache.hadoop.hive.serde2.lazy.LazyFloat:0.0
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyFloatObjectInspector:float
==> org.apache.hadoop.hive.serde2.lazy.LazyDouble:0.0
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyDoubleObjectInspector:double
==> org.apache.hadoop.hive.serde2.lazy.LazyBoolean:false
==> org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector:boolean
{noformat}

ORC:
{noformat}
13/05/11 02:56:16 INFO mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.ql.io.orc.OrcSerde with properties {transient_lastDdlTime=1368266162, serialization.null.format=\N, columns=ti,si,i,bi,f,d,b, serialization.format=1, columns.types=int,int,int,bigint,float,double,boolean}
==> org.apache.hadoop.hive.serde2.io.ByteWritable:-3
==> org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector:int
13/05/11 02:56:16 WARN mapred.LocalJobRunner: job_local_0003
org.apache.pig.backend.executionengine.ExecException: ERROR 6018: Error converting read value to tuple
        at org.apache.hcatalog.pig.HCatBaseLoader.getNext(HCatBaseLoader.java:76)
        at org.apache.hcatalog.pig.HCatLoader.getNext(HCatLoader.java:53)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:194)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.ByteWritable cannot be cast to org.apache.hadoop.io.IntWritable
        at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.getPrimitiveJavaObject(WritableIntObjectInspector.java:45)
        at org.apache.hcatalog.data.HCatRecordSerDe.serializePrimitiveField(HCatRecordSerDe.java:292)
        at org.apache.hcatalog.data.HCatRecordSerDe.serializeField(HCatRecordSerDe.java:192)
        at org.apache.hcatalog.data.LazyHCatRecord.get(LazyHCatRecord.java:53)
        at org.apache.hcatalog.data.LazyHCatRecord.get(LazyHCatRecord.java:97)
        at org.apache.hcatalog.mapreduce.HCatRecordReader.nextKeyValue(HCatRecordReader.java:203)
        at org.apache.hcatalog.pig.HCatBaseLoader.getNext(HCatBaseLoader.java:63)
        ... 8 more
{noformat}

(There is also an additional bug in how they are read for promotion, assuming Byte where it's ByteWritable, etc)

, I'm attaching a patch for this, by doing the following:

a) Removing promotion logic from HCatSchema, keeping that "pure" so it reflects the table type.
b) Doing to conversion to appropriate pig types inside PigHCatUTil. This breaks Travis' original intent of having HCatRecord/HCatSchema do promotions for all M/R programs, but given that there was a bug in that conversion anyway, this breakage is not a backward-incompatible breakage.
c) If we intend to add back that support, then the correct way to do that, imo, is to add that promotion to HCatRecord's accessors, but leave HCatSchema alone.
d) I've also added a new Testcase to mimic the e2e test that failed, and so we can build on that from now on. I've also refactored more Loader/Storer tests to run against orc as well.

, (patch attached), [~traviscrawford], could you please have a look at this?, Also, a few more notes : 

a) With my patch that fixes this bug, HCatRecordSerDe still is doing the promotion, so HCatRecord does have the promoted data when reading off it, so promotion is still configurable in the current way. I intend to refactor this out in a new patch(details below)
b) Only the HCatSchema has been made "pure" in that it reflects the underlying data.

--

My eventual goal, post-bugfix, to clean this up is as follows:

a) HCatRecord and HCatSchema reflect underlying raw data and do no promotions.
b) Introduce a ConversionImpl, which defines various datatype conversion functions, which all default to returning the input, and having a config that allows a user which conversions are implemented.
c) Introduce a PromotedHCatRecord & PromotedHCatSchema that wrap HCatRecord/HCatSchema and use a ConversionImpl.
d) Implement a PigLoaderConversionImpl/PigStorerConversionImpl in hcat-pig-adapter, which implements the following: Short->Int promotion, Short->Int promotion, Boolean->Int promotion
e) Have HCatLoader/HCatStorer use the promoted versions of HCatRecord/HCatSchema which use the PigConversionImpl.
f) Remove the current HCatContext promotion parameters and make them be HCatLoader/HCatStorer parameters.
, Attaching more ambitious patch that introduces notion of a DataTransformer and a TransformedHCatRecord.

This patch still has one important bug that needs to be ironed out - I attach it mostly to show what I intended by my previous comment.

The bug is that it still doesn't do conversion of shorts/bytes inside a nested structure, and so even if all the tests currently pass with this patch, this patch is broken(an indication that we need more tests :p). Essentially, a lot more of PigHCatUtil.getJavaObj needs to move into the DataTransformer, and out of PigHCatUtil.
, (Note, the new bug with not converting shorts/bytes in a nested structure is only in patch.2 , not in the first patch, which merely fixes the original bug), An additional note - the second patch file here was primarily as a request for comments, and still has a bug as noted on my 14th May comment, and is exploratory in nature. I'll open up another jira for that to clean up code. For the moment, the bug referred to in this jira is fixed by patch 1.
, +1 for the first patch., I've created HIVE-4816 to follow up on what patch.2 did, and I've uploaded patch.2 there. I'm going to delete it here so that it doesn't cause further confusion to others visiting this jira., The patch fails checkstyle., Seems to succeed for me:

--
checkstyle:
     [echo] hcatalog
[checkstyle] Running Checkstyle 5.5 on 421 files

BUILD SUCCESSFUL
Total time: 1 minute 33 seconds
--

Could you post up what error checkstyle brings up on your end?, Hmm, passes when I run no tests (ant -Dtestcase=nosuch test) but fails when I actually run any of the tests, it appears to be choking on the test results.  But this isn't the fault of this patch, so I'll mark this as patch available again., Patch checked in.]