[I can confirm that this applies to a MapReduce job writing HCat records to an Avro-backed table using HCatalog. 
{code}
java.lang.ClassCastException: org.apache.hadoop.io.NullWritable cannot be cast to org.apache.hadoop.io.LongWritable
        at org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat$1.write(AvroContainerOutputFormat.java:84)
        at org.apache.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:257)
        at org.apache.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:53)
        at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:639)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
{code}, I am running into this issue as well. If no one is currently working on this ticket, I would like to pick it up., I don't think anyone's working on it. I'd be happy to review once you post a patch., I think the correct fix for this is that HCatalog should be calling the {{OutputFormat}}s' {{getHiveRecordWriter}} rather than {{getRecordWriter}}. Since the purpose of HCatalog is to provide read and write interfaces and the Hive Metastore's services to non-Hive clients, existing SerDes should work out of the box.

Fixing it this way will also allow other SerDes, such as Parquet, to work with HCatalog as well since the ParquetSerDe currently has the same problem., If I understand correctly, it seems that this change will end up being quite extensive. HCatalog is currently using {{org.apache.hadoop.mapred.OutputFormat}} everywhere. It seems that in order to have HCatalog properly use Hive's write facilities, we would have to change HCatalog to use HiveOutputFormat rathjer than the Hadoop OutputFormat., The test coverage for this patch will be covered by running the HCatMapReduce tests against AvroSerDe and ParquetHiveSerDe. As a result, this ticket depends on HIVE-7286., I think I am close to getting this to work. Writing to Parquet seems to now be working for everything except for external tables. I am fairly certain that the reason why Avro is still not working is due to a table property for the Avro schema being missing., Correction: Parquet is working for everything except tables with static partitioning. I am pretty sure the root cause has to do with missing table properties., After fixing a {{NullPointerException}} in {{AvroSerDe.initialize}}, HCatalog is now calling {{AvroContainerOutputFormat.getHiveRecordWriter}}. However, {{AvroContainerOutputFormat}} is complaining that: {{Neither avro.schema.literal nor avro.schema.url specified, can't determine table schema}}.

I will check whether this should have been covered by HIVE-6806, but in any case, this should not be a difficult fix., Writing via HCatalog is now working for both Avro and Parquet Serdes for everything except static partitioning. For static partitioning, there is a mismatch between the expected schema and the schema set in the table properties due the partition column not being present; I am looking into this problem right now.

I am uploading a patch for initial review and to run through pre-commit tests., RB: https://reviews.apache.org/r/24136, Some notes about this patch:

 * {{\*OutputFormatContainer}} classes now wrap a {{HiveOutputFormat}} rather than a mapred {{OutputFormat}}.
 * {{\*RecordWriterContainer}} classes now wrap a {{FileSinkOperator.RecordWriter}} rather than a mapred {{RecordWriter}}.
 * {{InternalUtil.initializeOutputSerDe}} and {{InternalUtil.initializeDeserializer}} now take the properties from the {{TableDesc}} created from the table contained in {{HCatTableInfo}} rather than creating the properties manually. As a result, {{InternalUtil.setSerDeProperties}} has been removed.
 * Fixed a {{NullPointerException}} in {{AvroSerDe.initialize}} that occurrs if {{columnCommentProperty}} is null.

Test coverage:

 * Remove disabled Serde list from {{HCatMapReduceTest}} so that all {{HCatMapReduceTest}} suites are also run against {{AvroSerDe}} and {{ParquetHiveSerDe}}

To do:

 * Fix case where static partitioning is used.
 * Clean up if necessary
 * Remove diagnostic print statements., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12658803/HIVE-4329.0.patch

{color:red}ERROR:{color} -1 due to 21 failed/errored test(s), 5868 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.testPigPopulation
org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.testHCatPartitionedTable[0]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.testHCatPartitionedTable[1]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.testHCatPartitionedTable[2]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.testHCatPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.testHCatPartitionedTable[5]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.testHCatPartitionedTable[6]
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable[0]
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable[1]
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable[2]
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable[5]
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable[6]
org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.testHCatPartitionedTable[0]
org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.testHCatPartitionedTable[1]
org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.testHCatPartitionedTable[2]
org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.testHCatPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.testHCatPartitionedTable[5]
org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.testHCatPartitionedTable[6]
org.apache.hive.hcatalog.pig.TestHCatStorer.testStoreFuncAllSimpleTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/117/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/117/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-117/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 21 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12658803, Hi,

I'm against the goal of this patch requirement altogether, and this patch effectively breaks one of the core reasons for the existence of HCatalog, to be a generic wrapper for underlying mapreduce IF/OFs, for consumers that expect mapreduce IF/OFs. I apologize for not having spotted this jira earlier, since it seems a lot of work has gone into this, and I understand that there is an impedance mismatch here between HiveOutputFormat and OutputFormat, and one we want to fix, but this fix is in the opposite direction of the desired way of solving that impedance mismatch.

One of the longer term goals, for us, has been to try to evolve Hive's usage of StorageHandlers to a point where Hive stops using HiveRecordWriter/HiveOutputFormat altogether, so that there is no notion of an "internal" and "external" OutputFormat definition, so that third party mapreduce IF/OFs can directly be integrated into Hive, instead of having to change them to HiveOutputFormat/etc.

The primary issue discussed in this problem, that of FileRecordWriterContainer writing out a NullComparable is something that's solvable, since FileRecordWritableContainer's key format is a WritableComparable, and if AvroContainerOutputFormat does not already care about the key anyway, we should be ignoring it. If it's simpler, I would also be in favour of a hack like the FileRecordWriterContainer emiting a LongWritable in that case if it detects it's wrapping an AvroContainerOutputFormat instead of rewiring HCatalog to make it based on HiveOutputFormat., Hi Sushanth,

Thank you for taking a look at this ticket.

I agree that it would be ideal to get Hive to a point where a unified StorageHandler interface can replace the current use of HiveOutputFormat and FileSinkOperator.RecordWriter (which should really be named HiveRecordWriter). However, that is a larger, more long-term undertaking whereas this ticket is to fix the fact that it is currently not possible to write using HCatalog for storage formats whose (Hive)OutputFormats that only implement getHiveRecordWriter and not getRecordWriter.

The new tests I added as part of HIVE-7286 have demonstrated that only solving the type compatibility issue mentioned earlier in this ticket is not sufficient. The type error for AvroContainerOutputFormat masks the real issue which is that AvroContainerOutputFormat's getRecordWriter (as with ParquetHiveOutputFormat's) does nothing but throws an exception, which says that "this method should not be called."

This is why my fix for this issue is taking this approach, which is based on the approach taken by core Hive. To my understanding, Hive accepts both MR OutputFormats as well as HiveOutputFormats but ends up calling getHiveRecordWriter in both cases. For the case of MR OutputFormats, Hive detects that it is not a HiveOutputFormat and wraps it using HivePassThroughOutputFormat.

My understanding is that your main concern is that this patch may be turning HCatOutputFormat into a HiveOutputFormat. However, this is not the case. This patch does not change the HCatalog interface; it changes the way that HCatOutputFormat wraps the underlying OutputFormat so that it can properly handle HiveOutputFormats, which is required to make it possible to write using HCatalog for Avro and Parquet., Hi David,

Your patch uses HiveFileFormatUtils.getOutputFormatSubstitute to determine the underlying HiveOutputFormat substitute for the underlying OutputFormat, which is the route taken by core hive to take both MR OutputFormats and HiveOutputFormats. Unfortunately, this will not work, because that simply fetches a substitute HiveOutputFormat from a map of substitutes, which contain substitutes for only IgnoreKeyTextOutputFormat and SequenceFileOutputFormat. 

Although Hive's interface seems to allow any OF, it in reality accepts only these 2 apart from those that are specifically HiveOutputFormats. Thus, your call to that function will simply return null for mapreduce OutputFormats that are not HiveOutputFormats and are not the above two formats, and effectively does break runtime backward compatibility, even if not breaking compiletime backward compatibility.

If your patch were so that it fetches an underlying HiveOutputFormat, and if it were a HiveOutputFormat, using getHiveRecordWriter, and if it were not, using getRecordWriter, that solution would not break runtime backward compatibility, and would be acceptable - I tried something on that line over at https://issues.apache.org/jira/browse/HIVE-4524 (which wasn't eventually committed, because that problem was solved from the HBaseStorageHandler end rather than solving it from HCat's end) if you'd like to look at that. I think that might be a better way of solving the base issue of HiveOutputFormats not working from within HCatalog.
, Hi Sushanth,

I really appreciate you taking your time to look at this patch and for your tips. However, I am still a bit unclear about some of the concerns you mentioned.

bq. Unfortunately, this will not work, because that simply fetches a substitute HiveOutputFormat from a map of substitutes, which contain substitutes for only IgnoreKeyTextOutputFormat and SequenceFileOutputFormat.

From my understanding, {{HivePassThroughOutputFormat}} was introduced in order to support generic OutputFormats and not just {{HiveOutputFormat}}. According to {{[HiveFileFormatUtils. getOutputFormatSubstitute|https://github.com/apache/hive/blob/b8250ac2f30539f6b23ce80a20a9e338d3d31458/ql/src/java/org/apache/hadoop/hive/ql/io/HiveFileFormatUtils.java]}}, {{HivePassThroughOutputFormat}} is returned if the {{OutputFormat}} does not exist in the map but only if it is called with {{storageHandlerFlag = true}}. From [searching the codebase|https://github.com/apache/hive/search?utf8=%E2%9C%93&q=getOutputFormatSubstitute&type=Code], the only place where {{getOutputFormatSubstitute}} could be called with {{storageHandlerFlag}} set to true is in {{Table.getOutputFormatClass}} and if the {{storage_handler}} property is set.

As a result, I changed my patch to retrieve the {{OutputFormat}} class using {{Table.getOutputFormatClass}} so that HCatalog would follow the same codepath as Hive proper for getting the {{OutputFormat}}. Does this address your concern?

bq. If your patch were so that it fetches an underlying HiveOutputFormat, and if it were a HiveOutputFormat, using getHiveRecordWriter, and if it were not, using getRecordWriter, that solution would not break runtime backward compatibility, and would be acceptable

I tried this approach, but I think that it is cleaner to change {{OutputFormatContainer}} and {{RecordWriterContainer}} to wrap the Hive implementations ({{HiveOutputFormat}} and {{FileSinkOperator.RecordWriter}}) rather than introduce yet another set of wrappers. After all, Hive already has a mechanism for supporting both Hive OFs and MR OFs by wrapping MR OFs with {{HivePassThroughOutputFormat}}, and I think that HCatalog should evolve to share more common infrastructure with Hive.

I have attached a new revision of my patch that now fixes the original reason why this ticket is opened; writing to an Avro table via HCatalog now works. There are still a few remaining issues though:

 * The way that tables with static partitioning is handled is not completely correct. I have opened HIVE-7855 to address that issue.
 * Writing to a Parquet table does not work but more investigation is needed to determine whether this is caused by a bug in HCatalog or in the Parquet SerDe., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12665137/HIVE-4329.3.patch

{color:red}ERROR:{color} -1 due to 17 failed/errored test(s), 6153 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_sortmerge_join_8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.testSaslWithHiveMetaStore
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.testPigPopulation
org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.testHCatDynamicPartitionedTableMultipleTask[4]
org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.testHCatDynamicPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.testHCatDynamicPartitionedTableMultipleTask[4]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.testHCatDynamicPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalDynamicPartitioned.testHCatExternalDynamicCustomLocation[4]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalNonPartitioned.testHCatNonPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatExternalPartitioned.testHCatPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatMutableDynamicPartitioned.testHCatDynamicPartitionedTableMultipleTask[4]
org.apache.hive.hcatalog.mapreduce.TestHCatMutableDynamicPartitioned.testHCatDynamicPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatMutableNonPartitioned.testHCatNonPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatMutablePartitioned.testHCatPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.testHCatNonPartitionedTable[4]
org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.testHCatPartitionedTable[4]
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/550/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/550/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-550/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 17 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12665137, Hi David, making a change to PassThroughOutputFormat so that that gets used does theoretically solve my concern on why the previous patch wouldn't work. At this point, I'll retract my objections, and I'll tag [~ashutoshc], [~alangates] or [~mithun] to see if any of them want to review your patch to get it in, since it is useful and does introduce some much needed functionality. (One note, we should add a simple test case, maybe by making another DummyIF/DummyOF that composes a TextIF/TextOF, and add tests to see if that works clearly with your change.)

I will personally recuse myself from this, however, because while I'm completely agreed that Hive and HCatalog should use the same code to do I/O, I do still disagree with the direction of the change. HivePassThroughOutputFormat itself was intended to be a stopgap till we could fix Hive I/O to work off a generic M/R IF/OF and get rid of HIF/HOF.

[~ashutoshc], [~alangates], [~mithun] : Could any of you please have a look at this jira and take on reviewing this?, Thank you for your feedback, Sushanth.

bq. One note, we should add a simple test case, maybe by making another DummyIF/DummyOF that composes a TextIF/TextOF, and add tests to see if that works clearly with your change.

Agreed. That should not be difficult to add given the test fixture I added with HIVE-7286. I will add that test.

bq. HivePassThroughOutputFormat itself was intended to be a stopgap till we could fix Hive I/O to work off a generic M/R IF/OF and get rid of HIF/HOF

I also agree that {{HivePassThroughOutputFormat}} seems like a stopgap. Is there a JIRA ticket opened to discuss whether HIF/HOF should be deprecated in favor of MR IF/OF?

----

Before this patch is committed, I would like to make a few more changes to the tests, including adding Sushanth's suggestion above. I would like to make use of some code I added as part of HIVE-7420 for disabling specific test methods for storage formats. Thus, HIVE-7457 and HIVE-7420 should be committed before this patch. Can someone please take a look at those patches as well?, Attaching a new patch rebased on master, incorporating the test utils from HIVE-7286 to disable specific test methods for given storage formats., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12673066/HIVE-4329.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6563 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.testPigPopulation
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1131/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1131/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1131/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12673066, Could you open a follow-on ticket for the parquet static partitioning issue and link it to HIVE-8120?, +1 for 0.14, This patch needs a small rebase. I have it done and will upload. , 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12678086/HIVE-4329.5.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6614 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.testPigPopulation
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1548/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1548/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1548/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12678086 - PreCommit-HIVE-TRUNK-Build, Despite my initial reservations on approach, I've been trying to extend and make this patch work and get it in 0.14 because the functionality it introduces is important.  Last week, I'd pinged Vikram to get it okayed for 0.14. However, as of this time, on reviewing and debugging, this patch is still incomplete. 

The test failure from org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.testPigPopulation reported above is because this does not call FileSinkOperator.checkOutputSpecs, which thus, does not wind up populating the "actualOutputFormat", and thus, PassthroughOutputFormat thinks its underlying OutputFormat is null. Also, it's not a simple matter of simply calling that function, since that function depends on the FileSinkOperator having been instantiated, and having a TableDesc in its context. That, at least, is fixable, since HCatalog does have access to a TableDesc, in which case, HCatalog will then need to do some detection to see if the underlying OF is a PassthroughOutputFormat, and if so, then will need to instantiate PassthroughOutputFormat appropriately by calling a refactored FileSinkOperator.checkOutputSpecs that does not require the Operator itself.

This currently still breaks the traditional M/R OutputFormat usage under HCatalog usecase. At this point, I think it's easier to try and fix the underlying issue of making Avro work with HCatalog than to try rushing this patch into a 0.14 timeframe.

( Having said that, PassthroughOutputFormat is itself pretty broken, since it stores the realoutputFormat as a static string in HiveFileFormatUtils, which currently breaks current usecases like calling HBase through HS2, and then attempting to use any other M/R O/F like Accumulo (since HS2 winds up being a persistent process that retains the older versions of that static variable). It doesn't break in cases of hive commandline itself, if you write to only one M/R-OF based output in one query. That is a separate bug that is not this patch's fault, but this patch makes HCatalog depend on PassthroughOutputFormat, and HCat does get used in a multiple use per process scenario which affects it. (I'll file another jira on that issue soon - I've been debugging that issue) We may rely on PassthroughOutputFormat in the short term, but we really need to move off that and support M/R OFs natively(with native MR OutputCommitter semantics) in hive )
, (Unsetting patch-available for the time being - this patch still has issues and should not be committed in its current state), Created HIVE-8687 for Avro support for HCatalog by plumbing HCat and AvroContainerOutputFormat so that solves the underlying need for this. In the meanwhile, I still think this is worth doing eventually to combine how HCatalog and Hive do IO together., Setting "Major" because with HIVE-8687 it's not critical for hive .14 anymore., (FYI, the issue with PassthroughOutputFormat that I mentioned above is addressed by HIVE-8704 . Note that there is still a further issue with PTOF in that it still will not support having more than one OutputFormat proxied for writing by PTOF in the same thread, which is a HCat usecase (using HCatMultiOutputFormat) but not something which is a usecase for hive commandline or even HS2 (since even in multiwrite cases, since a separate process in the DAG will be the one using PTOF for writing))]