[

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12842365/HIVE-15048.01.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 10783 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=92)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join_filters] (batchId=118)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join0] (batchId=118)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join37] (batchId=118)
org.apache.hive.service.server.TestHS2HttpServer.testContextRootUrlRewrite (batchId=185)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2491/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2491/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2491/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12842365 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12842444/HIVE-15048.04.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10785 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_4] (batchId=92)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2504/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2504/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2504/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12842444 - PreCommit-HIVE-Build, failures are not related.

[~alangates], could you review please, I'm not sure I understand the change here.  The previous code looks like it was trying to avoid locking the whole table by figuring out which partitions would be read and only locking those partitions.  It looks like this goes wrong when there's a subquery involved, but in general should be sound.  If I understand your changes you're just moving it to always use dynamic partitioning.  But that locks the whole table, which we don't want., That is not what it does.  The code removes the table WriteEntity for target table and replaces it with some number of partition WriteEntity objects for that table.
So conceptually it does the same thing as before.

If you look at the new .q.out, the output shows the set inputs/outputs that it ends up with (not clearly highlight but they are there), WRT dynamic partitioning, that is also not new.  Update/delete statements have always ran with dyn part regardless of what WriteEntity objects there are there.
we.setDynamicPartitionWrite(original.isDynamicPartitionWrite()); just makes the lock management logic aware of.  HIVE-15032 is tracking improving this, Sorry, I missed that the new method updateOutputs was actually a breaking up of analyzeMerge into multiple methods.  I was reading it as a whole new method.  Ok, given that:

+1, committed to master
thanks Alan for the review]