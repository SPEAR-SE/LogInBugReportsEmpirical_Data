[[~thejas]/[~cartershanklin]

Could you provide data.csv file that caused the problem. Otherwise provide example of the data., temp.pl - perl file for generating data, This may not be relevant for you, but if you can use ORC then you can enable vectorized execution, and benefit from the vectorized implementation of TRIM, which should be much faster. See org.apache.hadoop.hive.ql.exec.vector.expressions.StringTrim., Here is the system configuration 
4 core, 8 GB RAM.
file format: Text
compression : NONE

1)  select count(l) from letters where l = 'l ';
 around 100 seconds.

2)  select count(l) from letters where trim(l) = 'l';
230 seconds

3)I created GenericUDF function for trim and the result was
  select count(l) from letters where gentrim(l) = 'l';
220 seconds.


This evaluate function is taking around 1500 nano seconds processing for each record. This  nano seconds accumulates and takes 230 seconds when we use  UDF function for 500M records.

This is the code that is used in evaluate.

        if (arguments[0].get() == null) {
                return null;
        }
       
        input = (Text) converters[0].convert(arguments[0].get());
        input.set(input.toString().trim());



[~ehans]]
I haven't tried ORC file format. I will try later.]