[trunk patch, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12739303/HIVE-10940.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 9007 tests executed
*Failed tests:*
{noformat}
org.apache.hive.beeline.TestSchemaTool.testSchemaInit
org.apache.hive.beeline.TestSchemaTool.testSchemaUpgrade
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4258/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4258/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4258/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12739303 - PreCommit-HIVE-TRUNK-Build, Failures are unrelated. [~prasanth_j] can you take a look? or tell me who is familiar with this code otherwise, I will take a look. , Looks like  {{scanDesc.getSerializedFilterExpr() == null}} always?

From a performance standpoint, it would be enough to serialize the expr during planning into the Desc., Why would it be null always?, See setFilterExpr in desc., Doesn't make sense, but let me re-test the patch on trunk build instead of LLAP., That was a kryo messup, the patch looks like it works exactly as expected on trunk., With more logging, it becomes slightly clearer 

{code}
2015-06-14 19:00:40,473 INFO [TezChild] io.HiveInputFormat: push down initiated with  filterText = (l_orderkey = 1212000001) filterExpr = GenericUDFOPEqual(Column[l_orderkey], Const bigint 1212000001) serializedFilterObj = null serializedFilterExpr = AQEAamF2YS51dGlsLkFycmF5TGlz9AECAQFvcmcuYXBhY2hlLmhhZG9vcC5oaXZlLnFsLnBsYW4uRXhwck5vZGVDb2x1bW5EZXPjAQFsX29yZGVya2X5AAABbGluZWl0Ze0BAm9yZy5hcGFjaGUuaGFkb29wLmhpdmUuc2VyZGUyLnR5cGVpbmZvLlByaW1pdGl2ZVR5cGVJbmbvAQFiaWdpbvQBA29yZy5hcGFjaGUuaGFkb29wLmhpdmUucWwucGxhbi5FeHByTm9kZUNvbnN0YW50RGVz4wEBAgcJgpztgwkBBG9yZy5hcGFjaGUuaGFkb29wLmhpdmUucWwudWRmLmdlbmVyaWMuR2VuZXJpY1VERk9QRXF1YewBAAABgj0BRVFVQcwBBW9yZy5hcGFjaGUuaGFkb29wLmlvLkJvb2xlYW5Xcml0YWJs5QEAAAECAQFib29sZWHu filterObject = null
{code}, Patch mostly looks good. Although it will be good to add some debug logging after each if null checks. Also from simple reference look up we don't seem be using textual representation of the filter expression anywhere. I don't think we need to set the text representation of filter expression. If we need text representation we have methods in PlanUtils to do so.

[~ashutoshc]/[~gopalv] Any idea why we set the filter expression in text form to job conf?, text representation is preserved for backward compat (if you mean the original one we used to serialize). Will add logging, +1, committed to trunk, This patch doesn't really work for 2 reasons:

   * It serializes the same or similar objects unnecessarily multiple times during planning. 
   * It ooms in dpp cases, because the expr references a reduce sink., .02 sets the expr as a phys opt. This should avoid the overheads and only do it after dpp is done. I'm wondering if I can unset the filter altogether then (in table scan), [~gopalv]/[~sershe]/[~prasanth_j] can you take a look?, {noformat}
// lets take a look at the operator memory requirements.
{noformat}
this comment looks like it was c/p-ed.

Can you add a comment to where the new optimizer is added indicating that it should be added at the end (for people who will be adding more optimizers)?

serializedFilterObject is never set anymore. Set or remove?, Thanks [~sershe]. Addressed comments in 03. I forgot to handle fliter object. Other than that I've added the requested comment and delete the copy/paste comment., evaluateMapWork and evaluateReduceWork does the same thing. Call evaluateOperators directly instead?, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12742955/HIVE-10940.03.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 9132 tests executed
*Failed tests:*
{noformat}
TestJdbcWithMiniKdcSQLAuthBinary - did not produce a TEST-*.xml file
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4454/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4454/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4454/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12742955 - PreCommit-HIVE-TRUNK-Build, Reverting old patch from LLAP., [~hagleitn]: this fixes the leak, but reintroduces the performance issue. Added log lines and it showed for query27

{code}
2015-07-06 13:08:31,521 INFO [InputInitializer [Map 5] #0] io.HiveInputFormat: hasObj = false, hasExpr=true
2015-07-06 13:08:31,522 INFO [InputInitializer [Map 5] #0] io.HiveInputFormat: hive.io.file.readcolumn.ids=0,6
2015-07-06 13:08:31,522 INFO [InputInitializer [Map 5] #0] io.HiveInputFormat: hive.io.file.readcolumn.names=d_date_sk,d_year
{code}

so it hits the serialize codepath still

{code}
 if (!hasObj) {
   serializedFilterObj = Utilities.serializeObject(filterObject);
 }
{code}, Never mind, it's because I reverted the old patch in my branch. This patch applies over the last fix.

LGTM - +1., Committed to master.]