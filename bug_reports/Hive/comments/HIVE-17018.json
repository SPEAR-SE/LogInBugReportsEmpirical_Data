[Ping [~csun] for comments., [~kellyzly] I'm not sure if I understand what you described. Can you come up with a small example query that demonstrates the problem? thanks., [~csun]: attached an example(HIVE-17018_data_init.q, HIVE-17018.q)
HIVE-17018
{code}
set hive.auto.convert.join.noconditionaltask.size=460;
explain select * from src,t1,t2,t3 where src.key=t1.key1 and src.value=t2.key2 and t1.value1=t3.value3;
{code}

the explain 
{code}
 STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-1 depends on stages: Stage-3
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
      DagName: root_20170706020353_8bc59675-8374-4ec7-ab9c-4904cd5fcadb:2
      Vertices:
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: t3
                  Statistics: Num rows: 2 Data size: 460 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: value3 is not null (type: boolean)
                    Statistics: Num rows: 2 Data size: 460 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 _col6 (type: string)
                        1 value3 (type: string)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-3
    Spark
      DagName: root_20170706020353_8bc59675-8374-4ec7-ab9c-4904cd5fcadb:3
      Vertices:
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: t1
                  Statistics: Num rows: 2 Data size: 460 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (key1 is not null and value1 is not null) (type: boolean)
                    Statistics: Num rows: 2 Data size: 460 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 key (type: string)
                        1 key1 (type: string)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 12), Map 4 (PARTITION-LEVEL SORT, 12)
      DagName: root_20170706020353_8bc59675-8374-4ec7-ab9c-4904cd5fcadb:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (key is not null and value is not null) (type: boolean)
                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 key (type: string)
                        1 key1 (type: string)
                      outputColumnNames: _col0, _col1, _col5, _col6
                      input vertices:
                        1 Map 3
                      Statistics: Num rows: 31 Data size: 6393 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col1 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col1 (type: string)
                        Statistics: Num rows: 31 Data size: 6393 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: string), _col5 (type: string), _col6 (type: string)
            Local Work:
              Map Reduce Local Work
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: t2
                  Statistics: Num rows: 2 Data size: 460 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key2 is not null (type: boolean)
                    Statistics: Num rows: 2 Data size: 460 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key2 (type: string)
                      sort order: +
                      Map-reduce partition columns: key2 (type: string)
                      Statistics: Num rows: 2 Data size: 460 Basic stats: COMPLETE Column stats: NONE
                      value expressions: value2 (type: string)
        Reducer 2 
            Local Work:
              Map Reduce Local Work
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col1 (type: string)
                  1 key2 (type: string)
                outputColumnNames: _col0, _col1, _col5, _col6, _col10, _col11
                Statistics: Num rows: 34 Data size: 7032 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col6 (type: string)
                    1 value3 (type: string)
                  outputColumnNames: _col0, _col1, _col5, _col6, _col10, _col11, _col15, _col16
                  input vertices:
                    1 Map 5
                  Statistics: Num rows: 37 Data size: 7735 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: ((_col0 = _col5) and (_col1 = _col10) and (_col6 = _col16)) (type: boolean)
                    Statistics: Num rows: 4 Data size: 836 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: _col0 (type: string), _col1 (type: string), _col5 (type: string), _col6 (type: string), _col10 (type: string), _col11 (type: string), _col15 (type: string), _col16 (type: string)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
                      Statistics: Num rows: 4 Data size: 836 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        Statistics: Num rows: 4 Data size: 836 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

{code}
There are 4 tables src(TS\[0\] size:6393), t1(TS\[1\] size:460),t2(TS\[2\] size:460),t3(TS\[3\] size:460)
the {{hive.auto.convert.join.noconditionaltask.size}} is 460. Only t3 can be converted to map join. But in above explain you can see that both t3 and t1 are converted to map join.
let's explain more detaily:
[maxSize|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java#L188]:460
the logical plan is  
{code}
TS[0]-FIL[23]-RS[5]-JOIN[8]-RS[10]-JOIN[13]-RS[15]-JOIN[18]-FIL[22]-SEL[20]-FS[21]
TS[1]-FIL[24]-RS[7]-JOIN[8]
TS[2]-FIL[25]-RS[12]-JOIN[13]
TS[3]-FIL[26]-RS[17]-JOIN[18]
{code}

after TS\[3\] is converted to map join
->
{code}
TS[0]-FIL[23]-RS[5]-JOIN[8]-RS[10]-JOIN[13]-MAPJOIN[27]-FIL[22]-SEL[20]-FS[21]
TS[1]-FIL[24]-RS[7]-JOIN[8]
TS[2]-FIL[25]-RS[12]-JOIN[13]
TS[3]-FIL[26]-RS[17]-JOIN[18]
{code}

TS\[2\] can not converted to a map join because {{connectedMapJoinSize + sizeOf(TS\[2\]}} >maxSize, and TS\[1\] is converted to a map join
{code}
TS[0]-FIL[23]-MAPJOIN[28]-RS[10]-JOIN[13]-MAPJOIN[27]-FIL[22]-SEL[20]-FS[21]
TS[1]-FIL[24]-RS[7]-JOIN[8]
TS[2]-FIL[25]-RS[12]-JOIN[13]
TS[3]-FIL[26]-RS[17]-JOIN[18]
{code}


the reason why TS\[1\] can be converted to a map join(JOIN\[8\] can be converted to a map join):
 [SparkMapJoinOptimizer#getConnectedMapJoinSize| calculates all the mapjoins in the parent path and child path. 
But the search stops when encountering UnionOperator or ReduceOperator. For {{RS\[5\]}}, it stop searching at RS\[10\], so {{SparkMapJoinOptimizer#getConnectedMapJoinSize}}
is 0. {{connectedMapJoinSize + totalSize}} is 0+ sizeOf(TS\[1\])=460, {{connectedMapJoinSize + totalSize) < maxSize}} matches. 



, t3.txt, HIVE-17018_data_init.q, HIVE-17018.q is used to reproduce the case in the comment, [~csun]: please spend some time to check whether this is a bug or not , have provided the example in the attachment,thanks!, Thanks for the examples [~kellyzly]. Are you trying to explain that HoS is overly aggressive in turning JOINs to MAPJOINs when there're chained JOIN operators? E.g., the above {{JOIN 8}} cannot be converted.

If so, I'm thinking this may be OK since the two MAPJOINs are in different work (one in Map 1 and another in Reducer 2)., [~csun]: 
{quote}
Are you trying to explain that HoS is overly aggressive in turning JOINs to MAPJOINs when there're chained JOIN operators?
{quote}
I can not explain. From the [code|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java#L364], I guess this is what author wanted. 

But from the definition of {{hive.auto.convert.join.noconditionaltask.size}}, I think this is confusing.
{noformat}
hive.auto.convert.join.noconditionaltask.size means the sum of size for n-1 of the tables/partitions for a n-way join is smaller than it, it will be converted to a map join. 
{noformat}

The code was committed by
{noformat}
HIVE-8943: Fix memory limit check for combine nested mapjoins [Spark Branch] (Szehon via Xuefu)  git-svn-id: https://svn.apache.org/repos/asf/hive/branches/spark@1643058 13f79535-47bb-0310-9956-ffa450edef68
{noformat}
, [~kellyzly]: yes, this is to prevent a Spark task from using unbounded memory for small tables. I agree that the config {{hive.auto.convert.join.noconditionaltask.size}} is a little confusing. It has already diverged from its original purpose for MR in several ways (e.g., Spark uses in-memory data size while MR uses file size). A better way might be to have a separate config just for HoS, and maybe a limit on small table memory per executor., [~csun]:  
{quote}A better way might be to have a separate config just for HoS, and maybe a limit on small table memory per executor.{quote}
what I confused is how to do this? the original code is to calculate whether the total mapjoin size in the same stage exceed the threshold or not.  Now create a new configure about calculating the threshold of all small tables according to the spark.executor.memory? If the total size of small tables in the same stage bigger than the spark.executor.memory, then not allow these small tables into the same stage but {{hive.auto.convert.join.nonconditionaltask.size}} is for caculating the total size of mapjoin size of small tables in the query? , What I'm thinking is that the new config (say A) will be a value smaller than {{spark.executor.memory}} and will be divided among all tasks in the executor (so A / {{spark.executor.cores}}).
Another way is to specify A as the maximum hashtable memory for a single Spark task. This is the limit of the sum of the sizes for all hash tables in a single work (MapWork or ReduceWork). I think no change is needed for the code related to {{connectedMapJoinSize}}., [~csun]: 
{quote}Another way is to specify A as the maximum hashtable memory for a single Spark task. This is the limit of the sum of the sizes for all hash tables in a single work (MapWork or ReduceWork).{quote}
Currently {{hvie.auto.convert.join.nonconditionaltask.size}} is the same behaviour as above., [~kellyzly] Yes. I think we don't need to change the existing behavior. I'm just suggesting that we might need a HoS specific config to replace {{hive.auto.convert.join.nonconditionaltask.size}}, so that it is less confusing., [~csun]: 
{quote}
Yes. I think we don't need to change the existing behavior. I'm just suggesting that we might need a HoS specific config to replace hive.auto.convert.join.nonconditionaltask.size
{quote}
rename {{hive.auto.convert.join.nonconditionaltask.size}} to {{hive.auto.convert.join.within.sparktask.size}}? and the description of the configuration 
is changed from
{noformat} 
the sum of size for n-1 of the tables/partitions for a n-way join is smaller than it
{noformat}

to 
{noformat}

the sum of size for n-1 of the tables/partitions for a n-way join is smaller than it in 1 MapTask or ReduceTask
{noformat}


Can you give some suggestion?
, My inputs:

That particular variable can't be renamed to something spark specific since all engines use it
Adding a net new variable for Spark would increase confusion rather than decrease it.

It would be good to have some sort of descriptive name that applies to both Tez and MR. As pointed out there is no relation between what that variable used to do and what it does today, and the implication of changing that parameter is difficult to guess.

Maybe a new variable like hive.auto.convert.join.max.hashtable.size could be introduced. Both engines switch to that variable at some point, then usage of the old variable could be deprecated and then removed.

Just my inputs. /cc [~ashutoshc], [~cartershanklin]:  
{quote}Maybe a new variable like hive.auto.convert.join.max.hashtable.size could be introduced. {quote}
{{hive.auto.convert.join.max.hashtable.size}} is more understandable for users than {{hive.auto.convert.join.noconditionaltask.size}}. Actually, before this jira, what i understood about {{hive.auto.convert.join.noconditionaltask.size}} is the max size of all hash tables not the max size of all hash tables which can be contained in 1 Spark/Tez task., Agreed. I think in the mean time at least we should put more doc on how this affects Tez/Spark respectively. In the long term we should replace this with a new variable, perhaps (at the same time) gradually deprecating the old one.]