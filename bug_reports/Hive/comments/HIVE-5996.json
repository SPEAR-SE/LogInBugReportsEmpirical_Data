[For information, the follow is the text from SQL-92[1] standard w.r.t SUM function:
{quote}
            b) If SUM is specified and DT is exact numeric with scale
              S, then the data type of the result is exact numeric with
              implementation-defined precision and scale S.
{quote}
For DT as long, currently Hive returns long, which doesn't violate the standard. However, such implementation is problematic as demonstrated in this JIRA. Plus, for decimal, Hive sum function accommodates at least 10 billion rows of data. Letting sum(long) return long is not able to uphold that assurance. Thus, we need to change the return type to make the function more useful.

[1] http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt
, Initial patch to kick off test., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12618065/HIVE-5996.patch

{color:red}ERROR:{color} -1 due to 36 failed/errored test(s), 4761 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join18_multi_distinct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join30
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join31
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_correlationoptimizer6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_count
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_cube1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_distinct_samekey
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_sets2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_rollup1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_multiMapJoin1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_predicate_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ql_rewrite_gbtoidx
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_skewjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_nested_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_short_regress
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/597/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/597/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 36 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12618065, -1

We should not be changing the output data types of expression results that are arguably reasonable. It causes code churn and can break existing apps. 

Having sum(bigint) return bigint is long standing behavior in Hive and is reasonable.

As a side note, SQL Server returns bigint for sum(bigint).

If users need more digits, they can cast the input to sum to a decimal., [~xuefuz] Can you please mark any jiras that make/propose such non backward compatible changes with the 'Incompatible change' flag ?
That would ensure that the community reviews such changes more carefully.
, Okay. Will do., {quote}
Having sum(bigint) return bigint is long standing behavior in Hive and is reasonable.

As a side note, SQL Server returns bigint for sum(bigint).

If users need more digits, they can cast the input to sum to a decimal.
{quote}

My concern is not about the number of digits that long can hold. Hive processes large number of rows that traditional DBs are shy of, and the chance of getting overflow error is bigger. With the proposed change, Hive can guarantee 10b (or certain number of) rows without worry about this problem. Without it, Hive has such guaranty, and two valid rows can overflow, as demonstrated in this JIRA.
, I am curious what the datatype for "sum(l)" is in mysql, where l is a bigint. Is it also using decimal ?
, {code}
mysql> create table test74 as select sum(l) from test;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    44
Current database: metastore

Query OK, 1 row affected (0.17 sec)
Records: 1  Duplicates: 0  Warnings: 0

mysql> desc test74;
+--------+---------------+------+-----+---------+-------+
| Field  | Type          | Null | Key | Default | Extra |
+--------+---------------+------+-----+---------+-------+
| sum(l) | decimal(41,0) | YES  |     | NULL    |       |
+--------+---------------+------+-----+---------+-------+
1 row in set (0.00 sec)
{code}
It seems that MySQL is using the 22 + p as the output precision, where p is the precision of the input. This is true for other types other than long also.
{code}
mysql> desc test1;
+-------+---------+------+-----+---------+-------+
| Field | Type    | Null | Key | Default | Extra |
+-------+---------+------+-----+---------+-------+
| i     | int(11) | YES  |     | NULL    |       |
+-------+---------+------+-----+---------+-------+
mysql> create table test75 as select sum(i) from test1;
Query OK, 1 row affected (0.19 sec)
Records: 1  Duplicates: 0  Warnings: 0

mysql> desc test75;
+--------+---------------+------+-----+---------+-------+
| Field  | Type          | Null | Key | Default | Extra |
+--------+---------------+------+-----+---------+-------+
| sum(i) | decimal(32,0) | YES  |     | NULL    |       |
+--------+---------------+------+-----+---------+-------+
{code}, Xuefu,

I'm all for new, useful functionality and better performance for Hive. And I'm all for getting correct results. I appreciate your contributions and your passion.

But I strongly believe changing behavior from one reasonable alternative to another in a way that breaks backward compatibility is not the way to go. I have a lot of experience with evolving a database (SQL Server) over a decade, and have talked to a many people who've been evolving the product longer than that. From this experience, I can say that changing backward compatibility (for either functionality or performance, but especially functionality) even in subtle ways can anger customers/users. 

Any changes to semantics like this should first of all be avoided, and if they can't be avoided, they need to be rolled out carefully, with a switch to enable backward compatibility. SQL Server has compatibility levels and "SET options" as switches, and a defined deprecation schedule. This is kind of process-heavy in the engineering effort, and also causes explosion of the test matrix. So I am not recommending necessarily that Hive go there, though maybe we need to have that discussion. I think we're better off being strict about not breaking backward compatibility unless really needed.

So, I ask that you please close this JIRA without making a patch.

There are a couple of other areas where there is an issue of ANSI SQL compatibility (result type of int/int and avg(int)). We could have a further discussion on those, though you know my preference would be to leave the semantics as-is on those since I think backward compatibility trumps ANSI SQL compatibility for those. If there is no issue of ANSI compatibility, and the current Hive behavior is reasonable, I'd like us to leave things as they are. I don't think there is a need to be across-the-board compatible with another system (MySQL or anything else).

Best regards,
Eric

P.S. Your specific argument that you can overflow a bigint sum, while technically accurate, I think is not a significant user issue. I've never heard a complaint about it with SQL Server, or PDW, our scale-out data warehouse appliance. Really big numbers, like the national debt in pennies, fit in a bigint, just to put it in perspective. Users can cast the input to decimal or double if they need more magnitude., Xuefu,

I see you want to make changes to have Hive be more in line with MySQL semantics. Can you explain why you're making these changes or considering them?

Thanks,
Eric, [~ehans] Thanks for sharing your thoughts and your inquiry. For your information, I'm not trying to make MySQL as the model. My first line of consideration is SQL standard. For a functionality if there is no SQL standard, Hive doesn't have to invent everything, thus, I do reference MySQL for ideas, mostly because MySQL and its technical documentation are readily available. However, this doesn't precludeme following other DB's practice. For instance, precision/scale determination for arithmetic operations in hive is following SQL server's formula. I'm not either anti- or pro- MySQL. Nor am I to SQL server, but I strongly believe that following well-established practices benefits Hive than doing something in a unique, unfortunate way. An example would be int/int in Hive.

However, a  lot of existing functionality in Hive was put into place when Hive is positioned as a tool rather than DB, and before all necessary data types were introduced. Take int/int again as an example, early developer probably didn't even think about SQL-compliance, and even if he/she did, there wasn't decimal data type as a consideration. As Hive is shift to a DB on bigdata positioning, I believe that we should start thinking in a perspective other than performance or backward compatibility. If we restrict ourselves based on  unconscious decisions made in the past, we may lose a lot of opportunities of doing the right things.

As I worked on decimal precision/scale support, I found a lot of problems in Hive about data types and their conversions and promotions. In many cases, Hive is not consistent itself. Let me ask you a question to see if you know the answer: what's the return type of 35 + '3.14', where 35 is from int column and '3.14' from a string column? Before I made the changes, you probably would say: wait, let me read the code first. And your answer might be different if my question were 35/'3.14'. Now, to answer the same questions, I can give right way, and I have a theory to tell why. In summary, it's a lot of effort to clean up the mess and inconsistency in Hive from the beginning of my work on decimal.

Now if we use either performance or backward compatibility to shut down what we have achieved, I don't see how Hive is shifting from a tool to a DB, and how Hive can become adopted as enterprise grade product.

Hive is still evolving, and that's why I think we have certain luxury of breaking backward compatibility for doing the right thing. As Ashutosh once mentioned, we don't want to be backward compatible to a bug. Once Hive is stabilized, it becomes much harder to make backward incompatible changes, as you know with your experience with SQL server.

I understand your concern about backward compatibility, especially your possible frustration over vectorization breaking or redoing. On the other hand, I think we are here to help hive become more useful. A blunt rejection without much consideration and communication doesn't seem as helpful and constructive as it should be., bq. Now, to answer the same questions, I can give right way, and I have a theory to tell why.
It would be great if you can document the theory, otherwise I still would need to look at code to understand the theory !  :)

I really appreciate the code cleanup you have been doing. But we have to be careful about backward compatibility. I also agree that we should not burden new users with historic problems.
Regarding "Once Hive is stabilized", how do we define that ? Maybe, once we create a list of non backward compatible changes that are important to make, we can make a major release version (1.x) , we can break the backward compatibility of certain things, and document it very well. Hopefully, that list of non-backward compatible changes can be kept small.

I discuss this in context of config defaults in HIVE-5875 .

, Regarding the specific change in this jira, I am not convinced that is an important  non-backward compatible change to make. You can have an overflow even with decimal type, if they are large enough, with just two rows.
On the other hand, the int division returning double is arguably a change to consider for a 1.0 candidate, as that is a SQL compliance issue.
, {quote}
It would be great if you can document the theory, otherwise I still would need to look at code to understand the theory
{quote}

I will put it somewhere on the wiki.

{quote}
You can have an overflow even with decimal type, if they are large enough, with just two rows.
{quote}

It's impossible to overflow output decimal type with just two rows because the precision of the output decimal type is 10 + the precision of the input type. In case of long input, the output decimal type is (29,0).
, Sorry, I meant even if the input row type is decimal , two rows can overflow, Yes, in theory, but unlikely. Current precision formula, 10 + p as the output precision, gives some room. Of course, if the input precision is already at  maximum, then output type precision will the same as the input precision. In that case, for really big numbers, it can overflow. Anyway, I'm going to close the JIRA, so the discussion., Thanks Xuefu for closing this one. You make some good points above about Hive's evolution & ANSI compliance. I think we still have to come to a conclusion on the other issues regarding int/int and avg(int) output type. We can take that up on the other JIRA(s). I'll cross-link.]