[Also, can you make sure that the old location gets deleted ?
(it can be different from the new location).

This can lead to a problem if another table (external) pointed to the old location,
but I guess we can live with it., Yes. We should delete the old data. Even today, if two tables/partitions share the same location, one delete will delete the data. 

Will upload a patch soon. The main problem here is to add testcases. (I can do tests on internal clusters.), Thats fine - I agree it will be difficult to add a unit test for this, 1. I dont think we should add a new varaible "hive.exec.partition.usetable.dfs" -
we should always use table dfs.
2. I think there is a bug when the table's location does not contain the fs (in SemanticAnalyzer).
		
      Path tabPath = dest_tab.getPath();	
      Path partPath = dest_part.getPartitionPath(); 
      if (conf.getBoolVar(HiveConf.ConfVars.HIVEPARTITIONUSETBLDFS)) {
        // if the table is in a different dfs than the partition,
        // replace the partition's dfs with the table's dfs.
        dest_path = new Path(tabPath.toUri().getScheme(), tabPath.toUri()
            .getAuthority(), partPath.toUri().getPath());
      } else {

We should pick the default fs then.

3. Path newPartPath = new Path(loadPath.toUri().getScheme(), loadPath
          .toUri().getAuthority(), oldPartPath.toUri().getPath());

same as above in Hive.java

4.           if (partPath != null	
              && !partPath.equalsIgnoreCase(tpart.getSd().getLocation())) {
            tpart.getSd().setLocation(partPath);
          }


Cant we always set the location ?

, +1

Will commit if the tests pass, All the tests are failing, made the change and running tests on my local., Committed. Thanks Yongqiang]