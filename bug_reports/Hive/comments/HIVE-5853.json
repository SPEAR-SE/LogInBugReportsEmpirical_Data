[It looks like you are using Hive Server?  Can you try Hive Server 2?, Hi,

We've never used HiveServer2. Is it fully compatible interface wise? If yes, i'd be happy to test with it.
, HS2 is strongly recommended over HS1 and in fact HS1 is considered deprecated.  I'd suggest you test with HS2. From a SQL perspective it should be compatible with HS1., Yep, the issue persists with HiveServer2 as well. I am connecting through c++ thrift interface. Connections continue to grow until the max connection limit is hit. As Harel Ben Attia mentioned, it works fine with concurrency support turned off.

Looks like Hive-4132 and Hive-4719 directly relate to this issue and should help the cause once they get fixed., HIVE-4132 could be related if you have many concurrent clients or aren't properly closing your session. I have tested HS2 and zk quite a bit via the jdbc driver and it does close connections to zk. IIRC 4132 is only and issue with >= 50 concurrent clients. , Didn't have the opportunity to try HS2 yet, but in our case the connection leak was happening even without any client concurrency. Just running a python script multiple times, each performing one simple query through hive-thrift. At first I thought it might be related to improper connection closing in the script itself, but that wasn't the case, as our production system uses the Java client, and the same result happens there as well.

I'd be glad to try and help - just tell me what you need.

, Yes I believe that HS1 will leak connections but is deprecated.

I believe you should switch to HS2. It will just involve deploying the HS2 server and switching you python/java clients over to the new thrift API.  Though, if you are using Java I'd just use JDBC to contact HS2., 
Thanks for the prompt answers.

I understand. I will try to understand the client-side implications of this kind of change, since we have a consumer-web production system running more than a 100 services, and around 30 instances of HS1. Also, we're using a lot of INSERT OVERWRITE LOCAL DIRECTORY calls, which cause the HS to actually write data to the local hard disk, and I'd need to check if there is any change of behavior there.
, Alright. This is fixed in HS2. I was able to verify it for myself. A better/appropriate clean up of resources e.g. session, connection etc gets around it., I'm also experiencing this in a HS2 environment - there appears to be a leak and number of open connections gradually creeps to the point that the limit is reached.]