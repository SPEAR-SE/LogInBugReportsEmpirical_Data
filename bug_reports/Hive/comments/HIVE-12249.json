[SHould there be spaces when concatenating the new thread id? Can you post RB?
Can you also show an example of how logs change? I skimmed the patch and didn't quite understand - does user have to set an ID in config to track logs?, Update with a test.

Sample output
{code}
vikram$ grep -r mrrTest * | grep syslog
target/hive/hive-logDir-nm-0_3/application_1445638125157_0001/container_1445638125157_0001_01_000001/syslog:2015-10-23T15:10:22,104 INFO [IPC Server handler 0 on 56693] app.DAGAppMaster: Running DAG: vikram_20151023151021_35b01963-5cf6-4dcc-b6ef-5279fb08e2a9:32, callerContext=context=HIVE, callerType=HIVE_QUERY_ID, callerId=mrrTestmainvikram_20151023151021_35b01963-5cf6-4dcc-b6ef-5279fb08e2a9, blob=
target/hive/hive-logDir-nm-0_3/application_1445638125157_0001/container_1445638125157_0001_01_000001/syslog:2015-10-23T15:10:28,274 INFO [IPC Server handler 0 on 56693] app.DAGAppMaster: Running DAG: vikram_20151023151028_024fa54e-2d53-4c20-8d82-f282cd438955:34, callerContext=context=HIVE, callerType=HIVE_QUERY_ID, callerId=mrrTestmainvikram_20151023151028_024fa54e-2d53-4c20-8d82-f282cd438955, blob=
target/hive/hive-logDir-nm-0_3/application_1445638125157_0001/container_1445638125157_0001_01_000001/syslog:2015-10-23T15:10:30,430 INFO [IPC Server handler 0 on 56693] app.DAGAppMaster: Running DAG: vikram_20151023151030_2c0cd0e9-3174-42f3-8258-205b480bde3f:36, callerContext=context=HIVE, callerType=HIVE_QUERY_ID, callerId=mrrTestmainvikram_20151023151030_2c0cd0e9-3174-42f3-8258-205b480bde3f, blob=
target/hive/hive-logDir-nm-0_3/application_1445638125157_0001/container_1445638125157_0001_01_000001/syslog:2015-10-23T15:10:32,802 INFO [IPC Server handler 0 on 56693] app.DAGAppMaster: Running DAG: vikram_20151023151032_2c6065c2-d54b-47f1-921f-f44e678bb942:38, callerContext=context=HIVE, callerType=HIVE_QUERY_ID, callerId=mrrTestmainvikram_20151023151032_2c6065c2-d54b-47f1-921f-f44e678bb942, blob=
vikram$ grep -r Test2 * | grep syslog
target/hive/hive-logDir-nm-0_3/application_1445638125157_0001/container_1445638125157_0001_01_000001/syslog:2015-10-23T15:10:34,545 INFO [IPC Server handler 0 on 56693] app.DAGAppMaster: Running DAG: vikram_20151023151034_0db3180c-4ccc-4076-81ba-217da09a4eb9:40, callerContext=context=HIVE, callerType=HIVE_QUERY_ID, callerId=Test2mainvikram_20151023151034_0db3180c-4ccc-4076-81ba-217da09a4eb9, blob=
{code}

In the client logs:
{code}
2015-10-23T15:10:34,278 INFO  [Test2main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(117)) - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
2015-10-23T15:10:34,278 DEBUG [Test2main]: hbase.HBaseReadWrite (HBaseReadWrite.java:flushCatalogCache(1909)) - Dumping metric: table cache hits 5
2015-10-23T15:10:34,278 DEBUG [Test2main]: hbase.HBaseReadWrite (HBaseReadWrite.java:flushCatalogCache(1909)) - Dumping metric: table cache misses 1
2015-10-23T15:10:34,278 DEBUG [Test2main]: hbase.HBaseReadWrite (HBaseReadWrite.java:flushCatalogCache(1909)) - Dumping metric: table cache overflows 0
2015-10-23T15:10:34,278 DEBUG [Test2main]: hbase.HBaseReadWrite (HBaseReadWrite.java:flushCatalogCache(1909)) - Dumping metric: partition cache hits 0
2015-10-23T15:10:34,278 DEBUG [Test2main]: hbase.HBaseReadWrite (HBaseReadWrite.java:flushCatalogCache(1909)) - Dumping metric: partition cache misses 0
2015-10-23T15:10:34,278 DEBUG [Test2main]: hbase.HBaseReadWrite (HBaseReadWrite.java:flushCatalogCache(1909)) - Dumping metric: partition cache overflows 0
2015-10-23T15:10:34,279 DEBUG [Test2main]: hbase.HBaseReadWrite (HBaseReadWrite.java:flushCatalogCache(1909)) - Dumping metric: storage descriptor cache hits 0
{code}, [~sershe] I think the addition of a space may be a good idea.

Yes. The user needs to set an id for tracking. I can maybe include the session id as a default in the absence of an id. Any other defaults that you can think of?, What is the usage pattern? You specify set ...id = blah on CLI and you get it in query logs?
I assume it is only helpful for HS2 use case where Tez AMs can be reused? Otherwise all the logs are your logs anyway.

Also is it possible to post RB?, Yes. The expectation is that a client such as Oozie would also be logging with the same id and when hive uses the same as well, we can correlate all the logs.

This is also helpful for the oozie use case where there may be multiple workflows for which a user can configure different ids. , Address Sergey's comments., It appears not all comments are addressed. My main question is whether this logic should run at all (e.g. set context) if the id is not set. The rest are nits about adding spaces and stuff, I think we should set a context regardless of whether the user configures it or not. If the user has setup one we use it else we use the session id., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12768682/HIVE-12249.5.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5795/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5795/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5795/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-shims ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-shims ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/shims/aggregator/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-shims ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/shims/aggregator/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/shims/aggregator/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/shims/aggregator/target/tmp/conf
     [copy] Copying 14 files to /data/hive-ptest/working/apache-github-source-source/shims/aggregator/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-shims ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-shims ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-shims ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/shims/aggregator/target/hive-shims-2.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-shims ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-shims ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/shims/aggregator/target/hive-shims-2.0.0-SNAPSHOT.jar to /home/hiveptest/.m2/repository/org/apache/hive/hive-shims/2.0.0-SNAPSHOT/hive-shims-2.0.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/shims/aggregator/pom.xml to /home/hiveptest/.m2/repository/org/apache/hive/hive-shims/2.0.0-SNAPSHOT/hive-shims-2.0.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Storage API 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-storage-api ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/storage-api/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/storage-api (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-storage-api ---
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-storage-api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-storage-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/storage-api/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-storage-api ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-storage-api ---
[INFO] Compiling 25 source files to /data/hive-ptest/working/apache-github-source-source/storage-api/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-storage-api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/storage-api/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-storage-api ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/storage-api/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/storage-api/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/storage-api/target/tmp/conf
     [copy] Copying 14 files to /data/hive-ptest/working/apache-github-source-source/storage-api/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-storage-api ---
[INFO] Compiling 4 source files to /data/hive-ptest/working/apache-github-source-source/storage-api/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-storage-api ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-storage-api ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/storage-api/target/hive-storage-api-2.0.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-storage-api ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-storage-api ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/storage-api/target/hive-storage-api-2.0.0-SNAPSHOT.jar to /home/hiveptest/.m2/repository/org/apache/hive/hive-storage-api/2.0.0-SNAPSHOT/hive-storage-api-2.0.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/storage-api/pom.xml to /home/hiveptest/.m2/repository/org/apache/hive/hive-storage-api/2.0.0-SNAPSHOT/hive-storage-api-2.0.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Common 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-common ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/common/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/common (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-common ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (generate-version-annotation) @ hive-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-source) @ hive-common ---
[INFO] Source directory: /data/hive-ptest/working/apache-github-source-source/common/src/gen added.
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-common ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-common ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-common ---
[INFO] Compiling 81 source files to /data/hive-ptest/working/apache-github-source-source/common/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-github-source-source/common/src/java/org/apache/hadoop/hive/common/JvmPauseMonitor.java: /data/hive-ptest/working/apache-github-source-source/common/src/java/org/apache/hadoop/hive/common/JvmPauseMonitor.java uses or overrides a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/common/src/java/org/apache/hadoop/hive/common/JvmPauseMonitor.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-github-source-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-github-source-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-github-source-source/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java:[2386,109] non-static variable LOG_PREFIX_LENGTH cannot be referenced from a static context
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [13.243s]
[INFO] Hive Shims Common ................................. SUCCESS [14.363s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [3.682s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [11.965s]
[INFO] Hive Shims Scheduler .............................. SUCCESS [2.375s]
[INFO] Hive Shims ........................................ SUCCESS [3.604s]
[INFO] Hive Storage API .................................. SUCCESS [4.493s]
[INFO] Hive Common ....................................... FAILURE [19.913s]
[INFO] Hive Serde ........................................ SKIPPED
[INFO] Hive Metastore .................................... SKIPPED
[INFO] Hive Ant Utilities ................................ SKIPPED
[INFO] Hive Llap Client .................................. SKIPPED
[INFO] Spark Remote Client ............................... SKIPPED
[INFO] Hive Query Language ............................... SKIPPED
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive Accumulo Handler ............................. SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HPL/SQL ...................................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:16.983s
[INFO] Finished at: Mon Oct 26 11:39:39 EDT 2015
[INFO] Final Memory: 50M/158M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-common: Compilation failure
[ERROR] /data/hive-ptest/working/apache-github-source-source/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java:[2386,109] non-static variable LOG_PREFIX_LENGTH cannot be referenced from a static context
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-common
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12768682 - PreCommit-HIVE-TRUNK-Build, fix compile issue in latest patch (make LENGTH static)., +1 based on latest RB, pending tests, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12768840/HIVE-12249.7.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 69 failed/errored test(s), 9630 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_alter_merge_2_orc
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_alter_merge_orc
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_alter_merge_stats_orc
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_create_merge_compressed
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_explainuser_1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge_incompat1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_orc_merge_incompat2
org.apache.hadoop.hive.hwi.TestHWISessionManager.testHiveDriver
org.apache.hadoop.hive.ql.io.orc.TestJsonFileDump.testJsonDump
org.apache.hive.beeline.TestBeeLineWithArgs.org.apache.hive.beeline.TestBeeLineWithArgs
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark
org.apache.hive.jdbc.TestJdbcWithMiniHS2.org.apache.hive.jdbc.TestJdbcWithMiniHS2
org.apache.hive.jdbc.TestJdbcWithMiniMr.org.apache.hive.jdbc.TestJdbcWithMiniMr
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark
org.apache.hive.jdbc.TestNoSaslAuth.org.apache.hive.jdbc.TestNoSaslAuth
org.apache.hive.jdbc.TestSSL.testConnectionMismatch
org.apache.hive.jdbc.TestSSL.testInvalidConfig
org.apache.hive.jdbc.TestSSL.testSSLConnectionWithProperty
org.apache.hive.jdbc.TestSSL.testSSLConnectionWithURL
org.apache.hive.jdbc.TestSSL.testSSLFetch
org.apache.hive.jdbc.TestSSL.testSSLFetchHttp
org.apache.hive.jdbc.TestSSL.testSSLVersion
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerPrimaryQueueMapping
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerQueueMapping
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerSecondaryQueueMapping
org.apache.hive.jdbc.TestSchedulerQueue.testQueueMappingCheckDisabled
org.apache.hive.jdbc.authorization.TestHS2AuthzContext.org.apache.hive.jdbc.authorization.TestHS2AuthzContext
org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext.org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext
org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthUDFBlacklist.testBlackListedUdfUsage
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization
org.apache.hive.jdbc.miniHS2.TestHiveServer2.org.apache.hive.jdbc.miniHS2.TestHiveServer2
org.apache.hive.jdbc.miniHS2.TestHiveServer2SessionTimeout.testConnection
org.apache.hive.jdbc.miniHS2.TestMiniHS2.testConfInSession
org.apache.hive.minikdc.TestHs2HooksWithMiniKdc.org.apache.hive.minikdc.TestHs2HooksWithMiniKdc
org.apache.hive.minikdc.TestJdbcWithMiniKdc.org.apache.hive.minikdc.TestJdbcWithMiniKdc
org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie
org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthBinary.org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthBinary
org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthHttp.org.apache.hive.minikdc.TestJdbcWithMiniKdcSQLAuthHttp
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testConfOverlay
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatement
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementAsync
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testExecuteStatementParallel
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testGetFunctions
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testGetInfo
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testOpenSession
org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithMr.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithMr
org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez
org.apache.hive.service.cli.operation.TestOperationLoggingLayout.org.apache.hive.service.cli.operation.TestOperationLoggingLayout
org.apache.hive.service.cli.session.TestSessionGlobalInitFile.testSessionGlobalInitDir
org.apache.hive.service.cli.session.TestSessionGlobalInitFile.testSessionGlobalInitFile
org.apache.hive.service.cli.session.TestSessionGlobalInitFile.testSessionGlobalInitFileAndConfOverlay
org.apache.hive.service.cli.session.TestSessionGlobalInitFile.testSessionGlobalInitFileWithUser
org.apache.hive.service.cli.session.TestSessionHooks.testProxyUser
org.apache.hive.service.cli.session.TestSessionHooks.testSessionHook
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatement
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testExecuteStatementAsync
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testGetFunctions
org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.testOpenSession
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatement
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testExecuteStatementAsync
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testGetFunctions
org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.testOpenSession
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5804/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5804/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5804/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 69 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12768840 - PreCommit-HIVE-TRUNK-Build, Fix for unit test failures., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12768949/HIVE-12249.10.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 9711 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_bitmap_auto_partitioned
org.apache.hadoop.hive.hwi.TestHWISessionManager.testHiveDriver
org.apache.hadoop.hive.ql.io.orc.TestJsonFileDump.testJsonDump
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5821/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5821/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5821/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12768949 - PreCommit-HIVE-TRUNK-Build, Test failures unrelated., Doc note:  This adds configuration parameter *hive.log.trace.id* to HiveConf.java, so it needs to be documented in the wiki for release 2.0.0.

Should it go in the Tez section of Configuration Properties, or in the general section?  If it belongs with Tez, it could go either at the end of the section or after *hive.tez.log.level*.

* [Configuration Properties -- Query and DDL Execution | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-QueryandDDLExecution]
* [Configuration Properties -- Tez | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Tez]
**  [hive.tez.log.level | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.tez.log.level]

, [~leftylev] This configuration is a general one. It can be used by any upstream project such as Oozie or templeton to configure hive to set the logging. The config helps us track the entire flow - from oozie to hive to hdfs/yarn/tez. This combined with HIVE-12254 makes it generic., Thanks Vikram!]