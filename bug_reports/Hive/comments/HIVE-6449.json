[Hi Resmus

One reason for this to happen is that parquet SerDe does not implement SerDeStats interface or parquet record writers does not implement StatsProvidingRecordWriter interface. The implementation of these interfaces are required for gathering raw data size. Statistics in explain will try to use the raw data size from the metastore. Raw data size should not be dependent on the operating system since its equivalent deserialized row size * number of rows. So I believe that parquet does not implement these interface and hence do not provide raw data size, in which case, file size is shown as the "Data size:". If the file size return by metastore or returned by filesystem.getContentSummary() api call is different then the statistics reported will be different. My suspicion is that the file sizes for the table are different for Windows vs Linux. Can you verify if the file size in windows is same as the file size in linux?, [~prasanth_j] thanks for the guidance. Since the difference reproes on ORC files, I focused on them now to eliminate any Parquet related problem. For my test ORC file, created as 
{code}
CREATE TABLE decimal_mapjoin STORED AS ORC AS 
  SELECT cdouble, CAST (((cdouble*22.1)/37) AS DECIMAL(20,10)) AS cdecimal1, 
  CAST (((cdouble*9.3)/13) AS DECIMAL(23,14)) AS cdecimal2,
  cint
  FROM alltypesorc;
{code}
I get the following stats in describe extended:
{code}
describe extended decimal_mapjoin;
...
Windows: {numFiles=1, COLUMN_STATS_ACCURATE=true, transient_lastDdlTime=1392727196, numRows=0, totalSize=126087, rawDataSize=0}
Linux:       {numFiles=1, transient_lastDdlTime=1392722507, COLUMN_STATS_ACCURATE=true, totalSize=126087, numRows=12288, rawDataSize=2165060} ...
{code}
So the problem is that neither ROW_COUNT nor RAW_DATA_SIZE are initialized properly. I'm investigating., In Windows .q tests FileSinkOperator.publishStats fails silently because of JDBC error java.lang.ClassNotFoundException: org.apache.derby.jdbc.EmbeddedDriver. this causes all subsequent problems because stats are 0/0, Fixed by adding Derby jar into the CLASSPATH, right there in hadoop.cmd. Just one more hack to get .q tests to run on Windows... I wish the whole pom->surefire->driver->hadoopCLI->task chain would work correctly vis-a-vis execution on a Windows environment, but is beyond my bandwith to fix it now. I'm explaining my hack fix for the unfortunate soul running into this later... and that includes myself 2 months later when I forgot what I did.]