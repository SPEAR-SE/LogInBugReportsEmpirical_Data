[Here is full exception trace:
{noformat}
java.io.IOException: cannot find dir =
hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/emptyFile
in partToPartitionInfo:
[xxx......., xxx......., xxx......., ...............
 hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1,
hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/2]
        at
org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(HiveFileFormatUtils.java:277)
        at
org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CombineHiveInputSplit.<init>(CombineHiveInputFormat.java:100)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:312)
        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:929)
        at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:921)
        at org.apache.hadoop.mapred.JobClient.access$500(JobClient.java:170)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:838)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:792)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1021)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:792)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:766)
        at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:610)
        at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:120)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:900)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:770)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:647)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:353)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{noformat}
, Amareshwari, more details about your example? From your example, i can not reproduce the problem., Sorry for the delay. 
The table has three partitions and 100 columns. It is stored as RCFile with compressed data.
The query we ran was "select count(\*) from <table>" with CombineHiveInputFormat as the input format. We were trying to test MAPREDUCE-1597 by setting hive.hadoop.supports.splittable.combineinputformat to true. Queries ran fine with Text files., @Amareshwari

in your example:
hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/emptyFile
in partToPartitionInfo:
[xxx......., xxx......., xxx......., ...............
 hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1,
hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/2]

If i put these into TestHiveFormatUtils, it can return correct value. Maybe there is some mismatch about 'xxx'?, I replaced the actual file names of xxx, because actual file/host names are internal to our organization. But the problem is CombineHiveInputFormat is looking for PartitionDesc in "hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/emptyFile" . This dir is not part of the table input data. I think this dir is getting added by FileSinkOperator. , bq. I replaced the actual file names of xxx.
I meant " I replaced the actual file/host names with xxx", Can you search 
hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1 (replacing xxx with actual file/host names)?

It should appear one time in partToPartitionInfo and another one time in "hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/emptyFile".
, It appears only once as "hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/". there is no "hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/emptyFile", so 'xxx' part is not the same in "hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/" and "hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/emptyFile"
?, Sorry If I misunderstood your comment. I looked for hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/ in partToPartitionInfo shown in the exception. Only hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/ appears. hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1/emptyFile does not appear in partToPartitionInfo. , For a given path, CombineHiveInputFormat does recursive lookup in partToPartitionInfo. If no match found, will lookup for the parent dir ("hdfs://xxx/.../hive_2010-09-07_12-15-00_299_4877141498303008976/-mr-10002/1") in partToPartitionInfo. In your case, it seems the parent dir exist in partToPartitionInfo. , Amareshwari, by adding a testcase in TestHiveFileFormatUtils, you will be able to find out the underlying problem, and then can you post a patch for it?, I was taking a look at reproducing the issue. The core reason why the exception is present is due to following.

* Input format is passed a set of input path.
* These set of path contains two kind of files, table data files and scratch/tmp files which are created by hive in hdfs.
* CombineHiveInputFormat tries to compute splits in these temp/scratch file, which causes the  getPartitionDescFromPathRecursively to fail. Causing the query to fail.

I hope this helps, I am still looking at the code, and trying to figure out where the actual addition to input paths are done. So basically I can back track from there. Any help on this would be great.

, This problem is caused in following scenario:

* {{NameNode}} is running on default port {{8020}}
* The data which is to be processed has atleast one empty partition.

The logic how empty partition is dealt is by creating an {{emptyFile}} in the scratch directory.

So when {{NameNode}} runs on default port, the URI which {{NameNode}} passes on does not contain the port information in authority part. Whereas typically the configuration for hive scratch directory contains the port information. This causes this issue., Attaching the patch which fixes this issue. It just makes the temporary empty file to qualified. Not sure of how to add a unit test case for the same.

, Making it Patch available., Yongqiang, can you take a look ?, +1 , I just committed! Thanks Sreekanth Ramakrishnan!, This bug seems to also show up in hive local mode, the empty temporary file path is not qualified with "file:/".

2013-03-11 09:34:30,767 INFO  io.CombineHiveInputFormat (CombineHiveInputFormat.java:getSplits(363)) - CombineHiveInputSplit creating pool for file:/var/folders/w7/fp4gml2n1xqg2434qdp799r00002cr/T/shuang/hive_2013-03-11_09-34-29_301_2567414763209147193/-mr-10000/1; using filter path file:/var/folders/w7/fp4gml2n1xqg2434qdp799r00002cr/T/shuang/hive_2013-03-11_09-34-29_301_2567414763209147193/-mr-10000/1
2013-03-11 09:34:30,772 INFO  mapred.FileInputFormat (FileInputFormat.java:listStatus(196)) - Total input paths to process : 1
2013-03-11 09:34:30,778 INFO  mapred.JobClient (JobClient.java:run(919)) - Cleaning up the staging area file:/data/hadoop/cache/analytics-mr.sv2/shuang/mapred/staging/shuang-1827099888/.staging/job_local_0001
2013-03-11 09:34:30,778 ERROR security.UserGroupInformation (UserGroupInformation.java:doAs(1180)) - PriviledgedActionException as:shuang (auth:SIMPLE) cause:java.io.FileNotFoundException: File does not exist: /var/folders/w7/fp4gml2n1xqg2434qdp799r00002cr/T/shuang/hive_2013-03-11_09-34-29_301_2567414763209147193/-mr-10000/1/emptyFile
2013-03-11 09:34:30,779 ERROR exec.ExecDriver (SessionState.java:printError(365)) - Job Submission failed with exception 'java.io.FileNotFoundException(File does not exist: /var/folders/w7/fp4gml2n1xqg2434qdp799r00002cr/T/shuang/hive_2013-03-11_09-34-29_301_2567414763209147193/-mr-10000/1/emptyFile)'
java.io.FileNotFoundException: File does not exist: /var/folders/w7/fp4gml2n1xqg2434qdp799r00002cr/T/shuang/hive_2013-03-11_09-34-29_301_2567414763209147193/-mr-10000/1/emptyFile
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:562)
        at org.apache.hadoop.mapred.lib.CombineFileInputFormat$OneFileInfo.<init>(CombineFileInputFormat.java:462)
        at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getMoreSplits(CombineFileInputFormat.java:256)
        at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:212)
        at org.apache.hadoop.hive.shims.Hadoop20SShims$CombineFileInputFormatShim.getSplits(Hadoop20SShims.java:347)
        at org.apache.hadoop.hive.shims.Hadoop20SShims$CombineFileInputFormatShim.getSplits(Hadoop20SShims.java:313)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:377)
        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:977)
        at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:969)
        at org.apache.hadoop.mapred.JobClient.access$500(JobClient.java:170)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:880)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:833)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:807)
        at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:671)
        at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:1092)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:197), We are still getting error message when trying to load empty table/file running on local mode. 
Tried adding "file://" in front of the path though it didn't help. Can someone please clarify the solution here? 

Hive Version: 0.10.0+121-1.cdh4.3.0.p0.16~precise-cdh4.3.0

Thanks

{code}
java.io.FileNotFoundException: File does not exist: /tmp/hdfs/hive_2016-01-19_21-40-07_727_4067638808884572526/-mr-10000/1/emptyFile
    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:807)
    at org.apache.hadoop.mapred.lib.CombineFileInputFormat$OneFileInfo.<init>(CombineFileInputFormat.java:462)
    at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getMoreSplits(CombineFileInputFormat.java:256)
    at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:212)
    at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getSplits(HadoopShimsSecure.java:411)
    at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getSplits(HadoopShimsSecure.java:377)
    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:387)
    at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1091)
    at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1083)
    at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)
    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:993)
    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:946)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:946)
    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:920)
    at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:448)
    at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:690)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Job Submission failed with exception 'java.io.FileNotFoundException(File does not exist: /tmp/hdfs/hive_2016-01-19_21-40-07_727_4067638808884572526/-mr-10000/1/emptyFile)'
Execution failed with exit status: 1
{code}]