[Is this different from hive.server2.thrift.resultset.max.fetch.size?, That appears to only be used in (some?) JDBC paths. All the fetchresults calls have different hardcoded values., [~thejas] [~prasanth_j] can you take a look?, Was discussing offline with [~vgumashta]  - There is already support for setting fetch row size from client side. Client side settings need to be able to override settings on server side for this parameter.

[~vgumashta]
Do we have a mechanism to figure out if the client wants a 'default' value to be used (ie, living it unset) or if it is setting a value explicitly ?

, [~thejas] there are different overloads with and without row count for various methods. The ones without currently hardcode the row count, I am replacing that with a config. Overriding the server setting just requires one to call a different overload., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12831375/HIVE-14876.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10654 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1379/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1379/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1379/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12831375 - PreCommit-HIVE-Build, The changes look good, except for the change in default value. 
I think 1k records is reasonable default, and I would prefer to side with the tradeoff for better reliability (no OOM) over speed in default setting.
I am not sure if going from 1k to 10k would produce a huge improvement in speed.
cc [~gopalv]

, I just took the largest of the existing 3 defaults that are being replaced. 10k rows seems like a very small amount, but I can change back to 1k., 10,000 is the default in the hard-coded codepaths.

{code}
-    return fetchResults(opHandle, FetchOrientation.FETCH_NEXT, 10000, FetchType.QUERY_OUTPUT);
{code}

From the patch, so that is not a change in impl in this patch - if anything, this makes it configurable.

There is a bandwidth issue related to the total # of calls, which affects desktop tools only because they tend to be VPN'd or proxied.

Speaking of proxying, Knox has overheads related to the cookies here (there's a possible bug actually, I logged a ticket for analysis #64616 which might need an Apache JIRA & fix once verified)., [~thejas] based on the above do you think 10k makes sense as a default?, Following are the details of RPC fetch from jdbc to hs2 and also the confusion over {{hive.server2.thrift.resultset.max.fetch.size}}:
When we create a new connection, we use a default value for fetch size if not specified in the connection string by the end user. In {{HiveConnection}}:
{code}
private int fetchSize = HiveStatement.DEFAULT_FETCH_SIZE;
{code}

If however, a user specifies the fetch size by using a connection string like this: {{jdbc:hive2://localhost:10000/default;fetchSize=10000}}, we override the default value with the user supplied value. In {{HiveConnection}}:
{code}
    if (sessConfMap.containsKey(JdbcConnectionParams.FETCH_SIZE)) {
      fetchSize = Integer.parseInt(sessConfMap.get(JdbcConnectionParams.FETCH_SIZE));
    }
{code}

When we run a {{HiveStatement.execute}}, we set the fetch size in the result set. In {{HiveStatement.execute}}:
{code}
    resultSet =  new HiveQueryResultSet.Builder(this).setClient(client).setSessionHandle(sessHandle)
        .setStmtHandle(stmtHandle).setMaxRows(maxRows).setFetchSize(fetchSize)
        .setScrollable(isScrollableResultset)
        .build();
{code}

Finally, when we issue a fetch rpc request, we send this value as part of the rpc request. In {{HiveQueryResultSet.next}}:
{code}
TFetchResultsReq fetchReq = new TFetchResultsReq(stmtHandle,
            orientation, fetchSize);
{code}

On the server side, the fetch request hits {{ThriftCLIService.FetchResults}}:
{code}
RowSet rowSet = cliService.fetchResults(
          new OperationHandle(req.getOperationHandle()),
          FetchOrientation.getFetchOrientation(req.getOrientation()),
          req.getMaxRows(),
          FetchType.getFetchType(req.getFetchType()));
{code}
The request eventually reaches {{SQLOperation.getNextRowSet}} which gets the fetch size specified in the RPC as the parameter.

Apologize for the confusion regarding {{hive.server2.thrift.resultset.max.fetch.size}}, but that is only used when ThriftJDBCSerde is used to write resultsets in tasks, to decide how many rows to serialize in a blob. I have created a jira for resolving the confusion and shall have a patch out soon: HIVE-14901. Meanwhile, to increase the default fetch size for the code path that doesn't use ThriftJDBCSerde, we should bump the value of HiveStatement.DEFAULT_FETCH_SIZE on the driver side.

cc [~ziyangz]:  you might want to follow the discussion here., [~vgumashta] this patch actually adds a config setting to make use of this setting everywhere (originally, I was going to add a separate setting).
"Bumping up" the hardcoded values is not an option (note there are 3 different hardcoded values).
Also not everyone uses JDBC. There's also (at least) ODBC, and maybe other callers., Ping?, [~sershe] Had some discussion with [~thejas] on Friday and I think there will be a need for a config for deciding the max rows to serialize in tasks with respect to HIVE-12427. Also, ODBC and JDBC currently end up using the other overloaded version of cliService.fetchResults, where the fetch size is specified from the client side (either explicitly or via a default value configured in the driver). 

I see the point where we need to specify a server specific default when the client does not specify a fetch size, but shouldn't this default value on server and driver be the same?, Doesn't this make it the same? Can you elaborate on the concerns with a patch specifically., Two suggestions - 
1. I think we should have a different "max" fetch size and a "default" fetch size.
The "max" would be a guard rail to prevent accidently adding extra zeroes to the param, resulting in an OOM in HS2. This config in this case is called "hive.server2.thrift.resultset.max.fetch.size" , it would be good to change the description of that and use a new config param for purpose of this jira ("hive.server2.thrift.resultset.default.fetch.size").

2. Default to 1k for server -
We have 3 default values getting currently used -
10k from ODBC driver (overrides server settings)
1k from JDBC driver (overrides server settings)
100 in server (CliDriver uses Operation.DEFAULT_FETCH_MAX_ROWS)

1k seems like a reasonable value to me. If we assume 1 KB average record size, it comes to around 1 MB per call. That should keep the network/rpc overheads low enough. Unless we have data that shows 10 KB provides significantly better performance, I think we can use 1k on the server side as well.
, 1. I can add another config setting. I will leave max untouched as that is a separate concern... The max seems to be used as a default somewhere (JDBC?) as per above discussion. 
2. We have a scenario right now where fetch results takes forever even with 10k due to fetching very large number of rows.
Given that the value is supposed to be relevant mostly for large fetches (the small ones will fit within a few requests anyway), I feel like with those in mind it should be set to a maximum reasonably safe value, not "some" reasonably safe value; it's easy to decrease if one gets an OOM. I would set it to 100k :) 1k seems too small. 
, Updated the patch., +1 on new one (maybe we can remove Operation.DEFAULT_FETCH_MAX_ROWS?), Sorry, is the  "HIVE_SERVER2_RESULTSET_DEFAULT_FETCH_SIZE("hive.server2.resultset.default.fetch.size"...)" in patch HIVE-14876.01.patch the same as "hive.server2.thrift.resultset.default.fetch.size" mentioned here? I mean is this a config only for ThriftJDBCBinarySerDe or a general one?, The general one., bq. it's easy to decrease if one gets an OOM.
Figuring out the reason for OOM is not easy, specially if you have many queries running against HS2.
Do you have any numbers on performance difference between 1k and 10k fetch size ?

cc [~gopalv] [~ziyangz]
, Committed to master some time ago., No pre-commit runs?, Doc note:  This changes the default value of *hive.server2.thrift.resultset.max.fetch.size* from 1000 to 10000 and adds *hive.server2.resultset.default.fetch.size* so they need to be documented in Configuration Properties.  (*hive.server2.thrift.resultset.max.fetch.size* was introduced in 2.1.0 by HIVE-12049 and isn't documented yet in the wiki.)  Added a TODOC2.2 label.

Edit (06/Mar/17):  HIVE-14901 removes *hive.server2.resultset.default.fetch.size* from the 2.2.0 release, so don't document it -- just document the new default for *hive.server2.thrift.resultset.max.fetch.size*.

* [Configuration Properties -- HiveServer2 | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-HiveServer2]

By the way, the most recent attached patch (01) doesn't include the change of default value.  Compare with commit 47d904675d04539bdef96ed26e00b1786251e9f2.

, [~sseth] hmm, the latest version didn't have them indeed, not sure why. They did run for a very similar version on Oct 3rd though/.]