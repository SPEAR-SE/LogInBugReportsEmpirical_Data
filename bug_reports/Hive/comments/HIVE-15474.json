[

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12844057/HIVE-15474.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 10822 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=234)
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_sort_array] (batchId=59)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2653/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2653/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2653/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12844057 - PreCommit-HIVE-Build, [~ashutoshc], could you take a look? Thanks, Didn't read the patch, but I'm trying to understand if it's valid to push limit to group by. People frequently use order by and limit get rank functionality., [~xuefuz], thanks for leaving the comment, it would be great if you could take a look at the patch too.

Propagating limit _N_ to GBy is valid iff GBy columns are a prefix of the OBy columns. This is due to the fact that GBy will not produce duplicates for those columns, while Hive implementation based on RS ensures that GBy output actually follows a certain order. Thus, we know that the GBy will output the top _N_ records.

I took a conservative approach as we need to be sure that we remain correct; it might be that the condition could be relaxed even further for some corner cases. However, we should not do it without double checking the theoretical background., Hi [~jcamachorodriguez], thanks for the explanation. 

Re: GBy will not produce duplicates for those columns, while Hive implementation based on RS ensures that GBy output actually follows a certain order.

This assumption isn't always true, actually. While MR-styled shuffle has that property (maybe Tez too), but it's not true for Hive on Spark where groups are not necessarily ordered.

Nevertheless, I'm still not 100% if there is any impact. In fact, looking at test cases in the patch, I'm sure of the plan difference. Thus, it would be very helpful if you can provide explain output for the example query for both w and w/o the optimization.

Thanks., [~xuefuz], thanks for the feedback. It is indeed true for MR and Tez. I am not so familiar with Spark as the backend. If it does not return ordered groups, are you sure the sequence of operators is still RS-GB-RS? That would mean that other optimizations, such as ReduceSinkDeDuplication, do not work properly (\?). In turn, I would argue that RS is a physical operator that has certain semantics that should be respected, no matter the backend. However, if the sequence of operators is still that one, I can just disable the optimization for Hive on Spark.

--

Given this query:
{code:sql}
explain
select key, value, count(key + 1) as agg1 from src 
group by key, value
order by key, value, agg1 limit 20;
{code}

Explain plan without patch:
{code}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: key (type: string), value (type: string), (UDFToDouble(key) + 1.0) (type: double)
              outputColumnNames: _col0, _col1, _col2
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: count(_col2)
                keys: _col0 (type: string), _col1 (type: string)
                mode: hash
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string), _col1 (type: string)
                  sort order: ++
                  Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col2 (type: bigint)
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: string), KEY._col1 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
              sort order: +++
              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
              TopN Hash Memory Usage: 0.3
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey2 (type: bigint)
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
          Limit
            Number of rows: 20
            Statistics: Num rows: 20 Data size: 200 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 20 Data size: 200 Basic stats: COMPLETE Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 20
      Processor Tree:
        ListSink
{code}

Explain plan with patch:
{code}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-2 depends on stages: Stage-1
  Stage-0 depends on stages: Stage-2

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: src
            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: key (type: string), value (type: string), (UDFToDouble(key) + 1.0) (type: double)
              outputColumnNames: _col0, _col1, _col2
              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: count(_col2)
                keys: _col0 (type: string), _col1 (type: string)
                mode: hash
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string), _col1 (type: string)
                  sort order: ++
                  Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                  TopN Hash Memory Usage: 0.3
                  value expressions: _col2 (type: bigint)
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: string), KEY._col1 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: bigint)
              sort order: +++
              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
              TopN Hash Memory Usage: 0.3
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string), KEY.reducesinkkey2 (type: bigint)
          outputColumnNames: _col0, _col1, _col2
          Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
          Limit
            Number of rows: 20
            Statistics: Num rows: 20 Data size: 200 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 20 Data size: 200 Basic stats: COMPLETE Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 20
      Processor Tree:
        ListSink
{code}

Observe the only difference is the TopN annotation in the first stage, which prevents shuffling all the data., [~lirui], could you also take a look at the proposal and comment here?, Hi [~jcamachorodriguez], I think it's an interesting optimization and please help me understand it better. For groupBy + orderBy query, we'll roughly have the following operator chain (assuming map side aggregation is on - GBY1 and RS2):
{{GBY1 -- RS2 -- GBY3 -- FS4 -- TS5 -- RS6 -- SEL7 -- FS8}}
Is the proposal to push the limit into GBY3, so that we'll have less data for the sorting stage? If so, I think that requires the output of GBY3 already being sorted, right? For MR (and probably Tez too) the shuffled data is sorted by key within each partition, which means the input to GBY3 is sorted. So you're implying that given a sorted input, GBY operator will maintain that order in output?
If my understanding is correct so far, this will have some problem with Hive on Spark. Because for Spark, the groupBy shuffling doesn't guarantee an order - the input to GBY3 may not be sorted. This makes sense in that groupBy semantic doesn't require a specific ordering. But maybe we can make some change to adapt to this optimization. Actually I'm also wondering: if we use parallel order by (to use a range partitioner rather than a hash partitioner in RS2), we can do the groupBy and orderBy in a single stage, which may improve performance in some cases., [~lirui], sorry for taking some time replying, I was away for a few days.

I have updated the description of this issue to try to explain better what I am trying to do. I think initial description was too brief and vague.

Given the physical plan you provide, we will propagate the limit from RS4 into RS2 (observe the explain plan above). RS2 produces the _top N_ keys for each partition; thus, GBY3 operator produces only results for those keys. Observe in the patch that there is no change for the GBY operator.

Concerning Spark. My current understanding is that the chain of operators is the same. But I was thinking further about it, and this optimization should not pose any problem in that context, since GBY logic has not changed. If Spark chooses to ignore RS2 since it is not sorting the input for GBY3, that should be fine: the limit is in the RS2 operator, not in GBY3. Spark will not benefit from the optimization, but it still remains correct.

{quote}
Actually I'm also wondering: if we use parallel order by (to use a range partitioner rather than a hash partitioner in RS2), we can do the groupBy and orderBy in a single stage, which may improve performance in some cases.
{quote}
It might be beneficial in some cases indeed. However, it is a complex cost-based decision which would need multiple extensions, as I can think on multiple factors that would influence it, e.g., data skew, number of records for the top N groups, the limit of records itself, etc., [~jcamachorodriguez], thanks very much for the detailed explanations :) For Spark, the operator chain is something like this:
{{GBY1 – RS2 – GBY3 – RS4 – SEL5 – FS6}}
Since RS2 can produce the top N keys, I think this optimization doesn't require the input to GBY3 to be sorted. I mean we still feed the top N keys to GBY3, but after shuffling, those keys may not be in a sorted order. And the result should remain correct. Is that right?, That is correct :)

I will update the patch to add the new q test to the MiniSparkCliDriver too., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12845376/HIVE-15474.01.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10914 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=233)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[case_sensitivity] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input_testxpath] (batchId=28)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_coalesce] (batchId=75)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_llap_counters] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
org.apache.hive.hcatalog.pig.TestTextFileHCatStorer.testPartColsInData (batchId=172)
org.apache.hive.hcatalog.pig.TestTextFileHCatStorer.testStoreMultiTables (batchId=172)
org.apache.hive.hcatalog.pig.TestTextFileHCatStorer.testWriteVarchar (batchId=172)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2764/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2764/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2764/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12845376 - PreCommit-HIVE-Build, Thanks [~jcamachorodriguez] for enabling the Spark test. It looks good to me.
One more question, you mentioned the optimization works iff GBy columns are a prefix of the OBy columns. How about when the OBy columns are prefix of GBy columns? Won't the optimization work for that too?, [~lirui], the combination with _ReduceDeduplication_ makes the trick in the case you describe.

Consider the following query:
{code:sql}
select key, value, value2, count(key + 1) as agg1 from src 
group by key, value, value2
order by key, value limit 20;
{code}
In this case, OBy columns are a prefix of GBy columns. RS[4] in this case will end up with columns _key, value, value2_. Then limit will be pushed by the new extension to the RS[2].

As I stated above, I took a conservative approach as we need to be sure that we remain correct; it might be that the condition could be relaxed even further for some corner cases. However, I did not want to do it without checking the theoretical background further., I see. Thanks [~jcamachorodriguez].
Instead of having another {{checkKeys}}, can we do some refactor to reuse the current one? I think they're essentially the same - checking whether some keys are prefix of another series?, [~ruili], thanks for the feedback.

I have created two methods that are more generic (_checkPrefixKeys_ and _checkPrefixKeysUpstream_) and moved them to ExprNodeDescUtils. Now they rely on the same logic. Could you take a look to the new patch? Thanks, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12845788/HIVE-15474.03.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10915 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=233)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[case_sensitivity] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input_testxpath] (batchId=28)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_coalesce] (batchId=75)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[offset_limit_ppd_optimizer] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_3] (batchId=92)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2800/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2800/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2800/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12845788 - PreCommit-HIVE-Build, Fails seem unrelated; none of the age=1 failures can be reproduced locally., Thanks for the update [~jcamachorodriguez]. +1, Pushed to master, thanks for reviewing [~lirui]!]