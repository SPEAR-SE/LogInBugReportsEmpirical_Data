[Here's a workaround:

1) In your hive source directory run 'ant clean'.
2) remove the contents of ~/.ant/cache/hadoop/core/sources
3) Download the following files to ~/.ant/cache/hadoop/core/sources:

      hadoop-0.17.2.1.tar.gz
      hadoop-0.17.2.1.tar.gz.asc
      hadoop-0.18.3.tar.gz
      hadoop-0.18.3.tar.gz.asc
      hadoop-0.19.0.tar.gz
      hadoop-0.19.0.tar.gz.asc
      hadoop-0.20.0.tar.gz
      hadoop-0.20.0.tar.gz.asc

4) For each hadoop-xxx.tar.gz file, compute the sha1 checksum using sha1sum, and verify that it matches the sha1 checksum in the corresponding .asc file.

If it does not match then the file is corrupt and you need to try downloading it again.

This step is not absolutely necessary since Ivy will check for you during the build process.

5) Try building Hive again following the instructions on the wiki. You shouldn't have any problems if you verified the checksums.

As an additional note, if you don't care about support for Hadoop 0.17.2.1, or 0.18, etc, you can disable support for these versions (and skip the download) by removing the references to these versions that shims/build.xml and shims/ivy.xml, * Make download errors less likely by only downloading the specified version of Hadoop instead of 0.17 through 0.20
* Add 'ivy-delete-cache' ant target so users can easily recover from corrupt ivy downloads. This target only deletes the hadoop artifacts in the Ivy cache.
* Add descriptions for all of the top-level targets in build.xml
* Move some common properties to build.properties.
, This should be a blocker for 0.5.0
, Any chance this can go into 0.5.0?, resolve:
[ivy:retrieve] :: Ivy 2.0.0-rc2 - 20081028224207 :: http://ant.apache.org/ivy/ ::
:: loading settings :: file = /data/users/njain/hive_commit3/hive_commit3/ivy/ivysettings.xml

BUILD FAILED
/data/users/njain/hive_commit3/hive_commit3/build.xml:142: The following error occurred while executing this line:
/data/users/njain/hive_commit3/hive_commit3/build.xml:89: The following error occurred while executing this line:
/data/users/njain/hive_commit3/hive_commit3/build-common.xml:73: java.lang.IllegalArgumentException: ivy.home must be absolute: ${env.IVY_HOME}


I got the following error while compiling after applying the patch, * Set ivy.home to ${user.home}/.ant if IVY_HOME is unset.
, @Namit: Updated patch should fix the compilation error.,     [junit] 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1161)
    [junit] 	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1029)
    [junit] 	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:979)
    [junit] 	at org.apache.hadoop.conf.Configuration.set(Configuration.java:404)
    [junit] 	at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:308)
    [junit] 	at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:318)
    [junit] 	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:243)
    [junit] 	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:332)
    [junit] 	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:312)
    [junit] 	at org.apache.hadoop.hive.ql.QTestUtil.<init>(QTestUtil.java:173)
    [junit] 	at org.apache.hadoop.hive.cli.TestCliDriver.setUp(TestCliDriver.java:39)
    [junit] 	at junit.framework.TestCase.runBare(TestCase.java:125)
    [junit] 	at junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit] 	at junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit] 	at junit.framework.TestResult.run(TestResult.java:109)
    [junit] 	at junit.framework.TestCase.run(TestCase.java:118)
    [junit] 	at junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit] 	at junit.framework.TestSuite.run(TestSuite.java:203)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
    [junit] Caused by: java.io.FileNotFoundException: /data/users/njain/hive_commit1/hive_commit1/build/hadoopcore/hadoop-0.20.0/conf/core-site.xml (Too many open files)
    [junit] 	at java.io.FileInputStream.open(Native Method)
    [junit] 	at java.io.FileInputStream.<init>(FileInputStream.java:106)
    [junit] 	at java.io.FileInputStream.<init>(FileInputStream.java:66)
    [junit] 	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)
    [junit] 	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)
    [junit] 	at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)
    [junit] 	at com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:186)
    [junit] 	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:771)
    [junit] 	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
    [junit] 	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)
    [junit] 	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:225)
    [junit] 	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:283)
    [junit] 	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:180)
    [junit] 	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1078)
    [junit] 	... 20 more
    [junit] Exception: java.io.FileNotFoundException: /data/users/njain/hive_commit1/hive_commit1/build/hadoopcore/hadoop-0.20.0/conf/core-site.xml (Too many open files)


I am getting a lot of errors like this, @Namit: 

Is /data/users/njain/hive_commit1/hive_commit1/* a valid path on your machine?

This is failing while running the tests with hadoop 0.20.0?

, -1 on the current patch based on comments from HIVE-487.
I think it makes sense to compile Hive with every supported major version of hadoop so that we only need to deploy a single set of hive for all versions of hadoop.

{quote}
Todd Lipcon added a comment - 30/Jul/09 05:34 PM
Patch looks good for me (just inspected it visually over here)
One question: once we use these shims, is it possible that we could have just a single hive distribution which works for all versions of Hadoop? I think we may be able to accomplish this by making the shim jar output be libs/shims/hive_shims-{$hadoop.version.prefix}.jar. Then either through ClassLoader magic or shell wrapper magic, we put the right one on the classpath at runtime based on which hadoop version is on the classpath.
Is this possible? Having different tarballs of hive for different versions of hadoop makes our lives slightly difficult for packaging.
{quote}


Can we keep building 4 versions of hadoop as hive is doing now?
, bq. Can we keep building 4 versions of hadoop as hive is doing now?

What if I modify the build so that all 4 versions are built when running the 'tar' target, but otherwise only the shim corresponding to hadoop.version is downloaded and built (which would be the case for the 'jar' and 'package' targets)?
, Given HIVE-1120, I think the need to compile with only one version of hadoop goes down a lot.
What do you think?
, bq. Given HIVE-1120, I think the need to compile with only one version of hadoop goes down a lot. What do you think? 

That assumes that folks are able to satisfy the dependency at least once in order to populate
their Ivy cache, and all of the traffic on hive-user seems to indicate this is a lot harder than
it looks.

Right now we always build shims for four different minor versions of Hadoop: 0.17, 0.18, 0.19
and 0.20. Users also have the option of building against five different versions of Hadoop
by setting hadoop.version to one that we don't already include. What's the long term plan
here? Are we eventually going to build shims for ten different versions of Hadoop every time
someone runs 'ant package'? 

As I said earlier I think it makes sense to compile shims for as many different versions as possible
when building a release tarball (or days from now when we want to build a Hive POM),
but I don't see why we should force a user to do this now when list a specific a Hadoop version
number using hadoop.version.
, I spoke with Zheng about this and here's what we came up with.  Carl, let me know if this works for you.

* If at all possible, we want to keep building all supported shims as part of ant package to make sure that when a change breaks one, the developer finds out early (before even submitting a bad patch)
* The long term plan does involve deprecating and eventually dropping support for older Hadoop versions.  The fact that Facebook still has some dependencies on 0.17 probably explains why that is currently the oldest version, but the standard voting procedure can be used at the project level for initiating a deprecation process going forward.
* Regardless of how many Hadoop versions we support, the current Hadoop+ivy situation is definitely broken, and we need to fix it ASAP since it can be a major impediment to new or existing contributors.
* Before doing anything else, I'm going to see if a more reliable source than archive.apache.org would address the problem.  I'll test this with my home network tomorrow, which usually fails with archive.apache.org.
* If a more reliable source would help, then we'll see if we can get mirror.facebook.net to provide all supported Hadoop versions (currently only apache.archive.org has the old ones), and if that's the case, then we'll check in a change to build.properties to make it the default source.
* If either of the above is not the case, then we can do what you proposed in HIVE-1171 (check the Hadoop dependencies into svn instead).
, 
bq.  If at all possible, we want to keep building all supported shims as part of ant package to make sure that when a change breaks one, the developer finds out early (before even submitting a bad patch)

Unless your change specifically mucks with the shim code I think it's unlikely that you're going to introduce a compile time error. It seems more likely that you would cause a test error, and that's something you will only catch if you run the full test suite against all supported versions -- something that we only expect Hudson to do.

Which brings up another point. How do we configure JIRA/Hudson to automatically test submitted patches? The Hadoop and Pig projects are both setup to do this, but I can't find any references to how it was done. Do either of you know how to set this up, or have objections to doing so?

bq. Before doing anything else, I'm going to see if a more reliable source than archive.apache.org would address the problem. I'll test this with my home network tomorrow, which usually fails with archive.apache.org.

Over the weekend I figured out that there are actually two different reasons why people are encountering errors during the download process, and wanted to make sure that everyone else is aware of this as well:

# Unable to connect to archive.apache.org: We can fix this by adding additional apache mirrors (see http://www.apache.org/mirrors/) to the hadoop-source resolver in ivysettings, and also by letting people know that they can explicitly set the mirror location using the hadoop.mirror property.

# -Dhadoop.version=0.20.1: When people set hadoop.version to 0.20.1 it causes ant to download both 0.20.0 *and* 0.20.1, which is unnecessary since the API does not change between patch releases. But the bigger problem is that 0.20.1's md5 checksum file on archive.apache.org contains an md5 hash along with a bunch of other garbage that breaks ivy. We can fix this either by disabling checksums for archive.apache.org (set ivy.checksums="" on that resolver), or by enhancing the build script so that it ignores patch release numbers and maps 0.20.1 to 0.20.0.

, Shims:  you may be right, but I guess the principle is that all source code checked in ought to be covered by the build if possible.  It's arguable that we should actually do even more in this respect (rather than less), since for example in HIVE-1136 we just hit a case where one of my changes was incompatible with an old Hadoop version (nothing to do with shims).  If we built against all supported Hadoop versions as part of ant test, this would have been caught when I ran tests myself (so Zheng would never have had to spend time testing my bad patch and rejecting it).  ant test might be a reasonable place for that, since test time will always be orders of magnitude longer than build time.  (But note:  I'm not proposing to run tests on all versions except in Hudson!)

Hudson automatically testing patches:  I don't know the answer to that one, but it sounds like a very high-value automation to me if the resources are available, and my opinion on the version download issue might change if this were working reliably with permanently committed resources.

archive.apache.org:  the default mirroring for Hadoop seems to be 0.18.3, 0.19.2, and 0.20.1 (that's what I see when I browse most of the mirrors), which doesn't match what Hive currently wants (0.17.2.1, 0.18.3, 0.19.0, and 0.20.0).  That's why I was thinking we might need a custom setup on mirror.facebook.net.
, I was able to verify that using a reliable server (tested with a privately hosted server of my own) allowed for successful artifact download from my home network.

Next step is to talk to some Facebook peeps to see if we can get what we need set up on mirror.facebook.net.
, Mirror is available now; see HIVE-984 for configuration patch to use it.
, Fixed in HIVE-1190.]