[[~prasanth_j] can you take a look at my proposed patch since it's related to HIVE-10286. Thank you., The direct fix would break the bloom filters for someone actually looking for "1 ", since hash("1 ") != hash("1").

The issue is tied to the fact that the there's no CHAR sargs, while hashcodes will be incorrect after a trip., We don't need a CHAR type in sargs. Instead we need to convert the constant literal to column type. For charColumn = "1", the literal "1" should be padded to the length specified in charColumn type info. ORC stores char columns  as is without stripping off the trailing white spaces. The column stats will also have spaces at the end. If we map the constant literal to the column type then we can get away with not having CHAR type in sargs.

[~gopalv]/[~owen.omalley] Can someone take a look at the patch?, The missing type semantic coercion is somewhere deep inside SemanticAnalyzer type inference.

{code}
        // Try to infer the type of the constant only if there are two
        // nodes, one of them is column and the other is numeric const
        if (genericUDF instanceof GenericUDFBaseCompare
{code}, Thanks for looking at this, [~prasanth_j]. Feel free to assign the JIRA to yourself., Addressed [~gopalv] comments. The conversion of string to char now happens during semantic analysis. ORC expects the string constants to be padded properly, else bloom filters will break. , [~sershe] Can you take a look since Gopal is out?, Is {noformat}
olumnType.startsWith(serdeConstants.CHAR_TYPE_NAME) {noformat} a proper way to check the type? Otherwise looks good., I have seen other places using contains() check (ConstantPropagateProcFactory, ColumnStatsSemanticAnalyzer). startswith is better for this case as we care only about char type (StatsUtils uses startswith)., It just seems brittle... why not use TypeInfoFactory.getPrimitiveTypeInfo()?, Addressed review comments. , Make sense. Fixed in latest patch., [~sershe] Could you take another look?, +1, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12775112/HIVE-11312.4.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 9870 tests executed
*Failed tests:*
{noformat}
TestHWISessionManager - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_partitioned
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_ppd_char
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_ppd_char
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mergejoin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.testGetPartitionSpecs_WithAndWithoutPartitionGrouping
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6198/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6198/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-6198/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12775112 - PreCommit-HIVE-TRUNK-Build, orc_ppd_char.q passes locally for me. Not sure why. Sitll investigating, Fixed ppd char test cases, Since the latest test failure is just golden file update and other failures are unrelated, I went ahead and committed the patch to branch-1, branch-2.0 and master branch., Thanks, [~prasanth_j] for getting this fix in!, Isn't it implied that something fixed in 2.0 is also fixed in 2.1?]