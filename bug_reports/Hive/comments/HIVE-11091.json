[Can you attach exception stack trace., I did a diff of Hive 0.11 vs. Hive 0.14 for the piece of code within MoveTask that is causing this error:

Hive-0.11:
{code}
Table table = db.getTable(tbd.getTable().getTableName());

        if (work.getCheckFileFormat()) {
          // Get all files from the src directory
          FileStatus[] dirs;
          ArrayList<FileStatus> files;
          FileSystem fs;
          try {
            fs = FileSystem.get(table.getDataLocation(), conf);
            dirs = fs.globStatus(new Path(tbd.getSourceDir()));
            files = new ArrayList<FileStatus>();
            for (int i = 0; (dirs != null && i < dirs.length); i++) {
              files.addAll(Arrays.asList(fs.listStatus(dirs[i].getPath())));
              // We only check one file, so exit the loop when we have at least
              // one.
              if (files.size() > 0) {
                break;
              }
            }
          } catch (IOException e) {
            throw new HiveException(
                "addFiles: filesystem error in check phase", e);
          }
          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {
            // Check if the file format of the file matches that of the table.
            boolean flag = HiveFileFormatUtils.checkInputFormat(
                fs, conf, tbd.getTable().getInputFileFormatClass(), files);
            if (!flag) {
              throw new HiveException(
                  "Wrong file format. Please check the file's format.");
            }
          }
        }
{code}

Hive-0.14:
{code}
Table table = db.getTable(tbd.getTable().getTableName());

        if (work.getCheckFileFormat()) {
          // Get all files from the src directory
          FileStatus[] dirs;
          ArrayList<FileStatus> files;
          FileSystem srcFs; // source filesystem
          try {
            srcFs = tbd.getSourcePath().getFileSystem(conf);
            dirs = srcFs.globStatus(tbd.getSourcePath());
            files = new ArrayList<FileStatus>();
            for (int i = 0; (dirs != null && i < dirs.length); i++) {
              files.addAll(Arrays.asList(srcFs.listStatus(dirs[i].getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER)));
              // We only check one file, so exit the loop when we have at least
              // one.
              if (files.size() > 0) {
                break;
              }
            }
          } catch (IOException e) {
            throw new HiveException(
                "addFiles: filesystem error in check phase", e);
          }
          if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {
            // Check if the file format of the file matches that of the table.
            boolean flag = HiveFileFormatUtils.checkInputFormat(
                srcFs, conf, tbd.getTable().getInputFileFormatClass(), files);
            if (!flag) {
              throw new HiveException(
                  "Wrong file format. Please check the file's format.");
            }
          }
        }
{code}, The only significant change I see in above code snippets is:

{code}
srcFs = tbd.getSourcePath().getFileSystem(conf);
            dirs = srcFs.globStatus(tbd.getSourcePath());
{code}

i.e. the way in which we get the file system handle and a list of the directories/files within the path provided. , Exception stack trace
===================
org.apache.hadoop.hive.ql.metadata.HiveException: addFiles: filesystem error in check phase
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:270)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1606)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1367)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1179)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:443)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:459)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:739)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.io.IOException: Error accessing file:/tmp/its_saga_upload_rt8_dat_1
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1510)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1548)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:574)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:262)
	... 22 more

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask
15/06/24 09:02:57 [main]: ERROR ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask
, This is tracked to HDFS-8767]