[Hi, [~ruili]
Could I take it over?, [~libing] sure, thanks for helping.
Some suggestions: instead of enforcing the configuration during RDD plan generation, maybe we can do it while we generate the SparkWork, like in {{GenSparkUtils.getEdgeProperty}}.
[~xuefuz], [~csun] would you mind share your opinions? Thanks., Agree. Maybe we can have some new edge properties to reflect this, and display them accordingly., This patch is based on branch-2.3.
With the above changes, I could get the explain result as below.

hive> {color:red}set hive.spark.use.groupby.shuffle=true;{color}
hive> explain select key, count(val) from t1 group by key;
OK
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        {color:red}Reducer 2 <- Map 1 (GROUP, 2){color}
      DagName: root_20170630080539_565b5a00-822e-46e9-a146-be84723ae7f6:2
      Vertices:
        Map 1
            Map Operator Tree:
                TableScan
                  alias: t1
                  Statistics: Num rows: 20 Data size: 140 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), val (type: string)
                    outputColumnNames: key, val
                    Statistics: Num rows: 20 Data size: 140 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(val)
                      keys: key (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 20 Data size: 140 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 20 Data size: 140 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: bigint)
        Reducer 2
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Time taken: 51.289 seconds, Fetched: 54 row(s)

hive> {color:red}set hive.spark.use.groupby.shuffle=false;{color}
hive> explain select key, count(val) from t1 group by key;
OK
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        {color:red}Reducer 2 <- Map 1 (GROUP PARTITION-LEVEL SORT, 2){color}
      DagName: root_20170630075518_b84add65-57db-466f-9521-3f1b14de6826:1
      Vertices:
        Map 1
            Map Operator Tree:
                TableScan
                  alias: t1
                  Statistics: Num rows: 20 Data size: 140 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), val (type: string)
                    outputColumnNames: key, val
                    Statistics: Num rows: 20 Data size: 140 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(val)
                      keys: key (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 20 Data size: 140 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 20 Data size: 140 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: bigint)
        Reducer 2
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Time taken: 49.372 seconds, Fetched: 54 row(s), Thanks [~libing] for working on this. Patch looks good overall. Can you create an RB for it? And a side note is, we usually base the patch against master. If you want to develop for a specific branch, you should name your patch accordingly (refer to this [wiki|https://cwiki.apache.org/confluence/display/Hive/HowToContribute#HowToContribute-CreatingaPatch]).

I think it's OK to remove the RepartitionShuffler, as it's effectively the same as SortByShuffler w/o total order. [~csun], [~xuefuz] please let me know if you think otherwise., Hi, [~ruili]
Thank you for the review!
I checked the latest code on master branch, the current patch could be applied to it directly.
So I won't create a new patch file for the master branch for this Jira.
I will pay attention to it in the future., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12875267/HIVE-16659.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10830 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5861/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5861/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5861/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12875267 - PreCommit-HIVE-Build, Refine the patch with an test case., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12875578/HIVE-16659.2.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 10830 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[columnstats_part_coltype] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5879/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5879/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5879/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12875578 - PreCommit-HIVE-Build, [~ruili], I updated the patch based on your comment and add the link of the review request. 
Thank you!, Thanks Bing for the update. I left some minor comments on RB., Refine GenSparkUtils.java based on Rui's comments on RB.
Thank you, [~ruili], +1 pending test, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12875693/HIVE-16659.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 10830 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=228)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5886/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5886/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5886/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12875693 - PreCommit-HIVE-Build, Pushed to master. Thanks Bing for the contribution., Hive 3.0.0 has been released so closing this jira.]