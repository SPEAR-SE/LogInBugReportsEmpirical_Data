[The  full stack trace, It is also worth mentioning that the namenode was under hardly any load when this happened. I have not seen a reproducible problem like this with normal map reduce jobs on that cluster., Added some debug logging to the close and process methods in FileSinkOperator and the close method is not called before the process method fails as one could guess from the stack trace., Looking into this a bit further the exception is actually caused by the reduce task timing out:
"Task attempt_200901071012_0373_r_000031_0 failed to report status for 616 seconds. Killing!"

This in turn closes the FileSystem/stream and the FileSink keeps trying to write, causing the exception seen in the log.
Now the question is why the task fails to report status., one question - which side was the big table on?

regardless though - perhaps the issue is that when we iterate through large reduce groups - hadoop is not reporting progress - maybe that's something that the application has to do.

what version of hadoop are u using?, The big table in the example query above is tablec, is this what you were referring to?
I'm using Hadoop 0.18.2.
, Just had a look and it doesn't seem like the reducer that fails gets any more data to it compared to the others., ok - got it. hadoop is ok - it's already reporting progress whenever any data is consumed or any data is emitted to output collector.

the issue is that we are not sending data to the output collector - rather we write out data to file system ourselves. the stack trace indicates that we have consumed the entire reduce group and we are writing to the filesystem. that means that hadoop gets no opportunity to report progress (it would have reported progress if we were writing to the output collector).

I am not sure why we haven't seen the problem in our environment yet - perhaps DFS IO is slow in ur environment. the fix is simple (we can report progress either in the filesinkoperator or the joinoperator). i think we should do this in the joinoperator (for the particular case where we have already gone through the entire reduce group) - since that's the only place we are vulnerable right now.

i can post a patch u can try out in a couple of hours .. , That sounds like a very reasonable explanation. We have fewer machines meaning fewer disks in total then Facebook, that could explain why we're seeing this error.
Out of curiosity, how come you're not using the standard output collector?, To verify that this was indeed the issue I have slapped together a patch. Tried it on the same cluster and the same query as above and it worked fine this time. It's not an ideal patch, it changes the initialize method etc. I'll leave a real solution up to someone with more detailed knowledge about this area., For the change to get the reporter reference into the operator structure - the interface change looks good. However - why don't we just store the reporter reference in the base Operator class rather than the FileSinkOperator specifically? If we run into other cases where we have to add progress indicators - this will make it easier.

+1 otherwise. , Updated with the changes mentioned., +1

thanks Johan!, If one of the committers have time to look at the patch it would be much appreciated, thanks., committed. Thanks Johan!!]