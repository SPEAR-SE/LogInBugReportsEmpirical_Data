[[~prasanth_j], I have been looking into this, but I do not understand the analysis fully yet...

Do you think that the problem in the stats annotation (way of calculating full GBY cardinality LGTM) or in the logic that creates the tasks for the operators (and that needs to consider if a certain input is already partitioned)? Or in another way: do we consider partitions/parallelism to compute stats for other operators, or do we just compute the full cardinality (i.e. all records that an operator emits)?

In turn, shouldn't partition column be taken into account when we decide if the intermediate reduction of the GBY can be computed using hash aggregation? Currently, it doesn't., [~prasanth_j] Are you suggesting that Map1 -> Reducer2 edge should have been broadcast instead of shuffle ? I am not sure we support broadcast edge between Map side GBY & Reduce side GBY?
, [~jcamachorodriguez] To give more context on this issue. Most TPCDS queries joins fact table with multiple dimension tables followed by some aggregation. The query provided in the description is one such kind. The plans generated for the query is attached with and without hive.transpose.aggr.join enabled. The execution time for all such queries is significantly slower (in order of 4x-6x). For this specific query, without this optimization it took 17.9s on 1TB and with hive.transpose.aggr.join enabled the execution time is 98.24s.

My initial understanding of this optimization is, only map-side GBY will be pushed through the join. This means reduction in the number of rows that will be broadcasted to the fact table. But looking at the plans, the GBY is pushed at the logical level which then gets compiled to map-side GBY and reduce-side GBY followed by JOIN. This shuffles approximately 600M before joining which I think is adding to the overall execution time. I filed this issue thinking that the map-side GBY will broadcast it's output to fact table for join.  Per [~ashutoshc]'s comment below, it seems like we don't support that. ]