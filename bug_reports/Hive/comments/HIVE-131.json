[We should start using the standard way to create side-effect files. This could remove the potential race condition.

http://hadoop.apache.org/core/docs/r0.17.2/api/org/apache/hadoop/mapred/FileOutputFormat.html#getWorkOutputPath(org.apache.hadoop.mapred.JobConf)
, i quickly glanced at the doc mentioned. seems like hadoop will move the files automatically to mapred.output.dir - but we don't have a single output directory for the task (since we can have multiple outputs).

anyway - i am not sure why the problem happens (it almost seems like map-reduce can declare a job complete while (speculative) tasks are still running) - but a trivial fix is to just create the tmp files in a completely different directory (say scratch dir per query) and then move from there. we can discard the scratch dir entirely on query completion. there's still some risk of these runaway files leaking inodes/space. but if these are known hive scratchdir locations - they can always be cleaned up later on., btw - if we can solve a problem at the application layer - i would really prefer doing that. we are trying to get hive to compile and run for different hadoop versions. the less dependency on non-critical hadoop apis/mechanisms - the less trouble we will have with portability., we already have this infrastructure in place - the intermediate results, describe output etc. are created in the scratch directory, which
are deleted at the end of query execution - Driver has a close() call which is supposed to be called at the end of query execution, Maybe the driver close is not getting called for this failure. Is this ctrl-C case? Or an actual failure. Can you put ut the stack trace of the failure., no - no failures here (from hive perspective). no ctrl-C. speculative map-reduce tasks are causing this problem., Dhruba said:

> 1. I see that execute returns values 1, 2, and 3. It will be good to document what these values mean.
> 2. Staring hadoop 0.19, it might make sense to set FileSystem.deleteOnExit() for files that are temporary.
> 3. It is interesting to note that now there is an extra step jobClose() that gets triggered on the client-side after the job is complete. Prior to this patch, a job would be successful even if the client-side has disappeared before the job is completed. This patch requires that the client remains active and healthy till the entire job is complete. This probably is ok for Hive, especially because Hive anyway requires job-chaining and I do not see any other way to do it

- incorporated  suggestion to use deleteOnExit where available.
- return codes are always accompanied by a corresponding message on the console/log. So don't see much point creating additional documentation around them.
- hive has always depended on client side code-patch for query completion., +1 Looks good to me.
, please commit this to 0.2 also since it's a pretty severe bug, trunk: Committed revision 745709.
branch-0.2: Committed revision 745710.

]