[[~lirui] is this a bug with {{CombineEquivalentWorkResolver}}?, Yes I think so. We don't consider DPP when we combine map works in CombineEquivalentWorkResolver. This is wrong because if two map works are to be pruned by different DPP sinks, their outputs are probably different and shouldn't be combined., The main challenge here is how to decide whether two DPP works are different. In {{CombineEquivalentWorkResolver}}, we visit child tasks before its parent. That means when we visit the target map works, we haven't seen the corresponding DPPs yet. The simplest solution is, if the DPP works' IDs (tracked by the target map works) are different, then we consider the target map works are different and don't combine them. The drawback is we'll lose some optimization opportunities - actually I'm not sure whether it's possible that two target map works share the same DPP in current implementation.

Another solution is we walk the parent tasks first, and combine equivalent DPP works. Two DPP works can be considered equivalent as long as they output same records. It shouldn't matter how these records are used to prune different tables. As we combine the DPP works, we update the information in the target map works accordingly (DPP works have reference to target map works). Then when we visit the target map works later, we know whether they should be combined. I'm working on a PoC patch to demonstrate the idea.
[~xuefuz], [~csun], [~stakiar], [~kellyzly] do you have any suggestions?, [~lirui]: I remember this problem when i developed HIVE-16948. But I can not reproduce this problem on hive(commit a51ae9c) now
{code}
set hive.explain.user=false;
set hive.spark.dynamic.partition.pruning=true;
set hive.tez.dynamic.partition.pruning=true;
set hive.auto.convert.join=false;
explain
select * from
  (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.key) a
join
  (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.value) b
on a.key=b.key;
{code}
the explain 
{code}
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
      DagName: root_20171022233200_990c146c-b49f-49b9-9a5b-a0028e34f200:2
      Vertices:
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Select Operator
                        expressions: _col0 (type: string)
                        outputColumnNames: _col0
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: string)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target column: ds (string)
                            partition key expr: ds
                            Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                            target work: Map 1
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: value is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: value (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Select Operator
                        expressions: _col0 (type: string)
                        outputColumnNames: _col0
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: string)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target column: ds (string)
                            partition key expr: ds
                            Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                            target work: Map 5

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 1), Map 4 (PARTITION-LEVEL SORT, 1)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1), Reducer 6 (PARTITION-LEVEL SORT, 1)
        Reducer 6 <- Map 5 (PARTITION-LEVEL SORT, 1), Map 7 (PARTITION-LEVEL SORT, 1)
      DagName: root_20171022233200_990c146c-b49f-49b9-9a5b-a0028e34f200:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string), ds (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col1 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col1 (type: string)
                        Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: string)
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string), ds (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col1 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col1 (type: string)
                        Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: string)
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: value is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: value (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
        Reducer 2 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col1 (type: string)
                  1 _col0 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col1 (type: string), _col0 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col1 (type: string)
                    sort order: +
                    Map-reduce partition columns: _col1 (type: string)
                    Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col0 (type: string)
        Reducer 3 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col1 (type: string)
                  1 _col1 (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 280 Data size: 28129 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 280 Data size: 28129 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col1 (type: string)
                  1 _col0 (type: string)
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col1 (type: string), _col0 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col1 (type: string)
                    sort order: +
                    Map-reduce partition columns: _col1 (type: string)
                    Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col0 (type: string)

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

{code}

I guess what you mean in the jira is that map8 and map9 are combined as one map in your env as the operators in these two map are same. The reason why there are not combined in my env is the filter operators in Map8 and Map9 are not same.
{code}
Map8
  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
{code}

{code}
Map9
					
					    Filter Operator
                    predicate: value is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
{code}

Can you provide your scripts? thanks!, [~kellyzly], the problem is map works for {{srcpart}} (in your case Map1 and Map5) are combined, while they shouldn't because they're targets of different DPPs and therefore are likely to output different results. I think you can disable CBO to see if the issue can be reproduced. Another way is to change the outer query into a union instead of a join., I can reproduce after disabling cbo
{code}

set hive.explain.user=false;
set hive.spark.dynamic.partition.pruning=true;
set hive.tez.dynamic.partition.pruning=true;
set hive.auto.convert.join=false;
set hive.cbo.enable=false;
explain
select * from
  (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.key) a
join
  (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.value) b
on a.key=b.key;
{code}

the explain
{code}
STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
      DagName: root_20171023004308_4b3c304e-3deb-4193-846d-12cf9e6a50ab:2
      Vertices:
        Map 8 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          Target column: ds (string)
                          partition key expr: ds
                          Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                          target work: Map 1

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 1), Map 4 (PARTITION-LEVEL SORT, 1)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1), Reducer 6 (PARTITION-LEVEL SORT, 1)
        Reducer 6 <- Map 1 (PARTITION-LEVEL SORT, 1), Map 7 (PARTITION-LEVEL SORT, 1)
      DagName: root_20171023004308_4b3c304e-3deb-4193-846d-12cf9e6a50ab:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: ds (type: string)
                      sort order: +
                      Map-reduce partition columns: ds (type: string)
                      Statistics: Num rows: 232 Data size: 23248 Basic stats: COMPLETE Column stats: NONE
                      value expressions: key (type: string)
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key (type: string)
                      sort order: +
                      Map-reduce partition columns: key (type: string)
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: value is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: value (type: string)
                      sort order: +
                      Map-reduce partition columns: value (type: string)
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
        Reducer 2 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 ds (type: string)
                  1 key (type: string)
                outputColumnNames: _col0, _col2
                Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: string), _col0 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col1 (type: string)
                    sort order: +
                    Map-reduce partition columns: _col1 (type: string)
                    Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col0 (type: string)
        Reducer 3 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col1 (type: string)
                  1 _col1 (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 280 Data size: 28129 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 280 Data size: 28129 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 ds (type: string)
                  1 value (type: string)
                outputColumnNames: _col0, _col2
                Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: string), _col0 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col1 (type: string)
                    sort order: +
                    Map-reduce partition columns: _col1 (type: string)
                    Statistics: Num rows: 255 Data size: 25572 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col0 (type: string)

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

{code}

There is only 1 Map about srcpart. The reason why the maps about srcpart can not be merged when enabling cbo is because the RS in Maps are considered different while they are considered same when disabling cbo(see attached [picture|https://issues.apache.org/jira/secure/attachment/12893484/17193_compare_RS_in_Map_5_1.PNG]), [~lirui]:
{quote}
1. The simplest solution is, if the DPP works' IDs (tracked by the target map works) are different, then we consider the target map works are different and don't combine them.
2. Another solution is we walk the parent tasks first, and combine equivalent DPP works. Two DPP works can be considered equivalent as long as they output same records.
{quote}
For #1, it can be implemented from the current code. For #2, how to compare the result of dpp work in the period of physical plan?  You mean directly comparing the estimated data size(Statistics: Num rows: 58 Data size: 5812)?

{code}
 Map 9 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: value is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: value (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Select Operator
                        expressions: _col0 (type: string)
                        outputColumnNames: _col0
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: string)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target column: ds (string)
                            partition key expr: ds
                            Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                            target work: Map 5
{code}


{code}
  Map 8 
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Select Operator
                        expressions: _col0 (type: string)
                        outputColumnNames: _col0
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          keys: _col0 (type: string)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                          Spark Partition Pruning Sink Operator
                            Target column: ds (string)
                            partition key expr: ds
                            Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                            target work: Map 1

{code}
, Hi [~kellyzly],
bq. how to compare the result of dpp work in the period of physical plan?
We can compare the DPP works the same way as we compare other works, i.e. if two works have the same operator tree and each operator has an equivalent counterpart, then the two works can be combined., {quote} The drawback is we'll lose some optimization opportunities - actually I'm not sure whether it's possible that two target map works share the same DPP in current implementation. {quote} As far as I know, this isn't possible. A DPP subtree can only be used to prune a single target {{MapWork}} - although that is something we want to change in HIVE-17178

{quote} Two DPP works can be considered equivalent as long as they output same records. {quote} I'm not sure how this would work, you don't know what a DPP work will output until the query actually starts to run.

I think a good fix here would to be just implement HIVE-17178 (I'm not sure, but this may be the same as HIVE-17877). If two DPP sinks are completely equivalent (same source table, filters, operations, etc.), but they only differ by the value of {{Target Work}}, then I think we should be able to combine them into a single DPP tree, with multiple target works. The value of the target work shouldn't change the value of the data that is written by a DPP subtree, so if the subtrees are equivalent, we can combine them. The main work will be to change the DPP code so that there can be multiple Target Works. , Hi [~stakiar], I meant we can compare DPP sink works the same way we compare other works. If two DPP works have the same operator tree, they will have the same output. I'll provide a PoC patch and more details in HIVE-17877., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12896851/HIVE-17193.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 17 failed/errored test(s), 11371 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dbtxnmgr_showlocks] (batchId=77)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[llap_text] (batchId=73)
org.apache.hadoop.hive.cli.TestContribNegativeCliDriver.org.apache.hadoop.hive.cli.TestContribNegativeCliDriver (batchId=240)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[unionDistinct_1] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning] (batchId=173)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=173)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=102)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[ct_noperm_loc] (batchId=94)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=111)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=206)
org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.testApplyPlanQpChanges (batchId=281)
org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.testReopen (batchId=281)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7742/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7742/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7742/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 17 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12896851 - PreCommit-HIVE-Build, Update to fix tests.
With the patch, two map works are considered targets of same DPP if:
# They share the same DPP sink operators.
# The DPP sink operators target the same columns of the map works.

Currently we don't combine sub trees within a base work, so this results in some suboptimal plans, and thus the update of the golden files. I think this is acceptable because correctness is more important. And it will be improved once we implement HIVE-17178., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12897542/HIVE-17193.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 11383 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dbtxnmgr_showlocks] (batchId=77)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[unionDistinct_1] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[ppd_union_view] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=102)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[ct_noperm_loc] (batchId=94)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=111)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=206)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7804/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7804/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7804/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12897542 - PreCommit-HIVE-Build, The test failures are not related.
[~kellyzly], [~stakiar], [~xuefuz] could you take a look? Thanks., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 43s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 48s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 19s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 40s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  6s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  9s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 15s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 48s{color} | {color:red} ql: The patch generated 1 new + 25 unchanged - 2 fixed = 26 total (was 27) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 15s{color} | {color:red} The patch generated 50 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 17m  2s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-10017/dev-support/hive-personality.sh |
| git revision | master / dc5a943 |
| Default Java | 1.8.0_111 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-10017/yetus/diff-checkstyle-ql.txt |
| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-10017/yetus/patch-asflicense-problems.txt |
| modules | C: itests ql U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-10017/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12917528/HIVE-17193.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 141 failed/errored test(s), 13580 tests executed
*Failed tests:*
{noformat}
TestBeeLineDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=252)
TestCopyUtils - did not produce a TEST-*.xml file (likely timed out) (batchId=230)
TestDbNotificationListener - did not produce a TEST-*.xml file (likely timed out) (batchId=246)
TestDummy - did not produce a TEST-*.xml file (likely timed out) (batchId=252)
TestExportImport - did not produce a TEST-*.xml file (likely timed out) (batchId=230)
TestHCatHiveCompatibility - did not produce a TEST-*.xml file (likely timed out) (batchId=246)
TestMiniDruidCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=252)
TestMiniDruidKafkaCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=252)
TestNegativeCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=95)
	[nopart_insert.q,insert_into_with_schema.q,input41.q,having1.q,create_table_failure3.q,default_constraint_invalid_default_value.q,database_drop_not_empty_restrict.q,windowing_after_orderby.q,orderbysortby.q,subquery_select_distinct2.q,authorization_uri_alterpart_loc.q,udf_last_day_error_1.q,constraint_duplicate_name.q,create_table_failure4.q,alter_tableprops_external_with_notnull_constraint.q,semijoin5.q,udf_format_number_wrong4.q,deletejar.q,exim_11_nonpart_noncompat_sorting.q,show_tables_bad_db2.q,drop_func_nonexistent.q,nopart_load.q,alter_table_non_partitioned_table_cascade.q,check_constraint_subquery.q,load_wrong_fileformat.q,check_constraint_udtf.q,lockneg_try_db_lock_conflict.q,udf_field_wrong_args_len.q,create_table_failure2.q,create_with_fk_constraints_enforced.q,groupby2_map_skew_multi_distinct.q,authorization_update_noupdatepriv.q,show_columns2.q,authorization_insert_noselectpriv.q,orc_replace_columns3_acid.q,compare_double_bigint.q,authorization_set_nonexistent_conf.q,alter_rename_partition_failure3.q,split_sample_wrong_format2.q,create_with_fk_pk_same_tab.q,compare_double_bigint_2.q,authorization_show_roles_no_admin.q,materialized_view_authorization_rebuild_no_grant.q,unionLimit.q,authorization_revoke_table_fail2.q,authorization_insert_noinspriv.q,duplicate_insert3.q,authorization_desc_table_nosel.q,stats_noscan_non_native.q,orc_change_serde_acid.q,create_or_replace_view7.q,exim_07_nonpart_noncompat_ifof.q,create_with_unique_constraints_enforced.q,udf_concat_ws_wrong2.q,fileformat_bad_class.q,merge_negative_2.q,exim_15_part_nonpart.q,authorization_not_owner_drop_view.q,external1.q,authorization_uri_insert.q,create_with_fk_wrong_ref.q,columnstats_tbllvl_incorrect_column.q,authorization_show_parts_nosel.q,authorization_not_owner_drop_tab.q,external2.q,authorization_deletejar.q,temp_table_create_like_partitions.q,udf_greatest_error_1.q,ptf_negative_AggrFuncsWithNoGBYNoPartDef.q,alter_view_as_select_not_exist.q,touch1.q,groupby3_map_skew_multi_distinct.q,insert_into_notnull_constraint.q,exchange_partition_neg_partition_missing.q,groupby_cube_multi_gby.q,columnstats_tbllvl.q,drop_invalid_constraint2.q,alter_table_add_partition.q,update_not_acid.q,archive5.q,alter_table_constraint_invalid_pk_col.q,ivyDownload.q,udf_instr_wrong_type.q,bad_sample_clause.q,authorization_not_owner_drop_tab2.q,authorization_alter_db_owner.q,show_columns1.q,orc_type_promotion3.q,create_view_failure8.q,strict_join.q,udf_add_months_error_1.q,groupby_cube2.q,groupby_cube1.q,groupby_rollup1.q,genericFileFormat.q,invalid_cast_from_binary_4.q,drop_invalid_constraint1.q,serde_regex.q,show_partitions1.q,check_constraint_nonboolean_expr.q,invalid_cast_from_binary_6.q,create_with_multi_pk_constraint.q,udf_field_wrong_type.q,groupby_grouping_sets4.q,groupby_grouping_sets3.q,insertsel_fail.q,udf_locate_wrong_type.q,orc_type_promotion1_acid.q,set_table_property.q,create_or_replace_view2.q,groupby_grouping_sets2.q,alter_view_failure.q,distinct_windowing_failure1.q,invalid_t_alter2.q,alter_table_constraint_invalid_fk_col1.q,invalid_varchar_length_2.q,authorization_show_grant_otheruser_alltabs.q,subquery_windowing_corr.q,compact_non_acid_table.q,authorization_view_4.q,authorization_disallow_transform.q,materialized_view_authorization_rebuild_other.q,authorization_fail_4.q,dbtxnmgr_nodblock.q,set_hiveconf_internal_variable1.q,input_part0_neg.q,udf_printf_wrong3.q,load_orc_negative2.q,druid_buckets.q,archive2.q,authorization_addjar.q,invalid_sum_syntax.q,insert_into_with_schema1.q,udf_add_months_error_2.q,dyn_part_max_per_node.q,authorization_revoke_table_fail1.q,udf_printf_wrong2.q,archive_multi3.q,udf_printf_wrong1.q,subquery_subquery_chain.q,authorization_view_disable_cbo_4.q,no_matching_udf.q,create_view_failure7.q,drop_native_udf.q,truncate_column_list_bucketing.q,authorization_uri_add_partition.q,authorization_view_disable_cbo_3.q,bad_exec_hooks.q,authorization_view_disable_cbo_2.q,fetchtask_ioexception.q,char_pad_convert_fail2.q,authorization_set_role_neg1.q,serde_regex3.q,authorization_delete_nodeletepriv.q,materialized_view_delete.q,create_or_replace_view6.q,bucket_mapjoin_wrong_table_metadata_2.q,udf_sort_array_by_wrong2.q,local_mapred_error_cache.q,alter_external_acid.q,mm_concatenate.q,authorization_fail_3.q,set_hiveconf_internal_variable0.q,udf_last_day_error_2.q,alter_table_constraint_invalid_ref.q,create_table_wrong_regex.q,describe_xpath4.q,join32.q,insert_sorted.q,describe_xpath2.q,authorization_role_grant_otheruser.q,masking_acid_merge.q,authorization_ctas.q,authorization_fail_5.q,alter_view_failure9.q,insert_into_acid_notnull.q,illegal_partition_type3.q,alter_table_constraint_invalid_pk_tbl.q,authorization_uri_import.q,database_drop_does_not_exist.q,date_literal3.q,archive_multi4.q,date_literal2.q,gby_star2.q,authorization_table_grant_nosuchrole.q,insert_into_with_schema2.q,join_cond_unqual_ambiguous_vc.q,archive_multi2.q,analyze1.q,invalid_distinct3.q,fs_default_name1.q,subquery_in_on.q,show_columns3.q,column_rename1.q,authorization_view_1.q,ptf_negative_JoinWithAmbigousAlias.q,groupby_rollup3.q,truncate_table_failure6.q,groupby_cube3.q,invalid_create_tbl1.q,illegal_partition_type.q,cachingprintstream.q,create_function_nonudf_class.q,exchange_partition_neg_table_missing2.q,dbtxnmgr_notablelock.q,create_view_failure1.q,create_view_failure2.q,alter_view_failure8.q,check_constraint_window_fun.q,update_notnull_constraint.q,authorization_drop_db_cascade.q,archive_partspec3.q,truncate_partition_column.q,alter_partition_partial_spec_dyndisabled.q,udf_format_number_wrong2.q,column_rename5.q,authorization_import.q,authorization_fail_2.q,script_error.q,archive_partspec5.q,script_broken_pipe2.q,update_no_such_table.q,exim_09_nonpart_noncompat_serdeparam.q,invalid_cast_from_binary_1.q,archive_partspec1.q,unionDistributeBy.q,drop_function_failure.q,authorization_priv_current_role_neg.q,archive_insert1.q,authorization_addpartition.q,archive_multi6.q,exim_05_nonpart_noncompat_coltype.q,druid_case.q,invalid_cast_to_binary_5.q,orderby_invalid_position.q,materialized_view_authorization_create_no_select_perm.q,exchange_partition_neg_with_fullacid_table.q,druid_address.q,delete_not_acid.q,temp_table_partitions.q,constraint_invalide_name.q,authorization_uri_load_data.q,udf_locate_wrong_args_len.q,duplicate_insert1.q,duplicate_insert2.q,udf_sort_array_by_wrong3.q,stats_publisher_error_2.q,show_tableproperties1.q,invalid_cast_to_binary_2.q,authorization_drop_admin_role.q,lockneg1.q,exim_16_part_noncompat_schema.q,database_switch_does_not_exist.q,ctas.q,exim_10_nonpart_noncompat_bucketing.q,unionOrderBy.q,addpart1.q,ptf_negative_NoWindowDefn.q,authorization_set_invalidconf.q,udtf_explode_not_supported3.q,ptf_negative_AmbiguousWindowDefn.q,create_external_with_check_constraint.q,udtf_invalid_place.q,join_cond_unqual_ambiguous.q,udf_format_number_wrong1.q,authorization_view_disable_cbo_6.q,exim_25_import_nonexist_authfail.q,authorization_role_cycles1.q,invalid_char_length_3.q,groupby_struct.q,join_alt_syntax_comma_on.q,exchange_partition_neg_incomplete_partition.q,udf_test_error_reduce.q,load_wrong_noof_part.q,authorization_export_ptn.q,drop_partition_failure.q,subquery_in_implicit_gby.q,udf_map_values_arg_num.q,udf_elt_wrong_args_len.q,alter_table_wrong_location.q,archive_insert4.q,authorization_grant_table_fail_nogrant.q,authorization_create_func1.q,dyn_part3.q,cte_with_in_subquery.q,column_change_skewedcol_type1.q,materialized_view_drop.q,selectDistinctStarNeg_2.q,exchange_partition_neg_with_mm_table.q,invalid_std_syntax.q,unset_view_property.q,authorization_view_3.q,subquery_exists_implicit_gby.q,authorization_set_role_neg2.q,authorization_grant_group.q,invalid_min_syntax.q,semijoin3.q,truncate_nonexistant_column.q,exchange_partition_neg_table_missing.q,gby_star.q,truncate_partition_column2.q,insertover_dynapart_ifnotexists.q,unionClusterBy.q,udf_qualified_name.q,udaf_invalid_place.q,spark_job_max_tasks.q,authorization_cannot_create_default_role.q,nonkey_groupby.q,spark_stage_max_tasks.q,ptf_negative_HavingLeadWithNoGBYNoWindowing.q,alter_view_as_select_with_partition.q,load_exist_part_authfail.q,archive_multi7.q,authorization_create_func2.q,authorization_grant_uri.q,line_terminator.q,load_view_failure.q,groupby_grouping_sets8.q,invalid_cast_from_binary_3.q,exim_21_part_managed_external.q,insert_into4.q,database_create_invalid_name.q,groupby_grouping_sets7.q,subq_insert.q,dyn_part2.q,alter_external_with_notnull_constraint.q,exchange_partition.q,lateral_view_join.q,allow_change_col_type_par_neg.q,create_function_nonexistent_db.q,create_function_nonexistent_class.q,authorization_not_owner_alter_tab_rename.q,strict_pruning.q,subquery_notexists_implicit_gby.q,orc_reorder_columns1.q,columnstats_partlvl_invalid_values.q,orc_reorder_columns2.q,authorization_dfs.q,udf_format_number_wrong7.q,exim_17_part_spec_underspec.q,druid_partitions.q,authorization_drop_role_no_admin.q,windowing_ll_no_over.q,subquery_corr_from.q,desc_failure2.q,load_non_native.q,windowing_ll_no_neg.q,authorization_role_grant2.q,lockneg4.q,lockneg3.q,drop_table_failure2.q,temp_table_authorize_create_tbl.q,dyn_part_max.q,orc_reorder_columns2_acid.q,change_hive_local_session_path.q,insert_into5.q,insert_into1.q,insert_into3.q,udf_in_2.q,udtf_explode_not_supported2.q,sample.q,udtf_explode_not_supported1.q,authorization_droppartition.q,orc_type_promotion2_acid.q,materialized_view_load.q,right_side_join.q,authorization_fail_1.q,authorization_cannot_create_all_role.q,invalid_max_syntax.q,udf_array_contains_wrong1.q,authorization_cannot_create_none_role.q,subquery_in_lhs.q,orc_replace_columns3.q,udf_size_wrong_args_len.q,create_skewed_table_dup_col_name.q,authorization_fail_7.q,authorization_invalid_priv_v1.q,invalidate_view1.q,union22.q,subquery_scalar_multi_columns.q,disallow_incompatible_type_change_on1.q,semijoin1.q,create_skewed_table_failure_invalid_col_name.q,udf_when_type_wrong.q,timestamp_literal.q,create_external_with_default_constraint.q,truncate_table_failure4.q,masking_acid_delete.q,check_constraint_violation.q,uniquejoin2.q,authorization_grant_table_dup.q,invalid_tbl_name.q,authorization_createview.q,alter_external_with_default_constraint.q,truncate_table_failure1.q,alter_partition_coltype_invalidtype.q,show_tablestatus_not_existing_part.q,authorization_msck.q,truncate_table_failure2.q,joinneg.q]
TestNonCatCallsWithCatalog - did not produce a TEST-*.xml file (likely timed out) (batchId=216)
TestReplicationOnHDFSEncryptedZones - did not produce a TEST-*.xml file (likely timed out) (batchId=230)
TestReplicationScenarios - did not produce a TEST-*.xml file (likely timed out) (batchId=230)
TestReplicationScenariosAcidTables - did not produce a TEST-*.xml file (likely timed out) (batchId=230)
TestReplicationScenariosAcrossInstances - did not produce a TEST-*.xml file (likely timed out) (batchId=230)
TestSequenceFileReadWrite - did not produce a TEST-*.xml file (likely timed out) (batchId=246)
TestTezPerfCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=252)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_table_stats] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[statsoptimizer] (batchId=62)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=174)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[groupby_groupingset_bug] (batchId=174)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_main] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[update_access_time_non_current_db] (batchId=171)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorization_div0] (batchId=170)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorized_dynamic_semijoin_reduction] (batchId=155)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=105)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.org.apache.hadoop.hive.cli.TestNegativeCliDriver (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[alter_notnull_constraint_violation] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_multi_into_notnull] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_overwrite_notnull_constraint] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[smb_bucketmapjoin] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[smb_mapjoin_14] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[sortmerge_mapjoin_mismatch_1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_aggregator_error_1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_aggregator_error_2] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_publisher_error_1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_corr_in_agg] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_notin_implicit_gby] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_bucketed_column] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_column_seqfile] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_map_values_arg_type] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_max] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_min] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_next_day_error_1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_printf_wrong4] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_reflect_neg] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_sort_array_by_wrong1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_sort_array_wrong1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_sort_array_wrong2] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_sort_array_wrong3] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_test_error] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_trunc_error1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_trunc_error2] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_trunc_error3] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_explode_not_supported4] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_not_supported1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_not_supported3] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[union2] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionSortBy] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[uniquejoin3] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[uniquejoin] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unset_table_property] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[updateBasicStats] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_bucket_col] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_non_acid_table] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_partition_col] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[view_update] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[windowing_invalid_udaf] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[windowing_leadlag_in_udaf] (batchId=96)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_scalar] (batchId=125)
org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.org.apache.hadoop.hive.cli.TestSparkPerfCliDriver (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query11] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query15] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query16] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query18] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query19] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query21] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query24] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query25] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query29] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query30] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query32] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query34] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query35] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query37] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query40] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query44] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query45] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query46] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query47] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query48] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query4] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query50] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query53] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query54] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query57] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query58] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query5] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query61] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query63] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query65] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query66] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query67] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query68] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query6] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query72] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query73] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query75] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query76] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query77] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query78] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query79] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query80] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query81] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query82] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query83] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query85] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query88] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query89] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query8] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query90] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query91] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query92] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query94] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query95] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query97] (batchId=254)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query99] (batchId=254)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=224)
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testMetastoreVersion (batchId=226)
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMatching (batchId=226)
org.apache.hadoop.hive.metastore.client.TestAppendPartitions.testAppendPartitionEmptyPartValues[Embedded] (batchId=210)
org.apache.hadoop.hive.metastore.client.TestAppendPartitions.testAppendPartitionEmptyPartValues[Remote] (batchId=210)
org.apache.hadoop.hive.metastore.client.TestAppendPartitions.testAppendPartitionNullPartValues[Embedded] (batchId=210)
org.apache.hadoop.hive.metastore.client.TestAppendPartitions.testAppendPartitionNullPartValues[Remote] (batchId=210)
org.apache.hadoop.hive.ql.TestAcidOnTez.testGetSplitsLocks (batchId=227)
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1 (batchId=231)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.testLlapInputFormatEndToEnd (batchId=237)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/10017/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/10017/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-10017/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 141 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12917528 - PreCommit-HIVE-Build, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 46s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 41s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 18s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 44s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  2s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  9s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 14s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 40s{color} | {color:red} ql: The patch generated 1 new + 25 unchanged - 2 fixed = 26 total (was 27) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 15s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 17m  2s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-10303/dev-support/hive-personality.sh |
| git revision | master / 760d472 |
| Default Java | 1.8.0_111 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-10303/yetus/diff-checkstyle-ql.txt |
| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-10303/yetus/patch-asflicense-problems.txt |
| modules | C: itests ql U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-10303/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12919424/HIVE-17193.4.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 56 failed/errored test(s), 14288 tests executed
*Failed tests:*
{noformat}
TestNonCatCallsWithCatalog - did not produce a TEST-*.xml file (likely timed out) (batchId=217)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[llap_smb] (batchId=92)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_vectorization_0] (batchId=17)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[results_cache_invalidation2] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[tez_join_hash] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[windowing_udaf] (batchId=68)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[results_cache_invalidation2] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_1] (batchId=171)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=105)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[alter_notnull_constraint_violation] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[avro_non_nullable_union] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[cachingprintstream] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[check_constraint_violation] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[compute_stats_long] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[default_constraint_invalid_default_value_type] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[dyn_part3] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[dyn_part_max_per_node] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[dynamic_partitions_with_whitelist] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_into_acid_notnull] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_into_notnull_constraint] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_multi_into_notnull] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_overwrite_notnull_constraint] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insertsel_fail] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[merge_constraint_notnull] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[script_broken_pipe2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[script_broken_pipe3] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[script_error] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[serde_regex2] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_aggregator_error_2] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_publisher_error_1] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_publisher_error_2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_corr_in_agg] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_in_implicit_gby] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_notin_implicit_gby] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_scalar_corr_multi_rows] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_scalar_multi_rows] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_assert_true2] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_assert_true] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_reflect_neg] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_test_error] (batchId=96)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_test_error_reduce] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_notnull_constraint] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[cluster_tasklog_retrieval] (batchId=98)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[local_mapred_error_cache] (batchId=98)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[mapreduce_stack_trace] (batchId=98)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[mapreduce_stack_trace_turnoff] (batchId=98)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[minimr_broken_pipe] (batchId=98)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=225)
org.apache.hadoop.hive.ql.TestAcidOnTez.testAcidInsertWithRemoveUnion (batchId=228)
org.apache.hadoop.hive.ql.TestAcidOnTez.testCtasTezUnion (batchId=228)
org.apache.hadoop.hive.ql.TestAcidOnTez.testNonStandardConversion01 (batchId=228)
org.apache.hadoop.hive.ql.TestAutoPurgeTables.testExternalNoAutoPurge (batchId=233)
org.apache.hadoop.hive.ql.TestAutoPurgeTables.testTruncateInvalidAutoPurge (batchId=233)
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1 (batchId=232)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/10303/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/10303/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-10303/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 56 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12919424 - PreCommit-HIVE-Build, The failures are not related. [~stakiar], could you take a look at the latest patch? Thanks., +1, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 50s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 34s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 14s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 43s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  4s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  9s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 15s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 42s{color} | {color:red} ql: The patch generated 1 new + 25 unchanged - 2 fixed = 26 total (was 27) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 14s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 16m 40s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-10474/dev-support/hive-personality.sh |
| git revision | master / 63923e7 |
| Default Java | 1.8.0_111 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-10474/yetus/diff-checkstyle-ql.txt |
| modules | C: itests ql U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-10474/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12920422/HIVE-17193.5.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 44 failed/errored test(s), 14293 tests executed
*Failed tests:*
{noformat}
TestMinimrCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=93)
	[infer_bucket_sort_num_buckets.q,infer_bucket_sort_reducers_power_two.q,parallel_orderby.q,bucket_num_reducers_acid.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_merge.q,root_dir_external_table.q,infer_bucket_sort_dyn_part.q,udf_using.q,bucket_num_reducers_acid2.q]
TestNonCatCallsWithCatalog - did not produce a TEST-*.xml file (likely timed out) (batchId=217)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_nullscan] (batchId=68)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_table_stats] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[autoColumnStats_4] (batchId=13)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_vectorization_0] (batchId=17)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[row__id] (batchId=80)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[acid_bucket_pruning] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[acid_vectorization_original] (batchId=173)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[check_constraint] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[default_constraint] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[dynpart_sort_optimization_acid] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[enforce_constraint_notnull] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[materialized_view_create_rewrite_4] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[materialized_view_create_rewrite_5] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_stats] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[acid_vectorization_original_tez] (batchId=106)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=105)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[cluster_tasklog_retrieval] (batchId=98)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[mapreduce_stack_trace] (batchId=98)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[mapreduce_stack_trace_turnoff] (batchId=98)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.testCliDriver[minimr_broken_pipe] (batchId=98)
org.apache.hadoop.hive.ql.TestAcidOnTez.testAcidInsertWithRemoveUnion (batchId=228)
org.apache.hadoop.hive.ql.TestAcidOnTez.testCtasTezUnion (batchId=228)
org.apache.hadoop.hive.ql.TestAcidOnTez.testNonStandardConversion01 (batchId=228)
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1 (batchId=232)
org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testWriteSetTracking3 (batchId=300)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgress (batchId=235)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=235)
org.apache.hive.jdbc.TestSSL.testSSLFetchHttp (batchId=239)
org.apache.hive.minikdc.TestJdbcWithDBTokenStore.testTokenAuth (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testCancelRenewTokenFlow (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testConnection (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testIsValid (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testIsValidNeg (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testNegativeProxyAuth (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testNegativeTokenAuth (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testProxyAuth (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testRenewDelegationToken (batchId=254)
org.apache.hive.minikdc.TestJdbcWithDBTokenStoreNoDoAs.testTokenAuth (batchId=254)
org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.testCookieNegative (batchId=254)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/10474/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/10474/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-10474/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 44 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12920422 - PreCommit-HIVE-Build, Pushed to master. Thanks for the review, [~stakiar].]