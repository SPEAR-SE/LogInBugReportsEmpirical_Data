[This patch implements the rules from PARQUET-113, which required some restructuring of the existing converters. The included TestArrayCompatibility tests will run on trunk and can be used to verify that the current array representation has not been changed and to see the current behavior for Avro, Thrift, and repeated types without annotations.

This patch has the following behavior consequences:
1. Avro and Thrift data structures that could be read previously will match the original Avro or Thrift type. This is the case when Avro stored, for example, a {{array<struct<f1: int>>}}. This structure matched Hive's 3-level representation of arrays, so it could be read, although the inner Avro record level was discarded by the SerDe and the type in Hive would be {{array<int>}}.
2. Lists must be annotated with {{LIST}} and maps with {{MAP}}. This was assumed by the previous version. This is a safe change because all Parquet object models have correctly used these annotations.
3. Repeated groups with 3 or more fields and repeated primitive types are now supported.

The Hive SerDe expects an extra {{ArrayWritable}} layer from the Parquet {{Converter}}. This expectation has been preserved and all list and map structures artificially include it so that the SerDe doesn't need to be changed. This should be done as a follow-up issue., [~spena], [~brocknoland], and [~mickaellcr], I think you will all be interested in this issue, which fixes Hive with data written by other formats., Excellent work! In terms of ordering this patch and HIVE-8359, I assume that HIVE-8359 should go in first?, Yes, HIVE-8359 should go in first. This should also be followed up with a patch that implements the write rules for PARQUET-113, which is really just renaming the fields that Hive currently produces. I'm also particularly interested in thorough testing for this, but I haven't done much Hive testing myself. That's why this includes unit tests but not HQL tests. If someone could help me out with that to make sure this doesn't cause regressions, I would be grateful!, [~rdblue], is this ticket related to the different nested types found on this document?
https://github.com/rdblue/incubator-parquet-format/blob/PARQUET-113-add-list-and-map-spec/LogicalTypes.md, Yes. It implements the rules for reading lists in existing data:

1. If the repeated field is not a group, then its type is the element type and elements are required.
2. If the repeated field is a group with multiple fields, then its type is the element type and elements are required.
3. If the repeated field is a group with one field and is named either "array" or uses the LIST-annotated group's name with "_tuple" appended then the repeated type is the element type and elements are required.
4. Otherwise, the repeated field's type is the element type with the repeated field's repetition.

It also structures the converters to match the other projects. LIST and MAP will use ElementConverter and KeyValueConverter and the list version supports these rules while matching the ArrayWritable structure expected by the SerDe (confirmed by tests that pass in both trunk and this patch).

Repeated groups that aren't annotated are deserialized into lists as before, but I changed this to put less work on the DataWritableGroupConverter that is now called StructConverter. Struct needs to support repeated inner groups, but rather than keeping a second array of objects, it passes its start() and end() calls to the repeated children converters, which use them to add the correct object to the struct. It's an easier-to-follow method that produces the same result. (By all means, please verify this!), FYI - I think this patch will need a rebase post HIVE-6914. Additionally once ready, please click {{Submit Patch}} to have the patch tested., Rebased patch on Sergio's changes. This didn't conflict except for the change to ArrayWritableGroupConverter, which was removed (so any change would conflict)., Reuploading patch with correct name., [~rdblue]

The are a few parquet query tests in the following path:
ql/src/test/queries/clientpositive/parquet_*.q
ql/src/test/results/clientpositive/parquet_*.q.out

The data files that are used by those queries tests are here (just read the *.q file to know which one is used):
data/files/*, After adding a .q file you generate the .q.out file via:

{noformat}
cd itests
mvn test -Dtest=TestCliDriver -Dqfile=my_parquet.q -Dtest.output.overwrite=true -Phadoop-2
{noformat}

https://cwiki.apache.org/confluence/display/Hive/HiveDeveloperFAQ#HiveDeveloperFAQ-HowdoIupdatetheoutputofaCliDrivertestcase?, I'm attaching a tarball with all of the files from the array compatibility tests. These can be used to add the .q tests., Updated patch with q tests for all of the cases in TestArrayCompatibility. I also added a q test that copies nested_complex.q that produces the same results., 
parquet_array_null_element
{noformat}
Caused by: parquet.io.ParquetDecodingException: Can not read value at 0 in block 0 in file pfile:/Users/noland/workspaces/hive-apache/hive/itests/qtest/target/warehouse/parquet_array_null_element/000000_0
  at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)
  at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:204)
  at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:102)
  at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:71)
  at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:71)
  at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:504)
  at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:594)
  ... 29 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
  at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.set(HiveStructConverter.java:96)
  at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter.end(HiveCollectionConverter.java:65)
  at parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:410)
  at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:194)
  ... 35 more
{noformat}, Not sure which test this is from:

{noformat}
Caused by: parquet.io.ParquetDecodingException: Can not read value at 0 in block 0 in file pfile:/Users/noland/workspaces/hive-apache/hive/itests/qtest/target/warehouse/parquet_jointable2/000000_0
  at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)
  at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:204)
  at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:102)
  at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:71)
  at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:71)
  at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:65)
  ... 16 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
  at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.set(HiveStructConverter.java:96)
  at org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$BinaryConverter.addBinary(ETypeConverter.java:219)
  at parquet.column.impl.ColumnReaderImpl$2$6.writeValue(ColumnReaderImpl.java:306)
  at parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:353)
  at parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:402)
  at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:194)
  ... 21 more
{noformat}

All failed parquet tests with the patch:
{noformat}
  <testcase name="testCliDriver_parquet_array_null_element" classname="org.apache.hadoop.hive.cli.TestCliDriver" time="4.945">
  <testcase name="testCliDriver_parquet_create" classname="org.apache.hadoop.hive.cli.TestCliDriver" time="4.416">
  <testcase name="testCliDriver_parquet_decimal" classname="org.apache.hadoop.hive.cli.TestCliDriver" time="5.478">
  <testcase name="testCliDriver_parquet_join" classname="org.apache.hadoop.hive.cli.TestCliDriver" time="8.928">
  <testcase name="testCliDriver_parquet_types" classname="org.apache.hadoop.hive.cli.TestCliDriver" time="4.094">
{noformat}, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12682794/HIVE-8909.3.patch

{color:red}ERROR:{color} -1 due to 18 failed/errored test(s), 6687 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_null_element
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_multi_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_optional_elements
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_required_elements
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_single_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_structs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_unannotated_groups
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_array_of_unannotated_primitives
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_avro_array_of_primitives
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_avro_array_of_single_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_create
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_thrift_array_of_primitives
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_thrift_array_of_single_field_struct
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_types
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hive.hcatalog.streaming.TestStreaming.testRemainingTransactions
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1856/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1856/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1856/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 18 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12682794 - PreCommit-HIVE-TRUNK-Build, I don't believe the test infra can unwrap the binary files since it uses svn. We'll have to verify t g em locally. 
, Subversion can handle binaries if they're checked in though. Could we add the test files and update the patch so they aren't included?, Typically we just run the tests manually but I don't see harm in checking in the binary files now and I just did so. Thus we won't want to submit them on the patch which fixes the failures I mentioned earlier., Hey [~rdblue]

Could you post the patch in the review board?, Adding patch without the data files., Looks good [~rdblue]. I run the tests locally and they're running correctly as well.
+1, I am still seeing the regressions mentioned here:

https://issues.apache.org/jira/browse/HIVE-8909?focusedCommentId=14220487&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14220487

[~spena] do those tests fail for you?, They didn't

{noformat}
mvn test -Phadoop-2 -Dtest=TestCliDriver -Dqfile=parquet_array_of_optional_elements.q,parquet_array_of_required_elements.q,parquet_array_of_single_field_struct.q,parquet_array_of_structs.q,parquet_array_of_unannotated_groups.q,parquet_array_of_unannotated_primitives.q,parquet_avro_array_of_primitives.q,parquet_avro_array_of_single_field_struct.q,parquet_nested_complex.q,parquet_thrift_array_of_primitives.q,parquet_thrift_array_of_single_field_struct.q

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.hive.cli.TestCliDriver
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 80.949 sec - in org.apache.hadoop.hive.cli.TestCliDriver

Results :

Tests run: 12, Failures: 0, Errors: 0, Skipped: 0
{noformat}

I run the first two tests (parquet_array_null_element,parquet_array_of_multi_field_struct) manually before running the 12 tests, and they passed., Fixed projection bug and added tests for map structures., Fixed typo in new map tests (thanks Sergio!), +1 again :), +1

thank you!!, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12682975/HIVE-8909.6.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6681 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1875/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1875/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1875/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12682975 - PreCommit-HIVE-TRUNK-Build, Thank you so much!! I have committed this to trunk!]