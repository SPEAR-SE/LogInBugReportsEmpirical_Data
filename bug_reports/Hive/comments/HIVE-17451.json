[Hexidecimal 0x138 = Decimal 312 (unscaled)
and, hexidecimal bytes 01 and 38 are a non-visible control character \001 and ASCII "8", I believe Avro stores the decimal as the serialization of a Java BigInteger.  That is, we call BigInteger's toByteArray function that returns a byte array with two's compliment reprentation.  Why do we use BigInteger?  Because decimal's max precion of 38 digits cannot be stored in a Java 64 bit signed long.  128 bits are needed.  I believe Spark represents decimal with precision > 18 as Java BigDecimal.  BigDecimal.unscaledValue() returns a BigInteger.

So logicalType decimal and stored as bytes makes sense.

Does specifying [col1:decimal(20,2)] in the data frame detail not work?, Seems like avro-tools.jar tojson isn't converting the binary (physical type) to decimal (logical type)., even in toText option : binary is displayed.

 while (fileReader.hasNext()) {
          ByteBuffer outBuff = (ByteBuffer) fileReader.next();
          outStream.write(outBuff.array());
         outStream.write(LINE_SEPARATOR); 
    }
     fileReader.close();, bq. Does specifying [col1:decimal(20,2)] in the data frame detail not work?

cannot convert from Bytes to Decimal in DF:

scala> df.withColumn("col2", 'col1.cast("decimal(20,2)")).select("col2").show()
org.apache.spark.sql.AnalysisException: cannot resolve 'cast(col1 as decimal(20,2))' due to data type mismatch: cannot cast BinaryType to DecimalType(20,2);
]