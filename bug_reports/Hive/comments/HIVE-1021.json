[When I ran the above query it didn't return any result which was expected.
{code}
hive> create table src (key string, value string);    
load data local inpath '../../../../data/files/kv1.txt' into table src;
hive> select k.key, v.value from src k join src v on k.key=v.key and k.value=v.value+1;               
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111012116_0002, Tracking URL = http://localhost:50030/jobdetails.jsp?jobid=job_201111012116_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2011-11-02 18:01:15,591 Stage-1 map = 0%,  reduce = 0%
2011-11-02 18:01:21,643 Stage-1 map = 100%,  reduce = 0%
2011-11-02 18:01:37,007 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201111012116_0002
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   HDFS Read: 6019 HDFS Write: 0 SUCESS
Total MapReduce CPU Time Spent: 0 msec
OK
Time taken: 39.108 seconds

{code}]