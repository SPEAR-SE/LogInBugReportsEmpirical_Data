[Try running this by disabling the FetchTask optimizer - {{set hive.fetch.task.conversion=none;}}, [~chillon_m]: that looks like a simple new line error (the conversion=none should trigger it for both cases).

If turning off FetchTask reproduces the issue for both cases, then {{set hive.query.result.fileformat= SequenceFile;}} (default in hive-2.1.0). , [bigdata@namenode hive-1.2.1]$ bin/beeline  -u jdbc:hive2://namenode:10000/default bigdata -n bigdata
Connecting to jdbc:hive2://namenode:10000/default
Connected to: Apache Hive (version 1.2.1)
Driver: Hive JDBC (version 1.2.1)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.2.1 by Apache Hive
0: jdbc:hive2://namenode:10000/default> set hive.fetch.task.conversion=none;
No rows affected (0.051 seconds)
0: jdbc:hive2://namenode:10000/default> with temp as (select msgType as  type,id,msgData from messages where Num='41433141' and erNum='99841977') 
0: jdbc:hive2://namenode:10000/default> select * from temp where id=163437 order by id;
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
WARN  : Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1456383638304_0009
INFO  : The url to track the job: http://namenode:8088/proxy/application_1456383638304_0009/
INFO  : Starting Job = job_1456383638304_0009, Tracking URL = http://namenode:8088/proxy/application_1456383638304_0009/
INFO  : Kill Command = /home/bigdata/hadoop-runtime/hadoop-2.5.2/bin/hadoop job  -kill job_1456383638304_0009
INFO  : Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0
INFO  : 2016-02-26 12:22:21,928 Stage-1 map = 0%,  reduce = 0%
INFO  : 2016-02-26 12:22:29,178 Stage-1 map = 100%,  reduce = 0%
INFO  : 2016-02-26 12:22:32,269 Stage-1 map = 100%,  reduce = 100%
INFO  : Ended Job = job_1456383638304_0009
+------------+----------+---------------+--+
| temp.type  | temp.id  | temp.msgdata  |
+------------+----------+---------------+--+
| -1000      | 163437   |   we come:      |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
| NULL       | NULL     | NULL          |
+------------+----------+---------------+--+
11 rows selected (15.594 seconds), FYI, what I meant was that without that optimization, even the withOrderBy will fail., set hive.query.result.fileformat = SequenceFile; do well.
thanks.
, it is a simple new line error when hive.query.result.fileformat=TextFile,it can't fix it.suggest set hive.query.result.fileformat = SequenceFile,it will ok.]