[Looking through code, this is not directly a HCat issue as much as it is an Orc issue. It looks like OrcOutputFormat only instantiates the appropriate compression override if instantiated from within getHiveRecordWriter as opposed to getRecordWriter, so this support would need to be added to orc to make it work from outside hive.

{code}
100   public RecordWriter<NullWritable, OrcSerdeRow>
101       getRecordWriter(FileSystem fileSystem, JobConf conf, String name,
102                       Progressable reporter) throws IOException {
103     return new
104       OrcRecordWriter(new Path(name), OrcFile.writerOptions(conf));
105   }
{code}

versus:

{code}
108   public FileSinkOperator.RecordWriter
109      getHiveRecordWriter(JobConf conf,
110                          Path path,
111                          Class<? extends Writable> valueClass,
112                          boolean isCompressed,
113                          Properties tableProperties,
114                          Progressable reporter) throws IOException {
115     OrcFile.WriterOptions options = OrcFile.writerOptions(conf);
116     if (tableProperties.containsKey(OrcFile.STRIPE_SIZE)) {
117       options.stripeSize(Long.parseLong
118                            (tableProperties.getProperty(OrcFile.STRIPE_SIZE)));
119     }
120 
121     if (tableProperties.containsKey(OrcFile.COMPRESSION)) {
122       options.compress(CompressionKind.valueOf
123                            (tableProperties.getProperty(OrcFile.COMPRESSION)));
124     }
125 
126     if (tableProperties.containsKey(OrcFile.COMPRESSION_BLOCK_SIZE)) {
127       options.bufferSize(Integer.parseInt
128                          (tableProperties.getProperty
129                             (OrcFile.COMPRESSION_BLOCK_SIZE)));
130     }
131 
132     if (tableProperties.containsKey(OrcFile.ROW_INDEX_STRIDE)) {
133       options.rowIndexStride(Integer.parseInt
134                              (tableProperties.getProperty
135                               (OrcFile.ROW_INDEX_STRIDE)));
136     }
137 
138     if (tableProperties.containsKey(OrcFile.ENABLE_INDEXES)) {
139       if ("false".equals(tableProperties.getProperty
140                          (OrcFile.ENABLE_INDEXES))) {
141         options.rowIndexStride(0);
142       }
143     }
144 
145     if (tableProperties.containsKey(OrcFile.BLOCK_PADDING)) {
146       options.blockPadding(Boolean.parseBoolean
147                            (tableProperties.getProperty
148                             (OrcFile.BLOCK_PADDING)));
149     }
150 
151     return new OrcRecordWriter(path, options);
152   }
{code}

The fix can be a simple (but not trivial) fix, as a change to OrcFile.writerOptions, to also take in overrides from conf., Attaching patch.

Changed Orc's RecordWriter instantiation to fall back to checking jobConf if the orc-specific properties are not present in the provided table properties. This way, users of OrcOutputFormat other than hive have a way of passing these parameters, such as the compression specification to orc.

In addition, changed HCat's FileOutputFormatContainer to have a way of special-casing instantiation. We already had one RCFile-specific instantiation, that is now refactored out to a separate class intended to be a collection of special cases, and added in the ability to copy orc-specific table properties from the TableDesc to job properties (which will then later be copied to jobconf by hive) so as pass on these parameters to orc.
, Testing related note : This bug is interesting in that hive as well as pig are able to read data irrespective of what compression format was actually used. i.e., the bug is that when we write to a compressed orc table while specifying a compression of SNAPPY,say, pig using HCat will write out the table using the default orc compression, which is ZLIB, irrespective of what the metadata indicates. This, however, is not a problem for hive in that the end data is still readable via hive and hcatalog/pig, so we don't get a read error. The read error occurs when external tools that are expecting the file to be snappy-compressed find that it is actually zlib compressed. It can also be a performance/size issue if snappy is desired over zlib, but we still retain zlib. Thus, testing by virtue of readability/non-readability or by way of checking for errors is not possible here.

Instead, to test, end-to-end tests are the way to go here, and I've done the following for this:

a) Create table using hive -e, specifying orc.compress=SNAPPY
b) use pig -useHCatalog, and write to the aforesaid table.
c) use hive --service orcfiledump on the file inside the table, it will show what compression format it sees. Without this patch, it indicates ZLIB, and with it, it indicates SNAPPY.

In addition, no other previous tests fail (there are no regressions), [~thejas]/[~prasanth_j] , could I bug either of you for a review of this?
, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12628073/HIVE-5504.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5086 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_auto_sortmerge_join_16
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1276/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1276/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12628073, The error reported by the precommit test seems to be unrelated to this fix., Attached reviewboard link : https://reviews.apache.org/r/18085/, Hrm, HIVE-5728 just hit, we should check if it conflicts with this patch (very likely)., Yeah, some parameters that were depended upon in OrcFile moved to HiveConf, so at the very least, this patch requires regeneration.

One good thing at least is that OrcOutputFormat itself did not change, so that is still an easy merge.

The one weird thing is that HIVE-5728 introduces logic to override the default behaviour using hive-site.xml based parameters, and removes certain default parameters from OrcFile. We need to change this patch to respect that.
, (canceling patch till I can regenerate), Actually, reading further, the defaults are taken care of in the WriterOptions constructor, and thus, not a problem for this patch. And the parameters names are still in OrcFile, it's only the default values that were moved to HiveConf. This patch works as-is, and is compatible with HIVE-5728 and will respect the new defaults behaviour as well. Resetting the patch as available., [~thejas]/[~ashutoshc] , could I please get a review of this patch?
, +1 on hcat part., [~thejas], could I please get a review of this jira? (Daniel has reviewed the hcat part of it, but this still needs hive-side verification)., Updated patch per reviewboard comments., +1, Re-canceling patch and resubmitting so that precommit tests pick it up, now that it's running again., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12631095/HIVE-5504.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 5218 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1604/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1604/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12631095, Looks like the tests finally ran! :)

And the error reported by the pre-commit tests do not seem related to this patch - the table in question that's failing on that test is not even created as an orc table., Patch committed to trunk.
Thanks for the contribution [~sushanth]]