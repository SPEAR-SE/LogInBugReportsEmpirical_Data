[[HIVE-17128] prevented file descriptors leaks by closing the log4j Appender used for Operation Logs.
Sometimes logging is attempted even after the Operation is closed.
The current appender, a RandomAccessFileAppender, will continue to try to
write to its log file even after it has been stopped.
To fix this, create a new class, HushableMutableRandomAccessAppender based on log4j's RandomAccessFileAppender.
https://github.com/apache/logging-log4j2/blob/master/log4j-core/src/main/java/org/apache/logging/log4j/core/appender/RandomAccessFileAppender.java
Unfortunately that class is final and hard to extend by delegation so the code is copied here.
The only substantive change is that a stopped HushableMutableRandomAccessAppender will no longer append
after it has been stopped.
{noformat}
  if (isStopped()) {
    // Don't try to log anything when appender is stopped
    return;
  }
{noformat}
Make log4j OperationLogging use the CloseableRandomAccessFileAppender
Add a test that writes to the appender after it has been stopped.
Add a minimal LogEvent implementation for testing, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12892871/HIVE-17826.1.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 11280 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[optimize_nullscan] (batchId=163)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_notin] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_scalar] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_select] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_views] (batchId=108)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query16] (batchId=243)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query94] (batchId=243)
org.apache.hadoop.hive.cli.TestTezPerfCliDriver.testCliDriver[query14] (batchId=241)
org.apache.hadoop.hive.cli.TestTezPerfCliDriver.testCliDriver[query16] (batchId=241)
org.apache.hadoop.hive.cli.TestTezPerfCliDriver.testCliDriver[query94] (batchId=241)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=204)
org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.testTriggerHighShuffleBytes (batchId=229)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7366/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7366/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7366/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12892871 - PreCommit-HIVE-Build, Test failures appear unrelated to my change.
[~aihuaxu] please can you review?, [~asherman] Thanks for working on it.

The idea sounds good. I'm wondering if we could also add delaying the stop() (e.g. for 5 seconds) so we will not miss any outputs typically? Since if the clients miss the output, it could be an issue for the client especially for a script. , Thanks [~aihuaxu] I did think about that but I am not sure how to do it in a simple and clean way.   cleanupOperationLog() is called from Operation.closeO so adding a delay inline will prevent the session from terminating which seems weird. And doing it asynchronously makes it more complicated. 

But as we just discussed IRL I will think about it some more., I looked into an alternative solution which is to use an [IdlePurgePolicy|https://logging.apache.org/log4j/2.x/manual/appenders.html]. This can be inserted into log4j when the RoutingAppender Is created in LogDivertAppender. The IdlePurgePolicy works by scheduling a thread to run at a configurable interval. When the thread runs it checks if any of the RoutingAppenderâ€™s sub-Appenders have been idle for more than a configurable time. Any that are found are stopped and the AppenderControl  is removed. 
I was able to use this instead of LogUtils.stopQueryAppender() to cause OperationLogs to close, providing an alternative mechanism for avoiding the file descriptor leak fixed in  [HIVE-17128].
 
The problem I see is that an IdlePurgePolicy may prematurely close the log for a long running operation if the operation is not logging. I experimented to see what happens when logging with a particular key restarts after being closed by IdlePurgePolicy. The good thing is that the logging does succeed but the bad thing is that the second log file appears to overwrite the original log file.

So I think that the original fix I proposed may be simpler and safer.

Edited to add: we may also need [HIVE-17373] to be completed in order to get bug fixes to  log4j
, What do you think [~aihuaxu] ?, [~asherman] Make sense. Let's just take your original fix then. +1. , Thanks [~aihuaxu], do we need more review? If not can you push this when convenient?, Pushed to master. Thanks [~asherman] for the work., Thank you for the review and push, This jira is resolved and released with Hive 3.0 If you find an issue with it, please create a new jira.]