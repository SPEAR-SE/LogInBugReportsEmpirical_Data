[[~gopalv] [~hagleitn] fyi, WIP patch. Currently stuck on the error where agent tarball is not found during create. Seems like there's no good way to find it from Hive; the API just assumes it's in the lib directory
{noformat}
Caused by: java.io.FileNotFoundException: File /grid/5/sershe/tez-autobuild/dist/hive/lib/slider-agent.tar.gz does not exist
   at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)
   at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:819)
   at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:596)
   at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
   at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)
   at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1965)
   at org.apache.slider.providers.ProviderUtils.addAgentTar(ProviderUtils.java:128)
   at org.apache.slider.providers.agent.AgentClientProvider.prepareAMAndConfigForLaunch(AgentClientProvider.java:270){noformat}, I think for now client will have to specify slider home. Or we may include a workaround like the one used to find spark speculatively in bin/hive, This changes LlapServiceDriver to have 2 modes - one (not default, triggered by --doNotStart argument) is the old one, the other starts LLAP cluster immediately after package generation using slider APIs (SLIDER_HOME may need to be supplied, see the patch).

[~gopalv] [~sseth] can you take a look?

RB at https://reviews.apache.org/r/56108/, I don't think start should be the default. That breaks existing behaviour which a lot of people are likely used to.
Thought we'd be doing this by continuing to generate a run.sh - which invokes a single java binary (instead of the 4 slider commands), accepting parameters for whether a destroy is required etc. That still saves most of the time, and retains compatibility. Having a package sit around which can be invoked multiple times is really useful while debugging. Also serves as a history for what was started (think this may still be the case - except without the run.sh). Most of the code would remain unchanged - except for the run.sh generation which points to the slider command or the new Java launch., run.sh is still generated, and could be used to restart the cluster. I can change the default behavior though. Having an extra JVM still has a lot of overhead given how big Hive classpath, configs, etc. is, and most of the time people want to start the cluster immediately after generating the package., Changed the default, If I'm reading this correctly, when the "startImmediately" option is specified, the following happens
1) Normal processing in the JavaServiceDriver
2) This then invokes package.py with a specific OUTPU_DIR, and invokes the start.
3) The java processing falls off, and we run package.py again - which generates the same contents as what we do today.

Is that correct?

On the patch itself
{code}
scriptArgs.add(OUTPUT_DIR_PREFIX + version);
{code}
Should this be to a temporary directory?

Move startCluster into SliderUtils?

{code}
if (rc == 0) {
          startCluster(options.getName(), version,
              HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));
        }
{code}
Some error checking  / message returned from calling python would be useful in case this fails.


Even if we run via this, was hoping we'd generate a run.sh which could invoke java instead of the command line slider tools. Re-strating with the same settings happens quite often while testing.

Unrelated to this jira: Any idea why package.py is in python. Is this something that can be moved to Java later, so that we don't have logic split in 2 places?, package.py exits immediately when invoked with the new arg and not from the java process. The one invoked from the java process generates everything still., Right. Missed that.
Are we ignoring the OUTPUT_DIR specified by the user in that case?, fixed that, +1 for the latest patch., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12851465/HIVE-15688.04.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10242 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3462/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3462/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3462/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12851465 - PreCommit-HIVE-Build, Committed to master. Thanks for the review, Should this be documented in the wiki?

So far, all we have for LLAP is the design doc:

* [Design Docs -- Completed -- LLAP | https://cwiki.apache.org/confluence/display/Hive/LLAP], Again, does this need to be documented?, Looks like the original is not documented... maybe? :)]