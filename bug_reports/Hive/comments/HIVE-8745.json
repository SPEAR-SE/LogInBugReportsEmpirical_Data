[Test file has same join twice, once with auto-convert once w/o. Different number of rows returned., Looks like HiveDecimal and HiveDecimalWritable have different comparison semantics in their equals() methods.
For HiveDecimal, 1.1 != 1.10
For HIveDecimalWritable, 1.1 == 1.10

This is due to the fact that HiveDecimal.equals() uses BigDecimal.equals(), where the precision/scale/value all need to be the same to be considered equal. HiveDecimal probably should have been using BigDecimal.compareTo(), which is what is used in HiveDecimalWritable.equals().

[~xuefuz], [~spena], would you agree with this?, I agree using compareTo() on HiveDecimal. 

Hive used to trim trailing zeroes before, so the equals() validation was always correct. Now, with the new change we should allow users to compare decimal values as before.
What do you think [~xuefuz]?, Agreed. 1.1 == 1.10 should return true., We should include vectorization in any new tests for this issue., Looking into this, it's not just HiveDecimal.equals() - BinarySortableSerde is serializing decimals in such a way that 1.1 is not the same as 1.10. This is why we're seeing the difference in the join behavior.

It looks like this difference in behavior is due to HIVE-7373. Before HiveDecimal was automatically trimming the trailing zeros and so 1.1 and 1.10 would both be represented as 1.1. Now that they have different internal representations, there seem to be some unexpected differences in behavior like we're seeing with BinarySortableSerde. We may want to consider backing out the changes from HIVE-7373.

If we were to try to fix this issue without reverting HIVE-7373, we would still have to trim trailing zeros within BinarySortableSerde so that 1.1 == 1.10. If we do this this will result in having the trimmed behavior that was deemed undesirable in HIVE-7373, but which would only exhibit itself when BinarySortableSerde is used (joins), which seems a bit odd.

Thoughts?, Marking for 0.14 - since this behavior change happened in 0.14, would like for this to be addressed before it gets released., I don't think reverting HIVE-7373 is a viable option. That's the trigger but not the root cause of the problem. The root cause is that BinarySortableSerde should be able to serailize decimal values in such way that comparison can be performed at byte level w/o requiring values be trailed for zeroes. I don't know how to do this, but I think that's the direction into which we should look. If the serde needs to remove trailing zeros during serialization, this is fine as long as it can get them back upon deserialization., {quote}
If the serde needs to remove trailing zeros during serialization, this is fine as long as it can get them back upon deserialization.
{quote}

What I'm saying is I don't think it's possible to get the trailing zeros back upon deserialization. Since this is BinarySortableSerde, the comparison is based on the bytes representing the decimal value.  If we had a way to differentiate 1.0 from 1.00 during deserialization, then there would have to be something in the BinarySortable representation of the decimal value to specify one trailing zero vs two trailing zeros, which would make the bytes comparison fail between 1.0 and 1.00. So the trailing zeros would be permanently trimmed., [~xuefuz] [~jdere] is right. You can't have it both ways. I don't see how you create an object that compares as equal on the byte-level but then magically reconstructs additional information on deserialization. You could add info to the value part of the MR key/value tuple but that's an unnecessarily complex solution. As [~jdere] says: This is a regression and I think we should revert HIVE-7373.

The other option would be to pad all values to the column spec and make sure we compute the spec as the max for the join keys. I'm not sure why you were against that in the first place - it seems that's what most DBs do. However, that's complicated and should be tackled in 0.15.0., {quote}
The other option would be to pad all values to the column spec and make sure we compute the spec as the max for the join keys. I'm not sure why you were against that in the first place
{quote}
I'm not sure what this refers to. Nevertheless, I think the serde should be able to trim the zeros and pad it back as long as it has the right metadata. It seems it does have the metadata for each colomns.

, [~spena], could you do some research on this? Thanks., {quote}
 Nevertheless, I think the serde should be able to trim the zeros and pad it back as long as it has the right metadata. It seems it does have the metadata for each colomns.
{quote}

We have the metadata for the column, but not for individual values in each row. If you have a decimal(10,5) column, but the values 
{noformat}
1
1.0
1.00
{noformat}

The only thing we could get from the column metadata is the precision=5, so we could pad everything to 1.00000. To know how many extra zeros we need for each value of the column, we would have to save something for each value., [~xuefuz] I'm going to revert HIVE-7373. I think that's reasonable given that it causes this issues. I'm also worried to change the behavior of decimals in hive .14 again if there's still questions about it. We've changed the behavior in .12 - .13 already and it caused a lot of grief to some folks. Given that BinarySortableSerde is involved, we also need to look into window functions, group by w/ w/o map-side aggr etc.

Jason also brings up another good point: Performance. The decision to maintain the trailing zeroes for each individual value instead of at the column level, means that we will never be able to simply encode decimals in two longs, which was the idea behind limiting the precision of decimals in the first place., Okay. Could you guys add this repro case so that when HIVE-7373 is revisted the issue here can be caught early?, Sounds like a good idea, will do., Attaching patch v1, just reverting the Java code changes associated with HIVE-7373, will wait for the precommit tests to see what q file golden files need to get updated. Patch did not apply cleanly so a few of these files needed manual changes. Ignored the changes for RecordReaderImpl, because it looks like the changes in HIVE-8461 take precedence.

[~hagleitn], can you verify the explain plans looks correct in decimal_join2.q.out? I'm pretty useless with explain plans. The result sets match., Verified the explain plan. Looks good to me., Update patch to include golden file changes from HIVE-7373, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12679944/HIVE-8745.1.patch

{color:red}ERROR:{color} -1 due to 67 failed/errored test(s), 6702 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_change_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_decimal_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_char_pad_convert
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_precision
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_trailing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_predicate_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_serde_regex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_when
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_update_all_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_aggregate_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_between_in
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_data_types
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_aggregate
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_expressions
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_precision
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_round_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_trailing
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_udf
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_reduce_groupby_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_decimal
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_navfn
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_rank
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mapjoin_decimal
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_update_all_types
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_between_in
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_data_types
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_6
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_aggregate
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_cast
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_expressions
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_precision
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_trailing
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_decimal_udf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_reduce_groupby_decimal
org.apache.hadoop.hive.common.type.TestHiveDecimal.testMultiply
org.apache.hadoop.hive.common.type.TestHiveDecimal.testPlus
org.apache.hadoop.hive.common.type.TestHiveDecimal.testPow
org.apache.hadoop.hive.common.type.TestHiveDecimal.testSubtract
org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testCeiling
org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testFloor
org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testNegate
org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testRound
org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testRoundWithDigits
org.apache.hadoop.hive.ql.exec.vector.expressions.TestDecimalUtil.testSign
org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastDecimalToString
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1672/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1672/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1672/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 67 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12679944 - PreCommit-HIVE-TRUNK-Build, Looks like a few more tests to fix.
Also, does it make sense to still change HiveDecimal.equals() to use BigDecimal.compareTo() rather than HiveDecimal.equals()?, Patch v3, fix test failures. Per [~hagleitn], leaving HiveDecimal.equals() alone for now, this is same behavior as it used to be., RB at https://reviews.apache.org/r/27715/, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12680049/HIVE-8745.3.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 6666 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_acid
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1684/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1684/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1684/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12680049 - PreCommit-HIVE-TRUNK-Build, Failures do not appear to be related to the patch., Not sure it's needed, because this is reverting, but: +1 for trunk and hive 0.14, Committed to trunk/branch-0.14, This has been fixed in 0.14 release. Please open new jira if you see any issues.
, [~leftylev] 

I believed you added a statement on documentation for HIVE-7373 fix; but this patch is reverting the trailing zeroes fix. So you might wanna revert that document statement as well., Thanks for the tip, [~spena].  I removed the TODOC14 label on HIVE-7373 and added a doc note (actually a no-doc note).]