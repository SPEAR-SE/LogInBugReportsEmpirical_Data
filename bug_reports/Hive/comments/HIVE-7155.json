[Patch attached. a new configuration "templeton.mapper.memory.mb" is introduced so that users can set this property in webhcat-site.xml to overwrite mapreduce.map.memory.mb for the webhcat controller job., [~shanyu]
One concern I have with the default value of templeton.mapper.memory.mb being set to 1GB is that the cluster might be having a larger mapreduce.map.memory.mb value, but webhcat would end up having a smaller limit for it.
I think we should take into consideration the current setting for mapreduce.map.memory.mb in the cluster. If the default value of mapreduce.map.memory.mb is greater than the value of templeton.mapper.memory.mb, I think we should use that. ie, templeton.mapper.memory.mb should be used for setting a higher value if necessary.

What has been a good value for templeton.mapper.memory.mb in your experience ?
, [~thejas], Thank you for your feedback! I think it could be a legitimate case when a cluster have larger mapreduce.map.memory.mb but smaller templeton.mapper.memory.mb, for example, when the users are only use templeton to submit jars. I think it is useful to separate the mapper's memory limit for general mapreduce jobs and templeton's launch mapper.

In practice, the default 1GB memory limit for templeton's launch mapper is sufficient for majority of job submission. But when we try to do hive query on huge tables we'll hit memory limit for the templeton's launch mapper, because hive tries to do quite a bit local processing. In that case, we want to increase memory limit (to 2GB) for templeton's launch mapper, but not for user's mapreduce jobs., I am thinking of this in terms of number of config parameters a user has to think about when getting started (the fewer that needs tweaking the better). 
Also, many recommendations that I have seen suggest setting the yarn.scheduler.minimum-allocation-mb to be equal to mapreduce.map.memory.mb. Using a smaller value for templeton tasks is not going to help.

I think an alternative to enable this (minimal configuration for most users), is to not set a default value for templeton.mapper.memory.mb. If the value of templeton.mapper.memory.mb is empty, then use mapreduce.map.memory.mb. Users can set templeton.mapper.memory.mb to get a higher or lower value of mapreduce.map.memory.mb for the launcher task.
, [~thejas] Yes, it's a good idea to make templeton.mapper.memory.mb defaults to empty, in which case we don't set mapreduce.map.memory.mb when submitting the controller job. This is safer for users that are not aware of this option yet flexible for people with the needs to change this configuration. I will modify the patch accordingly. Thanks for the suggestion!, Can you also please update the review board link ? Its easier to see the changes that way.

, Review board updated., [~shanyu] I can't comment on RB.  Did you perhaps not "publish" it?, [~ekoifman] I did publish it. I can add comments to it. Can you please double check? Thx., +1, +1, Although the parameter name *templeton.mapper.memory.mb* implies the units, you should make it explicit in the description:  "Templeton controller job's Launch mapper's memory limit, in megabytes.  (etc.)", [~leftylev] Thanks for your feedback. I've attached a new version calling out megabytes in the description., +1  Thanks [~shanyu]., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12648911/HIVE-7155.2.patch

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 5530 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_reduce_deduplicate_extended
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.testPropertiesParsing
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/413/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/413/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-413/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12648911, Patch committed to trunk. Thanks for the contribution Shanyu!
, Need to document *templeton.mapper.memory.mb* in the wiki with version information (0.14.0):

* [WebHCat Configuration:  Configuration Variables | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Configure#WebHCatConfigure-ConfigurationVariables], This has been fixed in 0.14 release. Please open new jira if you see any issues.
, Doc note:  *templeton.mapper.memory.mb* is documented in the WebHCat Configuration wiki, at the end of the table of configuration variables.  (Better late than never.)  I took the liberty of changing "Templeton" to "WebHCat" in the description -- should have thought of that before the commit.

* [WebHCat Configuration -- Configuration Variables | https://cwiki.apache.org/confluence/display/Hive/WebHCat+Configure#WebHCatConfigure-ConfigurationVariables]

]