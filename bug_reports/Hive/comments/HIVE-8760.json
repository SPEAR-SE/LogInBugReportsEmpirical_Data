[This is a perf regression for all well-written Hive hooks that exist.

There is previous information to indicate copying Configuration is not a fast operation. In HIVE-4486 we've gone from a query which took 347.66 seconds down to 218 seconds by throwing out unnecessary {{new HiveConf();}} calls.

If this is a thread-safety issue, then the hook spawning its own threads should synchronize - since this is class base config, which is pluggable that is very clearly the minimum impact fix.

{code}
@Override
  public void run(final HookContext hookContext) throws Exception {
    final long currentTime = System.currentTimeMillis();
+  final HiveConf confCopy = new HiveConf(hookContext.getConf());
    executor.submit(new Runnable() {
... // use local value off the closure capture in thread runnable
{code}, But why to make an assumption about what hook is doing? Isnt it prudent that Hive does a safe thing when it can., Immutable structures are prudent from an API design stand-point - copying always to get to a shared-nothing model potentially breaks anyone who relied on synchronization on that object elsewhere.

The stability impact of copying is currently invisible and unknown, but eventually a lot of System.identityHashcode is applied to debug those and System.err, because LOG.info() is synchronized.

The performance impact of however is well known (as quoted earlier). The core API issue over-all for me is that we don't have immutable Conf objects - I keep hitting these {{new Configuration()}} perf issues (track HADOOP-11223 for the impact on HDFS).

At the very least, I know the stability impact of copying in one Hook, the surface is rather narrow for that problem to trace through (i.e "ship Hook2.java, Hook3.java etc and test them without rebuilding all of hive").

On top of it, the biggest user of Hooks seems to be itests (which ships something like 20 single thread hooks). You'll be slowing down all of them, all the time., 

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12679919/HIVE-8760.patch

{color:green}SUCCESS:{color} +1 6700 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1670/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1670/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1670/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12679919 - PreCommit-HIVE-TRUNK-Build, Underlying hadoop bug is fixed, no workaround needed in Hive, Reopened because hadoop-common reverted changes., I'd like to propose a different change with smaller scope to get at least into hive .14. I think that'd be safer for 2 reasons:

- It only changes ATSHook, anyone else will not see a change to their hooks
- It's faster if you run w/o ATSHook, because the clone only happens there. Cloning is fairly expensive.
- It's a workaround for a hadoop issue. We should still try to get the hadoop problem fixed and take it back out for perf reasons. That limits the changes to only ATSHook again

Note: ExecHooks and hadoop conf have been around unchanged for quite a while so leaving the code path alone as much as possible for .14 seems a safe choice., bq. It's faster if you run w/o ATSHook, because the clone only happens there. Cloning is fairly expensive.

+1 - this is a more contained fix for 0.14. This does not change any existing behaviour or API contracts.

This will not break outside of ATSHook, which we can disable in case it shows issues.

Copying always will be slow, but it runs the risk of changing the API contract strictness too late for 0.14. Badly written hooks might fail, if they use the common conf to pass variables between different hooks.

For a cleaner solution long-term (0.15+), I think we need a {{new HiveConf(conf).asReadonly()}}, to enforce the API contract for such badly behaving hooks., Actually - cancelling patch. Since patch is ATSHook only, it's easier and quicker to verify locally., Committed to trunk and .14., 

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12680270/HIVE-8760.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1698/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1698/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1698/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-1698/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'metastore/src/gen/thrift/gen-py/hive_metastore/constants.py'
Reverted 'metastore/src/gen/thrift/gen-cpp/hive_metastore_constants.h'
Reverted 'metastore/src/gen/thrift/gen-cpp/hive_metastore_constants.cpp'
Reverted 'metastore/src/gen/thrift/gen-rb/hive_metastore_constants.rb'
Reverted 'metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/hive_metastoreConstants.java'
Reverted 'metastore/src/gen/thrift/gen-php/metastore/Types.php'
Reverted 'metastore/if/hive_metastore.thrift'
Reverted 'ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java'
Reverted 'ql/src/test/org/apache/hadoop/hive/ql/parse/TestUpdateDeleteSemanticAnalyzer.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java'
Reverted 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java'
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target shims/scheduler/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-unit-hadoop2/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hcatalog/hcatalog-pig-adapter/target accumulo-handler/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1637510.

At revision 1637510.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12680270 - PreCommit-HIVE-TRUNK-Build]