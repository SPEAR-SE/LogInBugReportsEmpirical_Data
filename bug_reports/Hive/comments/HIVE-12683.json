[Moving this jira to Hive for now. It will be good to start the discussion there as this pertains to Hive regardless of whether it uses MR or Tez as its internal execution engine. 

FWIW, issues such as this are usually better off being raised on the Hive user mailing list for discussion/analysis and a bug created later based on the findings. , \cc [~hagleitn] [~gopalv]

[~rohitgarg1989] Can you attach the yarn application logs for the query that was slow. , Additional info such as the query text as well as the explain would be useful. , Nevermind - just noticed that the query text is on the blog. , Thanks for the reply Hitesh. I didn't save the logs but I will re-launch EMR cluster and re-run the application and provide you with logs ASAP., ohh yes the query is there, From your blog - are you using 59Gb Tez containers?

set hive.tez.container.size=59205;, I tried lot of different settings. This was one of them and which worked. It came from the below formulas. Not sure if thats the right way to estimate it.

We used the following formulae to guide us in determining YARN and MapReduce memory configurations:

Number of containers =  min (2 * cores, 1.8 * disks, (Total available RAM) / min_container_size)
Reserved Memory = Memory for stack memory
Total available RAM = Total RAM of the cluster â€“ Reserved Memory
Disks = Number of data disks per machine
min_container_size = Minimum container size (in RAM). Its value is dependent on RAM available
RAM-per-container = max(min_container_size, (Total Available RAM) / containers)

For example, for our cluster, we had 32 CPU cores, 244 GB RAM, and 2 disks per node.

Reserved Memory = 38 GB
Container Size = 2 GB
Available RAM = (244-38) GB = 206 GB
Number of containers = min (2*32, 1.8* 2, 206/2) = min (64,3.6, 103) = ~4
RAM-per-container = max (2, 206/4) = max (2, 51.5) = ~52 GB

, Pretty sure all of that math is for 10k RPM disks - SSDs don't exactly follow the same rules.

For r3.8xl, my recommendation from measurement was 24 containers x 8 Gb containers for optimum performance.

EMR's default configs for Hive might not be the best for Tez, you might want to reconfigure hive-site.xml based on the Ambari default install instead.

Something like your Test 2 might be OOM'ing due to lack of S3 file closures - instead of increasing the memory Xmx so high, you might want to turn on the scalable partitioned insert from HIVE-6455.

Most of what you describe here isn't necessarily a bug., For SSDs, you should be able to run a few more containers per node. Maybe try with say 8 containers sized to 20 GB each ( Xmx 16G ) as a start. 

Also, you may want to try the large group-by query with "hive.map.aggr" set to false to help with the OOMs. 


 , Thanks for your inputs. I will try these changes and see if that would give me any performance boost over hive query engine.

This was the OOM error I was getting before I tweaked memory settings :

0 FATAL [Socket Reader #1 for port 55739] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[Socket Reader #1 for port 55739,5,main] threw an Error.  Shutting down now...
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1510)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:750)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:624)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:595)
2015-12-07 20:31:32,859 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.OutOfMemoryError: GC overhead limit exceeded
2015-12-07 20:31:30,590 WARN [IPC Server handler 0 on 55739] org.apache.hadoop.ipc.Server: IPC Server handler 0 on 55739, call heartbeat({  containerId=container_1449516549171_0001_01_000100, requestId=10184, startIndex=0, maxEventsToGet=0, taskAttemptId=null, eventCount=0 }), rpc version=2, client version=19, methodsFingerPrint=557389974 from 10.10.30.35:47028 Call#11165 Retry#0: error: java.lang.OutOfMemoryError: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at javax.security.auth.SubjectDomainCombiner.optimize(SubjectDomainCombiner.java:464)
	at javax.security.auth.SubjectDomainCombiner.combine(SubjectDomainCombiner.java:267)
	at java.security.AccessControlContext.goCombiner(AccessControlContext.java:499)
	at java.security.AccessControlContext.optimize(AccessControlContext.java:407)
	at java.security.AccessController.getContext(AccessController.java:501)
	at javax.security.auth.Subject.doAs(Subject.java:412)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
2015-12-07 20:32:53,495 INFO [Thread-60] amazon.emr.metrics.MetricsSaver: Saved 4:3 records to /mnt/var/em/raw/i-782f08c8_20151207_7921_07921_raw.bin
2015-12-07 20:32:53,495 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
2015-12-07 20:32:50,435 INFO [IPC Server handler 20 on 55739] org.apache.hadoop.ipc.Server: IPC Server handler 20 on 55739, call getTask(org.apache.tez.common.ContainerContext@409a6aa9), rpc version=2, client version=19, methodsFingerPrint=557389974 from 10.10.30.33:33644 Call#11094 Retry#0: error: java.io.IOException: java.lang.OutOfMemoryError: GC overhead limit exceeded
java.io.IOException: java.lang.OutOfMemoryError: GC overhead limit exceeded
2015-12-07 20:32:29,117 WARN [IPC Server handler 23 on 55739] org.apache.hadoop.ipc.Server: IPC Server handler 23 on 55739, call getTask(org.apache.tez.common.ContainerContext@7c7e6992), rpc version=2, client version=19, methodsFingerPrint=557389974 from 10.10.30.38:44218 Call#11260 Retry#0: error: java.lang.OutOfMemoryError: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
2015-12-07 20:32:53,497 INFO [Thread-60] amazon.emr.metrics.MetricsSaver: Saved 1:1 records to /mnt/var/em/raw/i-782f08c8_20151207_7921_07921_raw.bin
2015-12-07 20:32:53,498 INFO [Thread-61] amazon.emr.metrics.MetricsSaver: Saved 1:1 records to /mnt/var/em/raw/i-782f08c8_20151207_7921_07921_raw.bin
2015-12-07 20:32:53,498 INFO [Thread-2] org.apache.tez.dag.app.DAGAppMaster: DAGAppMaster received a signal. Signaling TaskScheduler
2015-12-07 20:32:53,498 INFO [Thread-2] org.apache.tez.dag.app.rm.TaskSchedulerEventHandler: TaskScheduler notified that iSignalled was : true
2015-12-07 20:32:53,499 INFO [Thread-2] org.apache.tez.dag.history.HistoryEventHandler: Stopping HistoryEventHandler
2015-12-07 20:32:53,499 INFO [Thread-2] org.apache.tez.dag.history.recovery.RecoveryService: Stopping RecoveryService
2015-12-07 20:32:53,499 INFO [Thread-2] org.apache.tez.dag.history.recovery.RecoveryService: Closing Summary Stream
2015-12-07 20:32:53,499 INFO [LeaseRenewer:hadoop@10.10.30.148:9000] org.apache.hadoop.util.ExitUtil: Halt with status -1 Message: HaltException, I will update here with the results., It seems like the application master is running out of memory. What is the Tez AM being configured for in terms of memory and Xmx?, yeah...it was application master..we read somewhere Tez AM memory and Xmx settings should be same as tez container. So, in one of our tests and which ran (as mentioned in the blog), we did

set tez.am.resource.memory.mb=59205;
set tez.am.launch.cmd-opts =-Xmx47364m;

We were tweaking the following properties mainly :

set tez.task.resource.memory.mb
set tez.am.resource.memory.mb
set tez.am.launch.cmd-opts 
set hive.tez.container.size
set hive.tez.java.opts
set tez.am.grouping.max-size, The Tez AM resource sizing has no relation to the task container sizing. That said, for various benchmarks done in the past, I dont believe anyone has needed to go beyond 16GB for the Tez AM for very large DAGs.

[~rohitgarg1989] What was the AM size configured to when the OOM happened? If you are running a version older than Tez 0.7.0, there were some memory issues that require a large AM size i.e. large being say 16 GB but for 0.7.0 and higher, even 4 GB should be sufficient for a decent sized DAG. You can set it to 8 GB to be safe for now with Xmx say 6.4 GB and that should be sufficient. If you still hit an OOM with 8 GB, a jira against Tez with the heap dump would be helpful. 

[~gopalv] anything to add? any configs that need to be tuned / turned off for Hive that ends up using more memory in the AM? Any implicit caching of splits, etc?  , Thanks Hitesh for the inputs. I am going to try the recommended settings and update the results here. The AM size was 1024MB. I was just using the bootstrap script provided by Amazon for initial testing. They had an older version of Tez (I think 0.4 or 0.5).
, The known 0.4.x OOMs were during split-generation for uncompressed text files (HIVE-10746) or when combine inputformat is used on S3 (HADOOP-11584).]