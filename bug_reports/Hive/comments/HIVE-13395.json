[

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12798855/HIVE-13395.6.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 4 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/139/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/139/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-METASTORE-Test-139/

Messages:
{noformat}
LXC derby found.
LXC derby is not started. Starting container...
Container started.
Preparing derby container...
Container prepared.
Calling /hive/testutils/metastore/dbs/derby/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/derby/execute.sh ...
Tests executed.
LXC mysql found.
LXC mysql is not started. Starting container...
Container started.
Preparing mysql container...
Container prepared.
Calling /hive/testutils/metastore/dbs/mysql/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/mysql/execute.sh ...
Tests executed.
LXC oracle found.
LXC oracle is not started. Starting container...
Container started.
Preparing oracle container...
Container prepared.
Calling /hive/testutils/metastore/dbs/oracle/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/oracle/execute.sh ...
Tests executed.
LXC postgres found.
LXC postgres is not started. Starting container...
Container started.
Preparing postgres container...
Container prepared.
Calling /hive/testutils/metastore/dbs/postgres/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/postgres/execute.sh ...
Tests executed.
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12798855 - PreCommit-HIVE-METASTORE-Test, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12798870/HIVE-13395.7.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 4 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/140/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-METASTORE-Test/140/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-METASTORE-Test-140/

Messages:
{noformat}
LXC derby found.
LXC derby is not started. Starting container...
Container started.
Preparing derby container...
Container prepared.
Calling /hive/testutils/metastore/dbs/derby/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/derby/execute.sh ...
Tests executed.
LXC mysql found.
LXC mysql is not started. Starting container...
Container started.
Preparing mysql container...
Container prepared.
Calling /hive/testutils/metastore/dbs/mysql/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/mysql/execute.sh ...
Tests executed.
LXC oracle found.
LXC oracle is not started. Starting container...
Container started.
Preparing oracle container...
Container prepared.
Calling /hive/testutils/metastore/dbs/oracle/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/oracle/execute.sh ...
Tests executed.
LXC postgres found.
LXC postgres is not started. Starting container...
Container started.
Preparing postgres container...
Container prepared.
Calling /hive/testutils/metastore/dbs/postgres/prepare.sh ...
Server prepared.
Calling /hive/testutils/metastore/dbs/postgres/execute.sh ...
Tests executed.
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12798870 - PreCommit-HIVE-METASTORE-Test, [~alangates] could you review please?, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12798870/HIVE-13395.7.patch

{color:green}SUCCESS:{color} +1 due to 6 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 9990 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_coltype_literals
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.testForcedLocalityPreemption
org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.testLocks
org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener.org.apache.hadoop.hive.ql.security.TestMultiAuthorizationPreEventListener
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.testSimplePrivileges
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testDelegationTokenSharedStore
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testMetastoreProxyUser
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testSaslWithHiveMetaStore
org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat
org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.testPartitionPublish
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7628/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7628/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7628/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12798870 - PreCommit-HIVE-TRUNK-Build, This is not a complete review but I have a few high level questions/comments.  Answering these will help me finish the rest of the review.

The description presents several possible courses of action.  Some comments on which one you chose and why would be good.

Why add a new WRITE_SET table?  Doesn't the TXN_COMPONENTS table have everything you need, since it tracks partitions/tables that were written to?  

Why the double check for a write conflict in commitTxn?  You're already checking the conflict when you acquire the shared-write lock.  Since the writer holds the lock no-one could possibly update that partition.  If we want to someday shift to let shared-write locks proceed together and do true first commit wins then checking in commitTxn makes sense.  Is that your plan?, This implements that idea that if 2 concurrent transactions have a Write/Write conflict, one must be aborted and First-Committer-Wins rule is used to decide which.

It's implemented using Write-Set tracking so that it can be extended to multi-statement txns in the future.   The check on lock acquisition is an optimization, while commitTxn() logic is the ultimate authority.  ( It also makes the logic cleaner in particular around properly mutexing operations, which have to be using RDBMS.  I started with your idea to check the latest writers txn id vs currently committing one but it ended up more difficult using a DB)

TXN_COMPONENTS/COMPLETED_TXN_COMPONENTS have different retention policies.  These are governed by compaction rather than transaction "liveness" which don't necessarily match.
, bq. TXN_COMPONENTS/COMPLETED_TXN_COMPONENTS have different retention policies [then WRITE_SET]. These are governed by compaction rather than transaction "liveness" which don't necessarily match.
Agreed, but the retention of TXN_COMPONENTS/COMPLETED_TXN_COMPONENTS > WRITE_SET (because you can't clean the compaction until all the readers are done).  I know in HIVE-13497 you propose to eliminate TXN_COMPONENTS.  So is your plan to have WRITE_SET and COMPLETED_TXN_COMPONENTS as the two tables?  That seems fine, as long as the upgrade path isn't hard on users., I wanted to have a solution that can be extended to multi-statement transactions.
In the general, you have to keep WriteSet info post transaction commit which means it can be cleaned.
For example, T[10,70] and S[35,36].  If T decides to write X after S commits, you still need to know if S wrote X.
, I agree we need something that works with multi-statement transaction.  But I think we'll find in that case that we cannot clean until all open transactions that could potentially see a set of changes (not just the txns with read locks) have closed.  That is, if I have a partition with base_10 and delta_11_20 and then compact so that I now have base_20 I can't clean that compaction until all transactions < 20 have committed or aborted.  Otherwise one of those transactions could try to read this partition and get the wrong version.  Since you have to remember all this I think this will force us to keep the necessary information in COMPLETED_TXN_COMPONENTS long enough.

AFAICT you have a different way of remembering all the same information, which is completely fine.  As long as we agree on what has to be remembered for how long I'm fine with doing it in a new WRITE_SET table and dropping the TXN_COMPONENTS table.

, patch 8 is just a rebase of 7, HIVE-13622 covers some optimizations for this, patch 11 (not final) reworks the implementation to get WriteSet information from TxnHandler.addDynamicPartitions() call rather than lock information (where applicable).  The later knows exactly which partitions have been written to. This increases concurrency dramatically.
For example "update T set a = 7 where b = 17".  Suppose b is not a partition column and the table has 10K partitions.  Suppose further that only 5 partitions match "b=17".  We'll currently lock all the existing partitions but a true WriteSet is the 5 partitions actually modified.

Further optimizations in HIVE-13622 are useful but not absolutely required., [~alangates] could you review patch 12 please, patch 13 is the same as 12, fix typo, patch 14 fixes a typo in patch 13, patch 15 fixes a couple of tests due to HIVE-13213 - no code changes, TxnHandler.java in OperationType:  it seems odd to create a new enum with a deprecated method.

In TxnHandler.commitTxn, would it make sense to rearrange this so that the check is made whether there are any operations that could conflict before the mutex is obtained and the transaction id checked?  If there's nothing to record in the write sets I don't see why you need to hold the mutex or even record a commit txn id.

TxnHandler.addDynamicPartitions we should fix this so that the operations is carried in the dynamic partition message now rather than fetched from components table.  We can do it in a separate JIRA but we should do it quickly.  Fetching back another row to answer that question is bogus.

TxnHandler.checkLock IIUC the if (!writeSet.isEmpty()) (line 2176) will never be triggered right now.  I'm not a fan of blocks of dead code.  When do you plan to alter checkLock so that it knows whether the requesting locker is dynamic or static?

TxnHandler lines 2229 through 2290 should be indented to match the following lines.  Right now they are 1 stop too far to the left., I have a followup HIVE-13622 which I hope to get to get shortly - I believe it covers all your concerns (except for the indentation).  Let me know if you disagree., Looks good.  If you fix the indentation on this I'm +1 on it., patch 16 only changes indentation/comments, {noformat}
Test Name Duration Age
 org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_join32	6.6 sec	1
 org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testSaslWithHiveMetaStore	1 min 9 sec	1
 org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.testDelayedLocalityNodeCommErrorImmediateAllocation	10 sec	1
 org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_select_read_only_encrypted_tbl	1 min 26 sec	2
 org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.testAddPartitions	3 sec	3
 org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure	4.8 sec	11
 org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.testFetchingPartitionsWithDifferentSchemas	3 sec	24
 org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.testGetMetaConfDefault	10 sec	25
 org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_insert_partition_static	1 min 57 sec	26
 org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_selectindate	13 sec	30
 org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avrocountemptytbl	9.6 sec	30
 org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order_null	35 sec	30
 org.apache.hive.hcatalog.api.repl.commands.TestCommands.org.apache.hive.hcatalog.api.repl.commands.TestCommands	20 sec	30
 org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_join_with_different_encryption_keys	1 min 32 sec	30
 org.apache.hive.minikdc.TestHiveAuthFactory.testStartTokenManagerForMemoryTokenStore	1.3 sec	30
 org.apache.hive.minikdc.TestHiveAuthFactory.testStartTokenManagerForDBTokenStore	0.38 sec	30
 org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.testGetPartitionSpecs_WithAndWithoutPartitionGrouping	3.3 sec	30
 org.apache.hive.minikdc.TestMiniHiveKdc.testLogin	1 min 27 sec	30
 org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_index_bitmap3	9.1 sec	30
 org.apache.hadoop.hive.cli.TestMinimrCliDriver.org.apache.hadoop.hive.cli.TestMinimrCliDriver	1 min 30 sec	30
{noformat}

test failures are not related, HIVE-13395.addendum.patch - obligatory forgotten file, committed to branch-1 and master

thanks Alan for the review, Doc note:  This adds *hive.writeset.reaper.interval* to HiveConf.java, so it will need to be documented in the wiki for releases 1.3.0 and 2.1.0.

* [Configuration Properties -- Query and DDL Execution | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-QueryandDDLExecution]

Added TODOC1.3 and TODOC2.1 labels., the typo fix looks good to me, +1, [~ekoifman] [~alangates]

I think the WRITE_SET table should some columns in the primary key. I expect most databases to organize the data in a b-tree with primary key as the index (or have an option to do so). That should help in reducing the search space for your prominent queries. As long as columns in the where clause match the prefix of the index, it should greatly reduce the search space.

You can add a autoincrement column to keep it unique if necessary. MySQL (innodb) anyway ends up organizing data on an autoincrement column, which is useless for the queries (see [post|https://blog.jcole.us/2013/05/02/how-does-innodb-behave-without-a-primary-key/] ).
, Created a follow up jira for the PK addition - HIVE-14047
, [~wzheng] documented *hive.writeset.reaper.interval* in the wiki so I removed the TODOC1.3 and TODOC 2.1 labels.  Thanks, Wei!

Here's the link:

* [Configuration Properties -- hive.writeset.reaper.interval | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.writeset.reaper.interval]]