[More details:
# Dropping a table did not impact the initiator thread, as it could already handle not finding a table.  However, it was assuming that all such cases were temp tables.
# Dropping a table before the worker thread or cleaner thread got to it caused bogus error messages.  In the case of the worker it also prevented forward progress on compacting any other requests, as the dropped table stayed at the head of the queue.
# Dropping a partition before any of the threads got to it caused the compaction do to be done against the table rather than the partition, which would result in an error once the compaction work started., The attached patch makes the following changes:
# The info message for initiator when it discovers a missing table now says that the table may have been a temp or may have been dropped.
# Worker and cleaner threads now properly check for the table to be compacting being null, and give an info message saying they assume the table has been dropped.
# All three threads now properly handle the case where the partition is dropped and give an info message rather than attempt to compact the table., [~sushanth], I'd like to add this to 1.2 as it can jam the compactor.  It's already patch available, so I should be able to get it in quickly.

[~ekoifman], would you have time to review this?, From the description, this is an NPE in a pretty core feature, so this qualifies as an outage. Added to https://cwiki.apache.org/confluence/display/Hive/Hive+1.2+Release+Status, And by that, I mean +1 for inclusion to branch-1.2, I'm not sure I understand how this works.
The Initiator (if the table/partition is no longer there) will not add anything to compaction queue.
So then there is nothing for Worker/Cleaner to do in this case.

How will data from TXNS, COMPLETED_TXN_COMPONENTS, TXN_COMPONENTS which relates to these table get cleaned up?, bq. So then there is nothing for Worker/Cleaner to do in this case.
Unless the table get's dropped between when the initiator initiates it and the worker picks it up, same for worker-> cleaner.  The issue with the worker can also be seen if a user initiates compactions of a deleted or non-existent table.

bq. How will data from TXNS, COMPLETED_TXN_COMPONENTS, TXN_COMPONENTS which relates to these table get cleaned up?
Hmm, that might be a general issue, but it isn't introduced by this.  We need to explore separately whether when compactions get dropped without being done (for whatever reason) that everything in TXNS etc. get's properly cleaned up., I filed a ticket to investigate the more general issue.
+1 this patch, Duplicating HIVE-10595.patch as HIVE-10595.1.patch to submit through precommit., 

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12730973/HIVE-10595.1.patch

{color:green}SUCCESS:{color} +1 8912 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3795/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3795/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3795/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12730973 - PreCommit-HIVE-TRUNK-Build, Patch 1 committed.  Thanks Eugene for the review., This issue has been fixed and released as part of the 1.2.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.]