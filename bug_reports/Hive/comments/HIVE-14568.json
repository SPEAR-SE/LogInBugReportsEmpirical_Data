[I think this is mostly by design. You have two columns: decimal(p1, s1) and decimal(p2,s2). We need to statically derive the type for the product of the two columns based on s = s1 + s2 and p1 = p1 + p2 +1. since your s1 = 28 and s2 = 10 in your case, then s = 38.  Similarly, p = 38 (which is the max). Thus, the result column has a type decimal(38, 38). This basically means that the result cannot have any integer part. On the other hand, if the result type is set as (38, 18), I can certainly construct example data which shows that the production of the two column loses the scale that I was expecting.

I understand that NULL may have been surprising to people. However, I wonder why a column defined as decimal (38,28) to be used to store data like 1.2, 1.44, etc. Is it reasonable to have a smaller precision/scale?

This sounds like a data modeling issue. the metadata needs to closely define the data.

It's a good point that an ERROR here might be better so that NULL doesn't slick in unnoticed. I believe that in MySQL there is a strict mode, which, when on, will generate error in this case. We don't have such mode defined in Hive, but it may make sense to introduce such a mode., Thanks Xuefu Zhang , Thank you for the elaborate explanation Zhang. We will workaround this issue by casting the column to a lower precision & scale. 
Since we have a few developers migrating from ORACLE and Postgres SQL, we thought this would be a feature request to ease the usage of Hive. Please let us know if there is a way to introduce such a mode on Hive and if that would have any performance impacts once implemented.

Regards,
Akhil]