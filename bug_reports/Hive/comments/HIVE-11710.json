[Currently we don't set SessionState.info to System.out which causes the query progress to output to System.err when set command is issued.

Now with the change, the progress is output to console as expected.
{noformat}
0: jdbc:hive2://> set a = 0;
No rows affected (0.003 seconds)
0: jdbc:hive2://> select count(*) from src;
Query ID = axu_20150910095139_f396d686-b4b1-4c5c-b8ed-4f74e2362920
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_local386038793_0002, Tracking URL = http://localhost:8080/
Kill Command = /Users/axu/Documents/workspaces/tools/hadoop/hadoop-2.6.0/bin/hadoop job  -kill job_local386038793_0002
Hadoop job information for Stage-1: number of mappers: 0; number of reducers: 0
2015-09-10 09:51:40,613 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local386038793_0002
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
+------+--+
| _c0  |
+------+--+
| 2    |
+------+--+
{noformat}
, I didn't create code review for this since it's a simple change. Submit the first patch to see if it would break anything., [~aihuaxu], thanks for working on this. Could you please point out in the code how set command changes the progress output to System.err without your patch?, Thanks Xuefu for reviewing the code.

Actually it's following magic line which sets sessionState.err to System.err while we didn't set sessionState.info. 

{noformat}
      // TODO: for hadoop jobs, progress is printed out to session.err,		
74	      // we should find a way to feed back job progress to client		
75	      sessionState.err = new PrintStream(System.err, true, "UTF-8");
{noformat}

In SessionState class, it will redirect to System.err for info stream when SessionState.info is null.

{noformat}
    public static PrintStream getInfoStream() {
      SessionState ss = SessionState.get();
      return ((ss != null) && (ss.info != null)) ? ss.info : getErrStream();
    }
{noformat}

After taking a close look, the patch may not be a perfect fix. Actually SessionState.err was closed after set command (in HiveCommandOperation.java), but the object itself is left there. So when info is null and err is not null (but invalid), nothing is printed. Maybe we should recover SessionState.err after the set command is done.

, Could we also check what Hive CLI is doing w.r.t this?, CLI sets as follows in CLIDriver.java.

{noformat}
      ss.out = new PrintStream(System.out, true, "UTF-8");
      ss.info = new PrintStream(System.err, true, "UTF-8");
      ss.err = new CachingPrintStream(System.err, true, "UTF-8");
{noformat}

, Actually after {{IOUtils.cleanup(LOG, parentSession.getSessionState().err);}}, System.err will always be in a closed state although the object is there.

Seems We shouldn't close System.err stream., Attach the new patch. 

We only close file stream, not System.err and System.out since if they are closed, later we won't be able to write to them anymore., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12755398/HIVE-11710.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 9422 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5242/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5242/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5242/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12755398 - PreCommit-HIVE-TRUNK-Build, The failures are not caused by the patch., In  HiveCommandOperation.tearDownSessionIO(), should we just close the result of this: new FileOutputStream(sessionState.getTmpOutputFile())? It looks like that we don't need to close either ss.out or ss.err. Otherwise, if one operation gets closed for any reason (such as timeout), then other operations in the same user session will get ss.out and ss.err that's closed. It's my understanding that session state is shared among operations in a hive session.
, Yeah. You are right. It really depends on how ss.out is used. Originally somehow I thought it's only used by this file, but actually it's not. I will remove that as well. , OK. Seems it's fine since from the close() call shown below, we are deleting that temp file any way at that moment. So at that moment when close() is called, we can't write to that tmp file anyway since the file is getting deleted ({{cleanTmpFile()}}).

{noformat}
  public void close() throws HiveSQLException {
    setState(OperationState.CLOSED);
    tearDownSessionIO();
    cleanTmpFile();
    cleanupOperationLog();
  }
{noformat}, Do you have any thoughts on this?
{quote}
It looks like that we don't need to close either ss.out or ss.err. Otherwise, if one operation gets closed for any reason (such as timeout), then other operations in the same user session will get ss.out and ss.err that's closed. 
{quote}, Xuefu, as I posted above, actually when close() is called, the temp file which ss.out is using will be deleted anyway so other operations shouldn't be allowed to use if close() is already called somewhere else. 

Yeah. I agree that seems like we may potentially run into such issue, but it would run into such issue even I'm not closing ss.out since the tmp file is deleted by {{cleanTmpFile();}}.

To tell the truth, I don't know exactly why we are setting ss.out in HiveCommandOptions. I can change to not to close ss.out, but it would leave to resource leaking since we are not closing the file. , [~xuefuz] Attached the new patch which doesn't close the ss.out or ss.err. Can you help take a look the new patch?, [~aihuaxu], your new patch looks good. One thing I'm not 100% sure is what happens when deleting file w/o closing it first. 

Secondly, we could leak either "new PrintStream" or "new FileOutputStream" or both if there is any exception or if we don't close ss.out. This seems minor but reliable code is ideal.

Thus, I suggest that we make sure that these objects are closed properly whether there is an exception or not.

As you can see that session state is complete thread-unsafe (ref. HIVE-11402). However, this has nothing to do the problem you're addressing.
, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12761666/HIVE-11710.2.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 9575 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5381/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5381/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5381/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12761666 - PreCommit-HIVE-TRUNK-Build, Yeah. The second patch will leak "PrintStream", so I think the first patch is reasonable actually in which will close the stream to the temp file before deleting the temp file. Let me check if the close() function will be called when an exception is thrown., [~xuefuz] I double checked. The first patch is good to handle closing the file handler when the current session is getting closing. The second patch will have the file handler leaking. 

Whether an exception occurs or not, hive will close the session, in which it will call {{HiveCommandOperation:close()}}. 

The first patch will see if the stream is a file stream or standard stream. If it's a file stream, then we should close it at last.

Xuefu, how do you think? Should we proceed with the first patch?, [~aihuaxu], looking at the two patches, I seem to see that either patch has a possibility of leaking resources, even if we consider in a single thread. The possible leaks are:

1. If new new FileOutputStream() is successful but new PrintStream() is not, then the FileOutputStream instance is never closed.
2. In SQLOperation class, we never close these streams.

I think the only change we need to make in HiveCommandOperation is to fix #1. The original tearDownSessionIO() seems fine with the changes in SQLOperation., [~xuefuz] Regarding #2, in SQLOperation class, the streams are using System.out and System.err. We shouldn't close them, right? If we did that, then later we wouldn't be able to write to System.out or System.err (actually that is the exact same issue as this issue).

For #1, that is a possible leak. I will fix that.

Let me know if I understand correctly on #2.

 , For #2, I meant that we need something similar to HiveCommanOperation.tearDownSessionIO() in which the streams are closed. Each SQLOperation will initialize the streams and close them when done. Isn't this expected?, For SQLOperation, the stream are System.out and System.err. We can't/shouldn't close them. 

That is how issue HIVE-11710 arises that we closed System.out in HiveCommandOperation, then later we can't write to System.out anymore for any sql., We will close File based stream, not standard System.out and System.err., {quote}
For SQLOperation, the stream are System.out and System.err. We can't/shouldn't close them.
{quote}
The might be some misunderstanding. We are opening the streams in the constructor (changes you made), and closing them when done (changes to be made). The same will happen to the subsequent queries. I don't understand why this still has the original problem. , Indeed that we have misunderstanding. :) Now I know what you mean. Actually when you close e.g., System.err or System.out in one place, later you will never reopen it again. So {{sessionState.out = new PrintStream(System.out, true, CharEncoding.UTF_8);}} will create the stream object but the internal "out" will be still in "closed" state.   That's why I have to detect if it's a file based stream or not. If it is, then we close the stream; if not, we should leave it open.

, I see. If we don't close the PrintStream instances, won't there be leaks? Looks like we need to close these wrapper streams while keeping sys.out, sys.err, etc. open., PrintStream.close() will do "flushing the stream and then closing the underlying output stream" from the doc. Seems I need to add "flushing the stream" while keeping the underlying output stream open".

, New patch which will flush the content when the session is closing., Does the following pseudo code helps a little bit?
{code}
public class MyPrintStream extends PrintStream {
  private boolean isFileIO = false;

  public MyPrintStream(...) {
  }

  public void close() {
    super.flush();
    if( file out stream) {
      super.close();
    } else {
       super.flush();
    }
  }
}
{code}, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12761965/HIVE-11710.3.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 9568 tests executed
*Failed tests:*
{noformat}
TestMiniTezCliDriver-vector_grouping_sets.q-scriptfile1.q-union2.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5408/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5408/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5408/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12761965 - PreCommit-HIVE-TRUNK-Build, Seems HIVE-11579 fixed one related issue, but the current incorrect behavior still exists.  1. There could be temp file handler leaking from HIveCommandOption class if somehow there are exception. 2. We still need to reset SessionState.out/info/err to System.out/err in SQLOperation, otherwise for embedded mode, the beeline client output will be redirected to the files, not to the console. 

[~Ferd] You worked on HIVE-11579 recently and probably know that code well. There is a possibility that the temp file handlers are not closed if there is an exception during the following code, correct? And also we need to flush the output rather than closing the stream if the stream points to System.out/.err. right?
{noformat}
      sessionState.out =
          new PrintStream(new FileOutputStream(sessionState.getTmpOutputFile()), true, CharEncoding.UTF_8);
      sessionState.err =
          new PrintStream(new FileOutputStream(sessionState.getTmpErrOutputFile()), true,CharEncoding.UTF_8);
{noformat}

[~xuefuz]  Sorry. Didn't get time to work on that.  MyPrintStream class will be cleaner, but it's not easy to differentiate if it's file based or not from the stream itself since System.out or System.err can also point to file based stream as well. So it's tight to the class HiveCommandOperation class themselves and we may need to pass a flag "flushOnClose" to the MyPrintStream class. Let me look into that., Thank you for reaching me. I Agree that we should never close the standard err and out stream. Seems HIVE-11579 didn't addressing the Command Handler code path. Could you add some unit test for this as well? , OK. Seems we don't need flush the string manually since autoFlush is set to true in PrintStream {{PrintStream(OutputStream out, boolean autoFlush, String encoding) }}., Thanks [~Ferd] I will try to add one unit test to cover that., Attached the new patch which will close the streams if there is an exception in HIveCommandOperation. Reset the streams for SQLOperation to standard out/err when initializing., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12766875/HIVE-11710.4.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 9698 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5682/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5682/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5682/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12766875 - PreCommit-HIVE-TRUNK-Build, [~xuefuz] can you help review the new patch? , +1, Xuefu, can you help submit the patch?, Committed to master. Thanks, Aihua.]