[+1, if a hadoop task is being failed - how is it that any side effect files created by hive code running in that task are getting promoted to the final output?

i think the forwarding is a red-herring. we should not commit output files from a failed task., @joydeep, the output file will not be committed if an exception occurred and close(abort=true) is called. This bug happened in a short time window after the exception occurred and before the close(abort) is called. Although the file got deleted, the dynamic partition insert already created a directory which later will be considered as an empty partition. , yeah - but then the directory itself should be created as a tmp directory. and we should promote the directory to it's final name only when closing successfully., Discussed with Joydeep offline. The side effects of failed task should be cleaned after the job finished. _tmp* files are already taken care of in the current code base. The only side effect that need to be taken care of is the empty directories created by failed dynamic partition inserts. This issue is addressed in HIVE-1655. 
, Should we close this one as Won't Fix?, Closing this for now]