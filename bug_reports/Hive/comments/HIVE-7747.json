[When I run the spark job locally (spark.master=local), it completes successfully. It repros only on the Spark cluster. Similar exception is seen in HIVE-7437. HIVE-7437 suggested shading jetty/servlet classes, but I still see the same exception., Problem is that we ship a wrong jar to Spark cluster. Instead of hive-exec, we ship hive-common. In SparkClient, we get the jar from HiveConf.getJar() which returns that jar that contains the initialization class. Initialization class given to HiveConf is different in HS2 versus CLI. In CliDriver (see run() method), SessionState.class (contained in hive-exec jar) is passed to HiveConf. In HS2 no initialization class is passed which defaults to HiveConf.class (contained in hive-common). 

The error thrown in Spark task is strange. Not sure if it is the standard error that is throw if no classes are found on classpath. Attaching a fix to pass SessionState.class as initilization class to HiveConf in HiveSessionImpl. It is a general fix, not specific to spark branch., +1

I think we can apply this to trunk if the tests pass!, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12662930/HIVE-7747.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6003 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestJdbcWithMiniMr.org.apache.hive.jdbc.TestJdbcWithMiniMr
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/412/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/412/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-412/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12662930, Test failure here is related to the change. Failure is complicated. It turns out that output of {{HiveConf(srcHiveConf, SessionState.class)}} is not same as srcHiveConf in terms of (property, value) pairs. Executed as part of constructor, the {{HiveConf.initialize}} method applies system properties on top of copied properties from srcHiveConf. So from the moment srcHiveConf is created to the moment of cloning HiveConf if there are any System properties set, cloned HiveConf inherits those properties. In the test case ({{MiniHS2}}) scratchdir property is modified in System properties (See [here|https://github.com/apache/hive/blob/trunk/itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java#L184]), but the default scratchdir value is {{$\{test.tmp.dir\}/scratchdir}} from hive-site.xml. Scrathdir set in {{MiniHS2}} is never used before, but with this change HS2 started using it. Scratchdir created in {{MiniHS2}} (See [here|https://github.com/apache/hive/blob/trunk/itests/hive-unit/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java#L183]) doesn't have 777 permissions, so whenever we have user impersonation there are issues (thats where the test is failing). Before this change, scratchdir is always {{$\{test.tmp.dir\}/scratchdir}} which is created in HS2 with 777 permissions (See [here|https://github.com/apache/hive/blob/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L3458]), so there were no issues with the impersonation. 

I think it is better to fix this in SparkClient by fetching the jar directly than through HiveConf, to avoid unexpected issues.

, Attaching v2 patch specific to spark-branch., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12663103/HIVE-7747.2-spark.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5958 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_opt_vectorization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union2
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_fs_default_name2
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/66/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/66/console
Test logs: http://ec2-54-176-176-199.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-66/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12663103, Wow, nice analysis! +1, Thank you so much Venki! I have committed this to spark!, 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 2.0 failed 4 times, most recent failure: Lost task 7.3 in stage 2.0 (TID 28, cloud1014113114.wd.nm.ss.nop.sogou-op.org): java.lang.IllegalStateException: unread block data
        java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2421)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1382)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:160)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace: (state=,code=0)

---

spark 1.1.1 ]