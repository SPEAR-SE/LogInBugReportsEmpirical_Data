[The reason we didn't get this error is because this happens in {{CopyFunction}}, which is only used when caching is enabled., @Chao, copyBytesWritable() method is also used in other places. I guess the interesting question is why we sometimes get different key type and what dertermines that. Also, it would be very confusing to have a copy method that takes BinaryComparable but returns a BytesWritable always. Could you do a little more research on this?, Rebasing on the latest update., [~xuefuz] OK, I'll take a look. Currently, this function is called in both {{CopyFunction}} and {{HiveBaseFunctionResultList}}, and in both cases it reads input from the HadoopRDD. So I'm not sure why this error didn't happen in {{HiveBaseFunctionResultList}}. I'll take a look and come back., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676184/HIVE-8545.1-spark.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6772 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tez_smb_1
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/249/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/249/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-249/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676184 - PreCommit-HIVE-SPARK-Build, OK, first, if the function is called in {{CopyFunction}}, then the input key & values are coming from {{RecordReader}}, and the value type extends from {{Writable}}, for instance, {{Text}}. In this case, we cannot cast it to {{BytesWritable}}.

Second, if the function is called from {{HiveBaseFunctionResultList::collect()}}, then the value type is actually {{BytesWritable}}, since it is returned from {{ReduceSinkOperator::makeValueWritable}}. In this method, a serializer will be used (the actual type is {{LazyBinarySerDe}}) to create new {{BytesWritable}} from a input object, which could be of type {{Text}}., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676195/HIVE-8545.2-spark.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 6771 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tez_smb_1
org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.testStatsAfterCompactionPartTbl
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/250/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/250/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-250/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676195 - PreCommit-HIVE-SPARK-Build, This patch mainly changes type signatures of {{MapInput}}, and uses {{WritableUtils.clone}} to copy {{Writable}}., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676376/HIVE-8545.3-spark.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 6772 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_load_dyn_part5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tez_smb_1
org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.testStatsAfterCompactionPartTbl
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/251/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/251/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-251/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676376 - PreCommit-HIVE-SPARK-Build, Addressing RB comments., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676491/HIVE-8545.4-spark.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 6772 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tez_smb_1
org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.testStatsAfterCompactionPartTbl
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/252/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/252/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-252/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676491 - PreCommit-HIVE-SPARK-Build, +1, Actually there seems a problem in the precondition assertion. Patch #5 fixes that, with a little bit code cleanup. [~csun], could you take a look at the patch?, [~xuefuz] Oops, it should be {{copyFunction != null}}. Sorry for this silly mistake., [~xuefuz] Yeah, I think it's better this way - I also considered to get rid of {{toCache}}., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676519/HIVE-8545.5-spark.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 6772 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tez_smb_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_multi_insert_move_tasks_share_dependencies
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/253/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/253/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-253/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676519 - PreCommit-HIVE-SPARK-Build, [~csun], I have a second thought. I think we can still keep HiveCopyFunction where it used to be. I think calling WritableUtils.clone() doesn't require Spark's JobConf. We can just create a default Configuration, conf = new Configuration() and pass it to WritableUtils.clone(). That way, HiveCopyFunction can keep its old way and stay where it was. After doing this, we can keep toCache variable in MapInput. This seems a little cleaner. What do you think?, [~xuefuz] I agree. This indeed would be a better approach. Let me make the change and re-upload a patch., The test failure on {{multi_insert_move_tasks_share_dependencies.q}} is likely related,  but the "Test logs" link doesn't work, so I cannot see the exact error message. I tried several times on my local machine, but couldn't reproduce it., OK, I managed to find the log file with help from Brock. I think the test failure is caused by this error:

{noformat}
2014-10-22 20:44:05,883 INFO  exec.Utilities (Utilities.java:getBaseWork(425)) - File not found: File
file:/home/hiveptest/54.177.19.115-hiveptest-0/apache-svn-spark-source/itests/qtest-spark/target/tmp/scratchdir/hiveptest/e91b92c0-6d0b-49a7-b7aa-5cb7ccb9e7da/hive_2014-10-22_20-44-05_260_8919336312617093153-1/-mr-10002/275fcb87-b139-4069-a544-b55d6c28ea44/reduce.xml
does not exist
2014-10-22 20:44:05,883 INFO  exec.Utilities (Utilities.java:getBaseWork(426)) - No plan file found:
file:/home/hiveptest/54.177.19.115-hiveptest-0/apache-svn-spark-source/itests/qtest-spark/target/tmp/scratchdir/hiveptest/e91b92c0-6d0b-49a7-b7aa-5cb7ccb9e7da/hive_2014-10-22_20-44-05_260_8919336312617093153-1/-mr-10002/275fcb87-b139-4069-a544-b55d6c28ea44/reduce.xml
2014-10-22 20:44:05,883 INFO  mr.ObjectCache (ObjectCache.java:cache(36)) - Ignoring cache key: __REDUCE_PLAN__
2014-10-22 20:44:05,883 DEBUG parse.SemanticAnalyzer (SemanticAnalyzer.java:genSelectPlan(3488)) - genSelectPlan: input = null{(key,_col0: string)(value,_col1: string)} {((tok_table_or_col
key),_col0: string)((tok_table_or_col value),_col1: string)}
2014-10-22 20:44:05,884 DEBUG parse.SemanticAnalyzer (SemanticAnalyzer.java:genSelectPlan(3643)) - Created Select Plan row schema: {((tok_table_or_col key),_col0: string)((tok_table_or_col
value),_col1: string)}
2014-10-22 20:44:05,884 DEBUG parse.SemanticAnalyzer (SemanticAnalyzer.java:genSelectPlan(3370)) - Created Select Plan for clause: insclause-3
2014-10-22 20:44:05,884 ERROR executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 135.0 (TID 283)
java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.init(SparkReduceRecordHandler.java:120)
  at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.call(HiveReduceFunction.java:44)
  at org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.call(HiveReduceFunction.java:27)
  at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:167)
  at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:167)
  at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
  at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
  at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:86)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
  at org.apache.spark.scheduler.Task.run(Task.scala:56)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:744)
{noformat}

It seems like a strange error, by looking at the log message I couldn't figure out why it happened. , [~csun], could you try to run this test locally? I tried yesterday and it passed for me somehow.

I think because of the missing reduce plan file, gWork is null below:
{code}
    if (gWork == null) {
      gWork = Utilities.getReduceWork(job);
      cache.cache(PLAN_KEY, gWork);
    } else {
{code}, [~xuefuz] Yes, I ran this test locally several times, and they all passed., I think we can get this in first, and log a followup JIRA to investigate the test failure above. [~csun], could you load up your latest patch with changes discussed above?, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12676628/HIVE-8545.6-spark.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 6771 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_sample_islocalmode_hook
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tez_smb_1
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_ppd_multi_insert
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/256/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-SPARK-Build/256/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-SPARK-Build-256/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12676628 - PreCommit-HIVE-SPARK-Build, +1. Please create the followup JIRA mentioned above to track the new test failure above., Committed to Spark branch. Thanks to Chao for the contribution.]