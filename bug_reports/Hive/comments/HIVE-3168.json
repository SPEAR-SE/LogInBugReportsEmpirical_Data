[review board link - https://reviews.facebook.net/D3777, @Thejas: I left some comments on phabricator. Thanks., Updated patch on phabricator. Fixes JavaBinaryObjectInspector and WritableBinaryObjectInspector as well. Also, tries to avoid copy of byte[] in BytesWritable ., @Thejas: Is the patch ready for review?, Yes, the updated patch on phabricator is ready for review. , HIVE-3168.2.patch - attaching patch that has already been published in phabricator., I downloaded hive-0.9.0 and applied the second patch "HIVE-3168.2.patch" using the command "patch -p0 < /home/neha/hive_patches/HIVE-3168.2.patch". 
After this, if I try to build the hive, it fails. Can you please let me know if I am doing anything incorrect. Thank you., One more question here. Do we need to apply both the patches attached?, The 2nd patch replaces the first, so you need to apply on the 2nd one. I tried applying it to a fresh checkout of hive 0.9 branch and I was able to build it fine. Please post the build error if you think it is related to the patch.
, Thanks Thejas. I applied only the second patch and the build was fine. However, I am still seeing the same incorrect values coming back. Could you please let me know if you have verified and this works fine for you.
e.g. I have following data in table.
a
ab
abc
abcd
abcde
z
And the values (this is in hexadecimal) coming back from HCatalog is :
61
616200
616263
616263640000
616263646500
7A6263646500, Please let me know if anyone tried and these patches are working fine. I need to look into my setup if this works fine for you. Would be very helpful if I get any clue here. Thanks a lot., Yes, I tried the patch and it worked for me with hcatalog. I tested it using MR job that uses hcat to read data. 
You might want to make sure that the you have deployed the updated hive jars and that hcatalog is using those.
hive build has some issues now, and I had to do "rm -rf ~/.ivy2/local/org.apache.hive/hive-* ; rm -rf ~/.ivy2/cache/org.apache.hive/hive-*" to clear the old jars. I am not sure if this affects only unit tests, or if it affects the package creation as well. But you might want to give that a try. 

Hive(ie hcat as well) expects the binary column be base64 encoded (at least for text format). The values you pasted don't have the padding with '==', and my  understanding is that base64 is expected to be padded to 4 bytes. Can you try with valid base64 encoding of printable ascii strings ?, The command for clearing ivy cache for hive is {code} "rm rf ~/.ivy2/local/org.apache.hive/hive* ; rm rf ~/.ivy2/cache/org.apache.hive/hive*" {code}
, Thanks a lot, Thejas. Will try this. 
The data that I have pasted above is the hive output. And, the data actually stored is in base64 encoding only., Tried the given suggestion. Replaced everything, but still same issue. Not sure where I am making the mistake. I am using hcatalog released version 0.4.0. Can you please confirm if there is no change in this hcatalog code., Neha,
Try 'ant clean' on the hive dir before building it. 
To verify that the patched hive code is getting used, try adding a debug message in LazyBinaryObjectInspector around line 43 (after applying patch) (note the debug message would come out of map task). Or change the data setting the first byte, and see if changes your result -
{code}
    System.arraycopy(bWritable.getBytes(), 0, data, 0, bWritable.getLength());
    ba.setData(data);
{code}
to 
{code}
    System.arraycopy(bWritable.getBytes(), 0, data, 0, bWritable.getLength());
    data[0]='Z';
    ba.setData(data);
{code}
I am not sure if I tested with hcat 0.4 branch or trunk. But I don't think there has been any change that should affect this behavior, it should work with hcat 0.4 release as well.
, Thanks a lot, Thejas. It is working fine now., Can any committer please review the 2nd patch ? (also in phabricator link), HIVE-3168.3.patch - updated patch based on Ashutosh's comments in phabricator. Also updated patch on phabricator.
, New patch is ready for reivew., Committed. Thanks, Thejas!, Integrated in Hive-trunk-h0.21 #1539 (See [https://builds.apache.org/job/Hive-trunk-h0.21/1539/])
    HIVE-3168: LazyBinaryObjectInspector.getPrimitiveJavaObject copies beyond length of underlying BytesWritable (Thejas Nair via Ashutosh Chauhan) (Revision 1360812)

     Result = SUCCESS
hashutosh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360812
Files : 
* /hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
* /hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyBinaryObjectInspector.java
* /hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBinaryObjectInspector.java
* /hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBinaryObjectInspector.java
* /hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
, Ashutosh,
Can you please apply this fix to 0.9 branch as well ?
, Integrated in Hive-trunk-hadoop2 #54 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/54/])
    HIVE-3168: LazyBinaryObjectInspector.getPrimitiveJavaObject copies beyond length of underlying BytesWritable (Thejas Nair via Ashutosh Chauhan) (Revision 1360812)

     Result = ABORTED
hashutosh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360812
Files : 
* /hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
* /hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyBinaryObjectInspector.java
* /hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBinaryObjectInspector.java
* /hive/trunk/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBinaryObjectInspector.java
* /hive/trunk/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
, This issue is fixed and released as part of 0.10.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.]