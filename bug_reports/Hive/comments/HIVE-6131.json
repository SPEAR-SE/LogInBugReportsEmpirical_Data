[Can you please clarify in your step #6 whether you're editing local file or HDFS file? If you're editing your local file, then you will need to load the data again as you do in step #3, or manually replace the file on HDFS with your local edited file., Editing the local file.  Step 7 is supposed to handle re-uploading the file using an OVERWRITE command like you say., Verified this issue with Apache Hive 0.10, 0.11, 0.12

The conclusion is: Hive 0.11 and 0.12 have this issue, while Hive 0.10 doesn't. That is to say, starting from Hive 0.11, the issue was introduced., Browsing the code, it seems like this was introduced by fix for HIVE-3833 in Hive 0.11. In that patch, the partition schema is used to read results, instead of the table schema as it was before. Since partition schema is a snapshot of table schema at the time of partition creation, it doesn't contain all the new columns added later. So, the result is read using stale schema and thus do not contain new column values, even though they are present in underlying data.

Clearly the intent of the 3833 patch is to use partition specific metadata, to allow for multiple serdes for partitions of a table (as i understand it). This issue seems to be a regression introduced by that patch.

One possible fix is to use partition metadata, except update column list from table metadata. It is quite possible that while this will work, this may not be the 'right' fix. [~namitjain]] [~ashutoshc], any thoughts on this?

, [~pala] Your suggested fix makes sense., Ok, i'll work on the patch.

[~ashutoshc], my fix assumes all serdes can work with latest table schema, even though partition data is in older schema. Is that valid? , Currently, hive only allows to add new columns in newer partition via alter table add column, so Hive should insert nulls for new columns which are absent in old partitions. Serdes should honor this. 
On the other hand, if user is doing alter table replace columns than all bets are off and they should know what they are doing, Hive doesnt handle those scenarios and as far as I see it, its very hard to support that., We saw this issue earlier while working with schema evolution for parquet and other serde, but had thought it was expected behavior (that different partition keep old column schema after HIVE-3833).  This will be a good fix to have., HIVE-6131: Honor latest table schema for partition data

    Ensure partition schema object for data read always picks
    up the schema from table metadata. Otherwise, it gets the
    metadata snapshot during partition creation which may now
    be stale.

    Added .q tests for verification., The above was intended to be code review description. [~ashutoshc], can you please review or have the right owner look at this change? Thanks., Honor latest table schema for partition data

Ensure partition schema object for data read always picks
up the schema from table metadata. Otherwise, it gets the
metadata snapshot during partition creation which may now
be stale.
Added .q tests for verification., Hi , I'm wondering if its possible to instead set the correct column metadata on the partition during 'alter table'? 

This patch changes hive to use table instead of partition metadata on initializing the de-serializer for partition.  While it works to return correct query result, the partition metadata (for example if you do 'describe partition' after 'alter table add columns') still shows the old columns before the alter, which is now inconsistent with the results returned., That thought did cross my mind. However, I wasn't sure if there are other dependencies that actually need the table schema snapshot during partition creation (which is stored as partition level schema). 

A related question is why store partition specific column schema if it is always identical to current table column schema? Then partition metadata should not include column schema at all and always pick it up from table metadata. 

I guess i am unsure about semantics of partition schema. Does it represent current table schema, and if so, why we keep a copy separate from table schema. If not, then it represents schema during partition creation, so it is possible that it is out of date, leading to the inconsistency you describe.
, bq. That thought did cross my mind. However, I wasn't sure if there are other dependencies that actually need the table schema snapshot during partition creation (which is stored as partition level schema).

You mean a use-case to keep partition.sd() as the original table.sd()?  Doesnt seem likely, but I could be wrong.

bq. A related question is why store partition specific column schema if it is always identical to current table column schema?  Does it represent current table schema, and if so, why we keep a copy separate from table schema. If not, then it represents schema during partition creation, so it is possible that it is out of date, leading to the inconsistency you describe.

Yea I wonder that as well, thats why I had originally assumed it was by design that partition columns can be different than table column.  Otherwise why waste all the metastore's memory/storage and store it in different places?   But again I dont have the full context, does [~ashutoshc] or others have some background, if theres a use-case of what Pala described?, [~ashutoshc], any thoughts on this?, There are lots of test cases in Hive covering these scenarios. It will be good to get this patch tested on those and see where we stand. You can follow https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing to get Hive QA to kick a CI build., Thanks, will do [~ashutoshc]. However, i need a login with apache jenkins. Could you or somebody else add a login for me?, Hi Pala, you can just re-upload the same patch again.  Jenkins job will pick it up automatically.  I think the first patch you uploaded got missed by the jenkins job during an outage.
, Sorry I assigned to myself accidentally while commenting, please assign back to yourself., [~szehon], i have reuploaded the patch with expected name. I still don't see a job in progress for jenkins Hive precommit build. Let me know if something else is needed. , It should be there, are you looking at [http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/|http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/]?   I think its either build 2054-2056 depending on when it was uploaded.  Let's wait for those and see., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637927/HIVE-6131.1.patch

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 5514 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat14
org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler
{noformat}

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2054/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/2054/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637927, I looked into the failures above and revisited HIVE-3833 with more context now:
1. LazyBinaryColumnarSerde requires partition level metadata to read existing data, it needs exact metadata used when serializing the data. So cannot use table level metadata which could have changed.
2. Other serdes/format, which support schema change, needs updated schema to support newly appended data with new columns.

So, it seems we should pass the table metadata or partition metadata selectively, depending on what the storage/serde supports. Is there a way to identify the serdes/format that do not support newer schema, programmatically? I don't see anything obvious. Alternative is to
a.  Add such metadata to serde info and populate that for all serdes. This may have been discussed briefly in HIVE-3833, and looks like this will be a large change because it essentially modifies interface for a plugin.
b.  Hardcode a white or blacklist of serdes and pass table/partition level metadata accordingly.

[~ashutoshc], [~szehon], any thoughts on the above, particularly are there other alternatives?

, Hm I understand file-format may differ between partition and table, that was the point of HIVE-3833.   But just for my understanding, did you find any use for the partition-columns being different from table-columns (being the original)? 

In my experience, I had seen that LazyBinaryColumnarSerde (and other serde) can use a schema with more columns to de-serialize data than what the data was written with.  If thats the case, cant we make the column set same for partition and table, during 'alter table'?, Yes, one case where partition column is different than table column is the type of an existing column is changed (e.g: from string to int) at the table level, after a partition is created. So column1 is string on partition, but int on table.

That is the cause of failure for the unit test 'org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat11' you see above.

So we can't alter the partition schema to match table schema, because underlying data in LazyBinaryColumnarSerde was written to adhere to old schema. If you just change the schema, but not the data, the data will be read incorrectly.

Yes serdes can handle additional columns by treating them as null valued. However, we are trying to address a more complex case: e.g: Table and partition has 3 columns. We add one more column at table level. Then at the partition, we append (not overwrite) data with 4th column too. Now the partition has some data with 3 columns, but some data with 4 columns. LazySimpleSerde can work in this scenario, but LazyBinaryColumnarSerde doesn't.  , Makes sense.  I had fixed LazyBinaryColumnarSerde to handle one of those case in HIVE-5788 (add column), but it did not fix handling change column-type as you observed.  Maybe we can try fixing that, but I guess its not easily supported by all the serdes.

An alternate thought I had is to introduce an 'alter partition' statement, to allow the user specify what schema to use on read., Do you mean allowing schema update statements also at partition level? Probably a more specific 'sync partition schema to table schema' command would be better, but even that i think is not natural.

I think best approach is to change implementation - use table schema whenever a serde can support latest schema correctly, otherwise use partition schema., Yea thats what I meant, 'alter table partition (spec) add column', etc, I wonder why its not natural?  Imo it would gives more flexibility to user.

My concern with your approach is it locking user to have only one choice per Serde.  For example take Rcfile's serdes, which would you pick?  If it is 'table-schema' , then you hit the partition..11.q issue (error after column-type change).  If it is 'partition-schema', then you hit the JIRA's issue (add column, new data loaded in that column is null) because partition schema is never updated.  There might be users interested in both cases (we ourselves are interested to get the latter use case for RCFile).







, You are right, types of existing columns may change so partition schema may never be same as table schema, so cannot pick one or the other. 

Let's say we support add columns DDL at partition level. What can be allowed? Can users add arbitrarily different columns compared to table, or should they only add columns that are present in table level, but are missing at partition level, in the same order? 

e.g: Initial schema: Table t (A, B, C, D), Partition p (A', B'). Can users only execute 'Alter table t partition (p) add columns C,D'? Or can they do something else also 'alter table t partition (p) add columns E, F, G'? 

If it is only the former, then we still can do the same programmatically, by 'merging' the partition and table schema at runtime. However, if the table schema itself can be wildly different compared to partition schema, then yes, DDL is the only option, and users have to manage it themselves., There is a related proposal over at HIVE-6835 to pass both to the serde and let them choose.

However, it doesnt seem to solve the issue about RCFile serde.   I agree its a mess, unfortunately the case like change column type for RCFile makes it possible for partition + table schemas to go out of sync with more than just #cols, and hard to do the 'merge' as you proposed.  I think it would be nice not to support it going out of sync this way, not sure if its possible to change at this point., how did this bug fix?, I met this bug too. And this patch really can solve my problem.
Could anyone have a look at this patch?, This is not a bug, you can use `alter table t1 replace columns (c1 string, c2 string) cascade`, see https://issues.apache.org/jira/browse/HIVE-8839
, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637927/HIVE-6131.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 69 failed/errored test(s), 9899 tests executed
*Failed tests:*
{noformat}
TestHWISessionManager - did not produce a TEST-*.xml file
TestMiniLlapCliDriver - did not produce a TEST-*.xml file
TestMiniTezCliDriver-auto_sortmerge_join_7.q-orc_merge9.q-tez_union_dynamic_partition.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-join1.q-mapjoin_decimal.q-union5.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-load_dyn_part2.q-selectDistinctStar.q-vector_decimal_5.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-mapjoin_mapjoin.q-insert_into1.q-vector_decimal_2.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-vector_distinct_2.q-tez_joins_explain.q-cte_mat_1.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-vector_interval_2.q-schema_evol_text_nonvec_mapwork_part_all_primitive.q-tez_fsstat.q-and-12-more - did not produce a TEST-*.xml file
TestMiniTezCliDriver-vectorized_parquet.q-insert_values_non_partitioned.q-schema_evol_orc_nonvec_mapwork_part.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-join4.q-groupby_cube1.q-auto_join20.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-parallel_join1.q-escape_distributeby1.q-auto_sortmerge_join_7.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_partition_change_col
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_table_cascade
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ivyDownload
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_data_after_schema_update
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat11
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_nonvec_mapwork_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_nonvec_mapwork_part_all_complex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_nonvec_mapwork_part_all_primitive
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_vec_mapwork_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_vec_mapwork_part_all_complex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_vec_mapwork_part_all_primitive
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_vecrow_mapwork_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_vecrow_mapwork_part_all_complex
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_schema_evol_text_vecrow_mapwork_part_all_primitive
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_partition_diff_num_cols
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver_hbase_queries
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_text_nonvec_mapwork_part
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_text_nonvec_mapwork_part_all_complex
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_text_vec_mapwork_part
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_text_vec_mapwork_part_all_complex
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_text_vec_mapwork_part_all_primitive
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_text_vecrow_mapwork_part
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_text_vecrow_mapwork_part_all_complex
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_schema_evol_text_vecrow_mapwork_part_all_primitive
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_partition_diff_num_cols
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_many
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join18
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_auto_join_reordering_values
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_simple_select
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_udf_udaf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_dynamic_rdd_cache
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_identity_project_remove_skip
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join_merge_multi_expressions
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_stats9
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_subquery_in
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.llap.tez.TestConverters.testFragmentSpecToTaskSpec
org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.testFinishableStateUpdateFailure
org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInRemote.org.apache.hadoop.hive.metastore.TestAuthzApiEmbedAuthorizerInRemote
org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf
org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs
org.apache.hadoop.hive.metastore.TestHiveMetaStoreStatsMerge.testStatsMerge
org.apache.hadoop.hive.metastore.TestMetaStoreInitListener.testMetaStoreInitListener
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics
org.apache.hadoop.hive.metastore.TestRemoteUGIHiveMetaStoreIpAddress.testIpAddress
org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.testRevokeTimedOutWorkers
org.apache.hadoop.hive.ql.security.TestExtendedAcls.org.apache.hadoop.hive.ql.security.TestExtendedAcls
org.apache.hadoop.hive.ql.security.TestFolderPermissions.org.apache.hadoop.hive.ql.security.TestFolderPermissions
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.testSimplePrivileges
org.apache.hive.hcatalog.api.repl.commands.TestCommands.org.apache.hive.hcatalog.api.repl.commands.TestCommands
org.apache.hive.minikdc.TestJdbcNonKrbSASLWithMiniKdc.org.apache.hive.minikdc.TestJdbcNonKrbSASLWithMiniKdc
org.apache.hive.minikdc.TestJdbcWithDBTokenStore.org.apache.hive.minikdc.TestJdbcWithDBTokenStore
org.apache.hive.service.cli.session.TestHiveSessionImpl.testLeakOperationHandle
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
{noformat}

Test results: http://ec2-54-177-240-2.us-west-1.compute.amazonaws.com/job/PreCommit-HIVE-MASTER-Build/315/testReport
Console output: http://ec2-54-177-240-2.us-west-1.compute.amazonaws.com/job/PreCommit-HIVE-MASTER-Build/315/console
Test logs: http://ec2-50-18-27-0.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-315/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 69 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637927 - PreCommit-HIVE-MASTER-Build, Hive provides new DDL to handle such situation in later versions after HIVE-3833 is applied in 0.11.0.
See: [Language Manual CASCADE|https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Add/ReplaceColumns]

??The CASCADE|RESTRICT clause is available in Hive 0.15.0. ALTER TABLE ADD|REPLACE COLUMNS with CASCADE command changes the columns of a table's metadata, and cascades the same change to all the partition metadata. RESTRICT is the default, limiting column changes only to table metadata.??

In this case you can change your step 5 to
5. Alter the column definitions: ALTER TABLE jvaughan_test REPLACE COLUMNS (col1 string, col2 string) *CASCADE*;, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637927/HIVE-6131.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 24 failed/errored test(s), 10723 tests executed
*Failed tests:*
{noformat}
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=143)
	[vectorized_rcfile_columnar.q,vector_elt.q,explainuser_1.q,multi_insert.q,tez_dml.q,vector_bround.q,schema_evol_orc_acid_table.q,vector_when_case_null.q,orc_ppd_schema_evol_1b.q,vector_join30.q,vectorization_11.q,cte_3.q,update_tmp_table.q,vector_decimal_cast.q,groupby_grouping_id2.q,vector_decimal_round.q,tez_smb_empty.q,orc_merge6.q,vector_decimal_trailing.q,cte_5.q,tez_union.q,cbo_rp_subq_not_in.q,vector_decimal_2.q,columnStatsUpdateForStatsOptimizer_1.q,vector_outer_join3.q,schema_evol_text_vec_part_all_complex.q,tez_dynpart_hashjoin_2.q,auto_sortmerge_join_12.q,offset_limit.q,tez_union_multiinsert.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=148)
	[auto_sortmerge_join_13.q,union_top_level.q,vector_left_outer_join2.q,schema_evol_text_vecrow_part_all_primitive.q,constprog_semijoin.q,update_where_partitioned.q,drop_partition_with_stats.q,smb_mapjoin_14.q,skiphf_aggr.q,vectorized_ptf.q,auto_join_filters.q,join0.q,insert_orig_table.q,mergejoin.q,join_filters.q,orc_split_elimination.q,subquery_in.q,vector_outer_join0.q,schema_evol_text_vec_part_all_primitive.q,vector_complex_all.q,auto_sortmerge_join_4.q,bucket_many.q,vectorization_15.q,union3.q,vectorization_nested_udf.q,windowing_windowspec2.q,auto_smb_mapjoin_14.q,vector_mr_diff_schema_alias.q,vector_join_filters.q,reduce_deduplicate_extended.q]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[alter_partition_change_col] (batchId=23)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[alter_table_cascade] (batchId=79)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_data_after_schema_update] (batchId=17)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat11] (batchId=6)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat12] (batchId=75)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat13] (batchId=58)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat14] (batchId=69)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_nonvec_part] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_nonvec_part_all_complex] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_nonvec_part_all_primitive] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vecrow_part] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vecrow_part_all_complex] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=92)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2487/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2487/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2487/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 24 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637927 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12637927/HIVE-6131.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 20 failed/errored test(s), 11000 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[alter_partition_change_col] (batchId=24)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[alter_table_cascade] (batchId=84)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_data_after_schema_update] (batchId=17)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat11] (batchId=7)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat12] (batchId=79)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat13] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat14] (batchId=73)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_nonvec_part] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_nonvec_part_all_complex] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_nonvec_part_all_primitive] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part_all_complex] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part_all_primitive] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vecrow_part] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vecrow_part_all_complex] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vecrow_part_all_primitive] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=180)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6504/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6504/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6504/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 20 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12637927 - PreCommit-HIVE-Build]