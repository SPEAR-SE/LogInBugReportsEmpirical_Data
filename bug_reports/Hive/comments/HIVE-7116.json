[Hi Vikram,

Couple of smaller comments on the patch.

{code}
LOG.info("Create dirs " + mkdirPath + " with permission " + fsPermission + " recursive " + recursive);
{code}

You probably don't need to manually concatenate the string but let log4j do it for you.

{code}
LOG.info("Create dirs {} with permission {} recursive {}", mkdirPath, fsPermission, recursive);
{code}

Secondly, I think that if you create a FileSystem using the newInstance method, it should be closed to avoid any resource leaks. Previosuly with the cached FS instance, the API was automatically handling that for you., Address Swarnim's comment., [~swarnim] Changing the log to that format affects readability as it is the way log has been done in Hive. I agree that is not a strong argument but, there is no advantage either ways.

The fs.close() is a good catch! Thanks for your quick review!, Should the fs.close() be in a finally so that an exception during mkdirs doesn't still cause a resource leak? We might as well use IOUtils.closeQuietly if we do unnecessarily want the fs.close() exception to propagate up the stack., Although not strictly required in this case, it makes sense to have the finally block do both the close and reset of the configuration flag. Updated., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646619/HIVE-7116.3.patch

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 5533 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_bucketsortoptimize_insert_5
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_load_dyn_part1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.common.metrics.TestMetrics.testScopeConcurrency
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/275/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/275/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-275/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646619, The try/finally block will mask exceptions in case there is a double failure (create fails then close fails in the finally block). Fixing that., Hey Vikram,

Looking deeper into the patch, in my opinion there could be some issues:

1. "hadoop-1" pulls in the 1.2.1 hadoop jar. The FileSystem object on this doesn't have the newInstance method on it. So with "-P hadoop-1" this would fail with a compilation error.
2. I think the latest patch is overly complicated for what it is trying to do. I think it could be simplified to just the following:

{code}
FileSystem fs = FileSystem.newInstance(mkdirPath.toUri(), conf);
    boolean retval = false;
    try {
      retval = fs.mkdirs(mkdirPath, fsPermission);
    } finally {
      org.apache.commons.io.IOUtils.closeQuietly(fs);
    }
{code}

3. Are we sure that it is the cached FileSystem instance that is causing the permissions to not get reflected? I wrote a simple test with cached FileSystem object[1] that sets permissions and it passed for me. Interestingly it only fails for "777" permissions so might be some issue specific to that additional "executable" sticky bit.

Just my 0.02 :)

[1] http://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/fs/FileSystem.html
[2] https://gist.github.com/swarnim87/afd29a06e6cc7fffae5c, [~swarnim] Good catch about the hadoop-1 compilation issue. [~vikram.dixit] To work around that you can set ""fs.%s.impl.disable.cache"=true in conf, and then call FileSystem.get(URI uri, Configuration conf)

[~swarnim] Regarding 2,  I think it is important to re-set the conf to its old state in case of a failure as well.

[~vikram.dixit] You can use a function to avoid code duplication within the finally block and outside. I think this log message is better logged at DEBUG level as it is also being called for every vertex creation in Tez mode.

[~swarnim] Regarding 3, try your example with a path name where more than one level in dir structure needs to be created "eg "/tmp/a/b/c". You will notice that only c gets created with FsPermission value. That is why the umask needs to be set in config so that "a" and "b" also get created with appropriate permissions. The cache becomes an issue only if you already have a FileSystem object in cache with old config. Your example is not resulting in a FileSystem object in cache with old config.





, I think this is better solved by the shims instead of writing ugly hacks that we have to keep around forever. Refreshed with comments and shim changes., +1 for shims. Very clean approach.

Should we have some tests to prove that this actually fixes the permissions issue? Also adding a new method to HadoopShims seems like would make it non-passive. Not sure how much we care about passivity in shims but just a thought. Since it is an interface, not sure if we can even do much about it anyways. :)

Much thanks for looking into this Vikram!, Moved the 1.0 code to the appropriate shim and fixed an indentation error., +1 LGTM
, +1. Thanks again. :), 

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647495/HIVE-7116.7.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/336/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/336/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-336/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.6.0_34/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-336/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
Reverted 'metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java'
Reverted 'metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java'
Reverted 'itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java'
Reverted 'common/src/java/org/apache/hadoop/hive/common/FileUtils.java'
++ egrep -v '^X|^Performing status on external'
++ awk '{print $2}'
++ svn status --no-ignore
+ rm -rf target datanucleus.log ant/target shims/target shims/0.20/target shims/0.20S/target shims/0.23/target shims/aggregator/target shims/common/target shims/common-secure/target packaging/target hbase-handler/target testutils/target jdbc/target metastore/target itests/target itests/hcatalog-unit/target itests/test-serde/target itests/qtest/target itests/hive-minikdc/target itests/hive-unit/target itests/custom-serde/target itests/util/target hcatalog/target hcatalog/core/target hcatalog/streaming/target hcatalog/server-extensions/target hcatalog/hcatalog-pig-adapter/target hcatalog/webhcat/svr/target hcatalog/webhcat/java-client/target hwi/target common/target common/src/gen service/target contrib/target serde/target beeline/target odbc/target cli/target ql/dependency-reduced-pom.xml ql/target
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1598675.

At revision 1598675.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647495, Rebased to latest trunk., +1 for 0.13.1, +1 for 0.13.1 .
, Ran tests locally and they passed.
, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647680/HIVE-7116.8.patch

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 5496 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/347/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/347/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-347/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647680, Patch committed to trunk. Thanks Vikram!
[~sushanth] Can you please include it in 0.13.1 ?
, [~vikram.dixit] Can you please create a rebased patch for 0.13 ?
, 0.13 patch added., Committed to 0.13.1, Thanks, Vikram!, This jira has been fixed as part of 0.13.1 release. If you find further issues, please create a new jira and link it to this one.
]