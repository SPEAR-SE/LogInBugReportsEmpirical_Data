[This solution allows for columns to be added to the end of structs. It looks like adding support for arbitrary column-schema evolution in structs would be very tricky.

(Note: The solution doesn't change {{HCatRecordReader}} at all, since the entire struct is projected correctly by the reader.), Any one any ideas? Seems we encountered the same problem when using pig to read hive parquet table after we added a sub field to a structure type field.
{noformat}
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 6018: Error converting read value to tuple
    at org.apache.hive.hcatalog.pig.HCatBaseLoader.getNext(HCatBaseLoader.java:76)
    at org.apache.hive.hcatalog.pig.HCatLoader.getNext(HCatLoader.java:57)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:204)
    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)
    at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.IndexOutOfBoundsException: Index: 13, Size: 13
    at java.util.ArrayList.rangeCheck(ArrayList.java:635)
    at java.util.ArrayList.get(ArrayList.java:411)
    at org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:468)
    at org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:451)
    at org.apache.hive.hcatalog.pig.PigHCatUtil.extractPigObject(PigHCatUtil.java:410)
    at org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:468)
    at org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:386)
    at org.apache.hive.hcatalog.pig.HCatBaseLoader.getNext(HCatBaseLoader.java:64)
{noformat}, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12818676/HIVE-14274.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10249 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_auto_join1] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join31] (batchId=81)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[multiMapJoin2] (batchId=152)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join31] (batchId=133)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3654/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3654/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3654/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12818676 - PreCommit-HIVE-Build, bq. Any one any ideas? 

Yes. :] The patch I have up on this JIRA should allow for the addition of columns to the end of a struct. Supporting the general case will be a very large change to HCatalog., For InputFormats that implement SelfDescribingInputFormatInterface, you can push down the desired type to the input format and it will handle schema evolution for you., This is a dupe. Closing in favour of HIVE-17794.]