[reproduced on Windows even by applying -Dfile.encoding=UTF8JavaOption., The root reason is SQLServer varchar and nvarchar behaves fundamentally different when unicode chars are involved. Hive meta data table PARTITION(..., PART_NAME varchar(767), ...) contains varchar column, but unicode chars like 'tag=䶵' are inserted, which leads to garbled representation like 'tag=?'. Of course a query will be failed, however, if nvarchar is used for column creation, N'tag=䶵' with 'N' prefix is correct thing to be inserted, without 'N', also garbled in representation.
I traced this issue down to bottom-most layer of HIVE, that is, DataNucleus that talks to MSSQLServer as metastore backend, and found DataNucleus returns NULL partition that fails LOAD DATA as described. I guess this relates SQLServer nvarchar issue.
I need more time to hack DataNucleus and HIVE metastore DDL/DML script to confirm my speculation., Good news, nvarchar really matters. Query runs fine as long as table is created with columns as nvarchar data types.
I tried two approaches:
1. alter package.jdo (DataNucleus O/R mapping xml) to use nvarchar, re-compile and ship hive-metastore jar for deployment, there some serious exceptions while launching hive CLI. Something from JDO are out of our control.
2. update hive-schema-0.13.0.mssql.sql to nvarchar specification, drop metastore DB, and manually create it, but keep package.jdo mapping file unchanged. This works fine.

Next step is to think about how to maintain compatability, e.g. old schema upgraded to new schema., If MSSQL is metastore and unicode chars support is expected, for a fresh installation or legacy system that doesn't involve multi-bytes support, varchar-to-nvarchar upgrade works fine, however, the upgrade seems impossible for on-going Hive system, because MSSQL doesn't allow schema change on varchar typed primary key or index, which is typical Hive use case.

Any thoughts on how to do this upgrade? Thanks!, Would it be possible to create a brand new table where you replace the varchar column(s) with nvarchar, copy the data from the old table, and then rename the new table to the old one?, Thanks [~jdere] for this export approach. Actually I changed all varchar columns in all tables so as not to hit inconsitency issue. , [~rbains] once you mentioned it's doable using scripts, can you add comments? Thanks!, Made a patch. Can anyone please review it? Thanks!, Made the 2nd patch! Considered 0.14 fresh install., - So you change the type of PART_NAME in package.jdo to be of type NVARCHAR. A potential issue here is, package.jdo is used for the schema and object-relational mapping for all databases, but it looks like the point of this issue is to change the column for SQLServer. What is the effect on the other DBMSs when this field is set to NVARCHAR, but the column in the table is still set as VARCHAR? Is there a way to specify that it only should be NVARCHAR in the case of SQLServer?
- Can you also update metastore/scripts/upgrade/mssql/hive-schema-0.15.0.mssql.sql with the same changes?
- metastore/scripts/upgrade/mssql/004-HIVE-8550.mssql.sql: What is the line with "GO"? Sorry I don't really know sql server.
- metastore/src/model/package.jdo: Is the nesting of the first <column> element in the diff correct?, [~ashutoshc] do you know what the effect is (if any) for the first point in [this comment|https://issues.apache.org/jira/browse/HIVE-8550?focusedCommentId=14187736&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14187736]?, Thanks [~jdere] for detailed reviews. Here are answers in order:
1. I removed package.jdo changes, since it has no effects in changing varchar into nvarchar by autoCreateSchema enabled.
2. changed hive-schema-0.15.0.mssql.sql for 0.15 fresh install
3. GO means executing the query in a batch or single transaction
4. NA due to 1, actually nesting does not matter, that's only indent spaces.

Made the 3rd path to be reviewed., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12677779/HIVE-8550.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6587 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1518/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1518/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1518/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12677779 - PreCommit-HIVE-TRUNK-Build, I think this looks ok, the change is now limited to the mssql scripts. Asked Ashutosh and he was ok with approach as well. +1
This has been tested with both fresh install and upgrading existing metastore tables?, +1 for hive 0.14., Yes [~jdere], tested both, see also review board. Pasted tests done here:
{noformat}
Legacy system upgrading:
1. stop all services
2. drop HIVE DB
3. manually create HIVE DB
4. enable datanucleus.autoCreateSchema
5. start all services
6. all tables are created by autoCreateSchema
7. run 004-HIVE-8550.mssql.sql to do upgrade

Fresh install:
1. stop all services
2. drop HIVE DB
3. manually create HIVE DB
4. run hive-schema-0.14.0.mssql.sql to create all tables
5. disable datanucleus.autoCreateSchema
6. start all services

It fixed the issue.
{noformat}, Committed to trunk. [~xiaobingo] can you generate a patch for hive .14? (The patch doesn't apply - my guess is you just need to drop the -0.15- file), Thanks [~hagleitn]. We can put a hold on 0.14 patch since [~jdere] proposed to do some verifications that make sure index on PART_NAME works well even after nvarchar change, although we think so., Confirmed two concerns from [~jdere]:
1. legacy system(varchar typed column) with non-unicode partition data in DB, running nvarchar upgrade, patch works well for unicode partitioned table creation and data loading.
2. run some samples queries on PARTITIONS table to make sure index is still working after upgrade.

For case 1, smoothy upgrade is seen; For case 2, I ran query on MSSQL, 
{noformat}
select * from dbo.PARTITIONS where part_name = 'ds=2008-04-08/hr=11' and TBL_ID = 7;
{noformat}
, the execution plans include index seek on the [PARTITIONS].[UNIQUEPARTITION] which is NonClustered index on part_name and TBL_ID. This is good certificate that index is working well after upgrade.

So It's safe to commit it., Made a patch for 0.14. [~hagleitn] can you get that into 0.14, thanks!, Committed to branch and trunk. Thanks [~xiaobingo] and [~jdere]!, Needs a fix version.

No documentation?, [~leftylev] I think we can claim unicode partitioned table is supported from 0.14 and above, including fresh install and upgrade from 0.13 to 0.14. Thanks., bq.   ... unicode partitioned table is supported from 0.14 and above, including fresh install and upgrade from 0.13 to 0.14.

Is that just for Windows support?  In other words, were unicode keys supported for non-Windows installations before 0.14?, Non-Windows installations already supported unicode keys. Yes, this is for Windows. Thanks., This has been fixed in 0.14 release. Please open new jira if you see any issues.
]