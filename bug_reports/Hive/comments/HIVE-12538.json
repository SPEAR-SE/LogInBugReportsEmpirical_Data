[I tried but wasn't able to reproduce., After debugging ,i find the problem is that ,the operation conf object SparkUtilities used to detect configuration change is different from session conf.
And the session conf object 's getSparkConfigUpdated method always return true after setting spark related config.
The code path where SQLOperation copy a new conf object from session conf:
https://github.com/apache/hive/blob/spark/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java#L467
{code}
/**
   * If there are query specific settings to overlay, then create a copy of config
   * There are two cases we need to clone the session config that's being passed to hive driver
   * 1. Async query -
   *    If the client changes a config setting, that shouldn't reflect in the execution already underway
   * 2. confOverlay -
   *    The query specific settings should only be applied to the query config and not session
   * @return new configuration
   * @throws HiveSQLException
   */
  private HiveConf getConfigForOperation() throws HiveSQLException {
    HiveConf sqlOperationConf = getParentSession().getHiveConf();
    if (!getConfOverlay().isEmpty() || shouldRunAsync()) {
      // clone the partent session config for this query
      sqlOperationConf = new HiveConf(sqlOperationConf);

      // apply overlay query specific settings, if any
      for (Map.Entry<String, String> confEntry : getConfOverlay().entrySet()) {
        try {
          sqlOperationConf.verifyAndSet(confEntry.getKey(), confEntry.getValue());
        } catch (IllegalArgumentException e) {
          throw new HiveSQLException("Error applying statement specific settings", e);
        }
      }
    }
    return sqlOperationConf;
  }
{code}
The code path where SparkUtilities detect the change and close the spark session :
https://github.com/apache/hive/blob/spark/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java#L122
{code}
public static SparkSession getSparkSession(HiveConf conf,
      SparkSessionManager sparkSessionManager) throws HiveException {
    SparkSession sparkSession = SessionState.get().getSparkSession();

    // Spark configurations are updated close the existing session
    if (conf.getSparkConfigUpdated()) {
      sparkSessionManager.closeSession(sparkSession);
      sparkSession =  null;
      conf.setSparkConfigUpdated(false);
    }
    sparkSession = sparkSessionManager.getSession(sparkSession, conf, true);
    SessionState.get().setSparkSession(sparkSession);
    return sparkSession;
  }
{code}

It shoud be easy to reproduce, i will dig more.

, A way to reproduce:
1, open beeline
2, run the following SQL:
{noformat}
create table if not exists test(id int);
set hive.execution.engine=spark;
set spark.yarn.queue=QueueA;
select count(*) from test;
select count(*) from test;
{noformat}
3,check yarn UI,and there will be two yarn applications .

Howerver,changing orders in step 2(the last setting command before actual query is not a spark related parameter),the problem will be gone:
{noformat}
create table if not exists test(id int);
set spark.yarn.queue=QueueA;
set hive.execution.engine=spark;
select count(*) from test;
select count(*) from test;
{noformat}

, Actually,there are two bugs:
1, Property isSparkConfigUpdated of HiveConf.java always get updated when setting values from client side(beeline).
    set spark.yarn.queue=QueueA; ---> isSparkConfigUpdated =true
    set hive.execution.engine=spark; ---> isSparkConfigUpdated =false
2, SparkTask uses an operation level conf object other than session level conf.
That makes "conf.setSparkConfigUpdated(false);" in SparkUtilities meaningless from session's view., [~jxiang], could you please check and verify if there are bugs on this? Thanks., [~nemon], good findings. Do you want to post a patch? Otherwise, I can do it. Thanks., 1,Fixing the bug : " isSparkConfigUpdated = isSparkRelatedConfig(name)"
2,Setting "isSparkConfigUpdated =false " at session level
3,Taking concurrency into consideration by adding a session level conf lock.As multiple queries can run in a single session., bq. 1,Fixing the bug : " isSparkConfigUpdated = isSparkRelatedConfig(name)"
The fix to HiveConf.java is good.

bq. 2,Setting "isSparkConfigUpdated =false " at session level
That means we reset the flag once we copy the hiveconf to the operation level. This may not work in some cases, right? For example, the first query has the flag set, and the second query has the flag not set, if the second query happens to create the spark session earlier, it will not create a new spark session.

I think it is better to fix SparkUtilities to use the session level conf in checking if a new spark session should be created, and use the operation level conf to create the speark session. What do you think?

bq. 3,Taking concurrency into consideration by adding a session level conf lock.As multiple queries can run in a single session.
Multiple queries can run in a single session asynchronously, right? If so, each one should has its own copy of conf object. If we are going to fix SparkUtilities, we may just need to synchronized the specific area of method SparkUtilities#getSparkSession().

, [~jxiang] ,thanks for review.
{quote}
That means we reset the flag once we copy the hiveconf to the operation level. This may not work in some cases, right? For example, the first query has the flag set, and the second query has the flag not set, if the second query happens to create the spark session earlier, it will not create a new spark session.
{quote}
Aggreed
{quote}
I think it is better to fix SparkUtilities to use the session level conf in checking if a new spark session should be created, 
{quote}
Aggreed
{quote}
and use the operation level conf to create the spark session. 
{quote}
Not quite follow.Is there anything special in operation conf for SparkSession? And when to set "isSparkConfigUpdated =false " ? 
Since each hive session only has one related spark session,i think it's should be fine to use session conf to start spark session.
{quote}Multiple queries can run in a single session asynchronously, right? If so, each one should has its own copy of conf object. If we are going to fix SparkUtilities, we may just need to synchronized the specific area of method SparkUtilities#getSparkSession().
{quote}
This depend on the question above : when to set "isSparkConfigUpdated =false "? Client may set another spark related parameter during SparkUtilities#getSparkSession(),that makes session level lock still needed., Uploading a new patch that only set session level's isSparkConfigUpdated to false in SparkUtilities#getSparkSession().
The problem with using the operation level conf to create the speark session is that,it may ignore some spark related parameters from client which are set after creation of operation level conf and before setting session level's isSparkConfigUpdated to false.
, bq. Not quite follow.Is there anything special in operation conf for SparkSession? And when to set "isSparkConfigUpdated =false " ?
We can set it to false for the session level conf only. So this flag in the operation level is totaly ignored, all the time.
Things are a little tricky actually. If we use the session level conf, we could miss some non-spark-related settings in the operation level conf.
If we use the operation level conf, we could miss some spark-related settings in the session level conf.
Instead of just maintaining a isSparkConfigUpdated flag, probably, we should have a separate map to store such changed spark-related settings temporarily.
This map can be reset upon SparkUtilities#getSparkSession() is invoked., Addressing issues mentioned in comments., Using ConcurrentHashMap instead of HashMap., Patch v2 looks good to me. No need to use ConcurrentHashMap since you use synchronized already, right?
For SparkUtilities#getSparkSession(), probably we just need to synchronize on the session level conf., Changing to session level spark session lock.
Changing back to HashMap.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12775471/HIVE-12538.4.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 9885 tests executed
*Failed tests:*
{noformat}
TestHWISessionManager - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_acid_globallimit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_udf_max
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_acid_globallimit
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mergejoin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.testGetPartitionSpecs_WithAndWithoutPartitionGrouping
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6211/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6211/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-6211/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12775471 - PreCommit-HIVE-TRUNK-Build, Patch v4 looks good to me. +1, Thought about it again, and chatted with Xuefu too. It seems to me there is no good fix for this issue. Due to asynchronous, if a query is going on, if someone changes the spark setting then submits another query, the previous query could be killed. Not sure how we can avoid such thing properly.

One solution is to prevent any spark related setting changes once an async query is already submitted in a session. [~xuefuz], [~szehon], what do you think?, My understanding is that a hive session may "own" more than one spark session in case of asynchronous queries. If a spark session is live (used to run a spark job), that spark session will not be used to run the next job. Therefore, whenever whenever a spark configuration change is detected in Hive session, we need to mark all the live Spark sessions as outdated.  When we are getting a session from the pool and check if the flag is set, then we destroy it and get a new one. Hope this will simply things a bit. , Currently, each hive session can have just one spark session. Yeah, if a hive session is allowed to have more than one spark session, this should work., [~jxiang]  Please feel free to take it over if you plan to support more than one spark session. :)
And here is my considerations:
I think it's bad practice to use one single hive connection for multiple threads on client side,
when each thread set different session level parameters and then submit queries separately.
Users shall be encouraged to set session level parameters serially.
For asynchronous queries submitted from a single thread and from a single hive connection,there is no 
mechanism to promise their query parameters does not influence each other(,unless these parameters 
are set by confOverlay?).I haven't seen this use case for now.
, I agree. I am wondering if we need to support running just one query at a time for a session. If so, the fix could be different. If we need to support multiple concurrent queries in one session, we need to come up a different fix. There may be clients already running several queries with the same session., HIVE-9223  seems that Hive on Tez has the same problem., Shall I provide a patch only fix issues mentioned here and keep track of supporting multiple concurrent queries in one session from a separate jira ?
, [~nemon], I think it's fine to solve one issue at a time. If you like to submit a patch, please specify the problem that you're trying to fix and then I can review it. Thanks., Uploading a patch addressing the following issues :
1, Property isSparkConfigUpdated of HiveConf has not been maintained properly , as shown in the unit test added by this patch .  
2, In case of a asynchronous query, isSparkConfigUpdated should be set from session level instead of operation level .
The second issue is hard to add a test case ., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12776744/HIVE-12538.5.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 9895 tests executed
*Failed tests:*
{noformat}
TestHWISessionManager - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_cbo_udf_max
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_order2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union9
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_insert_partition_dynamic
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_vectorized_dynamic_partition_pruning
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_mergejoin
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.testGetPartitionSpecs_WithAndWithoutPartitionGrouping
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery
org.apache.hive.jdbc.TestSSL.testSSLVersion
org.apache.hive.jdbc.miniHS2.TestHs2Metrics.testMetrics
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6318/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6318/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-6318/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12776744 - PreCommit-HIVE-TRUNK-Build, +1, [~xuefuz] Thanks for review. I have created HIVE-12689 to keep track of the concurrent issue discussed here., Committed to master. Thanks, Nemon!]