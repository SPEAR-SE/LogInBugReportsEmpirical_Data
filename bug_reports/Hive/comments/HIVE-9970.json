[Any exception in your hive.log? If you can attach your log here, it will be great.  If you see some /tmp/spark-events folder missing in your log, you can try to create that folder, or set spark.eventLog.enabled=false; and run your cli/query again.

By the way, you don't need to do this: "add jar /opt/spark-1.2.1/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.4.0.jar;".


, When spark.eventLog.enabled is set to true, you'd set spark.eventLog.dir to an existing folder (default /tmp/spark-events). I added this setting to the Hive on Spark Getting Started page., HIVE LOG (hive.log)

2015-03-17 16:36:52,654 INFO  [main]: ql.Driver (SessionState.java:printInfo(852)) - Launching Job 1 out of 1
2015-03-17 16:36:52,656 INFO  [main]: ql.Driver (Driver.java:launchTask(1630)) - Starting task [Stage-1:MAPRED] in parallel
2015-03-17 16:36:52,660 INFO  [Thread-68]: hive.metastore (HiveMetaStoreClient.java:open(365)) - Trying to connect to metastore with URI thrift://nn01:7099
2015-03-17 16:36:52,665 INFO  [Thread-68]: hive.metastore (HiveMetaStoreClient.java:open(461)) - Connected to metastore.
2015-03-17 16:36:52,688 INFO  [Thread-68]: session.SessionState (SessionState.java:start(488)) - No Tez session required at this point. hive.execution.engine=mr.
2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) - In order to change the average load for a reducer (in bytes):
2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) -   set hive.exec.reducers.bytes.per.reducer=<number>
2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) - In order to limit the maximum number of reducers:
2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) -   set hive.exec.reducers.max=<number>
2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) - In order to set a constant number of reducers:
2015-03-17 16:36:52,689 INFO  [Thread-68]: exec.Task (SessionState.java:printInfo(852)) -   set mapreduce.job.reduces=<number>
2015-03-17 16:36:52,696 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.connect.timeout -> 1000).
2015-03-17 16:36:52,697 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(113)) - load spark property from hive configuration (spark.eventLog.enabled -> false).
2015-03-17 16:36:52,697 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.rpc.threads -> 8).
2015-03-17 16:36:52,698 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.secret.bits -> 256).
2015-03-17 16:36:52,699 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.rpc.max.size -> 52428800).
2015-03-17 16:36:52,699 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(113)) - load spark property from hive configuration (spark.master -> spark://10.10.10.25:7077).
2015-03-17 16:36:52,702 INFO  [Thread-68]: spark.HiveSparkClientFactory (HiveSparkClientFactory.java:initiateSparkConf(130)) - load RPC property from hive configuration (hive.spark.client.server.connect.timeout -> 90000).
2015-03-17 16:36:54,480 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Spark assembly has been built with Hive, including Datanucleus jars on classpath
2015-03-17 16:36:57,761 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.connect.timeout=1000
2015-03-17 16:36:57,761 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.rpc.threads=8
2015-03-17 16:36:57,762 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.rpc.max.size=52428800
2015-03-17 16:36:57,763 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.secret.bits=256
2015-03-17 16:36:57,763 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Warning: Ignoring non-spark config property: hive.spark.client.server.connect.timeout=90000
2015-03-17 16:36:58,224 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 15/03/17 16:36:58 INFO client.RemoteDriver: Connecting to: nn01:50661
2015-03-17 16:36:58,240 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - Exception in thread "main" java.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT
2015-03-17 16:36:58,241 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at org.apache.hive.spark.client.rpc.RpcConfiguration.<clinit>(RpcConfiguration.java:46)
2015-03-17 16:36:58,241 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:137)
2015-03-17 16:36:58,241 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:528)
2015-03-17 16:36:58,242 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2015-03-17 16:36:58,242 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
2015-03-17 16:36:58,242 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2015-03-17 16:36:58,243 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at java.lang.reflect.Method.invoke(Method.java:606)
2015-03-17 16:36:58,243 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
2015-03-17 16:36:58,243 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
2015-03-17 16:36:58,243 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
2015-03-17 16:36:58,244 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
2015-03-17 16:36:58,244 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(530)) - 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2015-03-17 16:36:58,858 WARN  [Driver]: client.SparkClientImpl (SparkClientImpl.java:run(388)) - Child process exited with code 1.
, 
hive> insert into table test values(6,8797);
Query ID = hadoop2_20150317163636_4692aa68-56b6-4ea9-ad21-e0f46efe4bfc
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
, That means your jar files are not consistent, i.e, not compiled with the same code., My working Environment is
Centos 6.4
Hadoop - 2.6.0
Hive   - 1.1.0
Spark  - 1.3.0 (Builded spark,sql.yarn)

could you brief me about the error.Is that because of using higher end verions (or) my mistakes during building the spark.
, And also while using beeline i am getting this error




2015-03-18 16:03:21,458 ERROR [pool-3-thread-8]: DataNucleus.Datastore (Log4JLogger.java:error(115)) - An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1054)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4237)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4169)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2617)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2819)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2768)
	at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:949)
	at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:795)
	at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)
	at org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatement(AbstractTable.java:760)
	at org.datanucleus.store.rdbms.table.TableImpl.createIndices(TableImpl.java:648)
	at org.datanucleus.store.rdbms.table.TableImpl.validateIndices(TableImpl.java:593)
	at org.datanucleus.store.rdbms.table.TableImpl.validateConstraints(TableImpl.java:390)
	at org.datanucleus.store.rdbms.table.ClassTable.validateConstraints(ClassTable.java:3463)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3464)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:172)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:130)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:275)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:238)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:579)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:557)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:933)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:907)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy1.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database.getResult(ThriftHiveMetastore.java:8547)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database.getResult(ThriftHiveMetastore.java:8531)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

2015-03-18 16:03:21,458 ERROR [pool-3-thread-8]: DataNucleus.Datastore (Log4JLogger.java:error(115)) - An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1054)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4237)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4169)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2617)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2778)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2819)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2768)
	at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:949)
	at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:795)
	at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)
	at org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatement(AbstractTable.java:760)
	at org.datanucleus.store.rdbms.table.TableImpl.createIndices(TableImpl.java:648)
	at org.datanucleus.store.rdbms.table.TableImpl.validateIndices(TableImpl.java:593)
	at org.datanucleus.store.rdbms.table.TableImpl.validateConstraints(TableImpl.java:390)
	at org.datanucleus.store.rdbms.table.ClassTable.validateConstraints(ClassTable.java:3463)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3464)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:172)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:130)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:275)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:238)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:579)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:557)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_core(HiveMetaStore.java:933)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:907)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
	at com.sun.proxy.$Proxy1.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database.getResult(ThriftHiveMetastore.java:8547)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database.getResult(ThriftHiveMetastore.java:8531)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

2015-03-18 16:03:24,699 INFO  [pool-3-thread-8]: metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:<init>(132)) - Using direct SQL, underlying DB is MYSQL
2015-03-18 16:03:24,700 INFO  [pool-3-thread-8]: metastore.ObjectStore (ObjectStore.java:setConf(252)) - Initialized ObjectStore
2015-03-18 16:03:24,713 INFO  [pool-3-thread-8]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(732)) - 5: source:10.10.10.25 get_database: test
, mysql version
Server version: 5.1.73-log Source distribution, Mysql error solved after updating the version But hive on spark still in error state 
hive version 1.1.0
Spark 1.3.0

ERROR : Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:57)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.getSession(SparkSessionManagerImpl.java:116)
	at org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.getSparkSession(SparkUtilities.java:113)
	at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:95)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:75)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Timed out waiting for client connection.
	at com.google.common.base.Throwables.propagate(Throwables.java:156)
	at org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:104)
	at org.apache.hive.spark.client.SparkClientFactory.createClient(SparkClientFactory.java:80)
	at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.<init>(RemoteHiveSparkClient.java:88)
	at org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.createHiveSparkClient(HiveSparkClientFactory.java:58)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:55)
	... 6 more
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Timed out waiting for client connection.
	at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)
	at org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:94)
	... 10 more
Caused by: java.util.concurrent.TimeoutException: Timed out waiting for client connection.
	at org.apache.hive.spark.client.rpc.RpcServer$2.run(RpcServer.java:134)
	at io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)
	at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:123)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at java.lang.Thread.run(Thread.java:744)

ERROR : Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:57)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.getSession(SparkSessionManagerImpl.java:116)
	at org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.getSparkSession(SparkUtilities.java:113)
	at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:95)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:75)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Timed out waiting for client connection.
	at com.google.common.base.Throwables.propagate(Throwables.java:156)
	at org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:104)
	at org.apache.hive.spark.client.SparkClientFactory.createClient(SparkClientFactory.java:80)
	at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.<init>(RemoteHiveSparkClient.java:88)
	at org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.createHiveSparkClient(HiveSparkClientFactory.java:58)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:55)
	... 6 more
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Timed out waiting for client connection.
	at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)
	at org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:94)
	... 10 more
Caused by: java.util.concurrent.TimeoutException: Timed out waiting for client connection.
	at org.apache.hive.spark.client.rpc.RpcServer$2.run(RpcServer.java:134)
	at io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)
	at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:123)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at java.lang.Thread.run(Thread.java:744)
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask (state=08S01,code=1)
, Can you try bumping up "hive.spark.client.server.connect.timeout" to a higher value like "20000ms"?  Default was bumped in HIVE-9519, not sure if you picked that one up., my default hive.spark.client.server.connect.timeout=90000ms
now changed to 20000ms and Also finally to 220000ms

Same Error

Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
, now i tried in pre built spark same error found so is there any problem in hive version (1.1.0)?????, Hi all,


Have tried with prebuild spark  from Hortonworks 
Now i am able to access spark through Beeline but cannot using hive
and from Hortonworks documentation i came to know that yarn should be build on spark & they are starting thriftserver as

./sbin/start-thriftserver.sh --master yarn --executor-memory 512m --hiveconf hive.server2.thrift.port=10001

, Today, I also ran into the same issue. The issue is caused by using spark that has been built with hive. Hive classes in spark-assembly*.jar seems of older version. Removing all hive specific classes from spark-assembly*.jar resolved the issue or me., my hive version is 1.2.0.
and build spark1.3.1 on hadoop with "./make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.5".
That’s unfortunate,hive on spark still in error state,"Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask".

java.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT
	at org.apache.hive.spark.client.rpc.RpcConfiguration.<clinit>(RpcConfiguration.java:46)
	at org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:146)
	at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:480)



So,Have these questions had answers?, I have resolved the problem.
first hive cli will load $HIVE_HONE\lib\*.jar accurately.
then spark will load old version hive jar because in $SPARK_HOME\conf\spark-ent.sh
"export SPARK_CLASSPATH=$SPARK_HOME/lib/*:$HADOOP_HOME/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:$HIVE_HOME/lib/hive-contrib-0.12.0.jar:$HIVE_HOME/lib/hive-common-0.12.0.jar:$HIVE_HOME/bin/hive-cli    -0.12.0.jar:$HIVE_HOME/lib/hive-serde-0.12.0.jar:$HIVE_HOME/lib/:$EXTRA_CLASSPATH"

however HiveConf.class in hive-common-0.12.0.jar does not contain SPARK_RPC_CLIENT_CONNECT_TIMEOUT.
so, java.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT has occur., Hi , JoneZhang. According to your commitment , how can i resolve this problem ?, I am facing same issue when trying to run hive on spark in yarn mode.

hive 1.2.0 /1.1.0 -- tried with both
spark 1.3.1
hadoop 2.6.0

I have tried with both the prebuilt version and building with below command to remove any hive dependency
./make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.6"

5/07/15 17:44:09 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1436958102207_0007_000001
15/07/15 17:44:09 WARN conf.Configuration: mapred-default.xml:an attempt to override final parameter: mapreduce.cluster.local.dir;  Ignoring.
15/07/15 17:44:09 WARN conf.Configuration: mapred-default.xml:an attempt to override final parameter: mapreduce.cluster.temp.dir;  Ignoring.
15/07/15 17:44:09 WARN conf.Configuration: mapred-default.xml:an attempt to override final parameter: mapreduce.cluster.local.dir;  Ignoring.
15/07/15 17:44:09 WARN conf.Configuration: mapred-default.xml:an attempt to override final parameter: mapreduce.cluster.temp.dir;  Ignoring.
15/07/15 17:44:09 INFO spark.SecurityManager: Changing view acls to: root
15/07/15 17:44:09 INFO spark.SecurityManager: Changing modify acls to: root
15/07/15 17:44:09 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
15/07/15 17:44:09 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
15/07/15 17:44:09 INFO yarn.ApplicationMaster: Waiting for spark context initialization
15/07/15 17:44:09 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
15/07/15 17:44:09 INFO client.RemoteDriver: Connecting to: Impetus-dsrv16:41364
15/07/15 17:44:09 ERROR yarn.ApplicationMaster: User class threw exception: SPARK_RPC_CLIENT_CONNECT_TIMEOUT
java.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT
	at org.apache.hive.spark.client.rpc.RpcConfiguration.<clinit>(RpcConfiguration.java:46)
	at org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:146)
	at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:480)
15/07/15 17:44:09 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: SPARK_RPC_CLIENT_CONNECT_TIMEOUT)
15/07/15 17:44:19 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 100000 ms. Please check earlier log output for errors. Failing the application.
15/07/15 17:44:19 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: SPARK_RPC_CLIENT_CONNECT_TIMEOUT)
15/07/15 17:44:19 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1436958102207_0007
Please let me know if there is a resolution to it., 1.Make sure  build spark without hive.
If there is the following directory in spark-assembly-*-hadoop*.jar,this is not allowed.
spark-assembly-*-hadoop*.jar\org\apache\hive   
spark-assembly-*-hadoop*.jar\org\apache\hadoop\hive 
2. See if there is an older version of hive jar under CLASSPATH.Remove it If there exists., This seems indicating a non-issue. Feel free to reopen with additional information including a meaningful title., Hi Xuefu, 

I use spark trunk version of 1.6 + hadoop 2.6.0 + hive 1.2.1, when running the HQL in HIVE CLI, We got following error,

2015-11-12 16:31:17,245 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/12 16:31:17 ERROR Utils: uncaught error in thread SparkListenerBus, stopping SparkContext
2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - java.lang.AbstractMethodError
2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:62)
2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2015-11-12 16:31:17,246 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)

Any ideas?, This issue still persists my environment is :

hadoop = 2.6
hive = 1.1.1
spark = 1.5.1

below are logs in the hive.log :

15/11/21 19:56:27 [HiveServer2-Background-Pool: Thread-39]: ERROR exec.Task: Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:57)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.getSession(SparkSessionManagerImpl.java:116)
	at org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.getSparkSession(SparkUtilities.java:113)
	at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:95)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1638)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1397)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1183)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:145)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:70)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:209)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Cannot run program "/home/adt/server/spark1.5/spark-1.5.1set mapreduce.input.fileinputformat.split.maxsize=750000000set hive.vectorized.execution.enabled=trueset hive.cbo.enable=trueset hive.optimize.reducededuplication.min.reducer=4set hive.optimize.reducededuplication=trueset hive.orc.splits.include.file.footer=falseset hive.merge.mapfiles=trueset hive.merge.sparkfiles=falseset hive.merge.smallfiles.avgsize=16000000set hive.merge.size.per.task=256000000set hive.merge.orcfile.stripe.level=trueset hive.auto.convert.join=trueset hive.auto.convert.join.noconditionaltask=trueset hive.auto.convert.join.noconditionaltask.size=894435328set hive.optimize.bucketmapjoin.sortedmerge=falseset hive.map.aggr.hash.percentmemory=0.5set hive.map.aggr=trueset hive.optimize.sort.dynamic.partition=falseset hive.stats.autogather=trueset hive.stats.fetch.column.stats=trueset hive.vectorized.execution.reduce.enabled=falseset hive.vectorized.groupby.checkinterval=4096set hive.vectorized.groupby.flush.percent=0.1set hive.compute.query.using.stats=trueset hive.limit.pushdown.memory.usage=0.4set hive.optimize.index.filter=trueset hive.exec.reducers.bytes.per.reducer=67108864set hive.smbjoin.cache.rows=10000set hive.exec.orc.default.stripe.size=67108864set hive.fetch.task.conversion=moreset hive.fetch.task.conversion.threshold=1073741824set hive.fetch.task.aggr=falseset mapreduce.input.fileinputformat.list-status.num-threads=5set spark.kryo.referenceTracking=false#set spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch/bin/spark-submit": error=36, File name too long
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.hive.spark.client.SparkClientImpl.startDriver(SparkClientImpl.java:376)
	at org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:89)
	at org.apache.hive.spark.client.SparkClientFactory.createClient(SparkClientFactory.java:80)
	at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.<init>(RemoteHiveSparkClient.java:88)
	at org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.createHiveSparkClient(HiveSparkClientFactory.java:58)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:55)
	... 22 more
Caused by: java.io.IOException: error=36, File name too long
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:248)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 28 more
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
, This issue still persists please see logs below :

15/11/21 19:56:27 [HiveServer2-Background-Pool: Thread-39]: ERROR exec.Task: Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create spark client.)'
org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create spark client.
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:57)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl.getSession(SparkSessionManagerImpl.java:116)
	at org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.getSparkSession(SparkUtilities.java:113)
	at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:95)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1638)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1397)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1183)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:145)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:70)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:209)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Cannot run program "/home/adt/server/spark1.5/spark-1.5.1set mapreduce.input.fileinputformat.split.maxsize=750000000set hive.vectorized.execution.enabled=trueset hive.cbo.enable=trueset hive.optimize.reducededuplication.min.reducer=4set hive.optimize.reducededuplication=trueset hive.orc.splits.include.file.footer=falseset hive.merge.mapfiles=trueset hive.merge.sparkfiles=falseset hive.merge.smallfiles.avgsize=16000000set hive.merge.size.per.task=256000000set hive.merge.orcfile.stripe.level=trueset hive.auto.convert.join=trueset hive.auto.convert.join.noconditionaltask=trueset hive.auto.convert.join.noconditionaltask.size=894435328set hive.optimize.bucketmapjoin.sortedmerge=falseset hive.map.aggr.hash.percentmemory=0.5set hive.map.aggr=trueset hive.optimize.sort.dynamic.partition=falseset hive.stats.autogather=trueset hive.stats.fetch.column.stats=trueset hive.vectorized.execution.reduce.enabled=falseset hive.vectorized.groupby.checkinterval=4096set hive.vectorized.groupby.flush.percent=0.1set hive.compute.query.using.stats=trueset hive.limit.pushdown.memory.usage=0.4set hive.optimize.index.filter=trueset hive.exec.reducers.bytes.per.reducer=67108864set hive.smbjoin.cache.rows=10000set hive.exec.orc.default.stripe.size=67108864set hive.fetch.task.conversion=moreset hive.fetch.task.conversion.threshold=1073741824set hive.fetch.task.aggr=falseset mapreduce.input.fileinputformat.list-status.num-threads=5set spark.kryo.referenceTracking=false#set spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch/bin/spark-submit": error=36, File name too long
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.hive.spark.client.SparkClientImpl.startDriver(SparkClientImpl.java:376)
	at org.apache.hive.spark.client.SparkClientImpl.<init>(SparkClientImpl.java:89)
	at org.apache.hive.spark.client.SparkClientFactory.createClient(SparkClientFactory.java:80)
	at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.<init>(RemoteHiveSparkClient.java:88)
	at org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.createHiveSparkClient(HiveSparkClientFactory.java:58)
	at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.open(SparkSessionImpl.java:55)
	... 22 more
Caused by: java.io.IOException: error=36, File name too long
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:248)
	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 28 more
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
, [~tarushg], the error seems different from the original issue but very strange:
{code}
Caused by: java.io.IOException: Cannot run program "/home/adt/server/spark1.5/spark-1.5.1set mapreduce.input.fileinputformat.split.maxsize=750000000set hive.vectorized.execution.enabled=trueset hive.cbo.enable=trueset hive.optimize.reducededuplication.min.reducer=4set hive.optimize.reducededuplication=trueset hive.orc.splits.include.file.footer=falseset hive.merge.mapfiles=trueset hive.merge.sparkfiles=falseset hive.merge.smallfiles.avgsize=16000000set hive.merge.size.per.task=256000000set hive.merge.orcfile.stripe.level=trueset hive.auto.convert.join=trueset hive.auto.convert.join.noconditionaltask=trueset hive.auto.convert.join.noconditionaltask.size=894435328set hive.optimize.bucketmapjoin.sortedmerge=falseset hive.map.aggr.hash.percentmemory=0.5set hive.map.aggr=trueset hive.optimize.sort.dynamic.partition=falseset hive.stats.autogather=trueset hive.stats.fetch.column.stats=trueset hive.vectorized.execution.reduce.enabled=falseset hive.vectorized.groupby.checkinterval=4096set hive.vectorized.groupby.flush.percent=0.1set hive.compute.query.using.stats=trueset hive.limit.pushdown.memory.usage=0.4set hive.optimize.index.filter=trueset hive.exec.reducers.bytes.per.reducer=67108864set hive.smbjoin.cache.rows=10000set hive.exec.orc.default.stripe.size=67108864set hive.fetch.task.conversion=moreset hive.fetch.task.conversion.threshold=1073741824set hive.fetch.task.aggr=falseset mapreduce.input.fileinputformat.list-status.num-threads=5set spark.kryo.referenceTracking=false#set spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch/bin/spark-submit": error=36, File name too long
{code}
This is where Hive is building a process to launch a remote spark driver. it usually starts with something like "/home/adt/server/spark1.5/bin/spark-submit ...". It seems that the builder gets corrupted with a bunch of set commands. Could you describe how to reproduce this issue?, With the same problem "  ERROR util.Utils: uncaught error in thread SparkListenerBus, stopping SparkContext ", has anyone already solved it ??? 
---My spark 1.5.2   Hive 1.2.1 ,  build by myself with commands : 
   mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Dscala-2.10 -DskipTests clean package –e

-------Error Log ：
 15/11/26 09:39:40 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-11-26 09:39:40,245 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO exec.Utilities: Processing alias dc_mf_device_one_check
2015-11-26 09:39:40,245 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO exec.Utilities: Adding input file hdfs://cluster1/user/hive/warehouse/vendorzhhs.db/dc_mf_device_one_check
2015-11-26 09:39:40,735 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO log.PerfLogger: <PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-11-26 09:39:40,735 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO exec.Utilities: Serializing MapWork via kryo
2015-11-26 09:39:40,902 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:40 INFO log.PerfLogger: </PERFLOG method=serializePlan start=1448501980734 end=1448501980902 duration=168 from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-11-26 09:39:41,248 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.MemoryStore: ensureFreeSpace(599952) called with curMem=0, maxMem=555755765
2015-11-26 09:39:41,250 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 585.9 KB, free 529.4 MB)
2015-11-26 09:39:41,429 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.MemoryStore: ensureFreeSpace(43801) called with curMem=599952, maxMem=555755765
2015-11-26 09:39:41,429 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.8 KB, free 529.4 MB)
2015-11-26 09:39:41,433 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.69:39388 (size: 42.8 KB, free: 530.0 MB)
2015-11-26 09:39:41,437 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO spark.SparkContext: Created broadcast 0 from hadoopRDD at SparkPlanGenerator.java:188
2015-11-26 09:39:41,441 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 ERROR util.Utils: uncaught error in thread SparkListenerBus, stopping SparkContext
2015-11-26 09:39:41,441 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - java.lang.AbstractMethodError
2015-11-26 09:39:41,441 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:62)
2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:56)
2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:79)
2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1136)
2015-11-26 09:39:41,442 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
2015-11-26 09:39:41,467 INFO  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(569)) - 15/11/26 09:39:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}, We are seeing the exact same issue as [~TerrenceYTQ] and [~Qiuzhuang] which seems a regression as it has been reported to work with spark 1.4.1 (http://apache-spark-user-list.1001560.n3.nabble.com/Issue-with-spark-on-hive-td25372.html).]