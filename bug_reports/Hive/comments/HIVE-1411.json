[A little background on what DataNucleus is trying to do: http://www.datanucleus.org/extensions/plugins.html

Definition of pluginRegistryBundleCheck property: http://www.datanucleus.org/products/accessplatform_1_0/persistence_properties.html#general

, This patch adds the property "datanucleus.plugin.pluginRegistryBundleCheck" to hive-site.xml and sets the value to LOG. When not set this property defaults to EXCEPTION, which results in an exception being thrown if the CLASSPATH contains two or more JARs with the same name.
, Taking a look., Paul, are you still looking at this one?
, Whoops, got side tracked by another datanucleus issue. I'll review it today., @Carl - what is the motivation for turning off this exception? Wouldn't it be good to know if 2 jars were on the CP so that the user would know for sure which version was in use? , Here's some more background on what is happening:

When DataNucleus starts up it [scans every class that it finds on the CLASSPATH|http://www.datanucleus.org/extensions/plugins.html] looking for [OSGi format plugins|http://www.ibm.com/developerworks/opensource/library/os-ecl-osgi/]. Hadoop core happens to depend on Eclipse's core-3.1.1.jar (which contains OSGi format plugins), so this gets included in Hive's CLASSPATH since bin/hive delegates execution to bin/hadoop. If a user installed Hadoop from a tar ball and set HADOOP_HOME to the install directory we are all good, but if the user also ran 'ant' in $HADOOP_HOME we're in a world of pain since this results in two copies of core-3.1.1.jar (the original copy in $HADOOP_HOME/lib and another copy in $HADOOP_HOME/build/ivy/lib/Hadoop/common/) and bin/hadoop stupidly adds both jars to the CLASSPATH.

I want to highlight the following points:

 # Duplicate CLASSPATH entries are a fact of life with Hadoop and Hive due to the screwed up CLASSPATH construction code in bin/hadoop and bin/hive
 # The duplicate CLASSPATH check that DataNucleus enforces by default only applies to JARs that contain OSGi plugins, which in our case seems to only apply to the core-3.1.1 JAR.
 # The patch that I supplied causes this harmless condition to result in a LOG message instead of a fatal exception. I think this preferable.
 # This exception is easy to reproduce on 0.5.0, but the upgrade to datanucleus-2.3 in HIVE-1176 seems to have made it unreproducible on trunk. I think this indicates that there is a bug in datanucleus-2.3 since it is no longer honoring the contract set by datanucleus.plugin.pluginRegistryBundleCheck (the default value is still EXCEPTION). I recommend that we commit this patch anyway lest we encounter this bug in the future after upgrading to a newer version of datanucleus that fixes this problem.

, About #4, I was able to reproduce the exception and verify the fix by creating a copy of the datanucleus core jar in hive's lib directory.

Makes sense to me. Now that you mention it, I think I ran into a similar problem while using the 'hadoop jar' command. It would unpack the jar, but include both the jar and the unpacked classes in the classpath. If the jar contained datanucleus stuff, this would throw an exception.

Anyway, the fix looks good. +1, +1.  Will commit when tests pass.
, Carl, when you supply patches, please make sure they can be applied with patch -p0.  This one needed patch -p1.
, Committed to branch and trunk.  Thanks Carl!
]