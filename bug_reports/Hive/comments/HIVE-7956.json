[I thought that MR does this by setting the number of reducers equal to the number of buckets., Yes [~brocknoland], with {{set hive.enforce.bucketing = true;}}, spark also launches a proper number of reducers (according to # of buckets). But all mappers write shuffle data for just one reducer so all other buckets are empty. Therefore I think there's something wrong with the partitioning. I can try cloning the extra hashcode in {{HiveKey}}., Gotcha, Hi [~lirui], If your suspeicion is confirmed by your followup investigation, maybe this is an opportunity for us to use Hivekey instead of BytesWritable since join seemingly also relies on this. You can create a separate JIRA for that., [~xuefuz], I copied the extra fields ({{hashCode}},{{distKeyLength}}) for HiveKey. Now all the reducers get some data but I got another issue here: most of the records go to just one reducer and records with the same key can go to different reducers. I'll investigate more into this., Hi [~brocknoland] & [~xuefuz],

The problem is {{RowContainer}} spills records to disk as we collect the output from mappers. Since we're using {{BytesWritable}} as the key type, we read the key back as {{BytesWritable}} (while it should be {{HiveKey}} to hold the proper hash code) when spark tries to partition the data to reducers. So if I copy the key as {{HiveKey}}, when spark does the partition, some keys are {{BytesWritable}} (those spilled to disk) and some are {{HiveKey}} (those cached in RowContainer). Then the partition really gets messed up.
Changing our key type to {{HiveKey}} requires changing {{SparkTran}} which needs much refactor to current code. Even if we can use {{HiveKey}} as key, we may need to add {{write}} and {{readFields}} to {{HiveKey}} in order to keep the extra hash code. I doubt such a change is acceptable.
Lazy computing (HIVE-7873) may help mitigate the issue by reducing the chance that a record be spilled to disk, so spark can partition the record by (possibly) {{HiveKey}}.

Any ideas on this?, Excellent research!

bq. reducing the chance that a record be spilled to disk

I think we need something that works in a deterministic manner.

bq. We may need to add write and readFields to HiveKey in order to keep the extra hash code. I doubt such a change is acceptable.

Yes, we won't want to serialize the extra bytes for other use cases. I think there might be a few other solutions:

1) Create HiveSparkKey which does serialize the hashcode
2) Enhance RowContainer serialize hashcode based on a configuration option , [~lirui] Thank you very much for the detailed analysis. Besides what Brock outlined above, I think the following is probably the easiest:

1. Use HiveKey as the key type in our code. This is also necessary for join, and so we have to do this any way.
2. Define the row stored in RowContainer as <byte[], BytesWrtiable> instead of either the current <BytesWritable, BytesWritable> or <HiveKey, BytesWritable>. We get the bytes for HiveKey using kryo. We can do this when we copy the <key, value> pair in HiveBaseFunctionResultList.collect(). That is, we copy the the byte array representing HiveKey rather than copying it as a ByteWritable.

Please let me know what you think., Thanks [~brocknoland], [~xuefuz] for the comments.
I also think using HiveKey is the easiest solution, as long as we don't lose the specific hash code.
{quote}
We get the bytes for HiveKey using kryo.
{quote}
Will kryo serialize {{HiveKey.hashCode}} as well?, Yes. Kryo can serialize HiveKey and deseriablize it back as an whole. Check out class KryoSerializer and its usage in HiveMapFunction and other classes., [~xuefuz], do you mean we have to add serializeHiveKey/deserializeHiveKey to KryoSerializer (like the way we did for jobConf)? Please note that HiveKey doesn't override {{write}} or {{readFields}} methods., [~lirui] I meant we can use the generic serialize/deserialize method in KryoSerializer. Specifically, the following methods can be used:
{code}
  public static byte[] serialize(Object object) ;
  public static <T> T deserialize(byte[] buffer,Class<T> clazz);
{code}
This should take care of the whole object, including the field hashcode., [~xuefuz] that's great! I created HIVE-8017 and will do the refactor under that., Fixed via HIVE-8017]