[copying this from HIVE-19388 for reference:

{quote}
As for your observation about spark_vectorized_dynamic_partition_pruning.q, seems that's indeed another bug. The task fails during MapWork initialization. When we retry the task, we retrieve the MapWork from cache. At this point, some operator's state is State.INIT, although the previous initialization actually failed. So initialization is skipped and the task somehow finishes successfully. I think one way to fix it is to clear the work cache when initialization fails.
{quote}

Hi [~lirui] Can you please point me to the code which retries the task? Thanks!, Hi [~vihangk1], the task is retried by Spark, and it calls SparkMapRecordHandler::init to initialize the map operator. This is where we retrieve the MapWork [from cache|https://github.com/apache/hive/blob/rel/release-2.2.0/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkMapRecordHandler.java#L75].
I'm not sure whether we have a way to reset the operators to UNINIT state. If not, guess we have to clear the cache when initialization fails., BTW, the hash table is loaded when we init the dummy operators [here|https://github.com/apache/hive/blob/rel/release-2.2.0/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkMapRecordHandler.java#L113].]