[GroupByOperator uses HashMap's size() and iterator() API, which is not supported by HashMapWrapper right now. We should extended HashMapWrapper to support those. , Thanks Ning, we are trying to implement HashMapWrapper in our solution for Group By,  Wanted to know if this has been already done, It will be great if you can suggest on this., Hi Soundararajan, Do you mean you are trying to replace HashMapWrapper or extend it to support GroupByOperator? I am not actively working on this JIRA now. I think maybe Arvind is working on it? , Soundararajan, Ning - Yes I am planning on working on it starting next week. I expect this to take at least upto mid to late in the week in order to get a patch available for this. However, if that schedule does not work for you, please feel free to take this issue into your queue and go ahead. It will be great if you could confirm it either way first.

Arvind, I did some preliminary analysis for this JIRA and converted the {{HashMapWrapper}} to implement the {{java.util.Map}} interface. This required some changes all the way down to the underlying JDBM classes. 

However, this alone is not sufficient to plug it into the {{GroupByOperator}} implementation because the data stored in the {{HashMap}} is a mix of serializable Java objects as well as {{Writable}}s. Since {{Writable}}s cannot be directly serialized to Java, it follows that inorder to use this for fixing the memory problem we need _an external serialization_ mechanism that can handle arbitrary mixed type object graphs.

A trivial approach to address this would be to implement custom serialization using Java reflection but that would incur cost of excessive reflection and byte handling/marshaling.

If you have any other ideas regarding this, please add it to the comments of this issue for consideration.
, If there is interest, I can file a separate JIRA for modifying {{HashMapWrapper}} to support the {{java.util.Map}} interface and decouple that work from this JIRA. I think there is a lot of benefit in doing just that. Also, we could have this JIRA depend upon that as a prerequisite.

, Arvind, I thought the whole point of this JIRA was to make HashMapWrapper to support java.util.Map, no? If that would be a separate JIRA, what would this one be for? Sorry for being a bit dense here but if you could clarify that would be great.

Thanks,
Ashish
, Ashish - no problem - let me explain: The problem being addressed by this JIRA is that {{GroupByOperator}} and possibly other aggregation operators use in-memory maps to store intermediate keys, which could lead to {{OutOfMemoryException}} in case the number of such keys is large. It is suggested that one way to work around it is to use the {{HashMapWrapper}} class which would help alleviate the memory concern since it is capable of spilling the excess data to disk.

The {{HashMapWrapper}} however, uses Java serialization to write out the excess data. This does not work when the data contains non-serializable objects such as {{Writable}} types - {{Text}} etc. What I have done so far is to modify the {{HashMapWrapper}} to support full {{java.util.Map}} interface. However, when I tried updating the {{GroupByOperator}} to use it, I ran into the said serialization problem.

Thats why I was suggesting that perhaps we should decouple the serialization problem from enhancing the {{HashMapWrapper}} and let the later be checked independently., Arvind, I remember I got this problem (non-serializable) problem before and the problem boils down to support ObjectInspector in the same way as RowContainer does. In terms of a complete interface of Map, it would be good to have but I would be very cautious about performance penalty it may add. If the new API is not needed for now, I would vote for the ObjectInspector support first. , Ning, Aravind, I got this implemented and it looks good so far, I will try uploading the version I have modified after a thorough test, All I did was copy the HashMap implementation into the HashMapWrapper (leaving the existing functionality intact), now HashmapWrapper works exactly like hashmap, but I did not get to test out the serialization issues. will do that and update you guys. I think this should help us in our OOM issue around GroupBy..., to add, XMLEncoder/XMLDecoder works just fine and can handle our serde issues., Aravind/Ning, This implementation uses XMLEncoder/XMLDecoder to handle the serde, I am still experimenting on this, I got the base implementation from http://gitorious.org/persistenthashmap/pages/Home, This library is distributed under GPL, Added some functionality around threshold and combined iteration... , Soundararajan, thanks for the contribution. However it seems the persistent hash map package is under GNU v.3, which is not compatible with apache license. We also looked at other existing open source packages before implementing our own HashMapWrapper. One of the problems we found out then is the license compatibility issue. , right... Ning is there any open source serializers/deserializers that you are aware of that is reflections based, if so I can quickly implement a similar persistent map around that...., I'm not aware of an efficient serde that are reflection based. The XMLEncoder/Decoder is JAXB-based and are very inefficient. That's another reason we don't want to use it in the execution code path. A better way is to use the Hive SerDe (e.g., LazyBinarySerde) just as it is done in RowContainer.

Another way to tackle this problem is have more accurate estimate of how many rows can be fit into the main memory. The current code checks the amount of available memory and use 0.25 (by default) of them to hold the hashmap. We set it to 0.15 in our environment and it works for most cases. 0.15 is probably a little bit conservative. Some experiments need to be done to tune this parameter so that most cases will be fit into main memory and only for the exceptional cases the secondary storage will be used. , Thanks Ning, sounds logical, will try with 0.15 and tune accordingly in our environment, but on a long run I guess we may need a strong reflection based serde Map. I am still exploring if it can be achieved.. will keep the progress posted., Sounds good. , Hi all, I'm new to HIVE, and i find there is no spilling when hash agg having lots of entries in the hash map which could cause OOM.

So, is this issue still open, or any update?[~aprabhakar]]