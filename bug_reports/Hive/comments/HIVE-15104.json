[[~wenli] Can you give an example that I can run and compare? , try select count(distinct col1), count (distinct col2) from table,   and see the statistic for shuffle data size.

if you cann't reproduce, let me know.    i will find one table in tpch, and reproduce , then tell the details step.  

thank you~, Seems MR can just serialize the key as BytesWritable instead of HiveKey. We once hit some problem when only serializing the BytesWritable part. But I think it's worth investigating whether we can improve., This is changed by HIVE-8017. [~lirui] Do you recall what kind of issues it caused?, We need to use HiveKey because it holds the proper hash code to be used for partitioning. MR also uses HiveKey, but in OutputCollector, seems it only serializes the BytesWritable part. [~wenli], is this what you mean?
I suspect we'll need help from Spark if we want to do something similar., This is rather interesting. I know I originally reviewed HIVE-8017, but I didn't really know why ByteWritable works for MR while we need HiveKey for Spark. Since Spark is stable now, it would be interesting to find out at least why, whether we can optimize or not.

[~ruili], since you originally discovered the problem, could you revisit the issue? Thanks., [~xuefuz], here's what I find so far.
Firstly, MR uses HiveKey as the [key type|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java#L253]. So we're inline with MR. HiveKey extends BytesWritable. I think the main reason we need HiveKey is we don't want the hash code simply computed from the internal bytes. Instead, we somehow compute the hash code elsewhere and set it into HiveKey.

During shuffle, MR passes the HiveKey to OutputCollector. OutputCollector computes the proper partition for HiveKey (using the hash code), and uses [WritableSerialization|https://github.com/apache/hadoop/blob/release-2.7.2-RC2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/WritableSerialization.java#L97] to serialize it. At this point, it's OK to just serialize the BytesWritable part since the partition has already been figured out.

On reduce side, WritableSerialization is used again to deserialize input key as HiveKey, then feed it to ExecReducer. Of course at this point, the HiveKey's hash code is just 0. But it doesn't matter because it's not needed any more. And ExecReducer just cast the input key as BytesWritable.

Therefore, I think whether we can achieve the same depends on how Spark deals with the HiveKey we pass to it. If that's possible, we can register a custom Serializer for HiveKey and only ser/de the BytesWritable part. Here're some docs regarding how to do that:
[spark.kryo.registrator|http://spark.apache.org/docs/latest/configuration.html#compression-and-serialization]
[kryo registration|https://github.com/EsotericSoftware/kryo#registration], [~lirui] So what you are saying is, it depends on how spark shuffles the data and whether spark relies on such hashCode to shuffle the data? , [~lirui], thanks for sharing your findings. Can you confirm that Spark also uses BytesWritable.hashcode() to partition the RS output rows? If this is true, then there is no difference for Spark because the actual object Hive passed to Spark by RS is HiveKey, whose hashcode will be used for partitioning. 

If this is the case, then we should be able to define the output of our map function and reduce function just as <BytesWritable, BytesWritable>, for which we don't need a custom serializer because we don't need to declare the type as <HiveKey, BytesWritable>. It seems that there is still a gap in our understanding.

, I checked the source code and it seems that both Spark (Partitioner.scala) and MapReduce (HashPartitioner.java) calls key.hashCode() to get the partition number., [~xuefuz], [~aihuaxu], both MR and Spark need HiveKey.hashCode to compute the partition number. HiveKey's [hashCode|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/HiveKey.java#L58] is not computed on demand. Instead it's a field we set to the HiveKey.
I think what we need to investigate is, whether the hash code is still needed after the HiveKey is serialized, e.g. Spark somehow deserialize the HiveKey and access the hash code again. If not, we can just ser/de the BytesWritable part of HiveKey because the hash code is not needed any more., I will take a look at Spark to see if it's needed after it's serialized., Hi [~aihuaxu], are you still working on this? If not, do you mind if I take over?, [~lirui] I didn't have time to work on that . Feel free to take it over. , I looked at the shuffle writers of Spark and none of them seem to need the hashCode/partitionId after the HiveKey is serialized. But I got a problem during implementation. The plan is to implement this Spark trait:
{code}
trait KryoRegistrator {
  def registerClasses(kryo: Kryo): Unit
}
{code}
Then we set this implementing class to {{spark.kryo.registrator}}. At runtime, Spark will use reflection to instantiate our class and call its registerClasses to register the optimized SerDe for HiveKey.
However, Kryo is relocated in Hive. After build, the method signature of our class will actually be:
{{public void registerClasses(org.apache.hive.com.esotericsoftware.kryo.Kryo kryo)}}.
When Spark calls the method, we get an {{AbstractMethodError}}. I suppose this is because the {{public void registerClasses(com.esotericsoftware.kryo.Kryo kryo)}} method is not really implemented.
Does anybody know how this can be resolved?, Tried disabling relocation locally. It does solve the AbstractMethodError. However, seems Spark still needs the hashCode on reducer side. Will dig more ..., Spark needs the hash code on reducer side for the groupBy shuffling. Since groupBy does no ordering, reducer needs to put the shuffled data into a map to combine values by key, thus needing the hash code. We just need to keep the hash code during SerDe if groupBy shuffle is used.

Upload a PoC patch to demonstrate the idea. It disables kryo relocation which should not be acceptable.

Also did simple test to see the improvement. The test is to run a query: {{select key, count ( * ) from A group by key order by key;}}, where A contains 40000000 records with 20 distinct keys. The measurement is the number of bytes written during shuffle. I tested optimize HiveKey alone, as well as optimize HiveKey and BytesWritable. We can see even for simple classes like BytesWritable, the custom SerDe does better than a generic one.
|| ||Opt(N)||Opt(Y, Key)||Opt(Y, Key + Value)||
||GBY(Y)|2269|1953|1699|
||GBY(N)|2269|1713|1460|, [~lirui], great progress! Thanks for keeping up the effort.

As to Kryo class relocation, I think Hive did that to avoid version difference between Spark and Hive. (git history might confirm this.) I'm concerned that class conflicts might come back if we stop relocating Kryo. Any thoughts?, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12867727/HIVE-15104.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10688 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_3] (batchId=97)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=97)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.org.apache.hadoop.hive.cli.TestNegativeCliDriver (batchId=89)
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.testSparkQuery (batchId=225)
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.testTempTable (batchId=225)
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery (batchId=225)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5226/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5226/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5226/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12867727 - PreCommit-HIVE-Build, [~xuefuz], kryo was relocated in HIVE-5915. So it's not intended for Spark. Actually, we're on the same version as Spark-2.0.0: kryo-shaded-3.0.3.
bq. I'm concerned that class conflicts might come back if we stop relocating Kryo
You're right. I'm not sure whether it's a conflict or loading issue, but when I tried to run some TPC-H benchmark, I got a ClassNotFoundException, although the class is there in hive-exec.jar. I'll see how to workaround this.

BTW, the test in my last comment shuffles very little data. That's why optimizing the overhead can have a significant improvement. I guess this won't be the case in real world query. That's why I want to run some more serious benchmark., The CNF is due to how kryo is loaded in {{KryoMessageCodec}}. W/ relocation, kryo is in package {{org.apache.hive.com.esotericsoftware}}. So it's loaded from hive-exec.jar. Spark adds hive-exec.jar at runtime with some URL class loader. W/o relocation, we're using same kryo as Spark. Kryo's class loader is by default the one that loads it - therefore the AppClassLoader. However, AppClassLoader cannot load classes from hive-exec.jar and thus the CNF.
To solve it, we can make {{KryoMessageCodec}} use the current context loader., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12868055/HIVE-15104.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10698 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_ppd_decimal] (batchId=9)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=144)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testConcurrentStatements (batchId=225)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5260/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5260/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5260/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12868055 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12868935/HIVE-15104.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10738 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_3] (batchId=97)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=97)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query24] (batchId=231)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5349/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5349/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5349/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12868935 - PreCommit-HIVE-Build, Patch v3 compiles the registrators at runtime, so that we don't have to disable kryo relocation. I've also put it on RB. Will upload a benchmark result later., Attaching TPC-H benchmark result. It shows the improvement is more obvious for long queries when we need to shuffle a lot of data. And it's better to use with groupBy shuffle disabled., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12869112/TPC-H%20100G.xlsx

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5367/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5367/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5367/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-05-20 15:31:32.562
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-5367/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-05-20 15:31:32.564
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 7429f5f HIVE-16717: Extend shared scan optimizer to handle partitions (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)
+ git clean -f -d
Removing ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimal.txt
Removing ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimalMerge.txt
Removing ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgMerge.txt
Removing ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgTimestamp.txt
Removing ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumTimestamp.java
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 7429f5f HIVE-16717: Extend shared scan optimizer to handle partitions (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-05-20 15:31:33.510
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
patch: **** Only garbage was found in the patch input.
patch: **** Only garbage was found in the patch input.
patch: **** Only garbage was found in the patch input.
fatal: unrecognized input
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12869112 - PreCommit-HIVE-Build, The approach here can cause problem when we cache RDDs, e.g. combining equivalent works. The cached RDDs will be serialized when stored to disk or transferred via network, then we need the hash code after the data is deserialized. I think we have to ser/de the hash code anyway to be safe., Update patch v4:
1. Moved the registrator code to a resource file. Hopefully the patch is more readable.
2. To be safe, we still have to store the hash code. But that's still better than the generic serializer., I also run another round of TPC-DS. The overall shuffle data is reduced by 12%. The query time improvement is however negligible - about 1.5%.
[~xuefuz] do you think it's worth the effort?, [~lirui], I'm wondering if there is anything new (other than moving code around). Last time we benchmarked and found there was actual performance degradation. We can do that again, and if the perf degradation still exists, we may not want this at lest not by default. I wasn't able to figure out why this degradation might happen., Hi [~xuefuz], I can't reproduce the perf degradation on my side. Some case runs slower with the patch which I think is just variations. The overall perf (total time taken to run the benchmark) is still slightly improved. I don't have physical nodes to run the benchmark so it's done on VMs. It'd be great if you could rerun your benchmark, and we can take a closer look at the degradation. Thanks., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877028/HIVE-15104.4.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10889 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6004/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6004/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6004/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877028 - PreCommit-HIVE-Build, Hi [~xuefuz], with HIVE-17114 and HIVE-17321 the benchmark results become more stable and the improvement is a little higher. Here's the latest [100GB TPC-DS result|https://docs.google.com/spreadsheets/d/1ba-AbUpJOHNb0_5PZyWQHzrH4wfRljMxQP9vA9JACHg/edit?usp=sharing].
Would you mind share your benchmark tool so that I can look into the perf degradation? Thanks., Hi [~lirui], thanks for continuing the work here. The improvement is impressive and not much perf degradation is observed. Let me get back my old benchmarks and see if those patches help., [~lirui], I found it difficulty to backport HIVE-17114 to our code base, so I had to give up. However, since you have a configuration to turn this on/off, I think it's find to have this and postpone the verification on my side later until we upgrade our Hive.

I need some time to review your latest patch as it's different from the previous has some low-level class/jar manipulations., Thanks [~xuefuz] and take your time. I guess we can also run a round of QA test with the switch turned on., Run tests with the switch on., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12883637/HIVE-15104.5.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 11001 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=61)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testHttpRetryOnServerIdleTimeout (batchId=228)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6535/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6535/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6535/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12883637 - PreCommit-HIVE-Build, The patch looks good to me. My only concern is about the reliability of the runtime compilation and jar creating. I'd think it's best if we can avoid that.

I'm not 100% sure of the class loading problem we faced. If we define class HiveKryoRegistrator in Hive, with relocation, Spark's unrelocated kryo isn't able to find it?, [~xuefuz], my previous [comment|https://issues.apache.org/jira/browse/HIVE-15104?focusedCommentId=15998177&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15998177] has some explanations about the relocation problem. Basically, the problem is we need to implement some method defined by Spark, and the method accepts a kryo parameter. With relocation, Hive's kryo and Spark's kryo are in different packages. If we compile the class in Hive and runs it in Spark, Spark will find the method not implemented because it has a different signature., I see. It might be possible to put this class in a new package (jar), for which we don't relocate kryo? , [~xuefuz], I'll try if that's feasible. Do you think it's OK to create a package just for one single class?, Hi [~lirui], I think creating a trivial package is still better than the runtime compilation/packaging. Plus, non-hos developers doesn't need to bother with that package. I don't foresee any problem with a separate project., Hi [~xuefuz], sorry for taking so long to update. I tried out your proposal. The idea is to build the trivial package and add it with the {{--jars}} config when we launch the Spark app. So we need to locate the jar at runtime. To locate the jar, we can use reflection to get the class and call {{SparkContext.jarOfClass}} to get the URI. But since kryo is shaded, we don't have {{com.esotericsoftware.kryo.Kryo}} in our class path on Hive side. When I try to get the registrator class, I get a {{NoClassDefFoundError}} for {{com.esotericsoftware.kryo.Kryo}}.
I guess one workaround is to let user specify the path to the jar, but that seems not very friendly. Any suggestions?, One correction: the {{NoClassDefFoundError}} is for {{com.esotericsoftware.kryo.Serializer}}. That's because our HiveKey and BytesWritable serializer extend kryo's Serializer. When loading our classes, the super class also needs to be loaded and thus the error.

Since the serializers are static nested classes of HiveKryoRegistrator, I tried loading the class w/o linking it, i.e. by calling {{ClassLoader.loadClass()}}. And that can avoid the NoClassDefFoundError. But not sure whether this is reliable and independent from JVM implementations., Hi [~lirui], to locate the jar, can we assume that the jar is located somewhere in Hive's installation path? I'm not sure where (Hive, spark-submit, or remote driver) we need to find the location of the jar., [~xuefuz], we need to locate the jar on Hive side, before we call spark-submit. I made Hive include it in the {{HIVE_HOME/lib}} directory. I guess we can find the path to hive-exec.jar (which is also under lib) and search for the registrator jar under the same path (or relative ones). But that will totally depend on how Hive is installed., I think it's fairly safe to assume that hive-exec.jar and the new jar are in the same location. We can error out if the jar cannot be found in that location., Update patch v6 based on Xuefu's suggestions., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12892380/HIVE-15104.6.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7323/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7323/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7323/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-10-16 13:13:35.520
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-7323/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-10-16 13:13:35.522
+ cd apache-github-source-source
+ git fetch origin
From https://github.com/apache/hive
   6339936..da304ef  master     -> origin/master
+ git reset --hard HEAD
HEAD is now at 6339936 HIVE-17749: Multiple class have missed the ASF header (Saijin Huang via Rui)
+ git clean -f -d
Removing standalone-metastore/src/gen/org/
+ git checkout master
Already on 'master'
Your branch is behind 'origin/master' by 2 commits, and can be fast-forwarded.
  (use "git pull" to update your local branch)
+ git reset --hard origin/master
HEAD is now at da304ef HIVE-17798: When replacing the src table names in BeeLine testing, the table names shouldn't be changed to lower case (Marta Kuczora, via Peter Vary)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-10-16 13:13:39.565
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java: No such file or directory
error: a/itests/src/test/resources/testconfiguration.properties: No such file or directory
error: a/packaging/pom.xml: No such file or directory
error: a/pom.xml: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientUtilities.java: No such file or directory
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12892380 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12892511/HIVE-15104.7.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7341/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7341/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7341/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-7341/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-10-17 05:22:03.427
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 8c3f0e4 HIVE-17815: prevent OOM with Atlas Hive hook (Anishek Agarwal reviewed by Thejas Nair)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 8c3f0e4 HIVE-17815: prevent OOM with Atlas Hive hook (Anishek Agarwal reviewed by Thejas Nair)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-10-17 05:22:03.911
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
Going to apply patch with: patch -p0
patching file common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
patching file hive-kryo-registrator/pom.xml
patching file hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java
patching file itests/src/test/resources/testconfiguration.properties
patching file packaging/pom.xml
patching file pom.xml
patching file ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java
patching file ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java
patching file ql/src/test/queries/clientpositive/spark_opt_shuffle_serde.q
patching file ql/src/test/results/clientpositive/spark/spark_opt_shuffle_serde.q.out
patching file spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
patching file spark-client/src/main/java/org/apache/hive/spark/client/SparkClientUtilities.java
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
protoc-jar: protoc version: 250, detected platform: linux/amd64
protoc-jar: executing: [/tmp/protoc9130095883787762036.exe, -I/data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore, --java_out=/data/hiveptest/working/apache-github-source-source/standalone-metastore/target/generated-sources, /data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore/metastore.proto]
ANTLR Parser Generator  Version 3.5.2
Output file /data/hiveptest/working/apache-github-source-source/standalone-metastore/target/generated-sources/antlr3/org/apache/hadoop/hive/metastore/parser/FilterParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/parser/Filter.g
org/apache/hadoop/hive/metastore/parser/Filter.g
DataNucleus Enhancer (version 4.1.17) for API "JDO"
DataNucleus Enhancer : Classpath
>>  /usr/share/maven/boot/plexus-classworlds-2.x.jar
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDatabase
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFieldSchema
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MType
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MConstraint
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MOrder
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStringList
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartition
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MIndex
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRole
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRoleMap
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMasterKey
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDelegationToken
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MVersionTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMetastoreDBProperties
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MResourceUri
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFunction
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationLog
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationNextId
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMResourcePlan
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMPool
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMTrigger
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMMapping
DataNucleus Enhancer completed with success for 35 classes. Timings : input=193 ms, enhance=178 ms, total=371 ms. Consult the log for full details
ANTLR Parser Generator  Version 3.5.2
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveLexer.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
org/apache/hadoop/hive/ql/parse/HiveLexer.g
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
org/apache/hadoop/hive/ql/parse/HiveParser.g
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HintParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HintParser.g
org/apache/hadoop/hive/ql/parse/HintParser.g
Generating vector expression code
Generating vector expression test code
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.
[ERROR] COMPILATION ERROR : 
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[20,33] package com.esotericsoftware.kryo does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[21,33] package com.esotericsoftware.kryo does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[22,36] package com.esotericsoftware.kryo.io does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[23,36] package com.esotericsoftware.kryo.io does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[26,35] package org.apache.spark.serializer does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[34,45] cannot find symbol
  symbol: class KryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[36,31] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[41,50] cannot find symbol
  symbol:   class Serializer
  location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[43,23] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[43,34] cannot find symbol
  symbol:   class Output
  location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[49,25] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[49,36] cannot find symbol
  symbol:   class Input
  location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[57,56] cannot find symbol
  symbol:   class Serializer
  location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[59,23] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[59,34] cannot find symbol
  symbol:   class Output
  location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[64,31] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[64,42] cannot find symbol
  symbol:   class Input
  location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile (default-compile) on project hive-kryo-registrator: Compilation failure: Compilation failure:
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[20,33] package com.esotericsoftware.kryo does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[21,33] package com.esotericsoftware.kryo does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[22,36] package com.esotericsoftware.kryo.io does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[23,36] package com.esotericsoftware.kryo.io does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[26,35] package org.apache.spark.serializer does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[34,45] cannot find symbol
[ERROR] symbol: class KryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[36,31] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[41,50] cannot find symbol
[ERROR] symbol:   class Serializer
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[43,23] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[43,34] cannot find symbol
[ERROR] symbol:   class Output
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[49,25] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[49,36] cannot find symbol
[ERROR] symbol:   class Input
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[57,56] cannot find symbol
[ERROR] symbol:   class Serializer
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[59,23] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[59,34] cannot find symbol
[ERROR] symbol:   class Output
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[64,31] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[64,42] cannot find symbol
[ERROR] symbol:   class Input
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-kryo-registrator
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12892511 - PreCommit-HIVE-Build, Fix dependencies, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12892549/HIVE-15104.9.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 11276 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[unionDistinct_1] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[optimize_nullscan] (batchId=163)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_notin] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_scalar] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_select] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_views] (batchId=108)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query16] (batchId=243)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query94] (batchId=243)
org.apache.hadoop.hive.cli.TestTezPerfCliDriver.testCliDriver[query16] (batchId=241)
org.apache.hadoop.hive.cli.TestTezPerfCliDriver.testCliDriver[query94] (batchId=241)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=204)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighShuffleBytes (batchId=229)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7348/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7348/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7348/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12892549 - PreCommit-HIVE-Build, The sub-query failures are tracked by HIVE-17823. Others are not related.
I've put the 9th patch on RB. [~xuefuz], could you take another look? Thanks., Update to address review comments. Also changed the default switch back to false., +1, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12893666/HIVE-15104.10.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 11319 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=110)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=205)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=222)
org.apache.hadoop.hive.ql.parse.authorization.plugin.sqlstd.TestOperation2Privilege.checkHiveOperationTypeMatch (batchId=270)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighShuffleBytes (batchId=229)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7456/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7456/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7456/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12893666 - PreCommit-HIVE-Build, Pushed to master. Thanks Xuefu for the review!, Doc note:  This adds *hive.spark.optimize.shuffle.serde* to HiveConf.java, so it needs to be documented in the wiki.

* [Configuration Properties -- Spark | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Spark]

Added a TODOC3.0 label., Thanks [~leftylev] for the reminder. I've updated the wiki., Good doc, thanks [~lirui].  I removed the TODOC3.0 label.

Here's a direct link to the doc:

* [hive.spark.optimize.shuffle.serde | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.spark.optimize.shuffle.serde], Hey [~lirui] I found some time to do some internal testing of this patch. I ran a 1 TB Parquet TPC-DS benchmark (subset of 49 queries, run three times each) on a physical cluster and found similar results to your TPC-DS benchmark.
||||Baseline Run (default configuration)||Optimized Serde||Optimized Serde + No GroupBy||
||Shuffle Bytes Read|699.5 GB|530.7 GB|531.6 GB|
||Shuffle Bytes Written|690 GB|529.5 GB|530.3 GB|
||Total Latency (min)|202|191|190|

So about a 25% improvement on shuffle data and 5% performance improvement. I think the improvement for the shuffle data is significant and is a goodimprovement.

A few questions on the implementation.
 * The {{HiveKryoRegistrator}} still seems to be serializing the {{hashCode}} so where are the actual savings coming from?
 * I'm not sure I understand why the performance should improve when {{hive.spark.use.groupby.shuffle}} is set to {{false}}. It's still using the same registrator right?
 * You said that we need to serialize the {{hashCode}} because "{{The cached RDDs will be serialized when stored to disk or transferred via network, then we need the hash code after the data is deserialized"}} - why do we need the {{hashCode}} after deserializing the data?, [~stakiar], thanks for trying this out.
bq. The HiveKryoRegistrator still seems to be serializing the hashCode so where are the actual savings coming from?
I didn't look deeply into kryo, but I think the reason is generic kryo SerDe has some overhead to store class meta info, while 
 in {{HiveKryoRegistrator}} we just store the data. My earlier [comment|https://issues.apache.org/jira/browse/HIVE-15104?focusedCommentId=16007788&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16007788] shows custom SerDe can bring improvements for BytesWritable too.

bq. I'm not sure I understand why the performance should improve when hive.spark.use.groupby.shuffle is set to false.
I guess the difference is due to the different shuffle we used -- if {{hive.spark.use.groupby.shuffle}} is false, group-by-key shuffle is replaced with repartition-and-sort-within-partition shuffle. And yes, the registrator is same for the two cases.

bq. why do we need the hashCode after deserializing the data?
For MR, the hash code is not needed for deserialized HiveKey (see HiveKey::hashCode), because when HiveKey is deserialized, it's already been distributed to the proper reducer. For Spark, RDDs may get cached during the execution. So if we deserialize a cached RDD and try to partition it to a downstream reducer, we'll need the hash code available after deserialization., Hive 3.0.0 has been released so closing this jira.]