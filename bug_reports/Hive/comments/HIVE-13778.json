[How *many* files are needed to trigger this? cc: [~rajesh.balamohan], [~ashutoshc] I noticed that this happens usually around 25+ files., Can you provide details on the hive and hadoop-aws version details?. I tried it on Hive master + hadoop-2.8 and it worked fine., [~rajesh.balamohan]
I discovered a few more details as I was trying to reproduce this issue. I didn't realize that this is how it happened before. To reproduce, do the following:

 - In Hive, "create table purge_test_s3 (x int) location 's3a://[bucket]/purge_test_s3';"
 - Use the AWS CLI or the AWS Web interface to copy files to the above mentioned location.
 - In Hive, "drop table purge_test_s3 purge;"

The Metastore logs say:
2016-05-20 17:01:41,259 INFO  hive.metastore.hivemetastoressimpl: [pool-4-thread-103]: Not moving s3a://[bucket]/purge_test_s3 to trash
2016-05-20 17:01:41,364 INFO  hive.metastore.hivemetastoressimpl: [pool-4-thread-103]: Deleted the diretory s3a://[bucket]/purge_test_s3

However, the files are still there. The weird part is that the Hadoop S3A connector reads the files correctly but is not able to delete them.

If instead of the AWS CLI or the AWS Web interface, we use the hadoop CLI to copy the files, "drop table ... purge" works just fine. If we insert the files using Hive, it works fine as well.

This might be an issue of the HDFS Namenode not getting updated and might be more a problem for the HDFS folks., Thanks for the details [~sailesh].  Namenode should not be involved with s3a paths.

Can you re-run with some s3a logging on?  i.e. org.apache.hadoop.fs.s3a=DEBUG, Note this is the same as [IMPALA-3558|https://issues.cloudera.org/projects/IMPALA/issues/IMPALA-3558].  See that issue for my explanation that this is expected behavior., [~sailesh] can you assign this to me please?  I will resolve it., [~fabbri], I don't have Assign privileges here. I'll just resolve the issue myself. The root cause of the issue was found and is tracked by the following JIRA:
HADOOP-13230, Thanks.. You could also resolve as duplicated by., Duplicated by HADOOP-13230]