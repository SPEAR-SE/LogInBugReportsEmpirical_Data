[Fixing it by change void type in the field schema to string type. , Need code review. , I am not sure implicitly assuming string type is a good choice. I tested following query:
{code}
create table t2 as select null from t1;
{code} 

It fails on MS SQL Server and Oracle. Passes on postgres and mysql. But type of column in postgres is {{unknown}} and in mysql is {{binary}}

My preference is to fail this query in Hive as well, although with a better error message., 

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12745140/HIVE-11217.1.patch

{color:green}SUCCESS:{color} +1 9191 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4603/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4603/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4603/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12745140 - PreCommit-HIVE-TRUNK-Build, Without the patch, for query create table t2 as select null from t1, hive returns success with type void. I think the void type is not good for we can not create table with type void like create table foo (aaa void). And for CTAS with ORC format, hive throws the error.

So we have 3 choices:
1. Fix ORC, let it succeeds with type void.
2. Fix all the format scenarios, let hive return success with type string. (This patch)
3. Throw errors for all the formats. 
[~ashutoshc], I am fine with choice 2 or 3. Do you still prefer choice 3? 
[~gauravkohli], how do you think? 
Thanks, On second thought, maybe we should choose option 1. For option 2 and 3 may conflict with fixes for https://issues.apache.org/jira/browse/HIVE-4172 (JDBC2 does not support VOID type). 
Attach a new patch let orc support void type as other serde such as LazySimpleSerDe. 
, 

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12746140/HIVE-11217.2.patch

{color:green}SUCCESS:{color} +1 9227 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4668/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4668/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4668/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12746140 - PreCommit-HIVE-TRUNK-Build, [~spena] and [~ctang.ma], could you review the change? Thanks, [~ychena] This change seems reasonable to let ORC be compatible with the other file formats. We can file followup jira to discuss if we want to throw errors like other database since the customer has been using that for some time. [~ashutoshc] How do you think?, Patch seems reasonable to me. [~prasanth_j] what do you think ?, OrcProto is generated file and should never be modified. Also I don't think VOID is valid supported type in hive https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types
IMO, it should be handled at the hive level. Hive should not pass any type other than the ones supported. , For insert queries hive maps null values to destination column types. For CTAS, may be it should default to some type (string?)., [~prasanth_j], The first patch of this jira is mapping void to string type, do you think, it is a better solution? Thanks, [~ychena] Sorry I didn't look at the first patch. First patch looks reasonable to me. But I think all typecheck and conversions are happening in TypeCheckProcFactory. Can this change be done there instead?, [~prasanth_j], I can not find a proper place in TypeCheckProcFactory to put the code. For it is related to TypeInfo, could I put it into
hive/serde2/typeinfo/TypeInfoUtils ?
Attached the third patch to use TypeInfoUtils. Please review to see if it makes sense.
Thanks, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12756282/HIVE-11217.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 9448 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_nullvalueorc
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5306/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5306/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5306/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12756282 - PreCommit-HIVE-TRUNK-Build, Very recent changes in master affect the output of the qfile a little bit, patch 4 updated the ctas_nullvalueorc.q.out file. , [~ychena] My suggestion is to change the NullExprProcessor in TypeCheckProcFactory. Specifically, the following change
{code}
return new ExprNodeConstantDesc(TypeInfoFactory.getPrimitiveTypeInfoFromPrimitiveWritable(Text.class), null);
{code}

This changes NullWritable to Text. 

[~ashutoshc] IIRC, CBO changes void type to string. I recently found that when I was writing a test case for HIVE-11836. Will there be any consequence of the above change?
, [~prasanth_j], changes in TypeCheckProcFactory will cause assert error in my test case:
{noformat}
  TestCliDriver.testCliDriver_ctas_nullvalueorc:130->runTest:156 Unexpected exception java.lang.AssertionError
	at org.apache.calcite.rex.RexBuilder.makeLiteral(RexBuilder.java:908)
	at org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.convert(RexNodeConverter.java:389)
	at org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.convert(RexNodeConverter.java:129)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.genSelectLogicalPlan(CalcitePlanner.java:2745)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.genLogicalPlan(CalcitePlanner.java:2842)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:853)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:814)
	at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:113)
	at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:882)
	at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:149)
	at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:106)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:617)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:252)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10137)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:212)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:240)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:428)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1150)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1203)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1079)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1069)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1033)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1007)
	at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:146)
	at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ctas_nullvalueorc(TestCliDriver.java:130)

{noformat}

The failure is in
{noformat}
    case STRING:
      calciteLiteral = rexBuilder.makeLiteral((String) value);
      break;
{noformat}
makeLiteral assume the value is not null.

When I use eclipse debug in my local setup, the code is never called, so it passed. But it failed in the unit test. , I am still not convinced that silently assuming string type is a good idea. See, my very first comment on this jira. Further, as [~ychena] noted, with HIVE-4172 we are respecting VOID type. IMHO, either we should make ORC support VOID or throw an exception., Agreed with your initial comment. I also favor to throw exception and fail such queries much early instead of at runtime with ORC throwing exception. , For the LazySimpleSerDe already supports void, could we just do the same thing in orc? We'd better not break already supported features. Which means patch 2 is a better solution. , 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12756412/HIVE-11217.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 9450 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5319/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5319/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5319/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12756412 - PreCommit-HIVE-TRUNK-Build, I tried the following 
{code:title=TEXT}
hive> create table test_text as select null from loc_orc;
OK
hive> desc test_text;
OK
_c0                 	void     
hive> insert into test_text select '1' from loc_orc;
FAILED: UDFArgumentException void not found.
{code}

{code:title=RCFILE}
hive> create table test_rcfile stored as rcfile as select null from loc_orc;
OK
hive> desc test_rcfile;
OK
_c0                 	void      
hive> insert into test_rcfile select '1' from loc_orc;
FAILED: UDFArgumentException void not found.
{code}

{code:title=AVRO}
hive> create table test_avro stored as avro as select null from loc_orc;
OK
hive> desc test_avro;
OK
_c0                 	void      
hive> insert into test_avro select '1' from loc_orc;
FAILED: UDFArgumentException void not found.
{code}

{code:title=PARQUET}
hive> create table test_parquet stored as parquet as select null from loc_orc;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.UnsupportedOperationException: Unknown field type: void
{code}

{code:title=ORC}
hive> create table test_orc stored as orc as select null from loc_orc;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.IllegalArgumentException: Unknown primitive type VOID
{code}

Text, RCFile and Avro succeeds CTAS. The type is void but we cannot add any more rows to that table other than nulls which I don't find quite useful.

Parquet and ORC fails the query at runtime. 

I still don't see the usefulness of supporting void in ORC. I think it's better to fail (preferably much early) than supporting void type to which we cannot add any non-void type values., [~prasanth_j], you make a very good point. 
Do you want a change similar to https://issues.apache.org/jira/browse/HIVE-2615 ?
The patch in HIVE-2615 is never committed.

, [~ychena] Yes. IMO, that's the way we should take.
As Ashutosh mentioned in HIVE-2615, its better to the type checking in TypeCheckProcFactory instead of SemanticAnalyzer., [~prasanth_j], I do not quite understand why it is better in TypeCheckProcFactory.
To throw this error, need two conditions:
1. It is CTAS operation.
2. It has void type column. 
SemanticAnalyzer has all above information, so it is a reasonable place to change.
We do not throw error for all void columns. For example "select null, key from src " should return values. 
If we make changes in TypeCheckProcFactory ( for example in NullExprProcessor), it will be difficult to know it is in the context of a CTAS or just a simple select. 
I attach patch 5 to throw error in SemanticAnalyzer, could you review it to see if make sense? Or give me more suggestions. Thanks, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12761218/HIVE-11271.5.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 9453 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection
org.apache.hive.hcatalog.streaming.TestStreaming.testRemainingTransactions
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchCommit_Json
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5342/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5342/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5342/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12761218 - PreCommit-HIVE-TRUNK-Build, The four failures are not related.

org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation failed more than 100 times

The 3 org.apache.hive.hcatalog.streaming.TestStreaming tests is caused by:

Table/View 'TXNS' already exists in Schema 'APP'., [~ychena] SemanticAnalyzer in itself is a huge chunk of code and we really don't want to add more to it. In TypeCheckProcFactory.NullExprProcessor all you have to do is the following to know if it is a CTAS query
{code}
boolean isCTAS = SessionState.get().getCommandType().equals(HiveOperation.CREATETABLE_AS_SELECT.getOperationName());
{code}, [~prasanth_j], Thanks for review the change. 
I think the error has to be thrown in the context of void will be used as column field schema in the CTAS, if the NullExprProcessor used in subquery and not used in the CTAS column, the query should succeed. The place that I am changing will make sure the void will be used as field schema. 

Give you an example:
{noformat}
create table foo3 as select b.code from (select null, code from jsmall) b;
create table foo4 as select null, code from jsmall;
{noformat}
The first one should succeed while the second should fail. They both use NullExprProcessor and both are CTAS.
What I am trying to say that TypeCheckProcFactory may sometimes have difficulty to figure out the context of the expression.
, [~ychena] Thanks for the explanation. Make sense now. 
Some minor comments: Few lines below your change there is comment about assigning STRING type if column type is VOID. Since we throw SemanticException earlier the following code will be a dead code. Right? Can you please remove it?

{code}
        // Replace VOID type with string when the output is a temp table or
        // local files.
        // A VOID type can be generated under the query:
        //
        // select NULL from tt;
        // or
        // insert overwrite local directory "abc" select NULL from tt;
        //
        // where there is no column type to which the NULL value should be
        // converted.
        //
        String tName = colInfo.getType().getTypeName();
        if (tName.equals(serdeConstants.VOID_TYPE_NAME)) {
          colTypes = colTypes.concat(serdeConstants.STRING_TYPE_NAME);
        } else {
          colTypes = colTypes.concat(tName);
        }
{code}

nit: the colType check that you have added can be moved further up, immediately after the for loop iteration to save some allocation/cpu cycles., [~prasanth_j], the place that I put the code to throw exception is within 
if (field_schemas != null) which means will create a CTAS table.
If the field_schemas is null, the change from void to string code will be reached (this is for the case such as "select NULL from tt", something more like hive internal use void for temp table...) So we have to keep it. 

I will make more changes to save some cpu time. Thanks
, change the patch name to use the correct jira number., [~prasanth_j], when I try to improve the performance, I find most time before the exception thrown is used to get the colName. But the error message need the colName(it is very useful when use alias for the column). So we can not save much time. Could I just keep the patch as is? How do you think? Thanks, Sure. Not a problem. +1. Sorry for going back and forth on this issue. I will commit this patch shortly. , Thanks [~prasanth_j] for reviewing the patch., Committed to master and branch-1. Thanks [~ychena] for the contribution!]