[

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12646646/HIVE-7121.1.patch

{color:red}ERROR:{color} -1 due to 15 failed/errored test(s), 5462 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_decimal1
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket5
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucket_num_reducers
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_bucketed_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_dyn_part
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_num_buckets
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_truncate_column_buckets
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testDropTable
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testListPartitions
org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.testNameMethods
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/288/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/288/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-288/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 15 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12646646, Does this effect bucketed tables? I think it does and then we can not just change the hash code because that would break assumptions of what is in the bucket. IE i create a bucket in hive 12, and in hive 13 different data would be in the bucket.

I think this is why:

org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_num_buckets
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_quotedid_smb

These tests are failing. If this is the case we need a way or recording the hashcode in the metadata for the table., [~appodictic] I think you're right. This definitely affects bucketing. 

Options I see are:

- Only do it for queries that do not enter into bucketed tables, i.e.: leave the bucketing hash function as badly distributed as it is, but fix shuffle joins, group bys and inserts into other tables.
- Remember the hash function in table metadata. This is slightly tricky because we probably don't want a mix of hash functions in the same table (different partitions have different bucketing schemes - that would probably destroy any chance of SMB on that table.) Maybe we even want only one function per DB to make sure different tables in a DB can be joined without looking at the hash function used for each.

How come though these unit tests are failing? I didn't think we changed the bucketing scheme between hive 12 and 13. Did we?, [~hagleitn]: The unit tests are failing because I'm applying the same insert mechanic for flat & partitioned tables.

The patch works correctly when the following code fragment is hit

{code}
      // replace bucketing columns with hashcode % numBuckets
      int buckNum = 0;
      if (bucketEval != null) {
        buckNum = computeBucketNumber(row, conf.getNumBuckets());
        cachedKeys[0][buckColIdxInKey] = new IntWritable(buckNum);
      }
{code}

This is indeed setup correctly when doing dynamic partitioned inserts. Looks like this optimization is missed for the flat table inserts., Fix the table bucketed insert case by limiting the murmur hash to cases with variable number of reducers.

This neatly avoids using the new hash function where there are explicit bucket counts or reducer counts., Couldn't apply .2 cleanly. Not sure what happened. .3 should apply., Patch looks good. This doesn't affect bucketed tables, but we should still open another jira to handle that case too.

+1 assuming tests pass., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12647534/HIVE-7121.3.patch

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 5571 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input42
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_metadata_only_queries
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_ptf
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_scriptfile1
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_schema_evolution
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketmapjoin6
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_ctas
org.apache.hadoop.hive.ql.exec.tez.TestTezTask.testSubmit
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimal
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalX
org.apache.hive.hcatalog.pig.TestOrcHCatPigStorer.testWriteDecimalXY
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/355/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/355/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-Build-355/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12647534, This has an additional test failure which was not present in HIVE-7158 runs (which includes this patch).

{code}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_dynpart_sort_optimization
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_insert1
{code}

are not failing in https://issues.apache.org/jira/browse/HIVE-7158?focusedCommentId=14015118&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14015118

flaky tests?, Believe so. I've run all the failed tests locally and didn't see any new errors., Committed to trunk. Thanks [~gopalv]!]