[A patch that attempts to replace the long name under the assumption that they are not used for deserializer-based serdes. I'll test it tomorrow., [~jxiang] are you familiar with Avro stuff? I've no idea who knows that serde but you fixed some JIRAs :), I am not familiar with Avro serde either :(, [~xuefuz] [~sachingoyal] are you familiar with it? I wonder who is. most commits on these files are pretty old, you have one in 2014 :), Need to change the type name to keep metastore happy. Tested this on cluster with a giant Avro schema, I can create the table, query it (it's empty though) and describe correctly. At any rate, it's an improvement over existing truncated type name. [~ashutoshc] do you want to review or suggest a reviewer? :)

Btw, this case will also fail on Oracle (before the patch), as it doesn't allow the data to be truncated on insert., https://reviews.apache.org/r/38862/, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12764326/HIVE-11985.01.patch

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 9638 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_groupby_reduce
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5468/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5468/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5468/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12764326 - PreCommit-HIVE-TRUNK-Build, I would like to have other's opinion on this. 
Patch arbitrarily replaces a long type name (> 2K chars) with a typename {{struct<>}} for serde supplied schemas under the assumption that this typename is not used for anything for query processing (now and in *future*).  This lossy information (stored permanently in metastore) for type might not be a good idea, because this prevents us from using type information reliably in future. We can never be sure whether type info is correct or truncated one. Secondly, for its current use case (of describe table) it will atleast be confusing for user to see struct<> in type info in describe output, as oppose to actual type info.
IMHO, throwing an error for type name (> 2K) which is also part of the patch, should be sufficient.
 [~alangates] your thoughts?, The describe command correctly gets the type information from deserializer.
I can increase the limit to 4000 (length of column); at that point it's no worse than having incorrect, truncated type info in terms of usage - if it's used for anything it will fail anyway.
Throwing an error will make it impossible to use Hive with complex Avro schemas., [~ashutoshc] ping?, +1 to Ashutosh's comment, I don't think blowing off the type name is a good idea.  

Could we resolve the Avro case by having the option to store long type names in a separate column that's unbounded (text or clob depending on the RDBMS type).  This would take extra work on our side to detect the case and retrieve the name from the proper location but it should only penalize tables with very long column names., the key question is why would we do that. As far as I see column type should never be used for these serdes. If anything, I'd rather change the logic to write nothing, or a fixed coded type, for these cases. It's pointless to write it into the column, and error prone in case someone thinks that type in the column is actually a real thing and comes to rely on it, when real type is always derived from the schema, regardless of what's in the column., Updated the patch in accordance with the new description, First I have to admit that I don't have the enough knowledge to conclude if the approach here causes a problem. As far as I know, 4000 character limit is only a problem for Oracle, yet the patch seems rejecting any schema that is more than 2000 long. This sounds rather harsh, and a long of times users get around the problem by changing Oracle settings.

On a high level, I'd echo [~ashutoshc] and [~alangates]'s concerns. If we spend time on this, I'd rather solve the problem in the generic way, regardless the serde type and db type. The obvious inconsistency I see here is that we store for avro the schema if it's less than 2000 while storing a constant string for anything over that. If we determine that it's not necessary to store it for avro, don't store it at all. Or if we can solve the length problem for all serdes, then that's probably the the right way to go., [~xuefuz] the updated patch takes the route of not storing it at all.
Also, it's also problem for other RDBMSes (if it is ever used); Oracle fails, MySQL truncates it silently in my setup, others I haven't tested, probably one of the two., [~xuefuz] does the updated patch address your concern? :), I'm not sure if not storing the types in metastore poses a problem for the following use case: user is able to create an ARVO table without giving an external schema, such as 
{code}
create table j2 (a int) stored as avro;
{code}
See some comments in HIVE-11827. However, I guess the test will catch it if so.

On the other hand, I really think that we should address the 4000 character limitation somehow as the problem can happen to serdes other than AVRO. , 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12765687/HIVE-11985.02.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 9657 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_serde
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_partition_wise_fileformat17
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority2
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5594/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5594/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5594/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12765687 - PreCommit-HIVE-TRUNK-Build, Account for serdes that allow both options, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12766685/HIVE-11985.03.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 9697 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter1
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_explainuser_1
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver_explainuser_3
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5665/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5665/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5665/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12766685 - PreCommit-HIVE-TRUNK-Build, I don't understand the (old) alter serde logic at all... it only sets fields if the new serde doesn't have metastore-based schema. That doesn't make any sense., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12766940/HIVE-11985.05.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 9700 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_explode
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udtf_explode
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5688/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5688/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5688/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12766940 - PreCommit-HIVE-TRUNK-Build, The HCatalog test failed due to some unrelated cluster setup issue:
{noformat}
Caused by: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn/staging/history/done does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:376)
	at org.apache.hadoop.fs.DelegateToFileSystem.listStatus(DelegateToFileSystem.java:149)
{noformat}
It passes locally. The others were broken on master at the time.
[~xuefuz] [~alangates] [~ashutoshc] can you take a look at the latest patch?
, [~sershe], could you explained a little on your new approach. I cannot follow through the patch to the extent of full understanding. It would be nice if a RB entry can be provided as the changes have become non-trial., RB is available at https://reviews.apache.org/r/38862/.
The recent changes only fix the corner case like alter table add serde, otherwise approach is the same; when the serde is deserializer-based for schema, it doesn't store columns in Metastore. For Avro, based on how table is set up, both storing and not storing the schema is supported., Let me try to summarize what we get from this: we error out in case of long type names (>2000c) with this patch, while previously we just store whatever the user gives, which may end up with truncation. In previous case, user may be able to avoid the truncation by manipulating DB configuration, but now w/ this patch, such manipulation will not work any more.

Correct me if I'm off., In the previous case, we stored what was in the schema; on some DBs it could be truncated and on some DBs it would make it impossible to create the table (Oracle). In the former case the truncation will not actually be visible to the user, because metastore values are never used, wrong data is stored in metastore. In any case, it is not always possible to get rid of truncation because varchar (e.g. in Oracle) is limited, you cannot make the column bigger than the limit.
I think there's also a risk from storing values in metastore in this case, because if someone were to actually use the values, it would be incorrect.
For example, if schema is changed, and someone takes the type from metastore, type will be incorrect; or, if someone tries to make changes in metastore as normal from some new code in Hive, these changes will be ignored because schema comes from deserializer.
The patch changes logic to not store values when they should not be used., Thanks for the explanation. I reviewed the patch, but I'm afraid my knowledge is not good enough to grant a +1. It would be great if others can also review.

Nevertheless, I do favor A solution that solves the long type name issue., Filed HIVE-12269 to truncate the type name. I don't think it makes sense to add blob type column, which is handled differently between most rdbms-es, with upgrade scripts, doing a lot of work, testing, and adding support burden, for the problem that doesn't exist (these types in metastore are utterly pointless), For most things that muck with the typesystem in hive, [~jdere] is my go-to person to check with. Tagging him here., [~sershe] Can you confirm my understanding of the patch?
* If serde doesnt belong in config {{hive.serdes.using.metastore.for.schema}} schema is always stored in metastore. Patch introduces a check which will disallow type > 2K for this case.
* If serde doesn't belong in that config, then we enquire serde whether he wants to store its schema or not in metastore. If it say yes, then we store it with the above check.  Else, we store {{erived from deserializer}} as type for it.

I think this strategy does make sense. We can additionally get rid of  config {{hive.serdes.using.metastore.for.schema}} now that a method of same effect has been added., The method requires one to instantiate the SerDe. I wonder if it makes sense to keep it for the quick check. Btw there's also HIVE-12269 for less intrusive patch for now., I think we should proceed with this patch, rather than workaround unless someone feels otherwise. 
We instantiate Serde anyways, I think it makes sense to remove that config. But that we can take up on follow-on jira.
+1, Shouldn't we wait for HIVE-12274, which would solve root cause of the problem as well for this?, Although HIVE-12274 will ameliorate the problem, we will still need checks for max length, since on all RDBMS there is a limit on max length for varchar. In particular, Oracle where this problem was found length is limited to 4K, where we already are at. We can update length check for other DBs once, HIVE-12274 lands, but check is still needed, so I think this patch makes sense., Committed to master., No-doc note:  The configuration parameter *hive.serdes.using.metastore.for.schema* (which gets changed by this patch) is deliberately undocumented because it's for internal use only.  See the comments on HIVE-6681.]