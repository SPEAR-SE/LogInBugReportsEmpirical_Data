[Not sure this is a problem though. The next row may contain data with 18 decimal points, for which precision may get lost. I would think user shouldn't specific decimal(38, 18) for numbers that don't require such a scale.

Of course, we may want to check how other DBs handle this., For Java BigDecimal, there is a comment about the rounding and wonder if it can be used in Hive
{code}
Before rounding, the scale of the logical exact intermediate result (e.g. multiplier.scale() + multiplicand.scale()) is the preferred scale for that operation (e.g. multiply). If the exact numerical result cannot be represented in precision digits, rounding selects the set of digits to return and the scale of the result is reduced from the scale of the intermediate result to the least scale which can represent the precision digits actually returned. If the exact result can be represented with at most precision digits, the representation of the result with the scale closest to the preferred scale is returned.
{code}
I checked the MySQL which supports max precision 65 and max scale 30:
{code}
create table decimaltest (col1 decimal(65,14), col2 decimal(65, 14));
insert into decimaltest values (987654321001234567890123456789012345678901234567890.12345678901234, 10000000000000.12345678901234);
select col1 * col2 from decimaltest
--
returns 
9876543210012345678901234567890123456789012345678901234567890000.000000000
{code}
It is hard to interpret this result, its precision is 73 ( > max 65) and scale is 9 (instead of 28). But its metadata in a JDBC application is decimal with precision 65 and scale 28., Another use case if we use a decimal with small scale such as decimal (38, 6): 
{code}
create table test1 (a decimal(38, 6), b decimal(38, 6), c decimal(38, 6), d decimal(38, 6), e decimal(38, 6), f decimal(38, 6))
insert into test1 values (1.000000, 2.000000, 3.000000, 4.000000, 5.000000, 6.000000);
hive> explain select a*b*c*d*e*f from test1;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: test1
          Statistics: Num rows: 1 Data size: 53 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: (((((a * b) * c) * d) * e) * f) (type: decimal(38,36))
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 53 Basic stats: COMPLETE Column stats: NONE
            ListSink

hive> select a*b*c*d*e*f from test1;
OK
NULL
{code}, Here is the document about precision and scale in MS SqlServer (see https://msdn.microsoft.com/en-us/library/ms190476.aspx)
Basically the multiplication of two decimals (p1, s1) and (p2, s2) results in a decimal with precision p1 + p2 + 1 and scale s1 + s2. But precision or scale maximum should be 38. When the result precision is greater than 38, the corresponding scale is reduced to prevent the integral part of a result from being truncated.
-----
I did some test in Oracle whose decimal (numeric) max precision is also 38. For example:
dec(38,18) 92345678901234567890.123456789012345678 * 
dec(38,18) 20000000000000000000.00000000000000001
==> 1846913578024691357802469135780246914483
The result JDBC SQL type is numeric (0,0). 
It looks like that Oracle also truncates the result scale part in order to keep its integral part.
----
PostgreSQL supports max precision 131072 and scale 16383, which are large enough and should not have this issue.
, See also HIVE-13098 ;), I think as long as the derivation method is well defined it should be ok to return null or fail for values that don't fit. There's no good way to derive result type from the data in advance..., Thanks [~sershe] for chiming in. I think the issue here might be sightly different from that in HIVE-13098. For the example in HIVE-13098, select 999999 as decimal(5,0) should ideally throw out an exception since 999999 exceeds the decimal(5,0) data range. For the decimal multiplication, I think its result should be implicitly rounded up (or via a configuration?) as long as it is within the range supported by the specified decimal. Otherwise, the decimal use with multiplication will be dramatically limited because of the supported range. For the example in https://issues.apache.org/jira/browse/HIVE-14281?focusedCommentId=15384793&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15384793, the result data range is reduced to (-100, 100). Actually Hive cast also supports the round up in decimal, select cast(999.999 as decimal(5.0)) returns 1000., Hi all

Our customer faced this issue on production cluster. Any workaround? Any updates about fix? Please help., After applying HIVE-15331 issue HIVE-14281 was fixed too.]