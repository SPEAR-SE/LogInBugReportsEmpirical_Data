[Can you please run "set hive.tez.exec.print.summary=true;" in the session and attach the output?
Another thing to check is the value of "hive.exec.reducers.bytes.per.reducer" this controls the parallelism in the reducer phase., [~gss2002]: the only difference in codepath should be the split-generation - everything else is identical between all 3.

Both the MR and Spark impls ignore the actual FileInputFormat implementation and do not call FileInputFormat::getSplits().

Tez seems to be actually calling the FileInputFormat::getSplits() for TextInputFormat.

Can you run the same query with compressed input data?, Mostafa Mokhtar: This isn't happening in the reducer phase. It's occurring in the Map phase attempt. I've attached the Map Phase attempt output from the TezChild task., hive.exec.reducers.bytes.per.reducer=67108864; default?, Seems to be that single file with a group by/order by is generating 40040 splits... I think the map file is needed at this point to determine why this is happening correct?

2015-05-19 16:20:32,462 INFO [AsyncDispatcher event handler] impl.VertexImpl: Num tasks is -1. Expecting VertexManager/InputInitializers/1-1 split to set #tasks for the vertex vertex_1426958683478_171530_1_00
2015-05-19 16:20:32,707 DEBUG [InputInitializer [Map 1] #0] security.UserGroupInformation: PrivilegedAction as:gss2002 (auth:SIMPLE) from:org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:239)
2015-05-19 16:20:32,708 INFO [InputInitializer [Map 1] #0] dag.RootInputInitializerManager: Starting InputInitializer for Input: crc_arsn on vertex vertex_1426958683478_171530_1_00 [Map 1]
2015-05-19 16:20:32,722 INFO [InputInitializer [Map 1] #0] log.PerfLogger: <PERFLOG method=getSplits from=org.apache.hadoop.hive.ql.io.HiveInputFormat>
2015-05-19 16:20:32,723 INFO [InputInitializer [Map 1] #0] exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/431ae2bc-ebc9-48e7-bbb3-f03144198009/hive_2015-05-19_16-20-28_783_5570914503219655045-1/gss2002/_tez_scratch_dir/9da6870e-7388-40b1-bab6-9d0f242b1702/map.xml
2015-05-19 16:20:32,723 DEBUG [InputInitializer [Map 1] #0] exec.Utilities: Found plan in cache for name: map.xml
2015-05-19 16:20:32,744 INFO [InputInitializer [Map 1] #0] exec.Utilities: Processing alias crc_arsn
2015-05-19 16:20:32,744 INFO [InputInitializer [Map 1] #0] exec.Utilities: Adding input file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn
2015-05-19 16:20:32,747 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: hive.io.file.readcolumn.ids=
2015-05-19 16:20:32,747 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: hive.io.file.readcolumn.names=,arsn_cd,appl_user_id
2015-05-19 16:20:32,747 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: Generating splits
2015-05-19 16:20:32,780 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-19 16:20:32,780 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-19 16:20:32,780 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-19 16:20:32,780 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
2015-05-19 16:20:32,781 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-19 16:20:32,782 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7879a53d
2015-05-19 16:20:32,785 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-19 16:20:32,785 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-19 16:20:32,785 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-19 16:20:32,785 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
2015-05-19 16:20:32,786 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-19 16:20:32,786 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7879a53d
2015-05-19 16:20:32,876 DEBUG [InputInitializer [Map 1] #0] mapred.FileInputFormat: Time taken to get FileStatuses: 87
2015-05-19 16:20:32,876 INFO [InputInitializer [Map 1] #0] mapred.FileInputFormat: Total input paths to process : 1
2015-05-19 16:20:32,881 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-19 16:20:32,881 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-19 16:20:32,881 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-19 16:20:32,881 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
2015-05-19 16:20:32,882 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-19 16:20:32,883 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7879a53d
2015-05-19 16:20:32,907 DEBUG [InputInitializer [Map 1] #0] mapred.FileInputFormat: Total # of splits generated by getSplits: 40040, TimeTaken: 124
2015-05-19 16:20:32,916 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: number of splits 40040
2015-05-19 16:20:32,916 INFO [InputInitializer [Map 1] #0] log.PerfLogger: </PERFLOG method=getSplits start=1432066832722 end=1432066832916 duration=194 from=org.apache.hadoop.hive.ql.io.HiveInputFormat>
2015-05-19 16:20:32,917 INFO [InputInitializer [Map 1] #0] tez.HiveSplitGenerator: Number of input splits: 40040. 23542 available slots, 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat
2015-05-19 16:20:32,917 INFO [InputInitializer [Map 1] #0] exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/431ae2bc-ebc9-48e7-bbb3-f03144198009/hive_2015-05-19_16-20-28_783_5570914503219655045-1/gss2002/_tez_scratch_dir/9da6870e-7388-40b1-bab6-9d0f242b1702/map.xml
2015-05-19 16:20:32,918 INFO [InputInitializer [Map 1] #0] exec.Utilities: ***************non-local mode***************
2015-05-19 16:20:32,918 INFO [InputInitializer [Map 1] #0] exec.Utilities: local path = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/431ae2bc-ebc9-48e7-bbb3-f03144198009/hive_2015-05-19_16-20-28_783_5570914503219655045-1/gss2002/_tez_scratch_dir/9da6870e-7388-40b1-bab6-9d0f242b1702/map.xml
2015-05-19 16:20:32,918 DEBUG [InputInitializer [Map 1] #0] exec.Utilities: Loading plan from string: /tmp/hive/gss2002/431ae2bc-ebc9-48e7-bbb3-f03144198009/hive_2015-05-19_16-20-28_783_5570914503219655045-1/gss2002/_tez_scratch_dir/9da6870e-7388-40b1-bab6-9d0f242b1702/map.xml
2015-05-19 16:20:32,919 INFO [InputInitializer [Map 1] #0] log.PerfLogger: <PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-05-19 16:20:32,919 INFO [InputInitializer [Map 1] #0] exec.Utilities: Deserializing MapWork via kryo
2015-05-19 16:20:32,940 INFO [InputInitializer [Map 1] #0] log.PerfLogger: </PERFLOG method=deserializePlan start=1432066832919 end=1432066832940 duration=21 from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-05-19 16:20:32,941 DEBUG [InputInitializer [Map 1] #0] tez.SplitGrouper: Adding split hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000 to src new group? true, I am guessing this JIRA could be the root of this issue: https://issues.apache.org/jira/browse/HIVE-7156

"gss2002_20150520132600_e4199888_c149_4394_8231_238d9d9dee98_1.Map_1_crc_arsn" -> "gss2002_20150520132600_e4199888_c149_4394_8231_238d9d9dee98_1.Map_1" [ label = "Input [inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator]" ]




2015-05-20 13:26:03,760 INFO [IPC Server handler 0 on 33574] app.DAGAppMaster: JSON dump for submitted DAG, dagId=dag_1426958683478_173250_1, json={"dagName":"gss2002_20150520132600_e4199888-c149-4394-8231-238d9d9dee98:1","dagInfo":"{\"description\":\"\\nSELECT appl_user_id, arsn_cd, COUNT(*) as RecordCount FROM adw.crc_arsn GROUP BY appl_user_id,arsn_cd ORDER BY appl_user_id\",\"context\":\"Hive\"}","version":1,"vertices":[{"vertexName":"Map 1","processorClass":"org.apache.hadoop.hive.ql.exec.tez.MapTezProcessor","outEdgeIds":["196588160"],"additionalInputs":[{"name":"crc_arsn","class":"org.apache.tez.mapreduce.input.MRInputLegacy","initializer":"org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator"}]},{"vertexName":"Reducer 2","processorClass":"org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor","inEdgeIds":["196588160"],"outEdgeIds":["1320926067"]},{"vertexName":"Reducer 3","processorClass":"org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor","inEdgeIds":["1320926067"],"additionalOutputs":[{"name":"out_Reducer 3","class":"org.apache.tez.mapreduce.output.MROutput"}]}],"edges":[{"edgeId":"196588160","inputVertexName":"Map 1","outputVertexName":"Reducer 2","dataMovementType":"SCATTER_GATHER","dataSourceType":"PERSISTED","schedulingType":"SEQUENTIAL","edgeSourceClass":"org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput","edgeDestinationClass":"org.apache.tez.runtime.library.input.OrderedGroupedKVInput"},{"edgeId":"1320926067","inputVertexName":"Reducer 2","outputVertexName":"Reducer 3","dataMovementType":"SCATTER_GATHER","dataSourceType":"PERSISTED","schedulingType":"SEQUENTIAL","edgeSourceClass":"org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput","edgeDestinationClass":"org.apache.tez.runtime.library.input.OrderedGroupedKVInput"}]}
2015-05-20 13:26:03,762 INFO [IPC Server handler 0 on 33574] app.DAGAppMaster: Generating DAG graphviz file, dagId=dag_1426958683478_173250_1, filePath=/u01/hadoop/yarn/log/application_1426958683478_173250/container_1426958683478_173250_01_000001/dag_1426958683478_173250_1.dot


2015-05-20 13:26:05,142 DEBUG [InputInitializer [Map 1] #0] mapred.FileInputFormat: Total # of splits generated by getSplits: 40040, TimeTaken: 168
2015-05-20 13:26:05,144 DEBUG [Socket Reader #1 for port 33574] ipc.Server:  got #159
2015-05-20 13:26:05,145 DEBUG [IPC Server handler 0 on 33574] ipc.Server: IPC Server handler 0 on 33574: org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from 167.69.200.206:54162 Call#159 Retry#0 for RpcKind RPC_PROTOCOL_BUFFER
2015-05-20 13:26:05,145 DEBUG [IPC Server handler 0 on 33574] security.UserGroupInformation: PrivilegedAction as:gss2002@exa.example.COM (auth:TOKEN) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
2015-05-20 13:26:05,147 INFO [IPC Server handler 0 on 33574] ipc.Server: Served: getDAGStatus queueTime= 1 procesingTime= 2
2015-05-20 13:26:05,147 DEBUG [IPC Server handler 0 on 33574] ipc.Server: IPC Server handler 0 on 33574: responding to org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from 167.69.200.206:54162 Call#159 Retry#0
2015-05-20 13:26:05,147 DEBUG [IPC Server handler 0 on 33574] ipc.Server: IPC Server handler 0 on 33574: responding to org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from 167.69.200.206:54162 Call#159 Retry#0 Wrote 145 bytes.
2015-05-20 13:26:05,154 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: number of splits 40040
2015-05-20 13:26:05,154 INFO [InputInitializer [Map 1] #0] log.PerfLogger: </PERFLOG method=getSplits start=1432142764918 end=1432142765154 duration=236 from=org.apache.hadoop.hive.ql.io.HiveInputFormat>
2015-05-20 13:26:05,155 INFO [InputInitializer [Map 1] #0] tez.HiveSplitGenerator: Number of input splits: 40040. 23542 available slots, 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat, [~gss2002]: No, that JIRA is irrelevant to this issue - that has to do with column statistics, which your job does not have.

The issue you're hitting has its origins in the hadoop-1 TextInputFormat::getSplits(), which seems to generate 1 split per row.

Can you please test with a compressed input file & confirm if compressing the input makes a query faster?, So it looks like it turned a 3MB file into 5GB worth of reads since it did 40040 read ops due to the SPLITS?? 

BYTES_READ=5,149,746,588, BYTES_WRITTEN=0, READ_OPS=40040

hadoop fs -ls /example_dw/crc/arsn
Found 2 items
-rwxr-x---   6 loaduser hadoopusers          0 2015-05-17 20:03 /example_dw/crc/arsn/_SUCCESS
-rwxr-x---   6 loaduser hadoopusers    3883880 2015-05-17 20:03 /example_dw/crc/arsn/part-m-00000

2015-05-20 13:35:27,017 INFO [AsyncDispatcher event handler] history.HistoryEventHandler: [HISTORY][DAG:dag_1426958683478_173250_1][Event:TASK_FINISHED]: vertexName=Map 1, taskId=task_1426958683478_173250_1_00_000000, startTime=1432142771521, finishTime=1432143327014, timeTaken=555493, status=SUCCEEDED, successfulAttemptID=attempt_1426958683478_173250_1_00_000000_0, diagnostics=, counters=Counters: 28, org.apache.tez.common.counters.DAGCounter, RACK_LOCAL_TASKS=1, File System Counters, BYTES_READ=32, BYTES_WRITTEN=59817, READ_OPS=0, LARGE_READ_OPS=0, WRITE_OPS=0, BYTES_READ=5149746588, BYTES_WRITTEN=0, READ_OPS=40040, LARGE_READ_OPS=0, WRITE_OPS=0, org.apache.tez.common.counters.TaskCounter, SPILLED_RECORDS=11516, GC_TIME_MILLIS=19923, CPU_MILLISECONDS=890510, PHYSICAL_MEMORY_BYTES=1285681152, VIRTUAL_MEMORY_BYTES=5264326656, COMMITTED_HEAP_BYTES=3007840256, INPUT_RECORDS_PROCESSED=13440, OUTPUT_RECORDS=11516, OUTPUT_BYTES=218808, OUTPUT_BYTES_WITH_OVERHEAD=241846, OUTPUT_BYTES_PHYSICAL=59785, ADDITIONAL_SPILLS_BYTES_WRITTEN=0, ADDITIONAL_SPILLS_BYTES_READ=0, ADDITIONAL_SPILL_COUNT=0, HIVE, DESERIALIZE_ERRORS=0, RECORDS_IN_Map_1=13440, RECORDS_OUT_INTERMEDIATE_Map_1=11516, hadoop fs -cat /example_dw/crc/arsn/part-m-00000| wc -l 
13440


so 13440 rows... with a repfactor of 6...

What compression format are you looking for gz snappy etc?, "gzip -1 > part.gz", which should do the trick.

Also, you might want to do "hive --hiveconf hive.prewarm.enabled=true;" if you're worried about latency within the CLI - the default config is for throughput., With Compression with Snappy it ran in 7 seconds...

Status: DAG finished successfully in 7.93 seconds


METHOD                         DURATION(ms) 
parse                                1,081
semanticAnalyze                      1,488
TezBuildDag                            490
TezSubmitToRunningDag                  374
TotalPrepTime                        4,958

VERTICES         TOTAL_TASKS  FAILED_ATTEMPTS KILLED_TASKS DURATION_SECONDS    CPU_TIME_MILLIS     GC_TIME_MILLIS  INPUT_RECORDS   OUTPUT_RECORDS 
Map 1                      1                0            0             2.23              3,790                 29         13,440           11,516
Reducer 2                  1                0            0             0.81              2,150                  0         11,516           11,516
Reducer 3                  1                0            0             0.61              1,110                  0         11,516                0
OK
BB166674         P16     1, Just to clarify this data is tab delimited being loaded from SqoopV1... What is the difference between compressed vs uncompressed at this point?

Map 1: 0(+1)/1  Reducer 2: 0/1  Reducer 3: 0/1  
Map 1: 0(+1)/1  Reducer 2: 0/1  Reducer 3: 0/1  
Map 1: 0(+1)/1  Reducer 2: 0/1  Reducer 3: 0/1  
Map 1: 1/1      Reducer 2: 0/1  Reducer 3: 0/1  
Map 1: 1/1      Reducer 2: 0(+1)/1      Reducer 3: 0/1  
Map 1: 1/1      Reducer 2: 1/1  Reducer 3: 0(+1)/1      
Map 1: 1/1      Reducer 2: 1/1  Reducer 3: 1/1  
Status: DAG finished successfully in 523.42 seconds


METHOD                         DURATION(ms) 
parse                                   17
semanticAnalyze                      1,593
TezBuildDag                            585
TezSubmitToRunningDag                  187
TotalPrepTime                        3,522

VERTICES         TOTAL_TASKS  FAILED_ATTEMPTS KILLED_TASKS DURATION_SECONDS    CPU_TIME_MILLIS     GC_TIME_MILLIS  INPUT_RECORDS   OUTPUT_RECORDS 
Map 1                      1                0            0           516.72            752,950             15,318         13,440           11,516
Reducer 2                  1                0            0             0.81              1,890                 24         11,516           11,516
Reducer 3                  1                0            0             0.61              1,460                 19         11,516                0
OK
BB166674         P16     1, Debug logs from DAG with compressed it sets 1 split.. so how do we fix this issue?


2015-05-20 16:15:12,041 DEBUG [InputInitializer [Map 1] #0] exec.Utilities: Found plan in cache for name: map.xml
2015-05-20 16:15:12,055 INFO [InputInitializer [Map 1] #0] exec.Utilities: Processing alias gss_rsn2
2015-05-20 16:15:12,055 INFO [InputInitializer [Map 1] #0] exec.Utilities: Adding input file hdfs://xhadnnm1p.example.com:8020/apps/hive/warehouse/hue_debug.db/gss_rsn2
2015-05-20 16:15:12,057 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: hive.io.file.readcolumn.ids=
2015-05-20 16:15:12,058 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: hive.io.file.readcolumn.names=,arsn_cd,appl_user_id
2015-05-20 16:15:12,058 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: Generating splits
2015-05-20 16:15:12,087 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-20 16:15:12,087 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-20 16:15:12,087 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-20 16:15:12,087 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
2015-05-20 16:15:12,088 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-20 16:15:12,088 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c93595a
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-20 16:15:12,092 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c93595a
2015-05-20 16:15:12,216 DEBUG [InputInitializer [Map 1] #0] mapred.FileInputFormat: Time taken to get FileStatuses: 112
2015-05-20 16:15:12,216 INFO [InputInitializer [Map 1] #0] mapred.FileInputFormat: Total input paths to process : 1
2015-05-20 16:15:12,219 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-20 16:15:12,219 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-20 16:15:12,219 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-20 16:15:12,219 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
2015-05-20 16:15:12,220 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-20 16:15:12,220 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c93595a
2015-05-20 16:15:12,222 DEBUG [InputInitializer [Map 1] #0] mapred.FileInputFormat: Total # of splits generated by getSplits: 1, TimeTaken: 132
2015-05-20 16:15:12,222 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: number of splits 1
2015-05-20 16:15:12,222 INFO [InputInitializer [Map 1] #0] log.PerfLogger: </PERFLOG method=getSplits start=1432152912040 end=1432152912222 duration=182 from=org.apache.hadoop.hive.ql.io.HiveInputFormat>
2015-05-20 16:15:12,222 INFO [InputInitializer [Map 1] #0] tez.HiveSplitGenerator: Number of input splits: 1. 23542 available slots, 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/map.xml
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] exec.Utilities: ***************non-local mode***************
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] exec.Utilities: local path = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/map.xml
2015-05-20 16:15:12,223 DEBUG [InputInitializer [Map 1] #0] exec.Utilities: Loading plan from string: /tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/map.xml
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] log.PerfLogger: <PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] exec.Utilities: Deserializing MapWork via kryo
2015-05-20 16:15:12,239 INFO [InputInitializer [Map 1] #0] log.PerfLogger: </PERFLOG method=deserializePlan start=1432152912223 end=1432152912239 duration=16 from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-05-20 16:15:12,240 DEBUG [InputInitializer [Map 1] #0] tez.SplitGrouper: Adding split hdfs://xhadnnm1p.example.com:8020/apps/hive/warehouse/hue_debug.db/gss_rsn2/000000_0.snappy to src new group? true
2015-05-20 16:15:12,240 INFO [InputInitializer [Map 1] #0] tez.SplitGrouper: # Src groups for split generation: 2
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_
socket
2015-05-20 16:15:12,091 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-20 16:15:12,092 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c
93595a
2015-05-20 16:15:12,216 DEBUG [InputInitializer [Map 1] #0] mapred.FileInputFormat: Time taken to get FileStatuses: 112
2015-05-20 16:15:12,216 INFO [InputInitializer [Map 1] #0] mapred.FileInputFormat: Total input paths to process : 1
2015-05-20 16:15:12,219 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-20 16:15:12,219 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-20 16:15:12,219 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-20 16:15:12,219 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_
socket
2015-05-20 16:15:12,220 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-20 16:15:12,220 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c
93595a
2015-05-20 16:15:12,222 DEBUG [InputInitializer [Map 1] #0] mapred.FileInputFormat: Total # of splits generated by getSplits: 1, Tim
eTaken: 132
2015-05-20 16:15:12,222 INFO [InputInitializer [Map 1] #0] io.HiveInputFormat: number of splits 1
2015-05-20 16:15:12,222 INFO [InputInitializer [Map 1] #0] log.PerfLogger: </PERFLOG method=getSplits start=1432152912040 end=143215
2912222 duration=182 from=org.apache.hadoop.hive.ql.io.HiveInputFormat>
2015-05-20 16:15:12,222 INFO [InputInitializer [Map 1] #0] tez.HiveSplitGenerator: Number of input splits: 1. 23542 available slots,
 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/a760
104/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4
805-90a5-84b8c38fe1f4/map.xml
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] exec.Utilities: ***************non-local mode***************
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] exec.Utilities: local path = hdfs://xhadnnm1p.example.com:8020/tmp/hive/a76
0104/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-
4805-90a5-84b8c38fe1f4/map.xml
2015-05-20 16:15:12,223 DEBUG [InputInitializer [Map 1] #0] exec.Utilities: Loading plan from string: /tmp/hive/gss2002/646469af-0a8
7-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38
fe1f4/map.xml
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] log.PerfLogger: <PERFLOG method=deserializePlan from=org.apache.hadoop.hi
ve.ql.exec.Utilities>
2015-05-20 16:15:12,223 INFO [InputInitializer [Map 1] #0] exec.Utilities: Deserializing MapWork via kryo
2015-05-20 16:15:12,239 INFO [InputInitializer [Map 1] #0] log.PerfLogger: </PERFLOG method=deserializePlan start=1432152912223 end=
1432152912239 duration=16 from=org.apache.hadoop.hive.ql.exec.Utilities>
2015-05-20 16:15:12,240 DEBUG [InputInitializer [Map 1] #0] tez.SplitGrouper: Adding split hdfs://xhadnnm1p.example.com:8020/apps/hive
/warehouse/hue_debug.db/gss_rsn2/000000_0.snappy to src new group? true
2015-05-20 16:15:12,240 INFO [InputInitializer [Map 1] #0] tez.SplitGrouper: # Src groups for split generation: 2
2015-05-20 16:15:12,241 INFO [InputInitializer [Map 1] #0] tez.SplitGrouper: Estimated number of tasks: 40021 for bucket 1
2015-05-20 16:15:12,241 INFO [InputInitializer [Map 1] #0] split.TezMapredSplitsGrouper: Grouping splits in Tez
2015-05-20 16:15:12,241 INFO [InputInitializer [Map 1] #0] split.TezMapredSplitsGrouper: Desired splits: 40021 too large.  Desired splitLength: 20 Min splitLength: 16777216 New desired splits: 1 Total length: 807489 Original splits: 1
2015-05-20 16:15:12,242 INFO [InputInitializer [Map 1] #0] split.TezMapredSplitsGrouper: Using original number of splits: 1 desired splits: 1
2015-05-20 16:15:12,242 INFO [InputInitializer [Map 1] #0] tez.SplitGrouper: Original split size is 1 grouped split size is 1, for bucket: 1
2015-05-20 16:15:12,244 INFO [InputInitializer [Map 1] #0] tez.HiveSplitGenerator: Number of grouped splits: 1
2015-05-20 16:15:12,251 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
2015-05-20 16:15:12,251 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
2015-05-20 16:15:12,251 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
2015-05-20 16:15:12,251 DEBUG [InputInitializer [Map 1] #0] hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
2015-05-20 16:15:12,251 DEBUG [InputInitializer [Map 1] #0] retry.RetryUtils: multipleLinearRandomRetry = null
2015-05-20 16:15:12,251 DEBUG [InputInitializer [Map 1] #0] ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c93595a
2015-05-20 16:15:12,252 TRACE [InputInitializer [Map 1] #0] ipc.ProtobufRpcEngine: 85: Call -> xhadnnm1p.example.com/167.69.200.200:8020: getFileInfo {src: "/tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/map.xml"}
2015-05-20 16:15:12,253 DEBUG [InputInitializer [Map 1] #0] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms
2015-05-20 16:15:12,253 TRACE [InputInitializer [Map 1] #0] ipc.ProtobufRpcEngine: 85: Response <- xhadnnm1p.example.com/167.69.200.200:8020: getFileInfo {}
2015-05-20 16:15:12,254 TRACE [InputInitializer [Map 1] #0] ipc.ProtobufRpcEngine: 85: Call -> xhadnnm1p.example.com/167.69.200.200:8020: getFileInfo {src: "/tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/reduce.xml"}
2015-05-20 16:15:12,255 DEBUG [InputInitializer [Map 1] #0] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms
2015-05-20 16:15:12,255 TRACE [InputInitializer [Map 1] #0] ipc.ProtobufRpcEngine: 85: Response <- xhadnnm1p.example.com/167.69.200.200:8020: getFileInfo {}
2015-05-20 16:15:12,255 INFO [InputInitializer [Map 1] #0] dag.RootInputInitializerManager: Succeeded InputInitializer for Input: gss_rsn2 on vertex vertex_1426958683478_173564_1_00 [Map 1], After having offline discussion with Gopal V he determined the cause of this problem is that starting in Hive 0.14 org.apache.hadoop.mapred.TextInputFormat  uses whatever is defined in property: mapreduce.input.fileinputformat.split.minsize; In my case this was defined to "1"... Unfortunately that is 1 byte so it created 40040 splits creating 40400 reads of the single 3MB file...

Hope this helps someone else out.

Should be around half of the HDFS block size in my case 64MB since my block size is 128MB..
mapreduce.input.fileinputformat.split.minsize=67108864


Gopal V if no fix is coming should we resolve/close this JIRA?, [~gss2002]: Talking to the MRv2 folks to change the defaults to be saner than 1 byte.

Until that issue is resolved, I'll keep this open as a critical issue., Fixing legacy {{Mapred.TextInputFormat}} is fraught with issues.

Allow Tez split generation to set a sane default if the min-size is misconfigured during execution., +1. (Nit: ws issue, the closing brace for the if statement is not correctly aligned)., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12740251/HIVE-10746.1.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 9009 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_corr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4299/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4299/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4299/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12740251 - PreCommit-HIVE-TRUNK-Build, Test failures look unrelated.

Reformat before commit., Committed to 1.2.1, branch-1 and master., Please add to the release wiki ( https://cwiki.apache.org/confluence/display/Hive/Hive+1.2+Release+Status ) when you commit any patch to branch-1.2. I'll go ahead and add this one in.]