[Patch which applies over HIVE-6346 on hive/tez branch, The patch adds *hive.orc.zerocopy* to HiveConf.java, so it also needs to document the flag in hive-default.xml.template., 

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12626451/HIVE-6347.1.patch

Test results: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1151/testReport
Console output: http://bigtop01.cloudera.org:8080/job/PreCommit-HIVE-Build/1151/console

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as "{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as "{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as "{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as "{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:399:5: 
Decision can match input such as "{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW" using multiple alternatives: 2, 9

As a result, alternative(s) 9 were disabled for that input
warning(200): IdentifiersParser.g:524:5: 
Decision can match input such as "{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}" using multiple alternatives: 1, 3

As a result, alternative(s) 3 were disabled for that input
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ hive-exec ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-exec ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-exec ---
[INFO] Compiling 1546 source files to /data/hive-ptest/working/apache-svn-trunk-source/ql/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] Note: Some input files use or override a deprecated API.
[WARNING] Note: Recompile with -Xlint:deprecation for details.
[WARNING] Note: Some input files use unchecked or unsafe operations.
[WARNING] Note: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[21,48] cannot find symbol
symbol  : class DirectDecompressorShim
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[23,48] cannot find symbol
symbol  : class DirectCompressionType
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[51,48] cannot find symbol
symbol  : class ByteBufferPoolShim
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[52,48] cannot find symbol
symbol  : class ZeroCopyReaderShim
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[100,64] cannot find symbol
symbol  : class ByteBufferPoolShim
location: class org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[96,17] cannot find symbol
symbol  : class ZeroCopyReaderShim
location: class org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[27,48] cannot find symbol
symbol  : class DirectCompressionType
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[28,48] cannot find symbol
symbol  : class DirectDecompressorShim
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[82,11] cannot find symbol
symbol  : variable DirectCompressionType
location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[94,5] cannot find symbol
symbol  : class DirectDecompressorShim
location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[95,32] cannot find symbol
symbol  : variable DirectCompressionType
location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[153,5] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[165,5] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[228,45] cannot find symbol
symbol  : method getZeroCopyReader(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool)
location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[95,11] cannot find symbol
symbol  : variable DirectCompressionType
location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[107,5] cannot find symbol
symbol  : class DirectDecompressorShim
location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[108,32] cannot find symbol
symbol  : variable DirectCompressionType
location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[INFO] 17 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [4.645s]
[INFO] Hive Ant Utilities ................................ SUCCESS [6.787s]
[INFO] Hive Shims Common ................................. SUCCESS [3.329s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [2.245s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [2.600s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [1.428s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [3.198s]
[INFO] Hive Shims ........................................ SUCCESS [0.590s]
[INFO] Hive Common ....................................... SUCCESS [7.734s]
[INFO] Hive Serde ........................................ SUCCESS [8.743s]
[INFO] Hive Metastore .................................... SUCCESS [26.982s]
[INFO] Hive Query Language ............................... FAILURE [40.846s]
[INFO] Hive Service ...................................... SKIPPED
[INFO] Hive JDBC ......................................... SKIPPED
[INFO] Hive Beeline ...................................... SKIPPED
[INFO] Hive CLI .......................................... SKIPPED
[INFO] Hive Contrib ...................................... SKIPPED
[INFO] Hive HBase Handler ................................ SKIPPED
[INFO] Hive HCatalog ..................................... SKIPPED
[INFO] Hive HCatalog Core ................................ SKIPPED
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog HBase Storage Handler ............... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:52.063s
[INFO] Finished at: Sat Feb 01 08:39:12 EST 2014
[INFO] Final Memory: 55M/422M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[21,48] cannot find symbol
[ERROR] symbol  : class DirectDecompressorShim
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[23,48] cannot find symbol
[ERROR] symbol  : class DirectCompressionType
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[51,48] cannot find symbol
[ERROR] symbol  : class ByteBufferPoolShim
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[52,48] cannot find symbol
[ERROR] symbol  : class ZeroCopyReaderShim
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[100,64] cannot find symbol
[ERROR] symbol  : class ByteBufferPoolShim
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[96,17] cannot find symbol
[ERROR] symbol  : class ZeroCopyReaderShim
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[27,48] cannot find symbol
[ERROR] symbol  : class DirectCompressionType
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[28,48] cannot find symbol
[ERROR] symbol  : class DirectDecompressorShim
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[82,11] cannot find symbol
[ERROR] symbol  : variable DirectCompressionType
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[94,5] cannot find symbol
[ERROR] symbol  : class DirectDecompressorShim
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/SnappyCodec.java:[95,32] cannot find symbol
[ERROR] symbol  : variable DirectCompressionType
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.SnappyCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[153,5] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[165,5] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java:[228,45] cannot find symbol
[ERROR] symbol  : method getZeroCopyReader(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ByteBufferAllocatorPool)
[ERROR] location: interface org.apache.hadoop.hive.shims.HadoopShims
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[95,11] cannot find symbol
[ERROR] symbol  : variable DirectCompressionType
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[107,5] cannot find symbol
[ERROR] symbol  : class DirectDecompressorShim
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ZlibCodec.java:[108,32] cannot find symbol
[ERROR] symbol  : variable DirectCompressionType
[ERROR] location: class org.apache.hadoop.hive.ql.io.orc.ZlibCodec
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12626451, There are a number of cases there the if contains a negation
but there is an else condition. Thse should be swapped:

e.g

if (!bool) else => if (bool) else

{noformat}
+      if(zcr != null) {
+        while(len > 0) {
+          ByteBuffer partial = zcr.readBuffer(len, false);
+          result.add(new BufferChunk(partial, off));
+          int read = partial.remaining();
+          len -= read;
+          off += read;
+        }
+      } else {
{noformat}

{noformat}
+      if (ShimLoader.getHadoopShims().getDirectDecompressor(
+          DirectCompressionType.SNAPPY) != null) {
+        direct = Boolean.valueOf(true);
+      } else {
+        direct = Boolean.valueOf(false);
+      }
{noformat}, [~leftylev]: I will add the docs into the hive-site.xml in the next rev - was waiting for HIVE-6346 to get pushed to branch.

[~brocknoland]: a != null is actually positive test, because it implies there exists something (i.e it's presence). That is how rest of the ORC code deals with optional feature/args (like SARGs)., Address comments on review board (& added docs), munmap() is async and delayed action, Committed to branch. Thanks [~gopalv], *hive.exec.orc.zerocopy* is documented in hive-default.xml.template as of Hive 0.13.0 with the description "Use zerocopy reads with ORC." See HIVE-6360.

In the wiki its description has been expanded to "Use zerocopy reads with ORC. (This requires Hadoop 2.3 or later.)"

* [Configuration Properties: hive.exec.orc.zerocopy | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.orc.zerocopy], Hello,
I am running my cluster on Hadoop 2.7.1 (checksum fc0a1a23fc1868e4d5ee7fa2b28a58a), using Hive 1.2.1 (checksum ab480aca41b24a9c3751b8c023338231) and hive.exec.orc.zerocopy tends to cause failures.
All my hive queries run fines, but once I enable zerocopy, it seems to have some problems with native libraries:

{code}
set hive.exec.orc.zerocopy = true;
<execute my query>
Hadoop job information for Stage-1: number of mappers: 316; number of reducers: 90
2016-01-20 16:37:54,479 Stage-1 map = 0%,  reduce = 0%
2016-01-20 16:38:21,061 Stage-1 map = 100%,  reduce = 100%
2016-01-20 16:39:21,246 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_1452780282075_23380 with errors
Error during job, obtaining debugging information...
Diagnostic Messages for this Task:
Error: java.io.IOException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:266)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.<init>(HadoopShimsSecure.java:213)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:333)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:719)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:252)
	... 11 more
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect()I
	at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect(Native Method)
	at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressDirect(SnappyDecompressor.java:305)
	at org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor.decompress(SnappyDecompressor.java:341)
	at org.apache.hadoop.hive.shims.ZeroCopyShims$DirectDecompressorAdapter.decompress(ZeroCopyShims.java:101)
	at org.apache.hadoop.hive.ql.io.orc.SnappyCodec.directDecompress(SnappyCodec.java:100)
	at org.apache.hadoop.hive.ql.io.orc.SnappyCodec.decompress(SnappyCodec.java:67)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:214)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.read(InStream.java:227)
	at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.readValues(RunLengthIntegerReaderV2.java:54)
	at org.apache.hadoop.hive.ql.io.orc.RunLengthIntegerReaderV2.next(RunLengthIntegerReaderV2.java:302)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringDictionaryTreeReader.readDictionaryLengthStream(TreeReaderFactory.java:1674)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringDictionaryTreeReader.startStripe(TreeReaderFactory.java:1654)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringTreeReader.startStripe(TreeReaderFactory.java:1382)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StructTreeReader.startStripe(TreeReaderFactory.java:2040)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:795)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:205)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:539)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat$VectorizedOrcRecordReader.<init>(VectorizedOrcInputFormat.java:71)
	at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.getRecordReader(VectorizedOrcInputFormat.java:156)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.createVectorizedReader(OrcInputFormat.java:1088)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1102)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:67)
	... 16 more

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
{code}
, {code}
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect()I
{code}

Does look like the tasks are not able to locate the libhadoop.so binary (*or* the hadoop build was done without snappy-dev available).

Zero copy readers don't work if libhadoop.so is missing anyway.

But to fill in some later developments, turning on zerocopy=true needs cluster-wide configs to turn on YARN-1775.

The YARN memory counting counts memory-mapped files as container memory, so without that change you might see containers being killed for using too much memory as you scale past the terabyte levels., Thanks, [~gopalv],
Seems like tasks are not able to locate the libhadoop, indeed. However, they were able to do that when zerocopy was disabled (I am using snappy compression in my orc tables) and perform well. libhadoop.so and libsnappy are there and I never had any problem with those. This makes me think that there is a correlation between finding native libraries and zerocopy.
]