[[~xhao1], to clarify, this problem doesn't prevent the query from running successfully, right? Thanks., Yes, this problem doesn't prevent the query from running successfully., This problem does not seem specific to Spark.  I believe it happens when Hive starts a map-only MapReduce job. It doesn't generate a reduce.xml as there isn't a reduce phase, but later it tries to put reduce.xml in distributed cache when submitting the job, which cause this error., This was fixed in HIVE-11243

I don't have permissions to close this issue, but I think it can be resolved as Duplicate, HIVE-11243 changes the log level. Is it possible to avoid such log?, Ideally yes, but there are a few issues. The code that causes this exception is in the {{Utilities.getBaseWork}} method. This method basically fetches the {{Path}} to the plan file (e.g. {{reduce.xml}}), and tries to de-serialize it. If the plan file does not exist, the code will throw a {{FileNotFoundException}} which will then be caught and logged, and cause the method to return null. The problem is that the code doesn't differentiate between the scenarios where the plan file was accidentally deleted vs. no plan file was ever created in the first place (maybe a NPE gets thrown somewhere down the line, but there is guarantee). There is a comment in the code saying that the the {{FileNotFoundException}} "happens. e.g.: no reduce work.", Thanks [~stakiar] for the explanation. Then my question is can we avoid this on higher level? Specifically, why we need to call getReduceWork for map-only job?
One possible reason is we call {{HiveOutputFormatImpl.checkOutputSpecs -> Utilities.getMapRedWork}}. I think that can be fixed, at least for HoS, because for HoS we shouldn't try to get both MapWork and ReduceWork from one JobConf., Yes, I agree avoiding this on a higher level would be best. Let me take a closer look into the code and see what I can find. Thanks!, [~lirui] you are right. I debugged this some more, and found out the the {{FileNofFoundException}} is caused by {{HiveOutputFormatImpl.checkOutputSpecs -> Utilities.getMapRedWork}}. This happens even when running in MR mode.

I can work on a fix, but I'm not entirely sure how to modify the {{HiveOutputFormatImpl}} class to fix this. Given a {{JobConf}} object, is there any way to know if the current job corresponds to a Map-Only job?, My understanding is {{HiveOutputFormatImpl.checkOutputSpecs}} only looks for FS operator. FS indicates the end of a job. So if we find FS in MapWork, it means this is a map-only job and then we don't have to look for ReduceWork., Makes sense! I will implement the change and create a Patch + RB!, [~lirui] unfortunately, it seems that there are queries such as {{ANALYZE TABLE src COMPUTE STATISTICS}} that spawn Map Only Jobs, but don't have a {{FileSinkOperator}}. So I'm not sure if that approach will work.

Perhaps an alternative approach would be to set some type of flag in the {{JobConf}} indicating if a job contains {{MapWork}} or {{ReduceWork}}. This way, a Job can know if it there is no reduce phase. This new logic can probably all be encapsulated in the {{Utilities}} class., [~stakiar] - it seems not worth the extra complexity to just avoid annoying logs. I suggest we just apply the current solution, which should fix most of the cases, right? We can figure out a complete fix if there's need in future. Makes sense?, Yes, makes sense. The current solution should work for most cases.

Thanks for all the help!, [~lirui], I'm not sure if I understand the conclusion here, so a summary would be great. Annoying log is one thing, but another, more important problem is that trying to read reduce.xml for map-only tasks puts load on namenode. I'm wondering if it's possible to avoid making that call. Can you share your thoughts? Thanks.

BTW, this seems happening to Hive on MR as well., Hi [~xuefuz], the conclusion is we somehow try to read reduce.xml for map-only job, and yes it happens to MR as well. The call path is {{HiveOutputFormatImpl.checkOutputSpecs -> Utilities.getMapRedWork}}. The reason why HiveOutputFormatImpl needs to get the MapRedWork is it needs to do some check on all the FS operators. Since FS only exists at the end of a job, my suggestion is we firstly try to get MapWork. If the MapWork has an FS in it, it means this is a map-only job so we don't have to look for ReduceWork. But [~stakiar] found that some map-only job may not have FS in the MapWork, e.g. {{ANALYZE TABLE}}. To have a complete fix, we'll need some flag in the JobConf indicating if this is map-only. Or we can use my solution, which solves the issue for most cases.

Some special handling for HoS may be needed. For HoS, each map.xml and reduce.xml resides in a different path. We can use {{mapred.task.is.map}} to determine whether the JobConf is for MapWork or ReduceWork. And then call getMapWork or getReduceWork respectively., [~lirui], thanks for the summary. Following your idea, can we first check if the mapwork ends a RS and use this to determine if reduce.xml is expected?, I'm also working on this issue and I think it's better to avoid unnecessary NN calls, especially the # of mappers is huge.
Attaching an initial patch, which adopts the idea of using a separate conf flag. [~stakiar], [~ruili], [~xuefuz] can you give a review on this?
(sorry I have to own this JIRA in order to attach)., [~stakiar], let us know if you like to work on this. Otherwise, we will move this forward.

[~csun], thanks for working on this. I have two questions:
1. I saw ConditionalTask.hasReduce() is removed, which would change the semantics, right? I'm not sure if this breaks anything.
2. For SparkTask, I assume it's possible to have two MapTasks: one ends at FS, while the other connects to a reduce task. Would the first MapTask is still going to hit hdfs for reduce.xml?

[~lirui], please also take a look the approach. Thanks., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843132/HIVE-13278.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 39 failed/errored test(s), 10260 tests executed
*Failed tests:*
{noformat}
TestMiniLlapCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=133)
	[mapreduce2.q,orc_llap_counters1.q,bucket6.q,insert_into1.q,empty_dir_in_table.q,orc_merge1.q,script_env_var1.q,orc_merge_diff_fs.q,llapdecider.q,load_hdfs_file_with_space_in_the_name.q,llap_nullscan.q,orc_ppd_basic.q,transform_ppr1.q,rcfile_merge4.q,orc_merge3.q]
TestMiniLlapCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=134)
	[acid_bucket_pruning.q]
TestMiniLlapCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=135)
	[intersect_all.q,unionDistinct_1.q,orc_ppd_schema_evol_3a.q,table_nonprintable.q,tez_union_dynamic_partition.q,transform_ppr2.q,temp_table_external.q,global_limit.q,transform2.q,schemeAuthority.q,cte_2.q,rcfile_createas1.q,dynamic_partition_pruning_2.q,intersect_merge.q,transform1.q]
TestMiniLlapCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=136)
	[script_pipe.q,import_exported_table.q,except_distinct.q,orc_merge10.q,mapreduce1.q,explainuser_2.q,orc_merge4.q,rcfile_merge2.q,bucket5.q,llap_udf.q,external_table_with_space_in_location_path.q,load_fs2.q,script_env_var2.q,intersect_distinct.q,remote_script.q]
TestMiniLlapCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=137)
	[orc_merge2.q,insert_into2.q,reduce_deduplicate.q,orc_llap_counters.q,cte_4.q,schemeAuthority2.q,file_with_header_footer.q,rcfile_merge3.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=138)
	[join1.q,schema_evol_orc_acidvec_table_update.q,vector_decimal_5.q,insert_values_tmp_table.q,join32_lessSize.q,escape1.q,orc_predicate_pushdown.q,tez_union2.q,cte_mat_5.q,cte_mat_4.q,groupby3.q,smb_mapjoin_19.q,join46.q,dynpart_sort_optimization2.q,tez_bmj_schema_evolution.q,bucketmapjoin4.q,vector_include_no_sel.q,uber_reduce.q,schema_evol_orc_nonvec_part_all_complex.q,vector_interval_arithmetic.q,bucketsortoptimize_insert_2.q,smb_mapjoin_17.q,auto_sortmerge_join_3.q,vectorization_9.q,merge2.q,join_nulls.q,bucketsortoptimize_insert_6.q,ctas.q,cbo_udf_udaf.q,bucketmapjoin2.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=139)
	[skewjoinopt15.q,vector_coalesce.q,orc_ppd_decimal.q,cbo_rp_lineage2.q,insert_into_with_schema.q,join_emit_interval.q,load_dyn_part3.q,auto_sortmerge_join_14.q,vector_null_projection.q,vector_cast_constant.q,mapjoin2.q,bucket_map_join_tez2.q,correlationoptimizer4.q,vectorization_12.q,vector_number_compare_projection.q,orc_merge_incompat3.q,vector_leftsemi_mapjoin.q,update_all_non_partitioned.q,multi_column_in_single.q,schema_evol_orc_nonvec_table.q,cbo_rp_subq_in.q,cbo_rp_semijoin.q,tez_insert_overwrite_local_directory_1.q,schema_evol_text_vecrow_table.q,vector_count.q,auto_sortmerge_join_15.q,vector_if_expr.q,delete_whole_partition.q,vector_decimal_6.q,sample1.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=140)
	[bucket3.q,schema_evol_text_nonvec_table.q,mrr.q,orc_ppd_schema_evol_2b.q,orc_analyze.q,schema_evol_orc_acidvec_part_update.q,cbo_simple_select.q,cbo_rp_udf_udaf_stats_opt.q,subquery_views.q,multi_column_in.q,vector_interval_1.q,tez_join_result_complex.q,groupby1.q,ptf_matchpath.q,cbo_rp_udf_udaf.q,vector_decimal_aggregate.q,constprog_dpp.q,leftsemijoin_mr.q,unionDistinct_2.q,vectorization_14.q,update_all_types.q,cbo_stats.q,auto_sortmerge_join_6.q,vector_decimal_3.q,vector_groupby4.q,ptf.q,update_where_non_partitioned.q,insert_dir_distcp.q,vectorized_nested_mapjoin.q,schema_evol_text_nonvec_part.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=141)
	[insert_values_non_partitioned.q,union5.q,vectorized_math_funcs.q,vectorization_4.q,vectorization_2.q,vector_join_nulls.q,vector_decimal_math_funcs.q,vector_left_outer_join.q,tez_union_decimal.q,llap_partitioned.q,order_null.q,cbo_rp_views.q,smb_mapjoin_4.q,vector_date_1.q,lvj_mapjoin.q,partition_multilevels.q,varchar_udf1.q,select_dummy_source.q,limit_join_transpose.q,tez_multi_union.q,skewjoin.q,cte_mat_3.q,autoColumnStats_1.q,vector_decimal_round_2.q,semijoin.q,column_names_with_leading_and_trailing_spaces.q,update_two_cols.q,update_where_no_match.q,union_stats.q,authorization_2.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=142)
	[acid_vectorization_missing_cols.q,orc_merge9.q,vector_acid3.q,delete_where_no_match.q,vector_reduce1.q,stats_only_null.q,vectorization_part_project.q,vectorization_6.q,count.q,tez_vector_dynpart_hashjoin_2.q,parallel.q,delete_all_non_partitioned.q,delete_all_partitioned.q,vectorization_10.q,insert1.q,custom_input_output_format.q,vectorized_bucketmapjoin1.q,cbo_rp_windowing_2.q,vector_reduce3.q,smb_cache.q,hybridgrace_hashjoin_1.q,vector_count_distinct.q,schema_evol_orc_acid_part.q,hybridgrace_hashjoin_2.q,cross_join.q,parquet_predicate_pushdown.q,vector_varchar_mapjoin1.q,tez_smb_main.q,quotedid_smb.q,vector_bucket.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=143)
	[list_bucket_dml_10.q,mapjoin_emit_interval.q,acid_globallimit.q,vector_auto_smb_mapjoin_14.q,deleteAnalyze.q,database.q,smb_mapjoin_6.q,vector_reduce_groupby_decimal.q,vectorized_dynamic_partition_pruning.q,cbo_views.q,vectorization_part.q,auto_join30.q,dynamic_partition_pruning.q,cte_mat_1.q,cluster.q,vector_char_mapjoin1.q,bucketmapjoin7.q,vector_string_concat.q,cbo_rp_unionDistinct_2.q,limit_pushdown3.q,vector_outer_join2.q,smb_mapjoin_18.q,vector_varchar_4.q,vectorized_context.q,metadata_only_queries.q,ppd_union_view.q,union6.q,vector_decimal_4.q,cbo_subq_in.q,vectorized_timestamp_funcs.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=144)
	[vectorized_rcfile_columnar.q,vector_elt.q,explainuser_1.q,multi_insert.q,tez_dml.q,vector_bround.q,schema_evol_orc_acid_table.q,vector_when_case_null.q,orc_ppd_schema_evol_1b.q,vector_join30.q,vectorization_11.q,cte_3.q,update_tmp_table.q,vector_decimal_cast.q,groupby_grouping_id2.q,vector_decimal_round.q,tez_smb_empty.q,orc_merge6.q,vector_decimal_trailing.q,cte_5.q,tez_union.q,cbo_rp_subq_not_in.q,vector_decimal_2.q,columnStatsUpdateForStatsOptimizer_1.q,vector_outer_join3.q,schema_evol_text_vec_part_all_complex.q,tez_dynpart_hashjoin_2.q,auto_sortmerge_join_12.q,offset_limit.q,tez_union_multiinsert.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=145)
	[enforce_order.q,tez_dynpart_hashjoin_3.q,vectorization_7.q,tez_join.q,orc_ppd_schema_evol_1a.q,groupby_resolution.q,delete_where_non_partitioned.q,orc_merge11.q,subquery_exists.q,cbo_semijoin.q,schema_evol_orc_vec_table.q,mergejoin_3way.q,ppr_pushdown.q,orc_llap_nonvector.q,vector_reduce2.q,vector_interval_mapjoin.q,vectorization_16.q,tez_joins_explain.q,explainuser_4.q,create_merge_compressed.q,vector_aggregate_9.q,alter_merge_stats_orc.q,vectorization_not.q,windowing_gby.q,vectorization_part_varchar.q,having.q,vector_orderby_5.q,vector_outer_join6.q,delete_where_partitioned.q,union9.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=146)
	[load_dyn_part5.q,vector_complex_join.q,orc_llap.q,vectorization_pushdown.q,cbo_gby_empty.q,vectorization_short_regress.q,cbo_gby.q,auto_sortmerge_join_1.q,lineage3.q,cross_product_check_1.q,cbo_join.q,vector_struct_in.q,bucketmapjoin3.q,current_date_timestamp.q,orc_ppd_schema_evol_2a.q,groupby2.q,schema_evol_text_vec_table.q,vectorized_join46.q,orc_ppd_date.q,multiMapJoin1.q,sample10.q,vector_outer_join1.q,vector_char_simple.q,dynpart_sort_optimization_acid.q,auto_sortmerge_join_2.q,bucketizedhiveinputformat.q,leftsemijoin.q,special_character_in_tabnames_1.q,cte_mat_2.q,vectorization_8.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=147)
	[vectorized_parquet.q,vector_decimal_expressions.q,union_fast_stats.q,correlationoptimizer2.q,cbo_subq_not_in.q,identity_project_remove_skip.q,windowing.q,delete_tmp_table.q,bucket2.q,vector_aggregate_without_gby.q,schema_evol_text_nonvec_part_all_primitive.q,correlationoptimizer6.q,alter_table_invalidate_column_stats.q,union_remove_26.q,orc_create.q,vector_distinct_2.q,cbo_rp_limit.q,vectorization_13.q,mapjoin_mapjoin.q,update_all_partitioned.q,union2.q,vector_decimal_precision.q,dynpart_sort_opt_vectorization.q,exchgpartition2lel.q,dynpart_sort_optimization.q,vectorized_timestamp_ints_casts.q,tez_join_hash.q,multi_insert_lateral_view.q,non_native_window_udf.q,schema_evol_orc_vec_part_all_complex.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=148)
	[bucketsortoptimize_insert_7.q,cbo_windowing.q,load_dyn_part2.q,parquet_types.q,vectorization_5.q,schema_evol_stats.q,mapjoin46.q,column_access_stats.q,vector_between_in.q,merge1.q,cbo_rp_join.q,vectorized_string_funcs.q,vectorization_1.q,vector_coalesce_3.q,stats_noscan_1.q,vector_partition_diff_num_cols.q,vector_decimal_10_0.q,orc_merge5.q,bucketmapjoin6.q,vector_udf1.q,schema_evol_text_vec_part.q,vector_groupby_reduce.q,union8.q,lineage2.q,auto_sortmerge_join_16.q,auto_join29.q,correlationoptimizer1.q,vector_decimal_mapjoin.q,ptf_streaming.q,vectorized_case.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=149)
	[auto_sortmerge_join_13.q,union_top_level.q,vector_left_outer_join2.q,schema_evol_text_vecrow_part_all_primitive.q,constprog_semijoin.q,update_where_partitioned.q,drop_partition_with_stats.q,smb_mapjoin_14.q,skiphf_aggr.q,vectorized_ptf.q,auto_join_filters.q,join0.q,insert_orig_table.q,mergejoin.q,join_filters.q,orc_split_elimination.q,subquery_in.q,vector_outer_join0.q,schema_evol_text_vec_part_all_primitive.q,vector_complex_all.q,auto_sortmerge_join_4.q,bucket_many.q,vectorization_15.q,union3.q,vectorization_nested_udf.q,windowing_windowspec2.q,auto_smb_mapjoin_14.q,vector_mr_diff_schema_alias.q,vector_join_filters.q,reduce_deduplicate_extended.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=150)
	[vector_interval_2.q,schema_evol_orc_acid_table_update.q,schema_evol_orc_acid_part_update.q,orc_ppd_varchar.q,metadataonly1.q,limit_pushdown.q,auto_join_nulls.q,metadata_only_queries_with_filters.q,vector_inner_join.q,subquery_notin.q,schema_evol_text_nonvec_part_all_complex.q,vector_coalesce_2.q,alter_merge_orc.q,vector_between_columns.q,vector_char_cast.q,table_access_keys_stats.q,udaf_collect_set_2.q,update_after_multiple_inserts.q,offset_limit_ppd_optimizer.q,mapjoin_decimal.q,orc_merge_incompat1.q,columnstats_part_coltype.q,vectorized_parquet_types.q,union7.q,vector_char_2.q,schema_evol_orc_acidvec_part.q,vector_groupby_3.q,cbo_rp_gby.q,extrapolate_part_stats_partial_ndv.q,auto_sortmerge_join_9.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=151)
	[smb_mapjoin_15.q,insert_values_partitioned.q,selectDistinctStar.q,bucket4.q,vectorized_distinct_gby.q,vector_groupby_mapjoin.q,insert_values_dynamic_partitioned.q,vector_nvl.q,join_nullsafe.q,vectorized_mapjoin.q,schema_evol_orc_vec_part_all_primitive.q,vectorized_shufflejoin.q,tez_smb_1.q,cbo_union.q,tez_vector_dynpart_hashjoin_1.q,filter_join_breaktask2.q,vector_data_types.q,multiMapJoin2.q,filter_join_breaktask.q,schema_evol_orc_nonvec_part.q,alter_merge_2_orc.q,vectorization_3.q,union4.q,auto_sortmerge_join_8.q,stats_based_fetch_decision.q,disable_merge_for_bucketing.q,vectorized_date_funcs.q,auto_sortmerge_join_10.q,vector_varchar_simple.q,vector_decimal_udf2.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=152)
	[tez_union_group_by.q,auto_join1.q,escape2.q,tez_dynpart_hashjoin_1.q,correlationoptimizer3.q,vectorized_timestamp.q,cbo_subq_exists.q,vector_binary_join_groupby.q,lateral_view.q,vectorization_0.q,infer_bucket_sort_bucketed_table.q,optimize_nullscan.q,nonmr_fetch_threshold.q,vectorization_decimal_date.q,vectorized_casts.q,schema_evol_orc_vec_part.q,tez_self_join.q,vector_partitioned_date_time.q,schema_evol_text_vecrow_part_all_complex.q,tez_fsstat.q,stats11.q,vector_mapjoin_reduce.q,tez_join_tests.q,join_acid_non_acid.q,empty_join.q,auto_join21.q,schema_evol_text_vecrow_part.q,orc_ppd_timestamp.q,vector_decimal_1.q,vector_adaptor_usage_mode.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=153)
	[tez_schema_evolution.q,bucket_map_join_tez1.q,vector_multi_insert.q,insert_update_delete.q,temp_table.q,cross_product_check_2.q,cte_1.q,autoColumnStats_2.q,vectorization_17.q,orc_merge8.q,orc_merge_incompat2.q,bucket_groupby.q,vector_outer_join4.q,vector_nullsafe_join.q,orc_merge7.q,vector_decimal_udf.q,bucketpruning1.q,schema_evol_orc_acidvec_table.q,vector_grouping_sets.q,vector_outer_join5.q,vector_groupby6.q,bucketmapjoin1.q,smb_mapjoin_5.q,vector_char_4.q,auto_sortmerge_join_5.q,auto_join0.q,cbo_limit.q,load_dyn_part1.q,schema_evol_orc_nonvec_part_all_primitive.q,auto_sortmerge_join_11.q]
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=154)
	[auto_sortmerge_join_7.q,input16_cc.q]
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver (batchId=93)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query39] (batchId=222)
org.apache.hadoop.hive.ql.TestAcidOnTez.testMapJoinOnTez (batchId=203)
org.apache.hadoop.hive.ql.TestAcidOnTez.testMergeJoinOnTez (batchId=203)
org.apache.hadoop.hive.ql.TestAcidOnTezWithSplitUpdate.testMapJoinOnTez (batchId=206)
org.apache.hadoop.hive.ql.TestAcidOnTezWithSplitUpdate.testMergeJoinOnTez (batchId=206)
org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.testFetchResultsOfLogWithExecutionMode (batchId=211)
org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.testFetchResultsOfLogWithNoneMode (batchId=211)
org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.testFetchResultsOfLogWithPerformanceMode (batchId=211)
org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.testFetchResultsOfLogWithVerboseMode (batchId=211)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2566/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2566/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2566/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 39 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843132 - PreCommit-HIVE-Build, [~csun], thanks for the patch. I think Xuefu's concerns are valid. In SparkPlanGenerator, we clone JobConf for each BaseWork. So we can set the flag separately for each of them.
Actually I'm still not very clear when will {{HiveOutputFormatImpl.checkOutputSpecs}} be called. My understanding is, since HoS stores the plan file in different paths for different BaseWork, we can never get both MapWork and ReduceWork from a single JobConf. That means for HoS, we should always hit the FNF error. Or we should see FNF error for map.xml as well. It'll be better if we figure out these first., Thanks [~xuefuz] and [~lirui], I think you are right. Let me revise the patch.
[~lirui], to your question. I think for HoS the {{checkOutputSpecs}} is only called in {{SparkPlanGenerator}}, as discussed in https://issues.apache.org/jira/browse/HIVE-10073.
Also this only triggers for non-native storage (e.g., HBase), so we don't normally see it.

, Actually for HoS besides {{checkOutputSpecs}} I can't think of any case where we'll get the FileNotFoundException. 
[~xhao1], do you still have the query that caused this issue? did this happen with some non-native storage such as HBase?, [~csun], please feel free to address MR case first if we need more time for HoS case. Thanks. , [~csun], [~xuefuz] yes feel free to move this patch forward. Thanks., Thanks [~stakiar] :), [~csun], I think the error is found in container's log. SparkPlanGenerator runs in AM in yarn-cluster mode. If we find the error in any container other than AM, it means we somehow call the method on task side. I think Spark may call it too, e.g. in {{PairRDDFunctions::saveAsHadoopDataset}}., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843156/HIVE-13278.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10814 tests executed
*Failed tests:*
{noformat}
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[metadataonly1] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
org.apache.hive.service.server.TestHS2HttpServer.testContextRootUrlRewrite (batchId=186)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2569/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2569/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2569/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843156 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843161/HIVE-13278.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 10784 tests executed
*Failed tests:*
{noformat}
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=144)
	[vectorized_rcfile_columnar.q,vector_elt.q,explainuser_1.q,multi_insert.q,tez_dml.q,vector_bround.q,schema_evol_orc_acid_table.q,vector_when_case_null.q,orc_ppd_schema_evol_1b.q,vector_join30.q,vectorization_11.q,cte_3.q,update_tmp_table.q,vector_decimal_cast.q,groupby_grouping_id2.q,vector_decimal_round.q,tez_smb_empty.q,orc_merge6.q,vector_decimal_trailing.q,cte_5.q,tez_union.q,cbo_rp_subq_not_in.q,vector_decimal_2.q,columnStatsUpdateForStatsOptimizer_1.q,vector_outer_join3.q,schema_evol_text_vec_part_all_complex.q,tez_dynpart_hashjoin_2.q,auto_sortmerge_join_12.q,offset_limit.q,tez_union_multiinsert.q]
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[metadataonly1] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_4] (batchId=93)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2570/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2570/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2570/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843161 - PreCommit-HIVE-Build, Thanks [~lirui]. I think in Hive 2.x {{SparkPlanGenerator}} is the only likely place where the FileNotFound issue could be triggered, while in older versions of Hive (e.g., 1.1.0) it could be triggered in some other places.
I've attached a new patch, which I checked with some simple MR & Spark queries. Didn't see any FileNotFound message with the patch applied. Also, I don't think the test failures are related.
[~xuefuz], [~lirui], [~stakiar], could you take a look if you have time? Thanks., I like the new patch, which takes a simpler, easy-to-understand approach. On the other hand, this approach requires that planner set the flags correctly in order to avoid FNF problem, which shouldn't be an issue.

Though not essential, I noticed there are are a few cases where planers such as PartialScanTask.execute() set 0 as the number of reducers (no reducers), which might be better to set the flag as well. However, I think we can have a separate JIRA for thos.

+1 on my side. Please also provide your review feedback, [~lirui]. Thanks., Thanks [~xuefuz], I think you're right, for MR, we need to set the flag for any place where a {{submitJob}} is called. Otherwise the control flow will go to {{checkOutputSpecs -> getMapRedWork()}} and trigger FNF.
I think I missed two places: {{PartialScanTask}} and {{ColumnTruncateTask}}. Attaching patch v3 to address that., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843278/HIVE-13278.3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 10813 tests executed
*Failed tests:*
{noformat}
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[metadataonly1] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2578/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2578/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2578/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843278 - PreCommit-HIVE-Build, +1 on patch #3., Sorry about the delay. I have a concern about using flag: it seems difficult to make it exhaustive and maintain. What about the solution Xuefu mentioned:
bq. Following your idea, can we first check if the mapwork ends a RS and use this to determine if reduce.xml is expected?
Will this be cleaner and more reliable?, [~lirui], the concern is valid and shared, but on the other hand the current approach is simple and easy to understand. At least, the could be new cases where the problem may appear, but this doesn't make it worse and we don't expect too many such cases now or in the future.

Further thoughts?, Hi [~xuefuz], I just think it'll be even simpler to go the checking RS way - we can constrain the fix in just one method {{HiveOutputFormatImpl.checkOutputSpecs}}, rather than making changes to all these different tasks. Besides, with the flag it seems we're adding extra burden to ourselves to keep the logic consistent during plan generation.

On the other hand, if we decide to add the flag, I also have one suggestion. We can make {{has.map/reduce.work}} default to false. And we set them to true respectively in {{Utilities::setMapWork/setReduceWork}}. The logic behind this is if you haven't set a work with the JobConf, you shouldn't try to get one from it. Does this make sense?, [~lirui] sorry I didn't see your post about using the RS for this. I think this solution also looks good.
Note that for the flag approach is we can also set it in Task#initialize(), instead of setting it in different tasks.
This could make it cleaner.

For setting the default value to false, I think it may interrupt some Tez queries, which also calls the {{getMapRedWork}}
method. This patch doesn't handle Tez., Hi [~csun], I think Tez also calls setMapWork/setReduceWork to associate the work with the JobConf. Could you have a try? If it works, the fix will be transparent to different tasks. Thanks!, Yeah, let's try [~lirui]'s idea to cover more cases if possible. It's important to note that the current patch is at least improving Hive (might incomplete) and does no harm., Thanks guys. Attaching patch v4 following [~lirui]'s idea. Please take a look., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843458/HIVE-13278.4.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 10788 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=234)
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=144)
	[vectorized_rcfile_columnar.q,vector_elt.q,explainuser_1.q,multi_insert.q,tez_dml.q,vector_bround.q,schema_evol_orc_acid_table.q,vector_when_case_null.q,orc_ppd_schema_evol_1b.q,vector_join30.q,vectorization_11.q,cte_3.q,update_tmp_table.q,vector_decimal_cast.q,groupby_grouping_id2.q,vector_decimal_round.q,tez_smb_empty.q,orc_merge6.q,vector_decimal_trailing.q,cte_5.q,tez_union.q,cbo_rp_subq_not_in.q,vector_decimal_2.q,columnStatsUpdateForStatsOptimizer_1.q,vector_outer_join3.q,schema_evol_text_vec_part_all_complex.q,tez_dynpart_hashjoin_2.q,auto_sortmerge_join_12.q,offset_limit.q,tez_union_multiinsert.q]
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_sort_array] (batchId=59)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[metadataonly1] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2599/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2599/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2599/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843458 - PreCommit-HIVE-Build, Hi [~csun], sorry maybe I was being misleading. What I have in mind is something like this:
{code}
  // In Utilities::setMapWork
  public static Path setMapWork(Configuration conf, MapWork w, Path hiveScratchDir, boolean useCache) {
    conf.setBoolean(HAS_MAP_WORK, true);
    return setBaseWork(conf, w, hiveScratchDir, MAP_PLAN_NAME, useCache);
  }

  // In Utilities::getMapWork
  public static MapWork getMapWork(Configuration conf) {
    if (!conf.getBoolean(HAS_MAP_WORK, false)) {
      return null;
    }
    ....
{code}
Similar for set/get ReduceWork. So if we haven't called set work, we'll just get null when getting the work. Do you think it makes sense?, [~lirui] Yes I think this solution should work and is much more clean! Thanks for the suggestion. Attaching patch v5 to test., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843621/HIVE-13278.5.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 20 failed/errored test(s), 10820 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=234)
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_sort_array] (batchId=59)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[metadataonly1] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_1] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testCombinationInputFormat (batchId=254)
org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testCombinationInputFormatWithAcid (batchId=254)
org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testVectorReaderFooterSerialize (batchId=254)
org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testVectorReaderNoFooterSerialize (batchId=254)
org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testVectorization (batchId=254)
org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testVectorizationWithAcid (batchId=254)
org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.testVectorizationWithBuckets (batchId=254)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2613/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2613/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2613/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 20 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843621 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843770/HIVE-13278.6.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 10821 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=234)
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_sort_array] (batchId=59)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=92)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2628/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2628/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2628/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843770 - PreCommit-HIVE-Build, Thanks [~csun] for the update. The latest patch looks good to me. +1
I also tried some test failures locally and they can't be reproduced.
An improvement is maybe we can make TestInputOutputFormat also use Utilities to set the MapWork? We can leave it as a follow on., Committed to master. Thanks [~lirui] for the review!]