[Check if password is null or blank. If so, throw exception., 

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12704024/HIVE-9934.1.patch

{color:green}SUCCESS:{color} +1 7762 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3012/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3012/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3012/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12704024 - PreCommit-HIVE-TRUNK-Build, (cc [~prasadm] [~xuefuz]). I was able to reproduce the issue after disabling JDBC authentication and use the Hadoop provided {{SaslPlainServerFactory}}. I need to do the latter because Hive provided Sasl server implementation checks the case when password is empty, therefore the issue could be prevented. However, if the Hadoop version class gets loaded first (which doesn't check whether password is null or empty), then the issue could still happen.

In this patch I also included a simple uni test. Desirably we should write an end-to-end test, however that involves non-trivial work. I'll put that in a follow-up JIRA., +1, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12704870/HIVE-9934.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 7769 tests executed
*Failed tests:*
{noformat}
TestCustomAuthentication - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3047/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3047/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3047/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12704870 - PreCommit-HIVE-TRUNK-Build, There's a redundant field {{hiveServer2}} in the previous patch. This patch removes it - it shouldn't affect test results., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12704981/HIVE-9934.3.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 7770 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3053/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3053/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3053/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12704981 - PreCommit-HIVE-TRUNK-Build, Found this in log: 

{noformat}
2015-03-17 04:33:32,728 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 2015-03-17 04:33:32,725 INFO  [pool-1-thread-1] client.RemoteDriver (RemoteDriver.java:call(371)) - Failed to run job 681ccfbe-bf9f-491c-a2e7-ad513f62d1dc
2015-03-17 04:33:32,728 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - java.util.concurrent.ExecutionException: Exception thrown by job
2015-03-17 04:33:32,728 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:311)
2015-03-17 04:33:32,728 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:316)
2015-03-17 04:33:32,728 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:364)
2015-03-17 04:33:32,728 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:317)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at java.lang.Thread.run(Thread.java:744)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, ip-10-182-56-7.ec2.internal): java.io.FileNotFoundException: http://10.182.56.7:34690/jars/hive-exec-1.2.0-SNAPSHOT.jar
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:452)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:383)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
2015-03-17 04:33:32,729 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at java.lang.Thread.run(Thread.java:744)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - Driver stacktrace:
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
2015-03-17 04:33:32,730 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
2015-03-17 04:33:32,731 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
2015-03-17 04:33:32,731 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at scala.Option.foreach(Option.scala:236)
2015-03-17 04:33:32,731 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
2015-03-17 04:33:32,731 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
2015-03-17 04:33:32,731 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
2015-03-17 04:33:32,731 INFO  [stdout-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(537)) - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:137
{noformat}

I don't think this is relevant to my patch., Attached the same patch for another test run., 

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12705088/HIVE-9934.3.patch

{color:green}SUCCESS:{color} +1 7770 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3056/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3056/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3056/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12705088 - PreCommit-HIVE-TRUNK-Build, Looks fine to me.
BTW, testLdapEmptyPassword() is missing the Test annotation., [~prasadm], I think lacking @Test seems fine in this case, as the class is extended from TestCase. I also saw the added test case was run in previous test result. Thus, patch #3 is good as far as I can see. Let me know if you see differently.
 , That's fine. The test did get run in the pre-commit run for patch #3. sorry about the noise.

+1
, Committed to trunk. Thanks, Chao., bq. I was able to reproduce the issue after disabling JDBC authentication and use the Hadoop provided SaslPlainServerFactory. 
[~csun] Can you please elaborate on what you had do to reproduce this ? What do you mean by disabling JDBC authentication ? (I assume we need to set hive.server2.authentication=LDAP for this scenario). Also, since the SaslPlainServerFactory in hive in a different package, does changing the classpath alone result in the Hadoop provided SaslPlainServerFactory getting used ?

cc [~vgumashta]
, What documentation does this need?, Hive's SaslPlainServer actually throws an exception for empty or null password. When Hadoop implemented it's own plain Sasl server, we are potentially exposed to this LDAP vulnerability. The sasl service registration happens via static code block and hence we can't guarantee which Sasl server will be used.
Anycase, since this is LDAP specific behavior, it's better to guard it in LDAP provider rather than depending on the underlying Sasl implementation.
, Apache has special guidelines regarding security vulnerabilities. Here is the link:

http://www.apache.org/security/committers

We are all new to this, so what we have done so far may not comply to this. However, we should try to do so from now on.

For doc, please also refer to the document.

AS to the vulnerability, discussion is still ongoing in the community. Thus, we will act based on the conclusions., Hi [~thejas], for JDBC, I need to modify the code. In HiveConnection, when password is empty, it will change it to "anonymous". Since I'm using JDBC + Beeline to reproduce the issue, I need to change it so that the password will still remain empty.

For SaslPlainServerFactory, sorry my previous comment wasn't precise. Here, even though from different packages. they are added via different Providers with the same key ("SaslServerFactory.PLAIN"). Later, when searching for a particular key, it always choose the first Provider that matches. Since the Providers are added in static blocks, the order may not be deterministic. Hence, if the Hadoop one is picked, security issue could happen., Thanks [~csun] [~prasadm] !
However, looking at the hadoop code, it does not seem to get added via static code blocks (unlike the hive one). It gets initialized through calls to SaslRpcServer.init(). So it looks like the hive one would get added first, and the hadoop one would get added next (when hive functions such as HadoopThriftAuthBridge23.getHadoopSaslProperties are called. This is then getting stored in a HashTable, which means that the second one is what would get used. It seems like the hadoop one would always get used. (I haven't verified this by testing).
, Yes, hadoop doesn't use static blocks. But, since Hive class still have it, the order could still vary depending on when the class is loaded, right?
Looks like SaslRpcServer.init() is called in several places. I debugged it a little, there's one call site in org.apache.hadoop.ipc.RPC which is surrounded by a conditional stat. It might be called before the static block is called (although in different thread), if the condition is true. The execution didn't reach HadoopThriftAuthBridge23.getHadoopSaslProperties in my test.

Looks like new Provider is always added to the end of an ArrayList, and therefore the one added earlier will be used., This issue has been fixed and released as part of the 1.2.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.]