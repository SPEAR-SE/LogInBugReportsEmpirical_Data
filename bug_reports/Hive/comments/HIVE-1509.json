[With dynamic partitions, the number of intermediate files can grow very fast.

For example, consider a query with 10,000 mappers and 100 files per mapper - it can create up to 1 million files before merging them at the end.
The cluster may be down by the time the query finishes.

It is a good idea to track the number of files through a counter, and kill the query if the number exceeds a given threshold
, couple of comments:

- use ProgressCounter.CREATED_FILES directly instead of using valueOf("CREATED_FILES")
- can we move the check for total number of created files to inside checkFatalErrors? we are duplicating some code (for example we just fixed a problem where getCounters() can return null and ignoring that inside checkFatal).
, Good points Joy. Uploading a new patch with these changes. , can u try bucketmapjoin2.q in clientpositive. it's failing for me, found the bug. I'll a new patch after running the tests. , Uploading HIVE-1509.3.patch which fixed a bug. It passed hadoop 0.17 tests. I'm running 0.20 tests. , let me know once the tests pass 0.20 and i can commit.

one more question:
+    MAXCREATEDFILES("hive.exec.max.created.files", 100000),

i think u may have to append a 'L' to 100000 since u are trying to later on do a:

+      long upperLimit =  HiveConf.getLongVar(job, HiveConf.ConfVars.MAXCREATEDFILES);

(or switch to using getIntVar). i am a little surprised how this is working because the 100000 would be interpreted as Integer, go to the integer constructor which should leave the long default to -1. (or i guess i have forgotten how this works)
, Attaching HIVE-1509.4.patch to change 100000 to 100000L. I think it passed because we have the parameter set up in hive-default.xml as well.

All unit tests passed on 0.20 except index_compact_2.q. It is strange that this particular test passed when I run it individually. It also passed when I ran the whole tests again. , ok - i will run tests on 20 and commit if all clear., the test result for dyn_part3.q is not matching the one provided in the patch. it seems that testnegativeclidriver is not executing anything but the first query in the .q file:


    [junit] diff -a -I file: -I pfile: -I /tmp/ -I invalidscheme: -I lastUpdateTime -I lastAccessTime -I \
owner -I transient_lastDdlTime -I java.lang.RuntimeException -I at org -I at sun -I at java -I at junit -\
I Caused by: -I [.][.][.] [0-9]* more /data/users/jssarma/hive_trunk/build/ql/test/logs/clientnegative/dy\
n_part3.q.out
    [junit] 9a10,27
    [junit] > PREHOOK: query: create table nzhang_part( key string) partitioned by (value string)
    [junit] > PREHOOK: type: CREATETABLE
    [junit] > POSTHOOK: query: create table nzhang_part( key string) partitioned by (value string)
    [junit] > POSTHOOK: type: CREATETABLE
    [junit] > POSTHOOK: Output: default@nzhang_part
    [junit] > PREHOOK: query: insert overwrite table nzhang_part partition(value) select key, value from \
src
    [junit] > PREHOOK: type: QUERY
    [junit] > PREHOOK: Input: default@src
    [junit] > FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
    [junit] > PREHOOK: query: create table nzhang_part( key string) partitioned by (value string)
    [junit] > PREHOOK: type: CREATETABLE
    [junit] > POSTHOOK: query: create table nzhang_part( key string) partitioned by (value string)
    [junit] > POSTHOOK: type: CREATETABLE
    [junit] > POSTHOOK: Output: default@nzhang_part
    [junit] > PREHOOK: query: insert overwrite table nzhang_part partition(value) select key, value from \
src
    [junit] > PREHOOK: type: QUERY
    [junit] > PREHOOK: Input: default@src
    [junit] > FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
, It's strange. I didn't get this diff on both 17 and 20. I'll run TestNegativeCliDriver again., The TestNegativeCliDriver ran successfully. In terms of the number of queries in the .q files in neg, I think as long as there is no queries after the first exception, it should be fine. At least this is what dyn_part[12].q was doing (only the last query thrown expected exception)., strange - let me retry. can u check the patch one last time? (perhaps it's not up to date with contents of ur tree?), Yes, I check the svn diff in my working directory is the same as HIVE-1509.4.patch. , committed - thanks Ning.

it seems that the test problems were likely because there was a problem applying the patch.]