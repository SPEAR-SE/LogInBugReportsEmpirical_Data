[CC [~wzheng] [~sershe] , Hmm, we observed this with LLAP (which has advantage of table sharing in non-hybrid case), but not with containers to the best of my knowledge. It's possible it's slower even w/o accounting for sharing.
It's also possible that mapjoin table size was configured to be too small so there was too much needless spilling...
[~gopalv] any input?
We could disable it by default if it's hard to tune. Then for the cases where it can shine people can turn it on and tune the sizing., I think we can disable it by default  
The main motivation was actually avoiding OOMs as far as I understand. I don't thin anyone is working on perf improvements right now.
cc [~gopalv], Thanks [~sershe] for your input. I think the degradation in my test should cause by the unnecessary spilling. I take a look at the longest tasks for both enable/disable which processing the same number of records and have a roughly estimation for each phase with log.
!screenshot-1.png!

Extra reprocessing takes longer time and in disable case those data actually is not spilled into the disk. And from the following logs we can see that the spilled row numbers are actually very small (e.g. partition 0: 0 row, partition 3: 1 row) while the estimated memory is relative high (e.g. partition 0: 65636, partition 3: 589924). This is because the estimated memory is obtained from WBS from BytesBytesMultiHashMap. But in disabled case, there is limited overhead of creating BytesBytesMultiHashMap which maintains only one such hash map. I think we need to figure out a way to have better estimation of memory to avoid unnecessary spill caused by memory. Any thoughts or suggestions about this point?

{noformat}
2017-10-13 09:14:43,666 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Spilling hash partition 0 (Rows: 0, Mem size: 65636): /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-0-1870407313239959464.tmp
2017-10-13 09:14:43,666 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Memory usage before spilling: 1050880
2017-10-13 09:14:43,666 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Memory usage after spilling: 985244
2017-10-13 09:14:43,667 [INFO] [pool-29-thread-1] |common.FileUtils|: Local directories not specified; created a tmp file: /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-3-8788724383233259683.tmp
2017-10-13 09:14:43,667 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Trying to spill hash partition 3 ...
2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Spilling hash partition 3 (Rows: 1, Mem size: 589924): /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-3-8788724383233259683.tmp
2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Memory usage before spilling: 1509532
2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Memory usage after spilling: 919608
2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |common.FileUtils|: Local directories not specified; created a tmp file: /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-15-1832304146451074287.tmp
2017-10-13 09:14:43,669 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Trying to spill hash partition 15 ...
2017-10-13 09:14:43,676 [INFO] [pool-29-thread-1] |persistence.HybridHashTableContainer|: Spilling hash partition 15 (Rows: 2, Mem size: 589924): /ssd/ssd-pcie/hadoop/data/local-dirs/usercache/root/appcache/application_1506652027239_1242/container_1506652027239_1242_01_000042/tmp/partition-15-1832304146451074287.tmp
{noformat}



, > It's possible it's slower even w/o accounting for sharing. 
Could you please expand a little bit and explain in more detail?

> The main motivation was actually avoiding OOMs as far as I understand.
Agree, this feature can make map join more general when hash table can not fit into the memory.

> I don't thin anyone is working on perf improvements right now.
Logically it should have some performance benefits over the non hybrid grace hash join since it isn't required to scan the big table again during the reprocessing phase when hash table can not fit into the memory.
, [~Ferd] Sorry for the late reply. Yes the spilling part is the bottleneck and there's no easy way to get around it. In your case for the n-way joins, the optimizer stats estimation may not be accurate which makes the situation worse. Anyway, the ultimate way to solve this problem is to have a reliable memory manager which can provide memory usage/quota at any moment. Right now we're following a conservative approach, which is to use a soft (possibly inaccurate) memory limit. That way we can avoid unnecessary spilling if there is enough memory for loading the hashtable., Thanks [~wei.zheng] for your reply.
>  In your case for the n-way joins, the optimizer stats estimation may not be accurate which makes the situation worse.

AFAIK, the row size estimated should be the same with non-hybrid grace hash join case. It's strange why the spill happens in hybrid grace hash join case.
Another observation is o rows of data for one partition is occupying about 65636 bytes memory.

> Anyway, the ultimate way to solve this problem is to have a reliable memory manager which can provide memory usage/quota at any moment. Right now we're following a conservative approach, which is to use a soft (possibly inaccurate) memory limit. That way we can avoid unnecessary spilling if there is enough memory for loading the hashtable.

Interest. Any ticket addressing this part of work?]