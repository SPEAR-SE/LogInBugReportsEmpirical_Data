[[~JoyoungZhang@gmail.com], Spark application is associated with a user session. In case of Hive CLI, the user session exists until you quit it. Thus, this is designed behavior., Thank you very much.
This sitiation is different from hive on mapreduce.
I suggest that we should add some description about this in wiki., Hi,Xuefu Zhang, I find that the running application tends to block the running of other application if it is not killed.
can I kill the running applicaition after the query finished!, Yes, you can. However, a better way is to set a small session timeout so that these applications will exist themselves after the configured time., thanks for your helpful reply! how can I set the session timeout ? do you mean setting the timeout of hive cli?                         , is there a way that I can kill the applicaition without close the hive cli session?, is there a way that I can kill the applicaition without close the hive cli session?]