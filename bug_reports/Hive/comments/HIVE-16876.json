[At the moment RpcServer is a singleton. If we allow users to configure it at runtime, we have to give each session a dedicated RpcServer. I'm not sure if that's a good idea - seems strange that each server serves only one client.
[~xuefuz], any ideas? Thanks., [~lirui], I'm wondering what kind of configuration changes are in question. Overall, I think RPCServer configuration should be global to HS2, which are effective only upon HS2 restart. In addition, it doesn't seem ideal to have one RPCServer instance for each remote Spark driver. Thoughts!, [~xuefuz], examples are the server side connect timeout, and number of threads to be used for the event loop group. I agree not to have an RpcServer for each driver. Shall we document these configs cannot be changed at runtime?, Yeah. Documentation is good to have., I've made all Rpc configs immutable at runtime., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12872937/HIVE-16876.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10831 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query16] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query94] (batchId=232)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=216)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=216)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5641/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5641/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5641/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12872937 - PreCommit-HIVE-Build, +1, Pushed to master. Thanks Xuefu!, Doc note:  This adds eight hive.spark.client.* configs to the defaults for *hive.conf.restricted.list* so the wiki needs to be updated for release 3.0.0.

* [Configuration Properties -- hive.conf.restricted.list | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.conf.restricted.list]

Added a TODOC3.0 label., Thanks [~leftylev]. I've updated the wiki., Thanks for the docs, [~lirui].  I removed the TODOC3.0 label., Hive 3.0.0 has been released so closing this jira.]