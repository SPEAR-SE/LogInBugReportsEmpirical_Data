[For example, we have an ORC table named 'item'.

(a) Before running 'analyze table item compute statistics;',
the 'rawDataSize' was '884720592'.

The result of 'describe extended item':
Detailed Table Information      Table(tableName:item, dbName:bigbenchorc, owner:root, createTime:1421984899, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:i_item_sk, type:bigint, comment:null), FieldSchema(name:i_item_id, type:string, comment:null), FieldSchema(name:i_rec_start_date, type:string, comment:null), FieldSchema(name:i_rec_end_date, type:string, comment:null), FieldSchema(name:i_item_desc, type:string, comment:null), FieldSchema(name:i_current_price, type:double, comment:null), FieldSchema(name:i_wholesale_cost, type:double, comment:null), FieldSchema(name:i_brand_id, type:int, comment:null), FieldSchema(name:i_brand, type:string, comment:null), FieldSchema(name:i_class_id, type:int, comment:null), FieldSchema(name:i_class, type:string, comment:null), FieldSchema(name:i_category_id, type:int, comment:null), FieldSchema(name:i_category, type:string, comment:null), FieldSchema(name:i_manufact_id, type:int, comment:null), FieldSchema(name:i_manufact, type:string, comment:null), FieldSchema(name:i_size, type:string, comment:null), FieldSchema(name:i_formulation, type:string, comment:null), FieldSchema(name:i_color, type:string, comment:null), FieldSchema(name:i_units, type:string, comment:null), FieldSchema(name:i_container, type:string, comment:null), FieldSchema(name:i_manager_id, type:int, comment:null), FieldSchema(name:i_product_name, type:string, comment:null)], location:hdfs://bhx1:8020/user/hive/warehouse/bigbenchorc.db/item, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=4, transient_lastDdlTime=1421984899, COLUMN_STATS_ACCURATE=true, totalSize=83267548, numRows=563518, rawDataSize=884720592}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
Time taken: 0.527 seconds, Fetched: 24 row(s)

(b)After running 'analyze table TABLE_NAME compute statistics;'
the 'rawDataSize' will be changed to '0',

The result of 'describe extended item':
Detailed Table Information      Table(tableName:item, dbName:bigbenchorc, owner:root, createTime:1421984899, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:i_item_sk, type:bigint, comment:null), FieldSchema(name:i_item_id, type:string, comment:null), FieldSchema(name:i_rec_start_date, type:string, comment:null), FieldSchema(name:i_rec_end_date, type:string, comment:null), FieldSchema(name:i_item_desc, type:string, comment:null), FieldSchema(name:i_current_price, type:double, comment:null), FieldSchema(name:i_wholesale_cost, type:double, comment:null), FieldSchema(name:i_brand_id, type:int, comment:null), FieldSchema(name:i_brand, type:string, comment:null), FieldSchema(name:i_class_id, type:int, comment:null), FieldSchema(name:i_class, type:string, comment:null), FieldSchema(name:i_category_id, type:int, comment:null), FieldSchema(name:i_category, type:string, comment:null), FieldSchema(name:i_manufact_id, type:int, comment:null), FieldSchema(name:i_manufact, type:string, comment:null), FieldSchema(name:i_size, type:string, comment:null), FieldSchema(name:i_formulation, type:string, comment:null), FieldSchema(name:i_color, type:string, comment:null), FieldSchema(name:i_units, type:string, comment:null), FieldSchema(name:i_container, type:string, comment:null), FieldSchema(name:i_manager_id, type:int, comment:null), FieldSchema(name:i_product_name, type:string, comment:null)], location:hdfs://bhx1:8020/user/hive/warehouse/bigbenchorc.db/item, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{numFiles=4, transient_lastDdlTime=1421984899, COLUMN_STATS_ACCURATE=true, totalSize=83267548, numRows=563518, rawDataSize=884720592}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
Time taken: 0.527 seconds, Fetched: 24 row(s), Try using
{code}
analyze table TABLE_NAME compute statistics noscan
{code}
OR
{code}
analyze table TABLE_NAME compute statistics partialscan
{code}

This should get the raw data size properly. The reason why 'analyze table TABLE_NAME compute statistics' does not work in case of ORC is, ORC does not implement the SerDeStats which some formats implement. Implementing SerDeStats the traditional way requires ORC to pass serialized data size for each row. This is inefficient considering scanning of each row, getting stats and aggregating it. Since ORC already collects column stats we can utilize that information without scanning each row to compute the raw data size. Thats the reason we need noscan/partialscan at the end (both does the same)., Thanks Prasanth for the comment! We can resolve it in our local test environment now., Resolving the issue as it behaves as expected., [~prasanth_j] what you explained makes sense, but I think it will be fair on part of user to run analyze command without noscan/partialscan and expect it to work. Resetting rawDataSize to 0 as Hive does today, I will consider that as a bug. I think we should see check in analyze statement that its for ORC table and in such cases automatically do noscan analyze for them (which also will be more performant). 
Thoughts?, [~ashutoshc] Yeah. That make sense. I will put up patch to make that automatic switch in case of ORC., [~ashutoshc] Can you review the patch? The changes are small without the tests., Review Board is flaky to upload the patch., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12696664/HIVE-9560.1.patch

{color:red}ERROR:{color} -1 due to 52 failed/errored test(s), 7475 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_part
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_annotate_stats_table
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_skewtable
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_char_simple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_coalesce
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_decimal_cast
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_elt
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_if_expr
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_left_outer_join
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vector_varchar_simple
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_16
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_9
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_div0
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_limit
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_pushdown
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_case
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_casts
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_distinct_gby
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_math_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_nested_mapjoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_shufflejoin
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_vectorized_string_funcs
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_windowing_streaming
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_limit_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_char_simple
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_left_outer_join
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_varchar_simple
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_13
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_14
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_15
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_16
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_7
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_8
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_9
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_div0
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_limit
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_pushdown
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_distinct_gby
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_nested_mapjoin
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vectorized_shufflejoin
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2673/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2673/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2673/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 52 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12696664 - PreCommit-HIVE-TRUNK-Build, +1, 

{color:green}Overall{color}: +1 all checks pass

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12696991/HIVE-9560.2.patch

{color:green}SUCCESS:{color} +1 7482 tests passed

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2692/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2692/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2692/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12696991 - PreCommit-HIVE-TRUNK-Build, Committed to trunk., This issue has been fixed and released as part of the 1.2.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.]