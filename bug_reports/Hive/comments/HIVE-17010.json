[[~Ferd]: can you help review HIVE-17010.1.patch?
We can use double to replace long type to solve the overflow problem.
{code}
   //long max=9223372036854775807
      long a1= 9223372036854775807L;
      long a2=1022672;

      long res = a1+a2;
      System.out.println(res);  //-9223372036853753137


      //double max=1.7976931348623157E308
      double d1= 9223372036854775807L;
      double d2=1022672;

      double dres = d1+d2;
      System.out.println(dres);//9.223372036855798E18

{code}
, bq. We use Long type and it happens overflow when the data is too big.
I don't understand. How it could overflow with long type? how large is the dataset you used for testing?, [~csun]: found the problem on 3TB data. Actually the biggest table of tpc-ds "store_sales" does not exceed the max value of Long type(2^63-1). But [TPC-DS/query17|https://github.com/hortonworks/hive-testbench/blob/hive14/sample-queries-tpcds/query17.sql] is a query with many join.   We 
 use [numberOfBytes|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SetSparkReducerParallelism.java#L129] to collect the numberOfBytes of sibling of specified RS.  Here the sibling of specified RS maybe the result of join of big tables. The result execeed the max value of Long type., [~csun], we use a long to compute the sum of multiple longs. I guess that's in general a dangerous operation., [~lirui],[~csun], [~ferd]: in HIVE-17010.patch, use double to replace long type to solve the problem
similar bug was found in HIVE-8689.
in HIVE-8689, it use [StatsUtils.safeAdd|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java#L1626] to solve the problem. So which solution is better? 1. use double to replace Long 2. use StatsUtils.safeAdd , please give me your suggestion, With long you need to have ~9000 PB of {{numberOfBytes}} for the overflow to happen. It's interesting that this can occur with 3TB of input data. I'm just wondering if there's any bug in the code that caused this., [~csun]: 
the explain of the query17 without HIVE-17010.patch is in [link|https://issues.apache.org/jira/secure/attachment/12875204/query17_explain.log].  Reduce3's datasize  is 9223372036854775807 
{code}
   Reducer 3 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col28 (type: bigint), _col27 (type: bigint)
                  1 cs_bill_customer_sk (type: bigint), cs_item_sk (type: bigint)
                outputColumnNames: _col1, _col2, _col6, _col8, _col9, _col22, _col27, _col28, _col34, _col35, _col45, _col51, _col63, _col66, _col82
                Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: PARTIAL
                Reduce Output Operator
                  key expressions: _col22 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col22 (type: bigint)
                  Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: PARTIAL
                  value expressions: _col1 (type: bigint), _col2 (type: bigint), _col6 (type: bigint), _col8 (type: bigint), _col9 (type: int), _col27 (type: bigint), _col28 (type: bigint), _col34 (type: bigint), _col35 (type: int), _col45 (type: bigint), _col51 (type: bigint), _col63 (type: bigint), _col66 (type: int), _col82 
{code}

Map9's datasize is 1022672 
{code}
        Map 9 
            Map Operator Tree:
                TableScan
                  alias: d1
                  filterExpr: (d_date_sk is not null and (d_quarter_name = '2000Q1')) (type: boolean)
                  Statistics: Num rows: 73049 Data size: 2045372 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (d_date_sk is not null and (d_quarter_name = '2000Q1')) (type: boolean)
                    Statistics: Num rows: 36524 Data size: 1022672 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: d_date_sk (type: bigint)
                      sort order: +
                      Map-reduce partition columns: d_date_sk (type: bigint)
                      Statistics: Num rows: 36524 Data size: 1022672 Basic stats: COMPLETE Column stats: NONE
{code}
There is a join of Map 9 and Reducer3
{code}
        Reducer 4 <- Map 9 (PARTITION-LEVEL SORT, 1), Reducer 3 (PARTITION-LEVEL SORT, 1)
{code}
9223372036854775807 + 1022672 
cause the problem, Ah I see. Sometimes the stats estimation could generate negative values, in which case Hive will use {{Long.MAX_VALUE}} for both # of rows and data size. One case I observed previously:
{code}
not ((P1 or P2) or P3)
{code}
When no column stats are available, Hive will simply divide the # of input rows by 2 for each predicate evaluation. Suppose the total input rows is 10, then {{P1}}, {{P2}} and {{P3}} will yield 5 respectively. Operator {{or}} adds value from both sides so the expression {{((P1 or P2) or P3)}} generates 30 rows. The operator {{not}}, on the other hand, will subtract the value of its associated expression from the total input rows. Therefore in the end you will get {{10 - 30 = -20}}.

For the solution you proposed, I'm inclined to use {{StatsUtils.safeAdd}}, but either way should be fine., [~csun]: update HIVE-17010.2.patch to use {{StatsUtils.safeAdd}} to solve the overflow. help review, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12875874/HIVE-17010.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10832 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[infer_bucket_sort_reducers_power_two] (batchId=167)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testParallelCompilation (batchId=226)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5906/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5906/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5906/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12875874 - PreCommit-HIVE-Build, Hi [~kellyzly], I think it should be {{numberOfBytes = StatsUtils.safeAdd(numberOfBytes, sibling.getStatistics().getDataSize());}} right?
And thanks [~csun] for the detailed analysis. Seems the estimation logic deserves some improvement., [~lirui]: thanks for your catch, will update the patch soon, [~lirui]: help review HIVE-17010.3.patch., +1, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12876022/HIVE-17010.3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10834 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5914/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5914/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5914/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12876022 - PreCommit-HIVE-Build, [~Ferd]: as [~lirui] finished review, please help commit HIVE-17010.3.patch, Committed to the upstream. Thanks Liyun for the patch and thanks Chao and Rui for the reviews., Hive 3.0.0 has been released so closing this jira.]