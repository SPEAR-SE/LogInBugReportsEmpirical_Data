[The failure to get mem/core info is just a warning. So it's not related to executor killing tasks. You should check the AM log (if in yarn-cluster mode) to see why the driver commands a shutdown., It's running in yarn-client mode, actually I did not see why the driver commands a shutdown from AM log, just see the driver commands a shutdown. So could the Warning be the possible cause?, No I don't think the warning can be the cause. For yarn-client mode, you should look for the error in Hive's log., Hi [~lirui],
I also encountered this issue. I'm using yarn-client mode. My workload failed in 20 mins. 

[Settings]
set hive.spark.job.monitor.timeout=3600s;
set spark.network.timeout=3600s;

[Hive's log]
2017-03-09 17:22:29,771 WARN  [RPC-Handler-3]: client.SparkClientImpl (SparkClientImpl.java:rpcClosed(130)) - Client RPC channel closed unexpectedly.

[App master log]
17/03/09 17:22:30 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
17/03/09 17:22:30 INFO yarn.YarnAllocator: Canceling requests for 51 executor containers
17/03/09 17:22:30 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. 10.54.5.129:48757
17/03/09 17:22:30 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. master.titan.cluster.gao:48757
17/03/09 17:22:30 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
17/03/09 17:22:30 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
17/03/09 17:22:30 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
17/03/09 17:22:30 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1488974634357_0022
17/03/09 17:22:30 INFO util.ShutdownHookManager: Shutdown hook calle
, Hi [~yiyao], for yarn-client mode, you should check the hive log, which contains logs from both hive and RemoteDriver. Or you can upload the log here., Hi [~lirui],
Thanks for your quick response. Below is the hive log.

[LOG]
2017-03-09 17:23:29,761 WARN  [main]: impl.RemoteSparkJobStatus (RemoteSparkJobStatus.java:getSparkStageInfo(162)) - Error getting stage info
java.util.concurrent.TimeoutException
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:160)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)
        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:109)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1195)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:416)
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:432)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:726)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
2017-03-09 17:23:29,764 ERROR [main]: status.SparkJobMonitor (RemoteSparkJobMonitor.java:startMonitor(132)) - Failed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'
java.lang.IllegalStateException: RPC channel is closed.
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)
        at org.apache.hive.spark.client.SparkClientImpl$ClientProtocol.run(SparkClientImpl.java:550)
        at org.apache.hive.spark.client.SparkClientImpl.run(SparkClientImpl.java:145)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:158)
at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)
        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:109)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1195)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:416)
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:432)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:726)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
2017-03-09 17:23:29,766 ERROR [main]: status.SparkJobMonitor (SessionState.java:printError(997)) - Failed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'
java.lang.IllegalStateException: RPC channel is closed.
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)
        at org.apache.hive.spark.client.SparkClientImpl$ClientProtocol.run(SparkClientImpl.java:550)
        at org.apache.hive.spark.client.SparkClientImpl.run(SparkClientImpl.java:145)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:158)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)
        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:109)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1195)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:416)
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:432)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:726)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

2017-03-09 17:23:29,778 ERROR [main]: ql.Driver (SessionState.java:printError(997)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
, [~yiyao], you can see the log indicates the Rpc channel is closed unexpectedly. Therefore the error that makes the channel closed (and probably crashes the RemoteDriver) is the root cause. You should look for it in earlier part of your log., Hi [~lirui], I checked the earlier log, there's only 1 timeout warning.  I have specified 1 hour as Spark network timeout threshold. I did not find any network issues in my OS log. Not sure what causes the issue.

2017-03-09 17:23:29,761 WARN [main]: impl.RemoteSparkJobStatus (RemoteSparkJobStatus.java:getSparkStageInfo(162)) - Error getting stage info
java.util.concurrent.TimeoutException
at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)
at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:160)
at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96), Can you see any output from the RemoteDriver in your hive.log? Such logs are printed via the redirector, so you should find them by looking for something starting with {{stdout-redir-}} or {{stderr-redir-}}., I find some logs starting with 'stderr-redir-'

hive.log.2017-03-06:11216:2017-03-06 02:50:14,742 WARN  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(624)) - Error in redirector thread.

, [~yiyao], is that all the redirector log you have? Usually in yarn-client mode, the redirector should output quite a lot stuff because all the RemoteDriver log goes to the redirector. You should find something like creating SparkContext, starting all the schedulers in spark, etc. Even for the specific log you posted, it should have come with an exception: https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java#L678.
Btw, if you're using an old version of HoS, I'd suggest you try the case in a newer one., [~lirui], I found the following warning always occurs before the application fail.

java.util.concurrent.TimeoutException
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:160)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96)
        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:109)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:80)
, In the client log, we also found the following warning you mentioned.

2017-03-16 17:51:40,166 WARN  [stderr-redir-1]: client.SparkClientImpl (SparkClientImpl.java:run(641)) - Error in redirector thread.
java.io.IOException: Stream closed
        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)
        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)
        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)
        at java.io.InputStreamReader.read(InputStreamReader.java:184)
        at java.io.BufferedReader.fill(BufferedReader.java:154)
        at java.io.BufferedReader.readLine(BufferedReader.java:317)
        at java.io.BufferedReader.readLine(BufferedReader.java:382)
        at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:632)
        at java.lang.Thread.run(Thread.java:745)
, Hi [~lirui], I am using Hive2.2 on spark2.0.2, the issue also exists. 

2017-03-21 03:02:30,454 Stage-5_0: 241/241 Finished     Stage-6_0: 161(+1)/162  Stage-7_0: 0/2018       Stage-8_0: 0/1009       Stage-9_0: 0/1009
Failed to monitor Job[4] with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(java.util.concurrent.TimeoutException)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask

in hive's log I also found the TimeoutException, WARN as well as ERROR,

2017-03-21T03:02:31,466  INFO [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Closing channel due to exception in pipeline (org.apache.hive.spark.client.SparkClientImpl$ClientProtocol.handle(io.netty.channel.ChannelHandlerContext, org.apache.hive.spark.client.rpc.Rpc$MessageHeader)).
2017-03-21T03:02:31,468  WARN [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Expected RPC header, got org.apache.spark.SparkJobInfoImpl instead.
2017-03-21T03:02:31,468  INFO [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Closing channel due to exception in pipeline (null).
2017-03-21T03:02:31,469  WARN [RPC-Handler-3] client.SparkClientImpl: Client RPC channel closed unexpectedly.
2017-03-21T03:03:31,457  WARN [Thread-349] impl.RemoteSparkJobStatus: Failed to get job info.
java.util.concurrent.TimeoutException
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~[netty-all-4.0.29.Final.jar:4.0.29.Final]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:171) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getStageIds(RemoteSparkJobStatus.java:87) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:94) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:84) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
2017-03-21T03:03:31,457 ERROR [Thread-349] status.SparkJobMonitor: Failed to monitor Job[4] with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(java.util.concurrent.TimeoutException)'
org.apache.hadoop.hive.ql.metadata.HiveException: java.util.concurrent.TimeoutException
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:174) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getStageIds(RemoteSparkJobStatus.java:87) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:94) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:84) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
Caused by: java.util.concurrent.TimeoutException
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~[netty-all-4.0.29.Final.jar:4.0.29.Final]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:171) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        ... 8 more
2017-03-21T03:03:31,458 ERROR [Thread-349] SessionState: Failed to monitor Job[4] with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(java.util.concurrent.TimeoutException)'
org.apache.hadoop.hive.ql.metadata.HiveException: java.util.concurrent.TimeoutException
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:174)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getStageIds(RemoteSparkJobStatus.java:87)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:94)
        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:84)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60)
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:116)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79)
Caused by: java.util.concurrent.TimeoutException
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:171)
        ... 8 more

2017-03-21T03:03:31,458 ERROR [Thread-349] spark.SparkTask: Failed to get Spark job information
java.lang.IllegalStateException: RPC channel is closed.
        at com.google.common.base.Preconditions.checkState(Preconditions.java:149) ~[guava-14.0.1.jar:?]
        at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hive.spark.client.SparkClientImpl$ClientProtocol.run(SparkClientImpl.java:580) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hive.spark.client.SparkClientImpl.run(SparkClientImpl.java:151) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkJobInfo(RemoteSparkJobStatus.java:168) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getStageIds(RemoteSparkJobStatus.java:87) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.getSparkJobInfo(SparkTask.java:346) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:118) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
2017-03-21T03:03:33,233 ERROR [89254c41-8375-40ef-a364-e7b546d8ba48 main] ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask

in driver's log, found an InterruptedException:

17/03/21 03:02:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1181 on executor id: 38 hostname: hsx-node6.
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 244.0 in stage 7.0 (TID 657) in 573 ms on hsx-node4 (343/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 432.0 in stage 7.0 (TID 845) in 285 ms on hsx-node9 (344/2018)
17/03/21 03:02:31 WARN client.RemoteDriver: Shutting down driver because RPC channel was closed.
17/03/21 03:02:31 INFO client.RemoteDriver: Shutting down remote driver.
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Starting task 769.0 in stage 7.0 (TID 1182, hsx-node6, partition 769, PROCESS_LOCAL, 25685 bytes)
17/03/21 03:02:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1182 on executor id: 9 hostname: hsx-node6.
17/03/21 03:02:31 INFO scheduler.DAGScheduler: Asked to cancel job 4
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 392.0 in stage 7.0 (TID 805) in 365 ms on hsx-node10 (345/2018)
17/03/21 03:02:31 INFO client.RemoteDriver: Failed to run job 63390316-af66-4f84-8c8b-806112ef2b94
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)
	at org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:125)
	at org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:114)
	at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:169)
	at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:169)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.ready(package.scala:169)
	at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:264)
	at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:277)
	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:362)
	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:323)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Starting task 770.0 in stage 7.0 (TID 1183, hsx-node6, partition 770, PROCESS_LOCAL, 25685 bytes)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 469.0 in stage 7.0 (TID 882) in 350 ms on hsx-node7 (346/2018)
17/03/21 03:02:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1183 on executor id: 38 hostname: hsx-node6.
17/03/21 03:02:31 INFO cluster.YarnClusterScheduler: Cancelling stage 7
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception
java.lang.IllegalStateException: RPC channel is closed.
	at com.google.common.base.Preconditions.checkState(Preconditions.java:149)
	at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)
	at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)
	at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)
	at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:45)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)
	at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:94)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1252)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)
17/03/21 03:02:31 INFO server.ServerConnector: Stopped ServerConnector@10328ecb{HTTP/1.1}{0.0.0.0:0}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@282715e5{/stages/stage/kill,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6610a390{/api,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@a3e40d8{/,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@12a7530{/static,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7d6673e4{/executors/threadDump/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7603582f{/executors/threadDump,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6b8c0277{/executors/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@293aa151{/executors,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2db4db50{/environment/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@743d35fc{/environment,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@70265a43{/storage/rdd/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7207552{/storage/rdd,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5a10780b{/storage/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6a2dd645{/storage,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@574406ab{/stages/pool/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4a32fbe0{/stages/pool,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@70b816b5{/stages/stage/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1fa44e08{/stages/stage,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@25170f92{/stages/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5c6ccb81{/stages,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6af0b071{/jobs/job/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4d6efce4{/jobs/job,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3f28c543{/jobs/json,null,UNAVAILABLE}
17/03/21 03:02:31 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7849dc62{/jobs,null,UNAVAILABLE}
17/03/21 03:02:31 INFO cluster.YarnClusterScheduler: Stage 7 was cancelled
17/03/21 03:02:31 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.4:41594
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 7.0 (TID 443) in 733 ms on hsx-node4 (347/2018)
17/03/21 03:02:31 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (union at SparkPlan.java:70) failed in 0.744 s
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 7.0 (TID 499) in 720 ms on hsx-node4 (348/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 333.0 in stage 7.0 (TID 746) in 664 ms on hsx-node5 (349/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 425.0 in stage 7.0 (TID 838) in 400 ms on hsx-node2 (350/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 324.0 in stage 7.0 (TID 737) in 667 ms on hsx-node4 (351/2018)
17/03/21 03:02:31 WARN spark.ExecutorAllocationManager: No stages are running, but numRunningTasks != 0
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@3241bef1,org.apache.spark.executor.TaskMetrics@21953ed8)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 440.0 in stage 7.0 (TID 853) in 387 ms on hsx-node9 (352/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 7.0 (TID 532) in 715 ms on hsx-node10 (353/2018)
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskStart(7,-1,org.apache.spark.scheduler.TaskInfo@3e8a68b)
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@4959044b,org.apache.spark.executor.TaskMetrics@76d08da7)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 408.0 in stage 7.0 (TID 821) in 420 ms on hsx-node2 (354/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 482.0 in stage 7.0 (TID 895) in 358 ms on hsx-node8 (355/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 463.0 in stage 7.0 (TID 876) in 374 ms on hsx-node8 (356/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 7.0 (TID 481) in 731 ms on hsx-node4 (357/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 447.0 in stage 7.0 (TID 860) in 382 ms on hsx-node7 (358/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 446.0 in stage 7.0 (TID 859) in 384 ms on hsx-node8 (359/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 450.0 in stage 7.0 (TID 863) in 382 ms on hsx-node7 (360/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 7.0 (TID 550) in 716 ms on hsx-node5 (361/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 421.0 in stage 7.0 (TID 834) in 408 ms on hsx-node9 (362/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 7.0 (TID 579) in 710 ms on hsx-node4 (363/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 430.0 in stage 7.0 (TID 843) in 405 ms on hsx-node6 (364/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 7.0 (TID 530) in 723 ms on hsx-node4 (365/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 473.0 in stage 7.0 (TID 886) in 372 ms on hsx-node7 (366/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 305.0 in stage 7.0 (TID 718) in 679 ms on hsx-node10 (367/2018)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 445.0 in stage 7.0 (TID 858) in 388 ms on hsx-node9 (368/2018)
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@395b13ea,org.apache.spark.executor.TaskMetrics@4b141cd3)
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@6dfb0a2c,org.apache.spark.executor.TaskMetrics@5cf4c211)
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@3dd14e26,org.apache.spark.executor.TaskMetrics@56482fcb)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 479.0 in stage 7.0 (TID 892) in 366 ms on hsx-node8 (369/2018)
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@63d7f544,org.apache.spark.executor.TaskMetrics@19c94765)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 426.0 in stage 7.0 (TID 839) in 409 ms on hsx-node2 (370/2018)
17/03/21 03:02:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(7,0,ShuffleMapTask,Success,org.apache.spark.scheduler.TaskInfo@2e5e9a04,org.apache.spark.executor.TaskMetrics@39ffde73)
17/03/21 03:02:31 INFO scheduler.TaskSetManager: Finished task 379.0 in stage 7.0 (TID 792) in 665 ms on hsx-node8 (371/2018), Got the similar issue with Spark 2.2/Hive 2.5.3 (on HortonWork).  The terrible thing is that app got hung without shutdown the context (app didn't exit by itself except by killing it with yarn command. The same app runs fine with 2.1.1 or earlier.

java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)
	at org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:222)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:621)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
	at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:644)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:603)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:612)
	at com.att.nrs.spk.MainUtil.getVoiceData(MainUtil.java:222)
	at com.att.nrs.spk.RedialFinderMain.main(RedialFinderMain.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:635)

*Stderr:*
ERROR ApplicationMaster 91: Exception from Reporter thread.
org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt_1502715048011_224038_000002 doesn't exist in ApplicationMasterService cache.
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:445)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)
	at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:79)
	at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy18.allocate(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:277)
	at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:265)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:457)]