[New tasks should not be scheduled on them - because scheduling is based off of the activeInstanceSet.
For existing tasks, these will eventually timeout after communication failures.

Acting on these actively to disable the node needs to be done is a simple code change. However it needs testing. Need to get to writing an in-proc controllable llap test setup.
{code}
getContext().nodesUpdate(List<NodeReport>)
{code}, [~sseth] I can actually see problems because of this. Easy repro - start LLAP (e.g. 7 nodes), start the session (with AM), flex LLAP down (e.g. to 4), run some query. There can be a large delay in scheduling and the whole job can slow down a lot because nodes are not removed from instanceToNodeMap...
{noformat}
2016-09-02 16:51:41,428 [INFO] [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerManager] |tezplugins.LlapTaskSchedulerService|: Setting up node: DynamicServiceInstance [alive=true, host=cn109... with resources=<memory:83968, vCores:16>, shufflePort=15551, servicesAddress=..., mgmtPort=15004] with available capacity=16, pendingQueueSize=null, memory=83968
...
(of course nothing is actually removed)
2016-09-02 16:52:01,490 [INFO] [StateChangeNotificationHandler] |tezplugins.LlapTaskSchedulerService$NodeStateChangeListener|: Removed node with identity: f9b37b46-f629-4460-862f-f34183ba0a24
2016-09-02 16:52:01,567 [INFO] [StateChangeNotificationHandler] |tezplugins.LlapTaskSchedulerService$NodeStateChangeListener|: Removed node with identity: 12399334-c743-4a9b-8224-8c0cbc21dea7
2016-09-02 16:52:01,776 [INFO] [StateChangeNotificationHandler] |tezplugins.LlapTaskSchedulerService$NodeStateChangeListener|: Removed node with identity: c7b50156-b4f9-4353-89a4-3d1a1ccea604
...
2016-09-02 16:53:39,511 [INFO] [LlapScheduler] |tezplugins.LlapTaskSchedulerService|: Assigned task TaskInfo{task=attempt_1466700718395_1343_2_07_000000_1, priority=140, startTime=0, containerId=null, assignedInstance=null, uniqueId=24, localityDelayTimeout=0} to container container_222212222_1343_01_000025 on node=DynamicServiceInstance [alive=true, host=cn109... with resources=<memory:83968, vCores:16>, shufflePort=15551, servicesAddress=..., mgmtPort=15004]
{noformat}

Here, two attempts of the last reducer of the job failed with network errors, causing the query runtime to triple., [~prasanth_j] can you take a look? The problem is that the scheduling with requested hosts uses activeSet, but the random scheduling (or fallback, when not having location information) doesn't., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12826930/HIVE-14608.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10443 tests executed
*Failed tests:*
{noformat}
TestBeeLineWithArgs - did not produce a TEST-*.xml file
TestHiveCli - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[acid_bucket_pruning]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hive.service.cli.session.TestSessionManagerMetrics.testThreadPoolMetrics
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1093/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1093/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-1093/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12826930 - PreCommit-HIVE-MASTER-Build, [~prasanth_j] [~gopalv] ping? This can introduce large slowdowns during node failures, esp. for reducers, [~sershe]: I think the original loop was written with the assumption that .canAcceptTask() can flip to true at some point during the loop.

That is probably a bad assumption now that we are running things in milliseconds - LGTM +1., It mostly applies to reducers actually. The main problem is as indicated in the description - for whatever reason we don't remove nodes from the node list when they die. The per-node assignment explicitly checks active set to get around that(?) but the other path doesn't... so reducers with no location preference will be sent to dead nodes, potentially, Committed to branches. Thanks for the review!, The expectation was that canAcceptTask will return a false if the node is not available (as a result of serviceInstance.isAlive()). That is apparently not happening when a node goes away. The current patch works; however I think it's better to ensure the isAlive method on the ServiceInstance works as it should (public api and all that).]