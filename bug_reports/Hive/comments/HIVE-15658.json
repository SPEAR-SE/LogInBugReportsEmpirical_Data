[Another exception type, which can cause this error to happen is, when during creation of directories in HDFS the InterruptedException occurs. See the stack trace (once again, line numbers are off by, cause of https://github.com/cloudera/flume-ng and https://github.com/cloudera/hive):

{code}
2017-01-19 08:36:19,917 WARN org.apache.hadoop.ipc.Client: interrupted waiting to send rpc request to server
java.lang.InterruptedException
        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)
        at java.util.concurrent.FutureTask.get(FutureTask.java:187)
        at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1055)
        at org.apache.hadoop.ipc.Client.call(Client.java:1450)
        at org.apache.hadoop.ipc.Client.call(Client.java:1408)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy24.getFileInfo(Unknown Source) 
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:762)
        at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source) 
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy25.getFileInfo(Unknown Source) 
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1215)
        at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1211)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1211)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1412)
        at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:616)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:574)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:518)
        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.createPartitionIfNotExists(HiveEndPoint.java:358)
        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:276)
        at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:243)
        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(HiveEndPoint.java:180)
        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:157)
        at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:110)
        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:376)
        at org.apache.flume.sink.hive.HiveWriter$8.call(HiveWriter.java:373)
        at org.apache.flume.sink.hive.HiveWriter$11.call(HiveWriter.java:425)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

Its the same problem, only the exception that caused it is different., In the last catch of the SessionState.java part of the patch, can you add a call to tezSessionState.close, if present, protected by try-catch? 
Doesn't look like it should fail leaving it in open state, but I wonder if open (and esp. beginOpen) calls are atomic... we'd rather not have that leak. 

Also why is HiveSessionImpl change necessary? As far as I understand, the session at that point is already valid. At least, it's retained in the field (for future use?) even though we detach it., Hi Sergey, thank you for response, unfortunately, I am not the author of the original code and I have problems figuring out all the places where this should be fixed. I analyzed all the calls of .setCurrentSession and modified those parts, where I was it is not paired with .detach().
If, in HiveSessionImpl the session is already valid, why there is call to setSessionState, though. Maybe it may be reomved.

Maybe we need more complex change of this, to centralize the point where the Session is created & initialized atomically and then only call this one procedure. Standard way of doing this is in constructors. Detaching/closing is then done in .close(). I do not know why this time, the .start was chosen to decouple construction and starting of object, and why classes using SessionState are calling .detach() directly and not only .close(). The outer API of SessionState class is not clear to me.]