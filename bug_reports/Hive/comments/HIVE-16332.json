[
##  Reason Analysis
 ObjectInspectorConverters$ListConverter instance does not clean the data of previous record, 
 When the size of  array of current row is less than that of previous row, it data of list will not be fully overwrite 
 and the not overwrited data will be output.

## Code Analysis
In FetchOperator.nextRow. At first, it deserializes the value using the currSerDe, currSerDe is the SerDe of partition.
Second, ObjectConverter is an instance of ObjectInspectorConverters$StructConverter
```
         Object deserialized = currSerDe.deserialize(value);
          if (ObjectConverter != null) {
            deserialized = ObjectConverter.convert(deserialized);
          }
```
In method convert,  it read out every field value in turn, and it uses the consponding  converter to convert the field value.
After change the format of table to orc,  with the  type field array, the consponding convert is ObjectInspectorConverters$ListConverter.
```
@Override
    public Object convert(Object input) {
      if (input == null) {
        return null;
      }

      int minFields = Math.min(inputFields.size(), outputFields.size());
      // Convert the fields
      for (int f = 0; f < minFields; f++) {
        Object inputFieldValue = inputOI.getStructFieldData(input, inputFields.get(f));
        Object outputFieldValue = fieldConverters.get(f).convert(inputFieldValue);
        outputOI.setStructFieldData(output, outputFields.get(f), outputFieldValue);
      }

      // set the extra fields to null
      for (int f = minFields; f < outputFields.size(); f++) {
        outputOI.setStructFieldData(output, outputFields.get(f), null);
      }

      return output;
    }
  }
  ```
  
  In Method ObjectInspectorConverters$ListConverter.convert,  it first creates separate element converter for each element.
  Then, it call outputIO.resize(output,size). 
  Finally, it set every converted element to outputOI.
  ```
  @Override
    public Object convert(Object input) {
      if (input == null) {
        return null;
      }
      // Create enough elementConverters
      // NOTE: we have to have a separate elementConverter for each element,
      // because the elementConverters can reuse the internal object.
      // So it's not safe to use the same elementConverter to convert multiple
      // elements.
      int size = inputOI.getListLength(input);
      while (elementConverters.size() < size) {
        elementConverters.add(getConverter(inputElementOI, outputElementOI));
      }

      // Convert the elements
      outputOI.resize(output, size);
      for (int index = 0; index < size; index++) {
        Object inputElement = inputOI.getListElement(input, index);
        Object outputElement = elementConverters.get(index).convert(
            inputElement);
        outputOI.set(output, index, outputElement);
      }
      return output;
    }

  }
  ```
   The problem is in method resize, it does not clear all the data of previous, simply calls ensureCapacity. 
   When the size of  array of current row is less than that of previous row, it data of list will not be fully overwrite and the not overwrited data will be output.
  ```
  public Object resize(Object list, int newSize) {
      ((ArrayList) list).ensureCapacity(newSize);
      return list;
    }
  ```
  
  ## The method of amending.
```
  public Object resize(Object list, int newSize) {
  ((ArrayList) list).clear();
  return list;
}

```, IMHO, the ArrayList.ensureCapacity  does not clear all the data of previous row.
      When the size of array of current row is less than that of previous row, it data of list will not be fully   overwrite and the not overwrite data will be output., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12884219/HIVE-16332.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 11000 tests executed
*Failed tests:*
{noformat}
TestTxnCommandsBase - did not produce a TEST-*.xml file (likely timed out) (batchId=280)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=61)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=234)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=234)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=102)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6580/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6580/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6580/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12884219 - PreCommit-HIVE-Build]