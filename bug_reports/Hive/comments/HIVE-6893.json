[[~romainr] Can you provide more details about your setup? 
Was metastore running as separate process or embedded within HS2 ?
Were HS2 clients odbc or jdbc ? How many concurrent clients were there and how many queries they were firing?, Also, did you see any other error messages in your log like TException, TableAlreadyExistsException etc. or only the one you posted ?, [~romainr] This seems like a metastore concurrency issue. It seems that the metastore client socket is reading the RPC response from a different call, hence the out of sequence exception. I think we'll have a better idea once we learn more about [~ashutoshc]'s queries., [~romainr] As a followup, can you also try starting HiveServer2 with -hiveconf hive.metastore.uris=" " and see if you get the same error?, Mestastore is running as a standalone process/service outside the HS2 JVM. This issue was seen after the following occured (there is no indication of a problem prior)
1) Upgrade the HMS DB (mysql)
2) Running with slightly newer version of HUE.
3) About 15 parallel beeswax sessions to HS2 each running a query.

We haven't been able to reproduce this in-house using a JDBC Client that spins off threads that connect to HS2, run a query & exit. 

We suspected that it was one of the two causes
1) Socket timeouts during read by parallel invocations between HS2 (HMS Client) & HMS. 
2) Concurrency issues in HMS on invocations to HMS.

After noticing Socket timeout exceptions in the HS2 logs, we had the customer increase the socket read timeouts from 90s to 120s. Did not help, but also the hive logs indicate that the "out of sequence" errors occurred first then the socket  timeouts, pretty far apart in time. So the socket read timeout is not likely the root cause.

It must be concurrent access to the HMS by multiple beeswax sessions thats causing this issue. 

Hope this helps., This proposed fix makes HS2 use a synchronized wrapper for the metastore client when the HMS is running as a standalone process. In embedded mode, we would not be reading sequenced responses over a socket.

This fix does NOT synchronize all the calls to the HMS. For example, Hive.getMSC() returns an metastore client instance that is not thread-safe. There is a bit of risk synchronizing these calls, as it might have a regressive performance impact. 

I will be measure performance impact of using a synchronized wrapper all over and see if the reward outweighs the risk. , [~ngangam] Can you also upload the patch to review board?, Review posted to the review board.
https://reviews.apache.org/r/20569/

, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12641303/HIVE-6893.1.patch

{color:red}ERROR:{color} -1 due to 41 failed/errored test(s), 5417 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_auto_join32
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_filter_numeric
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby2_map_skew
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_sort_skew_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_infer_bucket_sort_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_6
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_list_bucket_dml_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_mapjoin_test_outer
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_nullgroup3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_orc_createas1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_ppd_join4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_select_dummy_source
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_stats_partscan_1_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_symlink_text_input_format
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_truncate_column_list_bucket
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udf_current_database
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_1
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_10
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_12
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_13
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_14
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_17
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_19
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_20
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_21
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_22
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_23
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_24
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_4
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_5
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_7
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_8
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_union_remove_9
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_bucketizedhiveinputformat
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_root_dir_external_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_dynamic_partitions_with_whitelist
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_stats_partialscan_autogether
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/9/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-Build/9/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 41 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12641303, As FYI, these are known test errors due to recent switch to hadoop-2.  Ref for a clean hadoop-2 build run two days back, see bottom of: [http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/precommit-hive-15/execution.txt|http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/precommit-hive-15/execution.txt], Patch in current form eschews underlying problem. I think its worth spending time to figure out whats going on. FWIW, I have seen {{TestRetryingHMSHandler.testRetryingHMSHandler}} test fail on trunk with HiveQA runs with following trace:
{code}
org.apache.thrift.TApplicationException: create_database failed: out of sequence response
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:76)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_database(ThriftHiveMetastore.java:511)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_database(ThriftHiveMetastore.java:498)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:534)
	at org.apache.hadoop.hive.metastore.TestRetryingHMSHandler.testRetryingHMSHandler(TestRetryingHMSHandler.java:76)
{code}

Above test failure which is independent of HS2 provides an independent repro test case. Though, in Hive QA runs it only fails rarely, hinting at some race condition. , I encountered the same issue, we get a socket read timeout and then out-of-sequence error. In one case we got an OOM in our client and I suspect it's the same underlying issue. Here is the metastore sequence of events. Our client tried to drop a table starting at 14:02:25. Note that we use a 20 seconds timeout for our client:
{code}
2014-06-23 14:02:25,181 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: 11: source:/10.20.93.47 drop_table : db=cloudera_manager_metastore_canary_test_db tbl=CM_TEST_TABLE
2014-06-23 14:02:25,181 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit: ugi=hue	ip=/10.20.93.47	cmd=source:/10.20.93.47 drop_table : db=cloudera_manager_metastore_canary_test_db tbl=CM_TEST_TABLE	
2014-06-23 14:02:25,182 INFO org.apache.hadoop.hive.metastore.HiveMetaStore: 11: source:/10.20.93.47 get_table : db=cloudera_manager_metastore_canary_test_db tbl=CM_TEST_TABLE
2014-06-23 14:02:25,182 INFO org.apache.hadoop.hive.metastore.HiveMetaStore.audit: ugi=hue	ip=/10.20.93.47	cmd=source:/10.20.93.47 get_table : db=cloudera_manager_metastore_canary_test_db tbl=CM_TEST_TABLE	
2014-06-23 14:02:46,596 INFO hive.metastore.hivemetastoressimpl: deleting  hdfs://jenkins-debian60-17.ent.cloudera.com:8020/user/hue/.cloudera_manager_hive_metastore_canary/HIVE_1_HIVEMETASTORE_627a77825bb851bf2db30317a698dded/2014_06_23_14_02_11/cm_test_table
2014-06-23 14:02:46,694 INFO hive.metastore.hivemetastoressimpl: Moved to trash: hdfs://jenkins-debian60-17.ent.cloudera.com:8020/user/hue/.cloudera_manager_hive_metastore_canary/HIVE_1_HIVEMETASTORE_627a77825bb851bf2db30317a698dded/2014_06_23_14_02_11/cm_test_table
{code}

On our client we get a socket timeout for the drop table call at 14:02:45:
{code}
2:02:45.209 PM 	WARN 	com.cloudera.cmon.firehose.polling.hive.HiveMetastoreCanary Metastore HIVE-1-HIVEMETASTORE-627a77825bb851bf2db30317a698dded: Failed to drop table 
com.cloudera.cmf.cdhclient.common.hive.MetaException: java.net.SocketTimeoutException: Read timed out
{code}
we then try to drop the database immediately afterwards and the next message in our logs is:
{code}
2:02:46.697 PM 	WARN 	com.cloudera.cmf.cdh4client.hive.MetastoreClientImpl 	Could not drop hive database: cloudera_manager_metastore_canary_test_db
com.cloudera.cdh4client.hive.shaded.org.apache.thrift.TApplicationException: get_database failed: out of sequence response
	at com.cloudera.cdh4client.hive.shaded.org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:76)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:412)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:399)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:736)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:479)
	at com.cloudera.cmf.cdh4client.hive.MetastoreClientImpl.dropDatabase(MetastoreClientImpl.java:160)
{code}

Note that the moved-to-trash message in the hive metastore is from 14:02:46,694 and the out-of-order exception is from 2:02:46.697. I know that order-in-time does not imply causation but is it possible that we are getting the drop-table acknowledgment message instead of the get_database message?, The fix in the proposed patch, is included in HIVE-10956. So this issue should be resolved by that fix. I am closing the jira.]