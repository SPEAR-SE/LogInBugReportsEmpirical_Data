[Looks like the second thread acquires the compile lock and is waiting from HDFS side to find out if that path is encrypted location or not. Do you see any errors in the namenode logs?, May also be related to "small files"... if the Hive data set has many small files in it, the Hive compile stage can take a long time to download all of the file information., [~vihangk1] [~belugabehr]Thank you for attention. We have multiple HiveServer2 instances for loading balance. We didn't find  any obvious errors in namenode logs. Once the hiveserver2 got stuck ,it would have thousands of CLOSED_WAIT connection status on port 10000. We find the execution sql of  *Thread-809399* which locked the resource  *0x00000003c0acca40*  is *select * from table_name limit 10* .  And FetchTask  (hive.fetch.task.conversion= more) is used,  so this sql job would not  produce mr jobs.  Therefore, we doubt that whether this sql generated a full table scan operation using FetchTask, and if the data of table is very large with many small files ,it would cause  HiveServer2 got stuck.
By the way, the data of table has been updated so we could't know the specific table data size when HiveServe2 got stuck , and its data size is 16 GB now. We test the sql of select * from table_name limit 10, and the return result is just ok. Maybe more data would cause HS2 got stuck. 
The above is my analysis. Do you think this is a correct analysis? Thansk., I am not so sure it is related to the data size. If you look at the error stack above the HS2 when it was compiling the query acquired the compile lock below

{code}
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1145)
	- locked <0x00000003c0acca40> (a java.lang.Object)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1139)
{code}

and then it issues HDFS RPC call

at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:350)

which must not be returning for a long time. All this while, no other query can be compiled on this particular HS2 node. You should look at namenode (or possibly datanode logs I am not very sure on that) around that time to see what was going on to find out the root cause. When you repeated the test did you make sure that the same HS2 was compiling the query since you are using load balancer it is possible that the query was compiled by some other HS2 which does not have this problem.]