[Hi Sush,

My feeling for the root cause probably is not as same as yours. Just want to provide another possibility. Please correct me if I am wrong. 

Exist databases(tables) were reported as non-exist: This is due to the connection to db (mysql/oracle) was bounced back due to the connection pool is small and the thread waiting time is too short. Currently this "internal error" exception was mistakenly casted to NoSuchObjectException. We have to fix the misleading error message. ( 

Parallelism execution: This is due to meta store usually hold connections for a very long time because lots of drop/add/alter operations have HDFS operations involved. Sometimes the table stats also are collected during the window. And connections to db is not shared by the meta store clients. So the best practice for parallelism is increasing the size of connection pools(DBCP for example). The db load is not heavy at all, we can utilize the concurrency of existing RDBMS. DirectSQL get_database definitely will hold connection for much less time than ORM get_database, so the connection shortage problem may not be obvious. 

I think "datanucleus.connectionPool.testSQL=SELECT 1" is the validation query for DBCP to validate the underneath connection to RDBMS. Have it set DBCP will guarantee the connection each time we borrow from the connection pool is valid. 

Thanks,
, Hi Selina,

Yes, I would agree that the connection pool (or jdbc driver, since I've since been able to see this happening a couple of times with DBCP as well) is probably raising some sort of internal error that is being incorrectly read as normal operation by DN, which results in a NSOE by the hive ObjectStore. I definitely agree that this is the underlying error that we need to reproduce and track down to fix.

In the case of a persistent remote metastore, I would agree that increasing the size of the connection pools makes sense, and should be the way to go. I generally do advise a larger pool, and always going through the metastore. 

But in the case of parallel hive fatclients, the embedded metastore is effectively single-threaded w.r.t to connections to the database, so I'm afraid I don't yet understand how having a larger pool would help in this case. Could you please expand on this bit?

(And yes, "datanucleus.connectionPool.testSQL=SELECT 1" is so that the overhead of DN testing connectivity to the db is minimized - without that, DN creates a bunch of deleteme* tables and drops them to test connectivity.)
, "DN creates a bunch of deleteme* tables ..."
It creates a _single_ DELETEME* table _when_ the RDBMS doesn't provide another mechanism for checking the catalog/schema in use _per PMF_ (and it isn't 'testing connectivity'). The only way there will be "a bunch" is if the application is using multiple PMFs (why would it need multiple), and even then the cost of a create/drop of a DELETEME table is so insignificant compared to the overall PMF startup cost.]