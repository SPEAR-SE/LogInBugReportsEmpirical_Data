[ It would better to call shim.closeAllForUGI to free up the FileSystem objects, like it is done for the kerberos mode  from HiveSessionImplwithUGI.close().

Calling shim.closeAllForUGI at end of  TUGIContainingProcessor.process is too early and ends up returning empty results for the queries.
, Is this issue related to HIVE-3098 or HIVE-3155 ?, I think HS1 has similar problems..., [~clarkyzl] Yes, HIVE-3098 and HIVE-3155 are related, as it is same kind of leak is seen, but with metastore and hive server1 instead.
, I have updated hiveserver2 setup instructions to disable the fs caches - https://cwiki.apache.org/confluence/display/Hive/Setting+up+HiveServer2, HIVE-4501.1.patch - disables fs cache by default, in non-kerberos mode with impersonation turned on. Makes hive work with default settings. 
, Thejas, I'm confused
patch sets fs.hdfs.impl.disable.cache to false
but wiki says set it to true, [~apivovarov]  Sorry about that, the patch should actually set it to true. (i wish we didn't use negative flags !) 
, After set them to false I see 4 Lease checker threads created on each query execution., Hi [~thejas]
Now, I try to address the issue HIVE-5296, which includes this issue.
I think, it's better to call FileSystem.closeAll at the end of the session than disable cache because a FileSystem object can be used more than 1 time in a statement., Sorry, I think that we use FileSystem.closeAll may be not good idea because a thread can block others threads by synchronizing. So, [~thejas] idea may be better., Provided patch doesn't solve the problem though, to my understanding. It makes it less pronounced for sure, but it doesn't go away., [~cos] Can you please elaborate ? The problem is memory leak in HS2 caused by FileSystem objects being cached in FileSystem.CACHE. Why won't disabling the cache stop the leak ?
, if cache is disabled then 4 lease checker threads are created for each query in hiveserver2 process.
These threads keep running even after client closes the connection.
I tested it in hive-0.11, I have corrected the description to say that these variables should be set to true to disable the cache.

[~apivovarov]
I thought you saw lease checker threads when you set the variables to false (ie cache was not disabled). , if cache is enabled then only one FileSystem per query is not closed
But if cache is disabled then 4 FileSystems are not closed - as a result 4 lease checker threads leak per query

How disabling FileSystem cache can help with closing FileSystem?
if we write smth to hdfs then Lease Checker thread is created. To stop the thread File System should be closed.
The only question is where.
As we already know calling close() at he end of TUGIContainingProcessor.process is too early.
So, where should it be called?, I think we need to close FileSystem object properly, right?, Probably we can set ctx.setHDFSCleanup(false) in Driver   (lines 415, 1126)
after that the fix reverted here should work https://github.com/apache/hive/commit/4de1b5acc0b0507787bc8e74259a15e414dcb49e

set HDFScleanup to false is ok because clean of tmp files on exit did not work because fs was never closed., Setting hive.server2.enable.doAs to false is a workaround., But it essentially prevents auth mechanism for HS2, right?, I did the fix which solves the problem with open FileSystems. (in case hive.server2.enable.doAs is true (which is default))

I remember all UGIs created during Statement execution/fetch and then close FSs for all these UGIs on Statement.close() event  (Filesystem CACHE should be enabled)
I tested it - it looks good. Can you review the fix?
https://github.com/WANdisco/amplab-hive/commit/ef7b4f618323796e730a0a4025455fa3a8fc66d6, Posting the patch for his fix on JIRA., Hi [~apivovarov] I revased your patch and found that FileSystem.CACHE doesn't leak., So, shall we have committed it to the trunk and 0.12?, I will review it today.

Closing the fs is certainly better than just disabling the cache. The benefit of explicitly closing the FS instead of just letting the GC collect it (by disabling the cache) seems to be that the deleteOnExit() calls will actually take effect. But I am not sure if hive code is relying on deleteOnExit() to happen on the client side. I haven't seen file/file handle leaks in long running HS2 . I believe the lease checker threads also go away when GC kicks in (based on my experience with long running HS2 with many queries).


, Making it patch available to kick off the tests.
, bq. I believe the lease checker threads also go away when GC kicks in

We haven't observed that behavior in our tests, not with explicit GC at least., 

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12605318/HIVE-4501.1.patch

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/942/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/942/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-942/source-prep.txt
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1527140.

At revision 1527140.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0 to p2
+ exit 1
'
{noformat}

This message is automatically generated., Regenerating patch with -p0, Wrong patch structure - needs to be regenerated with p0, 

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12605629/HIVE-4501.1.patch

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/947/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/947/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Tests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-947/source-prep.txt
+ mkdir -p maven ivy
+ [[ svn = \s\v\n ]]
+ [[ -n '' ]]
+ [[ -d apache-svn-trunk-source ]]
+ [[ ! -d apache-svn-trunk-source/.svn ]]
+ [[ ! -d apache-svn-trunk-source ]]
+ cd apache-svn-trunk-source
+ svn revert -R .
++ awk '{print $2}'
++ egrep -v '^X|^Performing status on external'
++ svn status --no-ignore
+ rm -rf
+ svn update

Fetching external item into 'hcatalog/src/test/e2e/harness'
External at revision 1527150.

At revision 1527150.
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0 to p2
+ exit 1
'
{noformat}

This message is automatically generated., Here's the trunk patch. The original one wasn't applicable for the trunk version and should have been clearly marked as such. Apologies., I reviewed the patch. The patch seems to make an assumption that same thread is used for whole session. This is not the case, same thread can get used for another session, once the call for a session has been serviced. This will close FileSystems associated with other sessions as well.

A clean way to deal with this would be to use same way as its done for impersonation in kerberos mode, ie using HiveSessionImplwithUGI .
, bq. This is not the case, same thread can get used for another session
Are you saying that the same FS ref. can be used across UGIs? If so, It looks like a nice big hole to me ;) The call for the close of FS is done at the point where the particular UGI's session is getting finished and all its resources are let go, e.g. operation handlers, metastore, etc. I am not sure if I follow your comment. Could you please elaborate?, bq. Are you saying that the same FS ref. can be used across UGIs? 
No, that is not happening in current code. But this patch will accumulate FS refs that belong to different sessions and when a session is closed it can end up closing FS objects that are associated with other sessions, and not closing the FS handles that belong to the session being closed. 
The change in TUGIContainingProcessor will just accumulate FS handles to a thread local variable.  But that thread can be used by different hive sessions. There is no 1:1 mapping between hive server 2 threads and hive sessions. I hope this clarifies.

, bq. The change in TUGIContainingProcessor will just accumulate FS handles to a thread local variable.
Correction (i meant to say): The change in TUGIContainingProcessor will just accumulate UGI objects to a thread local variable. The UGI objects being accumulated in the thread local hashset can belong to different sessions.

, What is the progress on this issue?, The changes in HIVE-6312 (in 0.13) should fix this issue, as the unsecure mode now follows same code path as secure mode, which calls closeAllForUGI .
I haven't verified with large number of requests and cache enabled, but if there is still any leak its going to have a different root cause.
]