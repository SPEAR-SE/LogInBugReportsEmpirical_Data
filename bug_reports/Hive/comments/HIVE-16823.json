[Possibly fixed with HIVE-16273.  What release did this problem occur on?, Hi, [~mmccline]. This exception was indeed triggered by [-HIVE-16273-|https://issues.apache.org/jira/browse/HIVE-16273], if I build Hive with code before this patch, this exception won't occur. Any comments and suggestion will be appreciated. Thx!
And in my opinion, it would be better to add some detailed description for  [-HIVE-16273-|https://issues.apache.org/jira/browse/HIVE-16273]., [~mmccline]: the exception was found after HIVE-16273.

before HIVE-16273
VectorGroupKeyHelper#copyGroupKey
{code}
 for(int i=0;i<stringIndices.length; ++i) {
      int keyIndex = stringIndices[i];
      BytesColumnVector inputColumnVector = (BytesColumnVector) inputBatch.cols[keyIndex];
      BytesColumnVector outputColumnVector = (BytesColumnVector) outputBatch.cols[keyIndex];
      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {
        // Copy bytes into scratch buffer.
        int start = buffer.getLength();
        int length = inputColumnVector.length[0];
        try {
          buffer.write(inputColumnVector.vector[0], inputColumnVector.start[0], length);
        } catch (IOException ioe) {
          throw new IllegalStateException("bad write", ioe);
        }
        outputColumnVector.setRef(outputBatch.size, buffer.getData(), start, length);
      } else {
        outputColumnVector.noNulls = false;
        outputColumnVector.isNull[outputBatch.size] = true;
      }
{code}

after HIVE-16273
{code}
  for(int i=0;i<stringIndices.length; ++i) {
      final int columnIndex = outputColumnNums[stringIndices[i]];
      BytesColumnVector inputColumnVector = (BytesColumnVector) inputBatch.cols[columnIndex];
      BytesColumnVector outputColumnVector = (BytesColumnVector) outputBatch.cols[columnIndex];
      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {
        // Copy bytes into scratch buffer.
        int start = buffer.getLength();
        int length = inputColumnVector.length[0];
        try {
          buffer.write(inputColumnVector.vector[0], inputColumnVector.start[0], length);
        } catch (IOException ioe) {
          throw new IllegalStateException("bad write", ioe);
        }
        outputColumnVector.setRef(outputBatch.size, buffer.getData(), start, length);
      } else {
        outputColumnVector.noNulls = false;
        outputColumnVector.isNull[outputBatch.size] = true;
      }
    }
{code}

The exception occurs  {noformat}outputBatch.cols[columnIndex]{noformat} (here the columnIndex is 1 while the size of outputBatch.cols is 1), so ArrayIndexOutOfBoundsException happened., some updates about the jira.
 The root cause of the problem is because the difference of sub-query {{select ds as ds, ds as `date` from srcpart group by ds}} between tez and spark mode.
the spark explain(the full spark explain is attached in [here|https://issues.apache.org/jira/secure/attachment/12883036/explain.spark] )
{code}
  Map 3 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  filterExpr: (ds = '2008-04-08') (type: boolean)
                  Statistics: Num rows: 1 Data size: 11624 Basic stats: PARTIAL Column stats: NONE
                  Select Operator
                    Statistics: Num rows: 1 Data size: 11624 Basic stats: PARTIAL Column stats: NONE
                    Group By Operator
                      keys: '2008-04-08' (type: string)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: '2008-04-08' (type: string)
                        sort order: +
                        Map-reduce partition columns: '2008-04-08' (type: string)
                        Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
 Reducer 4 
            Local Work:
              Map Reduce Local Work
            Reduce Operator Tree:
              Group By Operator
                keys: '2008-04-08' (type: string)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
{code}

the tez explain(the full tez explain is attached in [here|https://issues.apache.org/jira/secure/attachment/12883035/explain.tez] )
{code}
  Map 2 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  filterExpr: (ds = '2008-04-08') (type: boolean)
                  Statistics: Num rows: 1 Data size: 11624 Basic stats: PARTIAL Column stats: NONE
                  Select Operator
                    Statistics: Num rows: 1 Data size: 11624 Basic stats: PARTIAL Column stats: NONE
                    Group By Operator
                      keys: '2008-04-08' (type: string)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Reducer 3 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0
{code}

The Group By Operator appears in Map and Reducer in tez or spark mode. But the keys of GroupByOperator in Reducer is different. In tez, the key is  {{keys: KEY._col0 (type: string)}} while in spark the key is {{keys: '2008-04-08' (type: string)}}.  This difference causes [VectorizationContext#getVectorExpression|https://github.com/kellyzly/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java#L579 ] returns [getColumnVectorExpression|https://github.com/kellyzly/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java#L582] in tez mode while  returns [getConstantVectorExpression|https://github.com/kellyzly/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java#L660]., In HIVE-15269:Dynamic Min-Max/BloomFilter runtime-filtering for Tez, ConstantPropagate is removed in TezCompiler#runDynamicPartitionPruning. Similar code should be removed in SparkCompiler#runDynamicPartitionPruning
{code}
  private void runDynamicPartitionPruning(OptimizeTezProcContext procCtx, Set<ReadEntity> inputs,
      Set<WriteEntity> outputs) throws SemanticException {

    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING)) {
      return;
    }

    // Sequence of TableScan operators to be walked
    Deque<Operator<?>> deque = new LinkedList<Operator<?>>();
    deque.addAll(procCtx.parseContext.getTopOps().values());

    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
    opRules.put(
        new RuleRegExp(new String("Dynamic Partition Pruning"), FilterOperator.getOperatorName()
            + "%"), new DynamicPartitionPruningOptimization());

    // The dispatcher fires the processor corresponding to the closest matching
    // rule and passes the context along
    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, procCtx);
    List<Node> topNodes = new ArrayList<Node>();
    topNodes.addAll(procCtx.parseContext.getTopOps().values());
    GraphWalker ogw = new ForwardWalker(disp);
    ogw.startWalking(topNodes, null);

/** Similar code is removed in TezCompiler in HIVE-15269:Dynamic Min-Max/BloomFilter runtime-filtering for Tez***/
    // need a new run of the constant folding because we might have created lots
    // of "and true and true" conditions.
    // Rather than run the full constant folding just need to shortcut AND/OR expressions
    // involving constant true/false values.
    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {
      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);
    }

  }

{code}
[~lirui],[~stakiar]: can you help review?
, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12883056/HIVE-16823.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 10994 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_2] (batchId=171)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_3] (batchId=171)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_mapjoin_only] (batchId=170)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=100)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=235)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=235)
org.apache.hadoop.hive.common.TestFileUtils.testCopyWithDistCpAs (batchId=250)
org.apache.hadoop.hive.common.TestFileUtils.testCopyWithDistcp (batchId=250)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=180)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6484/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6484/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6484/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12883056 - PreCommit-HIVE-Build, update changes in q*out in HIVE-16823.1.patch. Most changes like following. This is because we remove ConstantPropagate in SparkCompiler#runDynamicPartitionPruning.  
{code}
            Map Operator Tree:
                 TableScan
                   alias: srcpart_date
-                  filterExpr: ((date = '2008-04-08') and ds is not null) (type: boolean)
+                  filterExpr: ((date = '2008-04-08') and ds is not null and true) (type: boolean)
                   Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
-                    predicate: ((date = '2008-04-08') and ds is not null) (type: boolean)
+                    predicate: ((date = '2008-04-08') and ds is not null and true) (type: boolean)
                     Statistics: Num rows: 1 Data size: 21 Basic stats: COMPLETE Column stats: NONE
{code}
Big changes in spark_vectorized_dynamic_partition_pruning.q.out as this file has not been updated for long time., explain more about the big changes in spark_vectorized_dynamic_partition_pruning.q.out on review board. [~lirui] and [~stakiar]: If have time, please help review, thanks!, [~kellyzly], thanks for working on this. I think the root cause is not in how we implement DPP. I can reproduce the issue with DPP disabled:
{noformat}
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.spark.dynamic.partition.pruning=false;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.vectorized.execution.enabled=true;
set hive.strict.checks.cartesian.product=false;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask = true;
set hive.auto.convert.join.noconditionaltask.size = 10000000;

set hive.cbo.enable=false;
set hive.optimize.constant.propagation=true;
{noformat}

Looking at the code in the vectorizer, I think it's because the vectorized GBY is not properly set when SparkHashTableSinkOperator is present. To verify this, if you change {{hive.auto.convert.join=false}} in the above configs, the query can run successfully. I'm still trying to understand the logic in the vectorizer. Please also update the JIRA if you find anything. Thanks., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12883287/HIVE-16823.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10994 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=235)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=235)
org.apache.hadoop.hive.common.TestFileUtils.testCopyWithDistCpAs (batchId=250)
org.apache.hadoop.hive.common.TestFileUtils.testCopyWithDistcp (batchId=250)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=180)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6499/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6499/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6499/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12883287 - PreCommit-HIVE-Build, Here's what I found so far. When we create vector GBY, the vector expression of the key is {{ConstantVectorExpression(val 2008-04-08) -> 1:string}}, with {{outputColumn == 1}}. The corresponding VectorizationContext is something like this:
{noformat}
Context name __Reduce_Shuffle__, level 0, sorted projectionColumnMap {0=KEY._col0}, scratchColumnTypeNames [string]
{noformat}
But in the constructor of the vector GBY, we create another VectorizationContext using the above one: https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java#L870.
Therefore, the vector GBY's VectorizationContext will be something like:
{noformat}
Context name GBY, level 0, sorted projectionColumnMap {0=_col0}, scratchColumnTypeNames []
{noformat}
Note that it doesn't have scratch columns. At runtime, the key expression's column index is 1, but we only have 1 column vector in the output batch, and thus getting the exception.
I don't fully understand the vectorization logics. But it seems the vector GBY relies on following operators to setup its VectorizationContext. If it's followed by SEL or RS, the VectorizationContext may be fixed as we get vector expressions for these operators (different operators' VectorizationContext share the same OutputColumnManager). [~mmccline] do you think it's an issue? Or let me know where else I should look at. Thanks., [~lirui]: I found if enable cbo with your settings, everything works fine
{code}
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.spark.dynamic.partition.pruning=false;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.vectorized.execution.enabled=true;
set hive.strict.checks.cartesian.product=false;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask = true;
set hive.auto.convert.join.noconditionaltask.size = 10000000;
set hive.optimize.constant.propagation=true;
{code}

when enabling cbo, the explain is
{code}
 Map 3 
            Map Operator Tree:                TableScan
                  alias: srcpart                  filterExpr: (ds = '2008-04-08') (type: boolean)
                  Statistics: Num rows: 1 Data size: 11624 Basic stats: PARTIAL Column stats: NONE
                  Select Operator
                    Statistics: Num rows: 1 Data size: 11624 Basic stats: PARTIAL Column stats: NONE
                    Group By Operator
                      keys: '2008-04-08' (type: string)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: string)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: string)
                        Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Reducer 4 
            Execution mode: vectorized
            Local Work:
              Map Reduce Local Work
            Reduce Operator Tree:
              Group By Operator
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0

{code}

when disabling cbo, the explain is
{code}
 Map 1 
            Map Operator Tree:
                TableScan
                  alias: srcpart                  filterExpr: (true and (ds = '2008-04-08')) (type: boolean)
                  Statistics: Num rows: 1 Data size: 11624 Basic stats: PARTIAL Column stats: NONE
                  Filter Operator                    predicate: true (type: boolean)
                    Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
                    Select Operator                      Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: '2008-04-08' (type: string)                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
                        Reduce Output Operator
                          key expressions: '2008-04-08' (type: string)                          sort order: +
                          Map-reduce partition columns: '2008-04-08' (type: string)
                          Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Reducer 2 
            Execution mode: vectorized
            Local Work:
              Map Reduce Local Work
            Reduce Operator Tree:
              Group By Operator
                keys: '2008-04-08' (type: string)
{code}

the difference is the key of GroupByOperator in the Reducer. But not know why cbo causes wrong explain. Need to investigate., [~kellyzly] I think that's because when CBO is on, ConstantPropagate won't be run., [~lirui]: Although ConstantPropagate influence the logical planï¼Œhive on tez will not throw the exception.

{code}
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.tez.dynamic.partition.pruning=true;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.vectorized.execution.enabled=true;
set hive.strict.checks.cartesian.product=false;
set hive.cbo.enable=false;
set hive.user.install.directory=file:///tmp;
set fs.default.name=file:///;
set fs.defaultFS=file:///;
set tez.staging-dir=/tmp;
set tez.ignore.lib.uris=true;
set tez.runtime.optimize.local.fetch=true;
set tez.local.mode=true;
set hive.explain.user=false;
select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
{code}

the explain(It seems the key of GroupByOperator is not right)
{code}
 Reducer 2 
            Execution mode: vectorized
            Reduce Operator Tree:
              Group By Operator
                keys: '2008-04-08' (type: string)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  Statistics: Num rows: 1 Data size: 11624 Basic stats: COMPLETE Column stats: NONE
                  Map Join Operator
{code}

Need more time to investigate why tez is not influenced when cbo is disabled. But i guess this is another problem, any suggestion?, [~kellyzly], the operator tree is different between spark and tez and as I said, the following operators might change the VectorizationContext of the GBY. This is why the query runs if map join is disabled, in which case GBY is followed by SEL/RS instead of SparkHashTableSinkOperator., I have a simpler query to reproduce the issue and it happens to Tez as well:
{noformat}
set hive.cbo.enable=false;
select count(*) from (select key from src group by key) s where s.key='98';
{noformat}, some update
{quote}
This is why the query runs if map join is disabled, in which case GBY is followed by SEL/RS instead of SparkHashTableSinkOperator.
{quote}
more explain about this.  
{code}
set spark.master=local;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.spark.dynamic.partition.pruning=false;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.vectorized.execution.enabled=true;
set hive.strict.checks.cartesian.product=false;
set hive.auto.convert.join=false;
set hive.cbo.enable=false;
set hive.optimize.constant.propagation=true;
select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
{code}
the cbo is disabled and the explain is not right, the key of GroupBy in Reducer is {{keys: '2008-04-08' (type: string)}} it should be {{keys: KEY._col0 (type: string)}} but the query finishes successfully. The reason is there is {{RS\[9\]}} after {{GBY\[4\]}} 
{code}
GBY[4]-SEL[5]-RS[9]
{code}

{{RS\[9\]}} called following stack, OutputColumnManager#allocateOutputColumn makes OutputColumnManager#getScratchColumnTypeNames returning value.
{code}
org.apache.hadoop.hive.ql.exec.vector.VectorizationContext$OutputColumnManager.allocateOutputColumn(VectorizationContext.java:478)
          at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:1153)
          at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:688)
          at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressions(VectorizationContext.java:590)
          at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpressions(VectorizationContext.java:578)
          at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.canSpecializeReduceSink(Vectorizer.java:3490)
          at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.vectorizeOperator(Vectorizer.java:4174)
          at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationNodeProcessor.doVectorize(Vectorizer.java:1632)
          at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$ReduceWorkVectorizationNodeProcessor.process(Vectorizer.java:1772)
{code}

in the log, we can see after VectorizationNodeProcessor#doVectorizer {{GBY\[4\]}} , the vectorization context is 
{code}
2017-08-25T03:40:21,316 DEBUG [cd30697a-7797-4bbe-ad92-1fcec8a89689 main] physical.Vectorizer: Vectorized ReduceWork reduce shuffle vectorization context Context name __Reduce_Shuffle__, level 0, sorted projectionColumnMap {0=KEY._col0}, scratchColumnTypeNames []
{code}
after  VectorizationNodeProcessor#doVectorizer {{RS\[9\]}}, the vectorization context is(here scratchColumnTypeNames  returns value)
{code}
2017-08-25T03:48:00,245 DEBUG [cd30697a-7797-4bbe-ad92-1fcec8a89689 main] physical.Vectorizer: vectorizeOperator org.apache.hadoop.hive.ql.plan.ReduceSinkDesc
2017-08-25T03:48:43,101 DEBUG [cd30697a-7797-4bbe-ad92-1fcec8a89689 main] physical.Vectorizer: Vectorized ReduceWork operator RS added vectorization context Context name SEL, level 1, sorted projectionColumnMap {}, scratchColumnTypeNames [string]
{code}

The difference in scratchColumnTypeNames causes different value in outputBatch in [VectorGroupKeyHelper|https://github.com/kellyzly/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java#L107].

I guess arrayIndexOutOfBoundsException can be reproduced in following condition whether in spark or tez mode.
1. cbo is disabled
2.  there is no RS follows GBY in the reducer
, [~lirui]: can you help review the patch?
i have 1 question about {{spark_vectorized_dynamic_partition_pruning.q}}, should we add {{-- SORT_QUERY_RESULTS}} to the file, otherwise 
the result of 
{code}
select distinct ds from srcpart
{code}
{code}
2008-04-09	
2008-04-08
{code}

while the result in the q.out is
{code}
2008-04-08	
2008-04-09
{code}, [~kellyzly], the v1 patch doesn't fix the root cause of the issue so it's not the right way to go. Let's figure out a fix for HIVE-17383 first and come back here. Besides, we do need to get rid of the "and true and true" conditions. Seems tez doesn't have such filters in the query plans., So HIVE-15269 doesn't actually remove the call to constant propagate. It just moves it to the end of {{TezCompiler#optimizeOperatorPlan}}. It seems the reason Tez has {{keys: KEY._col0 (type: string)}} in the explain plan, while Spark has {{keys: '2008-04-08' (type: string)}} is because Tez calls {{ConstantPropagate(ConstantPropagateOption.SHORTCUT)}} while Spark has been calling {{ConstantPropagate()}}. I changed this in HIVE-17405 to fix some other issues with the call to {{ConstantPropagate}}, and it looks like that made {{spark_vectorized_dynamic_partition_pruning.q}} work again.

So looks like HIVE-17405 should fix spark_vectorized_dynamic_partition_pruning.q, although sounds like HIVE-17383 is still a real bug.

Maybe a follow up JIRA would be to see what happens when we run {ConstantPropagate()}} at the end of {{SparkCompiler#optimizeOperatorPlan}}? Theoretically, it should improve performance? But sounds like there are some bugs we need to address before getting to that stage., [~stakiar]: {quote} 
Maybe a follow up JIRA would be to see what happens when we run {ConstantPropagate()}} at the end of SparkCompiler#optimizeOperatorPlan? Theoretically, it should improve performance? But sounds like there are some bugs we need to address before getting to that stage.
{quote}

is there any unit test failures if we put following code in the end of SparkCompiler#optimizeOperatorPlan?
{code}
  if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {
      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);
    }
{code}

I think it is better to put it in the end of SparkCompiler#optimizeOperatorPlan than in the runDynamicPartitionPruning. This is not related dpp just found bug in dpp unit test.  Beside, why it should improve performance? if you know, please tell me, thanks!, let's fix spark_vectorized_dynamic_partition_pruning.q  in the HIVE-17405 although the target of HIVE-17405 is not spark_vectorized_dynamic_partition_pruning.q  after HIVE-17383 is resolved., [~kellyzly] sounds good. I attached a new patch to HIVE-17405 with the call to constant propagate moved to the end of {{SparkCompiler#optimizeOperatorPlan}}. Let's see if there are any test failures. I'm not actually sure if an additional call to {{ConstantPropagate}} will improve performance, I would just assume if it changes things in the explain plan, then the execution of that plan should be faster; but thats just a theory.]