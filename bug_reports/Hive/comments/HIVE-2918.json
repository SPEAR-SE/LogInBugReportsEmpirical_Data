[Verified that this problem is present on 0.7.1 and trunk. I also discovered that we don't have any functioning test coverage for hive.exec.max.dynamic.partitions. This property is set in two clientnegative tests (dyn_part1.q and dyn_part3.q), but both tests actually fail for other reasons.
, cwsteinbach requested code review of "HIVE-2918 [jira] Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI".
Reviewers: JIRA

  HIVE-2918. Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI

  Dynamic Partition insert showing an error with the number of partitions created even after the default value of 'hive.exec.max.dynamic.partitions' is bumped high to 2000.

  Error Message:
  "Failed with exception Number of dynamic partitions created is 1413, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1413."

  These are the following properties set on hive CLI

  hive> set hive.exec.dynamic.partition=true;
  hive> set hive.exec.dynamic.partition.mode=nonstrict;
  hive> set hive.exec.max.dynamic.partitions=2000;
  hive> set hive.exec.max.dynamic.partitions.pernode=2000;

  This is the query with console error log

  hive>
      > INSERT OVERWRITE TABLE partn_dyn Partition (pobox)
      > SELECT country,state,pobox FROM non_partn_dyn;
  Total MapReduce jobs = 2
  Launching Job 1 out of 2
  Number of reduce tasks is set to 0 since there's no reduce operator
  Starting Job = job_201204021529_0002, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201204021529_0002
  Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_201204021529_0002
  2012-04-02 16:05:28,619 Stage-1 map = 0%,  reduce = 0%
  2012-04-02 16:05:39,701 Stage-1 map = 100%,  reduce = 0%
  2012-04-02 16:05:50,800 Stage-1 map = 100%,  reduce = 100%
  Ended Job = job_201204021529_0002
  Ended Job = 248865587, job is filtered out (removed at runtime).
  Moving data to: hdfs://0.0.0.0/tmp/hive-cloudera/hive_2012-04-02_16-05-24_919_5976014408587784412/-ext-10000
  Loading data to table default.partn_dyn partition (pobox=null)
  Failed with exception Number of dynamic partitions created is 1413, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1413.
  FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask

  I checked the job.xml of the first map only job, there the value hive.exec.max.dynamic.partitions=2000 is reflected but the move task is taking the default value from hive-site.xml . If I change the value in hive-site.xml then the job completes successfully. Bottom line,the property 'hive.exec.max.dynamic.partitions'set on CLI is not being considered by move task

TEST PLAN
  NONE

REVISION DETAIL
  https://reviews.facebook.net/D2703

AFFECTED FILES
  ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
  ql/src/test/queries/clientnegative/dyn_part_max.q
  ql/src/test/queries/clientnegative/dyn_part_max_per_node.q
  ql/src/test/results/clientnegative/dyn_part_max.q.out
  ql/src/test/results/clientnegative/dyn_part_max_per_node.q.out

MANAGE HERALD DIFFERENTIAL RULES
  https://reviews.facebook.net/herald/view/differential/

WHY DID I GET THIS EMAIL?
  https://reviews.facebook.net/herald/transcript/6189/

Tip: use the X-Herald-Rules header to filter Herald messages in your client.
, kevinwilfong has accepted the revision "HIVE-2918 [jira] Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI".

  +1 Will commit after running tests.

REVISION DETAIL
  https://reviews.facebook.net/D2703

BRANCH
  HIVE-2918-max-dynamic-parts
, kevinwilfong has requested changes to the revision "HIVE-2918 [jira] Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI".

  escape2.q seems to be broken

REVISION DETAIL
  https://reviews.facebook.net/D2703

BRANCH
  HIVE-2918-max-dynamic-parts
, @Kevin: I tried running escape1.q and escape2.q on trunk and got diffs in both tests. Are you able to run either of these tests?, @Carl: escape1.q and escape2.q both succeed for me on trunk in Linux, with this patch only escape2.q fails.

I tried running the tests on my Mac and they both fail on trunk there.  I am sure these tests were succeeding for me on my Mac as recently as Wednesday night., I created a task to resolve the test failures here
https://issues.apache.org/jira/browse/HIVE-2952, @Carl

The issue with escape2.q and this patch is that now, when the BlockMergeTask runs, the conf it gets from the Hive object has the value for hive.query.string populated, since the conf in the Hive objectis being updated more frequently than before this patch.  The query string for many of the concatenate commands in that test include characters which are illegal in XML 1.0, which it looks like Hadoop is trying to produce using the conf when a job is submitted.  This is an open issue in Hadoop https://issues.apache.org/jira/browse/HADOOP-7542

There are a couple ways I can think of so that we could deal with this issue:
1) sanitize the query String wherever we set it (Driver's execute method and SessionState's setCmd method)  This may have the added benefit of allowing users to execute queries (not just DDL commands) involving such characters.  This could potentially have the issue of escaping characters which were not escaped before and do not need to be depending on how we handle the sanitization process (this would happen for example, if we used the Apache commons library's Java escape method).
2) sanitize it, or remove it from the job conf in the BlockMergeTask.  The only two places we could run into this issue are in the BlockMergeTask and MapRedTask.  We already running into this issue in MapRedTask, and were only avoiding it in the BlockMergeTask (it appears) by luck, or somebody intentionally using the conf from the Hive object there rather than the one in the BlockMergeTask

, kevinwilfong has accepted the revision "HIVE-2918 [jira] Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI".

  +1 The escape2.q test issue was fixed by a separate revision, the tests pass.

REVISION DETAIL
  https://reviews.facebook.net/D2703

BRANCH
  HIVE-2918-max-dynamic-parts
, Committed.  Thanks Carl., Integrated in Hive-trunk-h0.21 #1397 (See [https://builds.apache.org/job/Hive-trunk-h0.21/1397/])
    HIVE-2918. Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI. (cwsteinbach via kevinwilfong) (Revision 1330417)

     Result = FAILURE
kevinwilfong : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1330417
Files : 
* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
* /hive/trunk/ql/src/test/queries/clientnegative/dyn_part_max.q
* /hive/trunk/ql/src/test/queries/clientnegative/dyn_part_max_per_node.q
* /hive/trunk/ql/src/test/results/clientnegative/dyn_part_max.q.out
* /hive/trunk/ql/src/test/results/clientnegative/dyn_part_max_per_node.q.out
, Looks like this has broken the trunk. See, https://builds.apache.org/job/Hive-trunk-h0.21/1397/, @Ashutosh, I can't seem to reproduce the problem in any of my environments.

@Carl, do you have any ideas about why this test might be failing?, Let's continue this discussion in HIVE-2984., 超过默认值1000了，重新设置以后不生效，是因为conf没有生效，可找到hive.java这个文件进行修改, Integrated in Hive-trunk-hadoop2 #54 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/54/])
    HIVE-2918. Hive Dynamic Partition Insert - move task not considering 'hive.exec.max.dynamic.partitions' from CLI. (cwsteinbach via kevinwilfong) (Revision 1330417)

     Result = ABORTED
kevinwilfong : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1330417
Files : 
* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
* /hive/trunk/ql/src/test/queries/clientnegative/dyn_part_max.q
* /hive/trunk/ql/src/test/queries/clientnegative/dyn_part_max_per_node.q
* /hive/trunk/ql/src/test/results/clientnegative/dyn_part_max.q.out
* /hive/trunk/ql/src/test/results/clientnegative/dyn_part_max_per_node.q.out
, This issue is fixed and released as part of 0.10.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.]