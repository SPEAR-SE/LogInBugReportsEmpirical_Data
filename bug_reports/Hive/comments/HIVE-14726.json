[Stack trace:
{code}
 (SessionState.java:printError(948)) - FAILED: IndexOutOfBoundsException Index: 8, Size: 2
java.lang.IndexOutOfBoundsException: Index: 8, Size: 2
        at java.util.ArrayList.rangeCheck(ArrayList.java:653)
        at java.util.ArrayList.get(ArrayList.java:429)
        at org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer$SortedDynamicPartitionProc.getPositionsToExprNodes(SortedDynPartitionOptimizer.java:588)
        at org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer$SortedDynamicPartitionProc.process(SortedDynPartitionOptimizer.java:217)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
        at org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.transform(SortedDynPartitionOptimizer.java:104)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:207)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10466)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10116)
        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:61)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)
        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnal
yzer.java:316)
        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:100)
{code}, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12827683/HIVE-14726.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10545 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynpart_sort_optimization_acid]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats0]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[acid_bucket_pruning]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1136/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1136/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-1136/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12827683 - PreCommit-HIVE-MASTER-Build, [~jcamachorodriguez] Can you take a quick look? Seemingly related dynpart_sort_optimization_acid.q is failing on master since last 15 builds., [~ashutoshc], could you elaborate why there is no advantage of running the optimization for update and delete? It seems we used to optimize these cases (l187 in the original SortedDynPartitionOptimizer class). If we apply the patch attached, we should remove the code in l187., Reason where this optimization is useful when there are large number of partitions being written. That time there are many orc writers which are buffering data. Update and delete statement gets rewritten as insert statement but they won't have this issue of many dynamic partitions being written into at same time. So, I am not sure why there is a handling of Update/Delete at place you pointed out, we should simply skip this optimization step in those cases. I will take another look and update the patch accordingly., On taking a second look it seems that delete/update statements can also be affected by wrath of orc buffers. e.g., for the query in test case RS sort and shuffle key is ROW__ID and partition column is in value of RS. If I understand correctly in Reducers FileSink operator need to have an orc buffer open for each partition. This may result in similar OOM cases. However, I am not sure if its possible to solve that problem through this optimization for delete/update statement. [~ekoifman] [~prasanth_j] any suggestions?, Update/Delete can certainly write many partitions at the same time, e.g.  Update T set c1=c1+1 where c2 > 100.
This may match 1 row in each partition and thus the Insert statement that is actually executed will write to each partition.  Same for Delete.

Since what is actually executed is an Insert statement, why is Update/Delete different from any other Insert?, There's no info on which query the stack trace came from.  [~deepesh], do you have the query and any information on the table?

Agreed that we don't want to blindly turn spdo off here., I think hive.optimize.sort.dynamic.partition was made to work with ACID update/delete in HIVE-13849 and HIVE-13750, [~alangates] test case in patch has a query which fails without fix, as far as I can tell it started failing here https://builds.apache.org/view/H-L/view/Hive/job/PreCommit-HIVE-MASTER-Build/1123/ which was for HIVE-14710, though I don't know how that could cause it, Does Jenkins print Commit hash for each run?  This test passed in run 1122 but failed in 1123.  cc [~sershe], I don't know what's causing dynpart_sort_optimization_acid.q to fail, but I don't think it's the same thing since we don't see the IndexOutOfBounds exception.  , I think the issue is that SortDynPartitionOptimizer is using the bucket column numbers to index into the array of columns.  UpdateDeleteSemanticAnalyzer is rewriting a delete query into an insert that contains only partition columns (see around line 150ff).  This means the bucket columns aren't in the rewritten query.  The bucket id is present since it's part of the ROW__ID, but we may need to write special code in SDPO to fetch it from there rather than look for the column., Thanks [~alangates] for looking into it. So, we begin with {{ROW__ID}} in sort and shuffle key and partition column as value of RS. In this optimization if we change RS sort and shuffle key to ROW__ID and partition column, we need not to keep orc buffer open and thus avoid OOM case. But is that ok with ACID writer in FileSink or it assumes sort and shuffle keys as {{ROW__ID}} and nothing else?, every update/delete statement in dynpart_sort_optimization_acid.q has "key" which is the bucketing column in the WHERE clause.  Perhaps that's why this works.

For ACID data only needs to be sorted in each bucket by ROW__ID.
Thus if we sort globally by partition/bucket/ROW__ID, acid would still work., Updated patch to make SPDO work with update/delete when bucket column is not in select list., Hmm, so was it related to HIVE-14710 thru some slight DB logic change, or was it just a coincidence?, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12828079/HIVE-14726.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10547 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[dynpart_sort_optimization_acid]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats0]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[acid_bucket_pruning]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1155/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1155/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-1155/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12828079 - PreCommit-HIVE-MASTER-Build, coincidence, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12828135/HIVE-14726.2.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10546 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats0]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[acid_bucket_pruning]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testConnections
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMethodCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1160/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-MASTER-Build/1160/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-MASTER-Build-1160/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12828135 - PreCommit-HIVE-MASTER-Build, Pushed to master. Thanks, Jesus for review!, [~ashutoshc] should this be in 2.1.1 as well?]