[I've added a quick fix on our cluster. The fix just erases mapred_input_dir from environment of child process.
ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java:
         ProcessBuilder pb = new ProcessBuilder(wrappedCmdArgs);
         Map<String, String> env = pb.environment();
         addJobConfToEnvironment(hconf, env);
+
+        LOG.info("HIVE-2372. HOTFIX. Removing mapred_input_dir from environment variables");
+        env.remove("mapred_input_dir");
+
         env.put(safeEnvVarName(HiveConf.ConfVars.HIVEALIAS.varname), String
             .valueOf(alias));

All queries are work fine now. If anyone proposes more correct decision (add a property which will disable this property), I'll try to make code changes and prepare a patch., Sergey Tryuber, can you put a patch?, Instead of simply removing the key, we should truncate the value to a shorter one if it is too long., Siying, what the limit we should set on this property? Limiting it to 132KB won't work on kernels prior to 2.6.23, because there are other properties exist and sum limit will exceed 132KB threshold for those kernels. May be we could introduce on more property that (if set to true) will remove the key?, I'm thinking a very short limit, like 1024 chars or so. A longer chars won't get user any information., I am surprised no one has ran into this sooner. I had assumed the limit was much shorter. Streaming is 'a neat hack' but with the UDF/UDAF frameworks I have never been convinced it is needed. Most often I see use it improperly to make hacky map side joins etc. Maybe your real problem is having to many input files and you should merge your input first, or use bucketing instead of two level partitioning.   , Edward, firstly, I also was the same problems/questions on other forums, unsolved. The issue is that we need hourly based access to our data in HDFS. Sometimes, we just need to process couple of ours quickly, and, sometimes, we need to process several monthes of data. We don't use "hacky map side joins". Our custom reducer performs only aggregations (quite complicated), nothing more.
Hive manages all our workflow quite well. Actually, this still be our only problem and we hope to use Hive later on.
Ok, what's the final decision: cut this option's lenght to 1,5,10KB? Or imply another option which enables this option removing from environment variables?, We are running into the same problem here, my list of input files is even larger. In my case we are using a TRANSFORM() python script to convert the output of the reducers to csv format. I confirmed that lowering the number of input files works by specifying a limited partition or merging the inputs. Either way, I don't think it should pass that entire list through environment variables. I can go with an option to remove that., I just wanted to note, that in my case where I use a stream script for TRANSFORM, which only happens at the very end, a workaround was to simply force it to have 2 stages so that by the last stage, the input list has been merged. I understand that REDUCE scripts will work differently. 

Also +1 on Sergey Triuber quick fix. But might not be a solid solution. Thanks., is there anyone still working on this?, That's a good question, if anyone official wants to vote on a patch or Sergey Tryuber HOTFIX. 

After applying the HOTFIX and running "ant test" I didn't see any issues. And this definitely fixed our current problems with the limitation. +1 Thanks., Well, guys, give me a week to prepare a patch (just have no time right now) that will truncate this variable to 20KB if string length is too large (I'm going to hardcode this value) and prints WARN to log if such thing happend. , Patch, 1st version, I've attached a patch (as an attachment, not by "submit patch", as described on wiki HowToContribute). When I cloned trunk and run tests without any changes, for about 5 hours, there was several test errors((( Build and testing with my changes showed the same errors count. So, please, review this patch and make remarks., Is 2048 per value too long?, Limitation in my patch is 20KB, I don't think, that smaller limit is good idea, because in some cases user might be really want to set long values. Actually, in some cases 2KB (as you proposed) is even not enough for for "hive.query.string". But, of course, if you insist, I'll set limitation to 1KB, just let me know about it., How about make it configurable? (though I hate to add more and more parameters.), That's was my initial idea (you even can see that in my comments). But after that I've changed my mind because I also "hate to add more and more parameters". This bug affects may be 0.001% of Hive users and I can't figure out the case of those 0.001% users when 20KB is not enough. Make this limit less then 20KB also has no sense, because that's not a bottleneck on modern environment. So, introducing one more useless option, prepare documentation for it and compel new Hive users to read/understand it... , As a hive user affected by this bug, I would have no problem with truncating the variable to 20KB. That should be enough to give you an idea of the input list. 2KB sounds low to me., Lets get this committed. I think we should introduce the variables. In the normal case we want someone to get all the input they would expect. If they run into an exception they can turn the variable on to get out of the problem. 

Lets do this. Rebase your patch and add hive.scriptoperator.truncate.env=true|false to control this feature. Default it to false which would be how hive works now. I will review and commit. , Ok, got your point. Edward, could you answer some questions:
1. As I understood from sources, there is no hive-default.xml (it is deprecated)? 
2. I'm going to add one more entry to HiveConf#ConfVars. Is that right way? 
3. Where is place (in code base) for inserting comments for properties, or it just hive wiki? , 1. There is a hive-default.xml.template you should add it there along with a short description
2. Yes adding a ConfVar is the right way
3. You can add some inline comments if you wish although generally if the property name is chosen correctly it usually explains itself., Patch that has hive.script.operator.truncate.env option that enables truncation., Edward, I've named my option hive.script.operator.truncate.env (instead of hive.scriptoperator.truncate.env as you proposed), to be analogous of hive.script.operator.id.env.var. Actually, I have a question, why do we have hive-default.xml.template if all options (their names and default values) are hardcoded in ConfVar?

Also, as I wrote in previous posts, there was some Hive tests failures in my "ant test" before. I just want to share this information with other developers: this problem was solved by changing locale in my system from Russian to EN. So, I have all tests successfully completed.

And the last notice. Edward, could you add *.iml files to gitignore. I use Idea as IDE and it creates all those files., I will take take a look at this. You should mark as match available when you are ready for review., +1 tests pass will commit, Integrated in Hive-trunk-h0.21 #1450 (See [https://builds.apache.org/job/Hive-trunk-h0.21/1450/])
    HIVE-2372 Argument list too long when streaming (Sergey Tryuber via egc) (Revision 1342841)

     Result = FAILURE
ecapriolo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1342841
Files : 
* /hive/trunk/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
* /hive/trunk/conf/hive-default.xml.template
* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
* /hive/trunk/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
, Integrated in Hive-trunk-hadoop2 #54 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/54/])
    HIVE-2372 Argument list too long when streaming (Sergey Tryuber via egc) (Revision 1342841)

     Result = ABORTED
ecapriolo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1342841
Files : 
* /hive/trunk/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
* /hive/trunk/conf/hive-default.xml.template
* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
* /hive/trunk/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java
, This issue is fixed and released as part of 0.10.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira., While creating a table with a large number of columns, a large hive variable is temporarily created using SET, the variable contains the columns and column descriptions.

A CREATE TABLE statement then successfully uses that large variable.

After successfully creating the table the hive script attempts to load data into the table using a TRANSFORM script, triggering the error:
java.io.IOException: error=7, Argument list too long

Since the variable is no longer used after the table is created, the hive script was updated to SET the large variable to empty.
After setting the variable empty the second statement in the hive script ran fine., Hi Ryan,

Yes, your issue is very related. Hive passes properties to TRANSFORM script via environment variables. In the scope of this ticket I've shortened only environment variable which stores information about partitions.

In case of user-defined variables (via SET statement), I'm not even sure that approach to shorten them is correct. May be it would be better just to fail with error before map-reduce job execution and ask user to unset the variable (as you did). But it is quite hard to judge what is length limitation, because it depends on OS (even in my patch, as I remember, I hardcoded the length and now it seems to be not the best choice). As an alternative, Hive can print a warning, but continue the execution. 

Anyway, this issue had been closed so much time ago (and applied patch really solves the problem in issue description) that I think it would be better to create a new one and lead all the discussion there. Don't you mind to do it?, Thanks Sergey, HIVE-7218 created for continued tracking]