[

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12791872/HIVE-13223.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 9847 tests executed
*Failed tests:*
{noformat}
TestSparkCliDriver-timestamp_lazy.q-bucketsortoptimize_insert_4.q-date_udf.q-and-12-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_orc_merge5
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_orc_merge6
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join1
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join4
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.testGetPartitionSpecs_WithAndWithoutPartitionGrouping
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7201/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7201/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7201/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12791872 - PreCommit-HIVE-TRUNK-Build, nit: both equals could be done once outside of the loop., Addressed Sergey's comment., I created an empty ORC file and ran orcfiledump on it. It threw exception

{code}
create table concat_incompat(key string, value string) stored as orc;
insert overwrite table concat_incompat select * from src where key > 1000; // return 0 rows

hive --orcfiledump file:///app/warehouse/concat_incompat/000000_0
Processing data file file:/app/warehouse/concat_incompat/000000_0 [length: 0]
Exception in thread "main" java.lang.IndexOutOfBoundsException
	at java.nio.Buffer.checkIndex(Buffer.java:540)
	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:510)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:361)
	at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:83)
	at org.apache.hadoop.hive.ql.io.orc.FileDump.getReader(FileDump.java:239)
	at org.apache.hadoop.hive.ql.io.orc.FileDump.printMetaDataImpl(FileDump.java:312)
	at org.apache.hadoop.hive.ql.io.orc.FileDump.printMetaData(FileDump.java:291)
	at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{code}, +1 pending tests. [~prasanth_j] wrong JIRA? :), AFAIK, filedump worked before this empty file bucket changes (HIVE-13040).  , You should test if it did and file a separate JIRA assigned to [~ashutoshc] if so ;), Ashutosh, Sergey, Is this patch ready to go in?  

Should have searched for this before debugging the recently hanging tests myself..
, REeattaching the patch for HiveQA, looks like it didn't get picked up, This patch is not ready. I think bug is in Spark itself, which is if you submit spark job with 0 splits, spark executors just hang. This got exposed by HIVE-13040 after which we were generating such jobs which was in turn effect of not generating splits for 0-length files. Note, MR & Tez dont have this issue. In this patch, I tried to generate splits even for 0-length files (by not skipping them) but that breaks later at job execution time because ORC reader is not resilient to 0-length file. 
To fix this issue we need to either figure out and fix Spark hang issue or extend Orc reader to handle 0-length file more gracefully (those failures were exposed in last Hive QA run), 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12795278/HIVE-13223.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 9941 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_orc_merge5
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_orc_merge6
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join1
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver_vector_outer_join4
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_vector_varchar_simple
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_temp_table
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorization_short_regress
org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFMonthString
org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFMonthTimestamp
org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFYearString
org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.testVectorUDFYearTimestamp
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7388/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7388/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7388/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12795278 - PreCommit-HIVE-TRUNK-Build, Hi [~ashutoshc] , sorry as I was not following HIVE-13040 and I couldnt see any description in that JIRA, what is the use case that leads to generating job on split or 0 splits and/or 0-length files?  Is it empty buckets?  How were empty buckets handled before HIVE-13040?  Thanks., HIVE-13040 has two changes. One is to not generate 0-length files when its known that they don't contain any data and other is to skip 0-length file while generating splits. This second change may result in a job which has no split e.g. when buckets are empty. For exact repro, you may run any of the failures reported by HiveQA here, e.g. orc_merge5,orc_merge6,vector_outer_join1,vector_outer_join4 etc., We can of course ask, and of course hanging is not a good behavior, but Spark community might consider a job with 0 splits as invalid.  It has always been not so-great behavior on Hive side to have empty files in buckets (I guess HIVE-12638 is a similar complaint).

Just to explore the options, is there any JIRA tracking ORC-side fix for handle 0-length files?, bq. Just to explore the options, is there any JIRA tracking ORC-side fix for handle 0-length files?
None that I am am aware of., [~prasanth_j] do you think this is something ORC can do feasibly?  Thanks, ORC reader handles only non-orc files correctly by throwing FileFormatException. The reader itself does not handle 0 length files. The way it's handled currently is OrcInputFormat just ignores 0 length files from split computation as it knows that it cannot be valid orc file. Also there are filters to prune hidden and _* files which are also not valid orc files. So ORC reader expects only valid ORC files. I think it should be handled at both places (split generation and reader) as both can be used together or independently. I can add a check to ORC reader to throw exception when 0 length files are encountered. , Thanks for reply, but I guess it will not help this case?

bq. I can add a check to ORC reader to throw exception when 0 length files are encountered., Throwing exception is going to fail these tests. I think the correct behaviour is to throw exception for invalid files. To read rows out of ORC, first we need to create Reader which reads footer, metadata, stripe information etc.. It is at this phase we have to fail as we cannot read footers out of invalid files. Only after successful creation of Reader we can create RecordReaders. If there are no rows then hasNext() will return false immediately. We won't be able to create RecordReaders with invalid files., For text formats, this makes sense as we don't have to read any footer/metadata to read rows out of it. Hence 0 length files can be handled just be returning false in hasNext(). But I don't think it's valid for row columnar formats that reads MAGIC bytes/footers etc.. , Fixed via HIVE-13525]