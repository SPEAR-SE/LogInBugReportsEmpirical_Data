[[~pxiong] FYI, this looks like a blocker for 2.3.0 release.

[~ngangam] Have you tried similar tests after the changes with postgres ?

cc [~aihuaxu], [~ngangam] Can you take a look? I remember you did the tests against various DBs?, I am taking a look. I have run upgrade tests with postgres as well., The problem will not reproduce on Hive tables that were present at metastore upgrade. It must be a (partitioned?) table, created after the upgrade., [~rusanu] Could you please post the full stack trace if you still have it? I do not know of these nuanes across these different DBs, so apologize if some of these questions seem obvious.

In the upgrade script, when we cast the existing String values as text, shouldnt the existing columns values also be treated the same as any new inserts? If not, is there an alternate way of making existing column values be the same format as new values during the upgrade?
{{alter table "COLUMNS_V2" alter column "TYPE_NAME" type text using cast("TYPE_NAME" as text);}}

If the JDO mapping file defines a column as CLOB, wouldnt the column type be of Clob irrespective of the underlying DB? Thanks , [~ngangam] I think the easiest is to set up a local PG metastore, inititalize it with schematool, then run the repro I attached.    

Like you, I would have expected the value to come as Clob, but is String in the debugger., [~rusanu] I have run some tests with my postgres installation running on {{Linux <snip> 2.6.32-504.16.2.el6.x86_64 #1 SMP Wed Apr 22 06:48:29 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux}} and the CLOB values inserted into the database are inline.
{code}
hive1=> \d "TABLE_PARAMS"
                      Table "public.TABLE_PARAMS"
   Column    |          Type          |            Modifiers            
-------------+------------------------+---------------------------------
 TBL_ID      | bigint                 | not null
 PARAM_KEY   | character varying(256) | not null
 PARAM_VALUE | text                   | default NULL::character varying
Indexes:
    "TABLE_PARAMS_pkey" PRIMARY KEY, btree ("TBL_ID", "PARAM_KEY")
    "TABLE_PARAMS_N49" btree ("TBL_ID")
Foreign-key constraints:
    "TABLE_PARAMS_TBL_ID_fkey" FOREIGN KEY ("TBL_ID") REFERENCES "TBLS"("TBL_ID") DEFERRABLE

hive1=> select "PARAM_VALUE" from "TABLE_PARAMS";
1
 true
 0
 46055
 1494964727
 0
 1
 46069
 true
 1494964728
 0
 0
 1
 true
 15812
 1494964729
 0
 0
 1494964729
 STRING,myField2 : STRING, myField3 : STRING, myField4 : STRING, myField5 : STRING, myField6 : STRING, myField7 : STRING, myField8 : STRING, myField9 : STRING, myField10 : STRING, myField11 : STRING, my
Field12 : STRING, myField13 : STRING, myField14 : STRING, myField15 : STRING, myField16 : STRING, myField17 : STRING, myField18 : STRING, myField19 : STRING, myField20 : STRING, myField21 : STRING, myFi
eld22 : STRING, myField23 : STRING, myField24 : STRING, myField25 : STRING, 
{code}

The last value has been snipped for brevity but it is a long value beyond the prior 4000 character limit. I am not sure why my "text" values are inline whereas your values show an integer. Could this be a database config setting? Thanks, What PG and PG client version do you have?, I am at 8.4.20 for both server and client., the last value reported above for column {{PARAM_VALUE}} is roughly 21k in length. Is your value much larger?
{code}
hive1=> select max(length("PARAM_VALUE")) from "TABLE_PARAMS";
  max  
-------
 20879
(1 row)

hive1=> select version();
                                                      version                                                      
-------------------------------------------------------------------------------------------------------------------
 PostgreSQL 8.4.20 on x86_64-redhat-linux-gnu, compiled by GCC gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-18), 64-bit
(1 row)
{code}, For me it happens with even tiny strings, like {{INT}}:
{code}
metastore=# select *, CAST(lo_get(CAST("TYPE_NAME" as bigint)) as TEXT) from "COLUMNS_V2"  LIMIT 1;
CD_ID | COMMENT | COLUMN_NAME | TYPE_NAME | INTEGER_IDX |  lo_get
-------+---------+-------------+-----------+-------------+----------
     2 |         | customer    | 21664     |           0 | \x696e74
{code}

Can you tell me what JDBC Driver do you use? My settings are:
{code}
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:postgresql://localhost:5432/metastore</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
    </property>
{code}
I think the classpath resolves the driver to {{postgresql-9.3-1102-jdbc3.jar}}. The PG server itself is 9.6.2:
{code}
rrusanu=# select version();
                                                    version
----------------------------------------------------------------------------------------------------------------
 PostgreSQL 9.6.2 on x86_64-apple-darwin15.6.0, compiled by Apple LLVM version 8.0.0 (clang-800.0.42.1), 64-bit
(1 row)
{code}

, BTW, to show {{PARAM_VALUE}} value in PG:
{code}
metastore=# select "PARAM_VALUE", lo_get(cast("PARAM_VALUE" as INT)) from "TABLE_PARAMS" limit 3;
 PARAM_VALUE |         lo_get
-------------+------------------------
 21665       | \x31343934333230383636
 21742       | \x30
 21743       | \x30
(3 rows)
{code}, I only have these 2 jars for the driver. It has to be one of them {{postgresql-9.0-801.jdbc4.jar}} and {{postgresql-9.1-901.jdbc4.jar}}. They are exactly the same size, so I am assuming they are the same.

{code}
hive> CREATE TABLE srcpart (key STRING COMMENT 'default', value STRING COMMENT 'default') PARTITIONED BY (ds STRING, hr STRING) STORED AS TEXTFILE;
OK
Time taken: 13.775 seconds
hive> LOAD DATA LOCAL INPATH "/Users/ngangam/apache/hive/data/files/kv1.txt" OVERWRITE INTO TABLE srcpart PARTITION (ds="2008-04-09",hr="11");
Loading data to table default.srcpart partition (ds=2008-04-09, hr=11)
OK
Time taken: 132.365 seconds
hive> select * from srcpart;
OK
238	val_238	2008-04-09	11
86	val_86	2008-04-09	11
311	val_311	2008-04-09	11
27	val_27	2008-04-09	11
165	val_165	2008-04-09	11
409	val_409	2008-04-09	11
....
200	val_200	2008-04-09	11
97	val_97	2008-04-09	11
Time taken: 17.693 seconds, Fetched: 500 row(s)
{code}

My hive-site.xml has the following .. should be the same as yours as far as connection params and a driver are concerned. 
{code}
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive1</value>
  </property>
{code}
, [~rusanu] Thank you for your continued help. 
I dont quite understand the need to use {{lo_get}} on a text column. For me, its working without any transformation. Could you please elaborate? Thanks, [~rusanu] I created a new database {{apachehive}} as a new user {{apachehive}} within postgres using {{create user apachehive createdb createuser password 'apachehive'; create database apachehive owner apachehive}}. I then installed the hive schema for 3.0.0. 
I can now see that the text columns are storing integers.
{code}
apachehive=# select * from "TBLS";
 TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME |  OWNER  | RETENTION | SD_ID |     TBL_NAME     |    TBL_TYPE    | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT | IS_REWRITE_ENABLED 
--------+-------------+-------+------------------+---------+-----------+-------+------------------+----------------+--------------------+--------------------+--------------------
      1 |  1495055598 |     1 |                0 | ngangam |         0 |     1 | srcpart          | MANAGED_TABLE  |                    |                    | f
      2 |  1495055786 |     1 |                0 | ngangam |         0 |     3 | largeserdeparams | EXTERNAL_TABLE |                    |                    | f
      3 |  1495055859 |     1 |                0 | ngangam |         0 |     4 | largetblparams   | EXTERNAL_TABLE |                    |                    | f
(3 rows)

apachehive=# select * from "TABLE_PARAMS" where "TBL_ID"=2;
 TBL_ID |       PARAM_KEY       | PARAM_VALUE 
--------+-----------------------+-------------
      2 | EXTERNAL              | 24881
      2 | transient_lastDdlTime | 24880
(2 rows)
{code}

However, I am able to retrieve the values correctly using both JDO and DirectSQL, without any changes to the code. Please see the results and the hive CLI output in the attached files. I am wondering if somehow your JDO mapping file does NOT reflect this change to CLOB so JDO layer is treating it as a String while the value is CLOB. 

Would you be able to retry by starting with a fresh DB and installing just 3.0.0 schema (instead of upgrading from an older hive release)? In my prior tests, I was using an upgraded schema (initially installed hive1.1.0 schema and then upgraded to hive3.0.0 using the upgrade scripts). In that database, the {{text}} values for columns still seem to be stored in-line. I am wondering if this was due to the fact that it was initally a {{character}} type and then converted to {{text}}.
Thanks, [~ngangam] Thanks! I Can you please run the repro I attached originally?
{code}
CREATE TABLE srcpart (key STRING COMMENT 'default', value STRING COMMENT 'default') PARTITIONED BY (ds STRING, hr STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH "${hiveconf:test.data.dir}/kv1.txt" OVERWRITE INTO TABLE srcpart PARTITION (ds="2008-04-09", hr="11");
select * from srcpart;
{code}
There is a reason for this specific repro. If simply look at any CLOB field, like {{TABLE_PARAMS.PARAM_VALUE}}, then this field may well be loaded by JDO, via the ObjectStore. JDO knows how to handle this field appropriately. But my repro triggers a code path which goes through the [MetasoreDirectSql.getPartitionsFromPartitionIds|https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java#L787]:
{code}
   // Get FieldSchema stuff if any.
    if (!colss.isEmpty()) {
      // We are skipping the CDS table here, as it seems to be totally useless.
      queryText = "select \"CD_ID\", \"COMMENT\", \"COLUMN_NAME\", \"TYPE_NAME\""
          + " from \"COLUMNS_V2\" where \"CD_ID\" in (" + colIds + ") and \"INTEGER_IDX\" >= 0"
          + " order by \"CD_ID\" asc, \"INTEGER_IDX\" asc";
      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {
        @Override
        public void apply(List<FieldSchema> t, Object[] fields) {
          t.add(new FieldSchema((String)fields[2], extractSqlClob(fields[3]), (String)fields[1]));
        }});
    }
{code}
This particular code is the one I'm reporting the problem on. For me, this does not handle Clobs appropriately and reads the lob handle value instead of the lob content.

, [~rusanu] I did run the exact repro you provided. Thats the first test in the attached HiveCLIOutput.txt file., Can you confirm with a debugger that that line is being executed? I want to figure out whether is a different Hive setting that causes this code not to run in your case, or the line is being executed but behave differently (loads the lobs as strings), which would point toward a difference in Env/PG/JDO/Driver., I will use a standalone JDBC client to be able to play around with this., I am able to reproduce the issue now. Nothing has changed since last night except I rebuilt the product from scratch this morning. So the HiveConf changes for it to use DirectSQL might not have picked up last night.

{code}
hive> CREATE TABLE srcpart (key STRING COMMENT 'default', value STRING COMMENT 'default') PARTITIONED BY (ds STRING, hr STRING) STORED AS TEXTFILE;
OK
Time taken: 11.637 seconds
hive> LOAD DATA LOCAL INPATH "/Users/ngangam/THIRD/apache/hive/data/files/kv1.txt" OVERWRITE INTO TABLE srcpart PARTITION (ds="2008-04-09",hr="11");
Loading data to table default.srcpart partition (ds=2008-04-09, hr=11)
OK
Time taken: 132.753 seconds
hive> select * from srcpart;
OK
Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Error: type expected at the position 0 of '25200:25201' but '25200' is found.
Time taken: 24.4 seconds
hive> 
{code}

Let me debug thru this and see if I can come up with a fix. So from ur experience, how do we determine if text is being stored in-line or out-of-line by the database?, While debugging this I found that there is a relevant JDO driver setting: {{DatastoreAdapter.CLOB_SET_USING_SETSTRING}}, that controls this, see [ClobRDBMSMapping.setString|https://github.com/datanucleus/datanucleus-rdbms/blob/master/src/main/java/org/datanucleus/store/rdbms/mapping/datastore/ClobRDBMSMapping.java#L60]:
{code}
    public void setString(PreparedStatement ps, int param, String value)
    {
        if (getDatastoreAdapter().supportsOption(DatastoreAdapter.CLOB_SET_USING_SETSTRING))
        {
            super.setString(ps, param ,value);
        }
        else
        {
            setObject(ps, param, value);
        }
    }
...
    public String getString(ResultSet rs, int param)
    {
        if (getDatastoreAdapter().supportsOption(DatastoreAdapter.CLOB_SET_USING_SETSTRING))
        {
            return super.getString(rs, param);
        }
        return (String) getObject(rs, param);
    }
{code}

However, I could not find any way to *configure* this. It is pre-set for [MySQL Adapter|https://github.com/datanucleus/datanucleus-rdbms/blob/master/src/main/java/org/datanucleus/store/rdbms/adapter/MySQLAdapter.java#L119], but not for PG. I don't know if the connection URL/string can somehow set this preference/setting. My experience with Datanucleus is rather limited., I have tried a couple of means to force PosgresSQL to store the values inline instead of TOASTed values via OIDs.  According to their documentation, setting storage to {{PLAIN || MAIN}} should force the DB to store them in-line.

{code}
The TOAST code recognizes four different strategies for storing TOAST-able columns:
PLAIN prevents either compression or out-of-line storage; furthermore it disables use of single-byte headers for varlena types. This is the only possible strategy for columns of non-TOAST-able data types.

EXTENDED allows both compression and out-of-line storage. This is the default for most TOAST-able data types. Compression will be attempted first, then out-of-line storage if the row is still too big.

EXTERNAL allows out-of-line storage but not compression. Use of EXTERNAL will make substring operations on wide text and bytea columns faster (at the penalty of increased storage space) because these operations are optimized to fetch only the required parts of the out-of-line value when it is not compressed.

MAIN allows compression but not out-of-line storage. (Actually, out-of-line storage will still be performed for such columns, but only as a last resort when there is no other way to make the row small enough.)
{code}
Even with either of the setting, I cannot get it to store these values in-line.

I am still researching .. based on a hint in a usergroups, I found this
{code}
select * from "COLUMNS_V2";
CD_ID | COMMENT | COLUMN_NAME | TYPE_NAME | INTEGER_IDX 
-------+---------+-------------+-----------+-------------
    1 | default | key        | 27118    |          0
    1 | default | value      | 27119    |          1
(2 rows)

select "CD_ID", "COMMENT", "COLUMN_NAME", convert_from(loread(lo_open("TYPE_NAME"::int, x'40000'::int), x'40000'::int),  'UTF8') as "TYPE_NAME" from "COLUMNS_V2" where "CD_ID" in (1) and "INTEGER_IDX" >= 0 order by "CD_ID" asc, "INTEGER_IDX" asc;
CD_ID | COMMENT | COLUMN_NAME | TYPE_NAME 
-------+---------+-------------+-----------
    1 | default | key        | string
    1 | default | value      | string
{code}

While the above conversion works fine in native client, the same does not work via hive/JDO. However, the overall request to getPartitions succeeds because it falls back to using datanucleus when DirectSQL fails.

{{2017-05-19T11:24:46,604  WARN [pool-7-thread-2] metastore.MetaStoreDirectSql: Getting partitions:query=select "CD_ID", "COMMENT", "COLUMN_NAME", convert_from(loread(lo_open("TYPE_NAME"::int, x'40000'::int), x'40000'::int),  'UTF8') as "TYPE_NAME" from "COLUMNS_V2" where "CD_ID" in (1) and "INTEGER_IDX" >= 0 order by "CD_ID" asc, "INTEGER_IDX" asc

2017-05-19T11:24:46,605  WARN [pool-7-thread-2] metastore.ObjectStore: Falling back to ORM path due to direct SQL failure (this is not an error): SQL query "select "CD_ID", "COMMENT", "COLUMN_NAME", convert_from(loread(lo_open("TYPE_NAME"::int, x'40000'::int), x'40000'::int),  'UTF8') as "TYPE_NAME" from "COLUMNS_V2" where "CD_ID" in (1) and "INTEGER_IDX" >= 0 order by "CD_ID" asc, "INTEGER_IDX" asc" requires 3 parameters yet none have been supplied at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:636) at }}

, I have looked thru the code to see if there is a way to set {{CLOB_SET_USING_SETSTRING}}. There is no API to add to {{supportedOptions}} list. and like you said it is only enabled for MySQL., [~rusanu] I have tried a couple of means to change the query in direct sql to get it to read thru the {{text}} columns. They work via native postgresql client {{psql}} but fail via JDO. I have also tried to alter the table definition to force in-line storage of the column {{plain || main}} value but it seems to have no impact on how it is actually persisted. This is going to need a bit more research.

Unblocking the release is now a priority. Until we can figure out a proper solution, we can fix it so that extractSqlClob throws an exception if the value is not clob type and that should get it to retry using ORM. I have test this fix locally and seems to be working. Thoughts?, Attaching a patch that would allow the fallback to ORM for Clob datatypes, so the pre-commits gets run to ensure there are no test failures. This should unblock the release., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12869329/HIVE-16667.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 10742 tests passed

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5386/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5386/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5386/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12869329 - PreCommit-HIVE-Build, I don't see how your patch will address the problem of mixing  pre v3.0 upgraded tables and post v.30 created one. the new ones will store the {{TYPE_NAME}} as a LOB handle, while the upgraded ones will have the inlined string value.

Can we explore instead reverting the column type mapping to {{VARCHAR}} in the package.jdo, (or {{LONGVARCHAR}} if need be) instead of {{CLOB}}? Keep the metastore upgrade scripts, change the underlying storage tyopes to respective large types, but have JDO map them to String, not Clob. On my testing with PG, this works correctly on all cases I tested (my repro, your large serde DDL from HIVE-12274), and should handle upgrade correctly. But I did not test this with other metastore engines, Oracle, MySQL etc., [~rusanu] I have had a chance to test with VARCHAR in the JDO mappings file with Oracle, Postgres and Derby. Seems to be working without any issues.
I am uploading the patch with the changes. Could you please review it when you get a chance? Thanks

{code}
2017-06-06T15:50:41,716 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 280.51028ms + 0.084425ms, the query is [select "PARTITIONS"."PART_ID" from "PARTITIONS"  inner join "TBLS" on "PARTITIONS"."TBL_ID" = "TBLS"."TBL_ID"     and "TBLS"."TBL_NAME" = ?   inner join "DBS" on "TBLS"."DB_ID" = "DBS"."DB_ID"      and "DBS"."NAME" = ? ]
2017-06-06T15:50:41,858 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 141.674264ms + 0.379869ms, the query is [select "PARTITIONS"."PART_ID", "SDS"."SD_ID", "SDS"."CD_ID", "SERDES"."SERDE_ID", "PARTITIONS"."CREATE_TIME", "PARTITIONS"."LAST_ACCESS_TIME", "SDS"."INPUT_FORMAT", "SDS"."IS_COMPRESSED", "SDS"."IS_STOREDASSUBDIRECTORIES", "SDS"."LOCATION", "SDS"."NUM_BUCKETS", "SDS"."OUTPUT_FORMAT", "SERDES"."NAME", "SERDES"."SLIB" from "PARTITIONS"  left outer join "SDS" on "PARTITIONS"."SD_ID" = "SDS"."SD_ID"   left outer join "SERDES" on "SDS"."SERDE_ID" = "SERDES"."SERDE_ID" where "PART_ID" in (1) order by "PART_NAME" asc]
2017-06-06T15:50:42,002 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 142.592725ms + 0.294649ms, the query is [select "PART_ID", "PARAM_KEY", "PARAM_VALUE" from "PARTITION_PARAMS" where "PART_ID" in (1) and "PARAM_KEY" is not null order by "PART_ID" asc]
2017-06-06T15:50:42,142 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 139.308419ms + 0.229592ms, the query is [select "PART_ID", "PART_KEY_VAL" from "PARTITION_KEY_VALS" where "PART_ID" in (1) and "INTEGER_IDX" >= 0 order by "PART_ID" asc, "INTEGER_IDX" asc]
2017-06-06T15:50:42,281 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 139.136081ms + 0.092378ms, the query is [select "SD_ID", "PARAM_KEY", "PARAM_VALUE" from "SD_PARAMS" where "SD_ID" in (2) and "PARAM_KEY" is not null order by "SD_ID" asc]
2017-06-06T15:50:42,423 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 141.601391ms + 0.08076ms, the query is [select "SD_ID", "COLUMN_NAME", "SORT_COLS"."ORDER" from "SORT_COLS" where "SD_ID" in (2) and "INTEGER_IDX" >= 0 order by "SD_ID" asc, "INTEGER_IDX" asc]
2017-06-06T15:50:42,564 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 140.216113ms + 0.114448ms, the query is [select "SD_ID", "BUCKET_COL_NAME" from "BUCKETING_COLS" where "SD_ID" in (2) and "INTEGER_IDX" >= 0 order by "SD_ID" asc, "INTEGER_IDX" asc]
2017-06-06T15:50:42,703 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 139.086542ms + 0.094296ms, the query is [select "SD_ID", "SKEWED_COL_NAME" from "SKEWED_COL_NAMES" where "SD_ID" in (2) and "INTEGER_IDX" >= 0 order by "SD_ID" asc, "INTEGER_IDX" asc]
2017-06-06T15:50:42,847 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 142.786353ms + 0.433393ms, the query is [select "CD_ID", "COMMENT", "COLUMN_NAME", "TYPE_NAME" from "COLUMNS_V2" where "CD_ID" in (1) and "INTEGER_IDX" >= 0 order by "CD_ID" asc, "INTEGER_IDX" asc]
2017-06-06T15:50:42,990 DEBUG [pool-7-thread-4] metastore.MetaStoreDirectSql: Direct SQL query in 142.115654m
{code}, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12871687/HIVE-16667.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 264 failed/errored test(s), 6622 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_index] (batchId=228)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=1)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=10)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=11)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=13)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=14)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=16)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=17)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=18)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=2)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=20)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=21)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=22)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=23)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=24)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=25)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=26)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=27)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=28)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=29)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=30)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=31)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=32)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=33)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=34)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=35)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=36)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=4)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=40)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=41)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=43)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=44)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=45)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=46)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=48)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=49)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=50)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=52)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=53)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=55)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=56)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=58)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=59)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=6)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=62)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=63)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=65)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=66)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=67)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=68)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=69)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=7)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=71)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=73)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=74)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=75)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=76)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=77)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=78)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=79)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=8)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=80)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=81)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=82)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=83)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=84)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=85)
org.apache.hadoop.hive.cli.TestCliDriver.org.apache.hadoop.hive.cli.TestCliDriver (batchId=9)
org.apache.hadoop.hive.cli.TestCompareCliDriver.org.apache.hadoop.hive.cli.TestCompareCliDriver (batchId=233)
org.apache.hadoop.hive.cli.TestContribCliDriver.testCliDriver[serde_s3] (batchId=231)
org.apache.hadoop.hive.cli.TestContribNegativeCliDriver.org.apache.hadoop.hive.cli.TestContribNegativeCliDriver (batchId=234)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=154)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=155)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver (batchId=161)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=169)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver (batchId=98)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver (batchId=99)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.org.apache.hadoop.hive.cli.TestNegativeCliDriver (batchId=88)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.org.apache.hadoop.hive.cli.TestNegativeCliDriver (batchId=89)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.org.apache.hadoop.hive.cli.TestNegativeCliDriver (batchId=90)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query78] (batchId=232)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=100)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=101)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=102)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=103)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=104)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=105)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=106)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=108)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=109)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=111)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=112)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=113)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=114)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=115)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=118)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=120)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=122)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=124)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=125)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=127)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=128)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=129)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=130)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=131)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=132)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=134)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=135)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=136)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=137)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=138)
org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=239)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[ambiguous_join_col] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[duplicate_alias] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[garbage] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[insert_wrong_number_columns] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[invalid_create_table] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[invalid_dot] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[invalid_function_param2] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[invalid_index] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[invalid_select] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[macro_reserved_word] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[missing_overwrite] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[nonkey_groupby] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[quoted_string] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_column1] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_column2] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_column3] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_column4] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_column5] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_column6] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_function1] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_function2] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_function3] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_function4] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_table1] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[unknown_table2] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[wrong_distinct1] (batchId=236)
org.apache.hadoop.hive.ql.parse.TestParseNegativeDriver.testCliDriver[wrong_distinct2] (batchId=236)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testColumnarStorePushdown (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testColumnarStorePushdown2 (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testConvertBooleanToInt (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testDatePartitionPushUp (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testGetInputBytes (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testProjectionsBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testReadDataBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testReadDataPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testReadMissingPartitionBasicNeg (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testReadPartitionedBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testSchemaLoadBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testSchemaLoadComplex (batchId=180)
org.apache.hive.hcatalog.pig.TestAvroHCatLoader.testSchemaLoadPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testColumnarStorePushdown (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testColumnarStorePushdown2 (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testConvertBooleanToInt (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testDatePartitionPushUp (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testGetInputBytes (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testProjectionsBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadMissingPartitionBasicNeg (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadPartitionedBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testSchemaLoadBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testSchemaLoadComplex (batchId=180)
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testSchemaLoadPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestParquetHCatLoader.testColumnarStorePushdown (batchId=180)
org.apache.hive.hcatalog.pig.TestParquetHCatLoader.testConvertBooleanToInt (batchId=180)
org.apache.hive.hcatalog.pig.TestParquetHCatLoader.testGetInputBytes (batchId=180)
org.apache.hive.hcatalog.pig.TestParquetHCatLoader.testReadDataPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestParquetHCatLoader.testSchemaLoadBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestParquetHCatLoader.testSchemaLoadComplex (batchId=180)
org.apache.hive.hcatalog.pig.TestParquetHCatLoader.testSchemaLoadPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testColumnarStorePushdown (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testColumnarStorePushdown2 (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testConvertBooleanToInt (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testDatePartitionPushUp (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testGetInputBytes (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testProjectionsBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testReadDataBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testReadDataPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testReadMissingPartitionBasicNeg (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testReadPartitionedBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testSchemaLoadBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testSchemaLoadComplex (batchId=180)
org.apache.hive.hcatalog.pig.TestRCFileHCatLoader.testSchemaLoadPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testColumnarStorePushdown (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testColumnarStorePushdown2 (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testConvertBooleanToInt (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testDatePartitionPushUp (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testGetInputBytes (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testProjectionsBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testReadDataBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testReadDataPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testReadMissingPartitionBasicNeg (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testReadPartitionedBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testSchemaLoadBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testSchemaLoadComplex (batchId=180)
org.apache.hive.hcatalog.pig.TestSequenceFileHCatLoader.testSchemaLoadPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testColumnarStorePushdown (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testColumnarStorePushdown2 (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testConvertBooleanToInt (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testDatePartitionPushUp (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testGetInputBytes (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testProjectionsBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testReadDataBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testReadDataPrimitiveTypes (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testReadMissingPartitionBasicNeg (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testReadPartitionedBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testSchemaLoadBasic (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testSchemaLoadComplex (batchId=180)
org.apache.hive.hcatalog.pig.TestTextFileHCatLoader.testSchemaLoadPrimitiveTypes (batchId=180)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.testDataTypes (batchId=226)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5555/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5555/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5555/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 264 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12871687 - PreCommit-HIVE-Build, The unit test failures are related to the patch. I had not specified VARCHAR length in the mapping hoping the datanucleus would map it max length permitted for {{java.lang.String}}. Based on the error message in the test failures {{Caused by: java.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR '{"BASIC_STATS":"true","COLUMN_STATS":{"c_bigint":"true","c_b&' to length 255}}
it appears that it is attempting to limit to 255 which is too short. I then tried testing with max length which also caused problems with DERBY as it has a checks the lengh
https://fossies.org/dox/db-derby-10.13.1.1-src/interfaceorg_1_1apache_1_1derby_1_1iapi_1_1reference_1_1Limits.html#a4e223303a7751ae27dfdd8e62e993588

So for I think we are kinda limited to ~32k for max value. As this is an improvement compared to where we were at and only a limitation for derby, we should use this value for this release and fix the postgres issue in the next release.

I am uploading a patch with a length in the JDO mapping file. , 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12871754/HIVE-16667.3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10820 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query78] (batchId=232)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5563/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5563/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5563/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12871754 - PreCommit-HIVE-Build, These failures do not appear to be related to the patch. The prior build also has these failures. [~rusanu] Could you please review the patch when you get a chance? Thanks, LGTM

+1

Thanks!, Fixed with https://git1-us-west.apache.org/repos/asf?p=hive.git;a=commit;h=5861b6af52839794c18f5aa686c24aabdb737b93

Note that any PostgreSQL metastore DB created between {{b3462503ec6cc6aebb375c30d9295e59411a4ea7}} and {{5861b6af52839794c18f5aa686c24aabdb737b93}} will have to be recreated as it contains invalid values (IDs). , [~ngangam], you committed this to branch-2.3 as well as master, so the fix version should include 2.3.0.  (Are you also going to commit it to branch-2 for 2.4.0+?)

See commit 8563794b4e7f4175773a70af4145032dfcc6454e., [~leftylev] I intend to commit this to branch-2 as well for 2.3. I have seeing build issues with branch-2 which I have been able to resolve. I will commit it today. Thanks, Thanks Naveen, and sorry for pestering you.  ;), Hive 3.0.0 has been released so closing this jira.]