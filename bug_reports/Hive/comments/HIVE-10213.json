[

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12709312/HIVE-10213.1.patch

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 8700 tests executed
*Failed tests:*
{noformat}
TestMinimrCliDriver-smb_mapjoin_8.q - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter_partitioned
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3281/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3281/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3281/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12709312 - PreCommit-HIVE-TRUNK-Build, The borked tests are unrelated, I think. , This patch set off some warning flags for me with regards to the traditional M-R usecase, but it's because it's been a while since I looked at this piece of code. The traditional M-R usecase is still fine, because the DynamicPartitionFileRecordWriterContainer.close() will register an appropriate TaskCommitterProxy, and a commit on the OutputCommitter will be called in the same process scope, thus making it okay. For pig-based optimizations also, it'd continue to be okay as the singleton retains it in memory.

+1, and I'm okay with committing this patch as-is, tests have already run on this, and this section of code has not changed since then., Committed to 1.2 and master. Thanks, Mithun!, Thanks for the commit, [~sushanth].

I just verified this fix again with the YHive-13 and trunk, using the attached program. (The code reshuffles the specified 'source' data into a differently partitioned 'target' table, using dynamic partitioning.)

Here's the exception-trace for the bug. I've verified that the partitioning happens correctly with this patch applied:

{code}
Error: java.io.IOException: No callback registered for TaskAttemptID:attempt_1428474791204_201112_m_000000_0@hdfs://crystalmyth.myth.net:8020/tmp/myth/mythdb/foobar_partitioned_dt_grid/_DYN0.6055391511914422/dt=__HIVE_DEFAULT_PARTITION__/grid=__HIVE_DEFAULT_PARTITION__
        at org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry.commitTask(TaskCommitContextRegistry.java:74)
        at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitTask(FileOutputCommitterContainer.java:143)
        at org.apache.hadoop.mapred.Task.commit(Task.java:1163)
        at org.apache.hadoop.mapred.Task.done(Task.java:1025)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:345)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
{code}, This issue has been fixed and released as part of the 1.2.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.]