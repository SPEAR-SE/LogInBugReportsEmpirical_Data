[[~sseth] Could you do a quick review?

The patch contains:
- New test case to verify new nodes with resued IPs are not destroyed by performing the background check (see TestCloudExecutionContextProvider)
- Remove the destroyed node from the terminatedHosts set (See CloudExecutionContextProvider), Hi [~spena], thank you for working on this. Is this the root cause for recent failures in precommitï¼Ÿ, It could be. This is one of 2 issues found. The other is due to a possible thread leak on tomcat. However, the thread leak has been there for a while without causing other issues. I need to dig more on this one, and try to reproduce it.

For more context, we recently moved the ptest server from Amazon to Google Compute. Amazon was giving us different IPs all the time, but GCE is giving us only IPs on the range of 10.0.0.x; so only 255 possible values, and reusing those is happening too often., [~brocknoland] [~szehon] Could you take a look at this patch? We moved ptest from AWS to GCE, and I started seeing ptest killing nodes that are still in use., +1

I think Brock wrote this original code so he might know more, but yes it does look like a bug to me.  Only small comment is, you can annotate this method with @VisibleForTesting annotation., Looks like performBackgroundWork is meant to clean up nodes for which previous destroy invocations may have failed. If we're committing this change, that entire thread can be removed. Also the tracking variable mTerminatedHosts.

The intent of the background work could be achieved by ensuring IPs in mTerminatedHosts don't exist in the live host list (mLiveHosts).
, [~spena] Any reason to not commit this? I think we might be hitting this again., This is not an issue anymore. I thought it was killing nodes with the same IP that weren't part of Hive ptest, but during my tests and more understanding of the code I found that Hive ptest was getting those IPs related only to Hive ptest labels, so this is not an issue.

I will close the jira., Btw, why do you think we are hitting this issue again? If so, then maybe this is related? , In some of the recent test runs some batches are timing out (ofcourse randomly but rarely). I looked into log of one such failure and found it contains following:
{code}
2017-11-18T10:30:01,231  WARN [Fetcher_O {Map_1} #0] orderedgrouped.FetcherOrderedGrouped: Failed to connect to hive-ptest-slaves-aff.c.gcp-hive-upstream.internal:0 with 1 inputs
java.io.IOException: Failed to connect to http://hive-ptest-slaves-aff.c.gcp-hive-upstream.internal:0/mapOutput?job=job_1511029513075_0001&dag=203&reduce=0&map=attempt_1511029513075_0001_203_00_000000_0_11576, #connectionFailures=3
        at org.apache.tez.http.HttpConnection.connect(HttpConnection.java:168) ~[tez-runtime-library-0.9.1-SNAPSHOT.jar:0.9.1-SNAPSHOT]
{code}

Above suggested to me that some slaves went away in middle of test execution resulting in those time outs., [~szita] I see you fixed this issue on HIVE-18263, can I close this issue as duplicate or this issue still exists?, Hi [~spena], as I can see it, ptest executions seem to have stabilised, they take around 75 mins. I think this ticket can be closed.]