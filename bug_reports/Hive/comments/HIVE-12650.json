[Hi [~vanzin], any idea on this?, Hi [~lirui], since application master in the context of Hive on Spark takes a container from yarn. In a busy cluster, spark-submit may wait up to spark.yarn.am.waitTime to launch the master. On the other hand, Hive waits for  hive.spark.client.server.connect.timeout  before declaring that the remote driver is not connecting back. If the latter is less than the former, it's possible that Hive prematurely disconnects, causing an unstable condition. [~JoyoungZhang@gmail.com] had a description of the problem in the user list.

I think we need at least to make hive.spark.client.server.connect.timeout greater than spark.yarn.am.waitTime by default. To further guard against the problem, Hive can increase hive.spark.client.server.connect.timeout automatically based on the value of spark.yarn.am.waitTime;

[~vanzin], please share your thoughts as well., {{spark.yarn.am.waitTime}} is not the time Spark waits for the master to launch. It's the time the Spark AM waits for the SparkContext to be created after the AM has been launched.

That being said, it's ok for the Hive timeout to be larger. 90s already seems like a really long time to wait, so I doubt the extra 30s will help, but it won't hurt., Thanks for the clarification, [~vanzin]. I agree with you. Do you know what factors (such as a lack of available executors) might make Spark AM wait for SparkContext to be initialized for longer period of time (say, a minute)? The problem seems to be that Hive times out first while the AM still appears running, waiting for the context to be initialized. It will eventually fail either the context gets initialized for timeout occurs. This might look a bit confusing. I'm think if we make Hive waits longer than that, then we can avoid the scenario. Any further thoughts?
, bq.  Do you know what factors (such as a lack of available executors) might make Spark AM wait for SparkContext to be initialized for longer period of time (say, a minute)?

The only factor is possible problems in the user's {{main}} method, since that's the code that creates the SparkContext. The AM container is *already running* at that time, so it can't really fail for not being able to allocate the container..., Thanks guys for your inputs. My understanding is that {{hive.spark.client.server.connect.timeout}} is the timeout between RPC server and client handshake. In {{RemoteDriver}}, RPC client is created before SparkContext. And if {{spark.yarn.am.waitTime}} is the timeout waiting for SparkContext to be created, maybe it won't help here. I mean we can try increasing {{hive.spark.client.server.connect.timeout}}, but according to something else.
BTW, is it possible the timeout is caused by the schedule delay within yarn? Is the issue only encountered with yarn-cluster?, Here is the log that provided the the JIRA creator:
{code}
Logs of Application_1448873753366_121022 as follows(same as application_1448873753366_121055):
Container: container_1448873753366_121022_03_000001 on 10.226.136.122_8041
============================================================================
LogType: stderr
LogLength: 4664
Log Contents:
Please use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future
Please use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future
15/12/09 16:29:45 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]
15/12/09 16:29:46 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1448873753366_121022_000003
15/12/09 16:29:47 INFO spark.SecurityManager: Changing view acls to: mqq
15/12/09 16:29:47 INFO spark.SecurityManager: Changing modify acls to: mqq
15/12/09 16:29:47 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mqq); users with modify permissions: Set(mqq)
15/12/09 16:29:47 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
15/12/09 16:29:47 INFO yarn.ApplicationMaster: Waiting for spark context initialization
15/12/09 16:29:47 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
15/12/09 16:29:47 INFO client.RemoteDriver: Connecting to: 10.179.12.140:38842
15/12/09 16:29:48 WARN rpc.Rpc: Invalid log level null, reverting to default.
15/12/09 16:29:48 ERROR yarn.ApplicationMaster: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.
java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)
        at org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:156)
        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:483)
Caused by: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.
        at org.apache.hive.spark.client.rpc.Rpc$SaslClientHandler.dispose(Rpc.java:449)
        at org.apache.hive.spark.client.rpc.SaslHandler.channelInactive(SaslHandler.java:90)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
        at org.apache.hive.spark.client.rpc.KryoMessageCodec.channelInactive(KryoMessageCodec.java:127)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:233)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:219)
        at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:769)
        at io.netty.channel.AbstractChannel$AbstractUnsafe$5.run(AbstractChannel.java:567)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:380)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
        at java.lang.Thread.run(Thread.java:745)
15/12/09 16:29:48 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.)
15/12/09 16:29:57 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 150000 ms. Please check earlier log output for errors. Failing the application.
15/12/09 16:29:57 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished.)
15/12/09 16:29:57 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1448873753366_121022
15/12/09 16:29:57 INFO util.Utils: Shutdown hook called

LogType: stdout
LogLength: 216
Log Contents:
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option UseCompressedStrings; support was removed in 7.0
{code}

The interesting part of the log is:
{code}
15/12/09 16:29:57 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 150000 ms. Please check earlier log output for errors. Failing the application.
{code}

I have shared with the thread in the user mailing list for reference via email., I'm specially interested in case where Hive calls spark-submit to submit the application while there is no container available. I'm not sure if spark-submit will wait. If it does, then Hive can time out first before the AM starts to run., Hi [~xuefuz], the exception you posted doesn't seem to be a timeout, at least it's not related to {{hive.spark.client.server.connect.timeout}}, because the elapsed time is much less than 90s. I found the code that prints the log you mentioned:
{code}
      while (sparkContextRef.get() == null && System.currentTimeMillis < deadline && !finished) {
        logInfo("Waiting for spark context initialization ... ")
        sparkContextRef.wait(10000L)
      }

      val sparkContext = sparkContextRef.get()
      if (sparkContext == null) {
        logError(("SparkContext did not initialize after waiting for %d ms. Please check earlier"
          + " log output for errors. Failing the application.").format(totalWaitTime))
      }
{code}
You can see the while loop can exit either on timeout or finished being set to true. Since time elapsed is short, it must because user thread (RemoteDriver) has finished abnormally:
{code}
    val userThread = new Thread {
      override def run() {
        try {
          mainMethod.invoke(null, userArgs.toArray)
          finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
          logDebug("Done running users class")
        } catch {
          case e: InvocationTargetException =>
            e.getCause match {
              case _: InterruptedException =>
                // Reporter thread can interrupt to stop user class
              case SparkUserAppException(exitCode) =>
                val msg = s"User application exited with status $exitCode"
                logError(msg)
                finish(FinalApplicationStatus.FAILED, exitCode, msg)
              case cause: Throwable =>
                logError("User class threw exception: " + cause, cause)
                finish(FinalApplicationStatus.FAILED,
                  ApplicationMaster.EXIT_EXCEPTION_USER_CLASS,
                  "User class threw exception: " + cause)
            }
        }
      }
    }
{code}
In conclusion, the problem here is not we timed out creating SparkContext. My guess is that something goes wrong before we create SparkContext (you can refer to the constructor of RemoteDriver). Also found another property {{hive.spark.client.connect.timeout}} which defaults to 1000ms. It's used when RemoteDriver creates RPC client so it could be related, although I'm a little confused about the difference between the 2 configurations.

Regarding your last question, I tried submitting application when no container is available. Spark-submit will wait until timeout (90s)., [~lirui], thanks for your analysis. Yeah, I saw the actually elapsed time is very short, while the message says timeout 150s, which is very confusing.

[~vanzin], could you please explain a little bit the use of the two timeout? Also, what timeout value does spark-submit use if the application cannot be submitted?

[~JoyoungZhang@gmail.com], could you please reproduce the problem and provide more info such as hive.log?

Thanks, folks!, bq. could you please explain a little bit the use of the two timeout?

There's nothing complicated about them.

- RSC timeout: time between the RSC launching the Spark app and the Spark driver connecting back.
- Spark AM timeout: time between Spark AM launching the user's "main" method and a SparkContext being created.

Both overlap but one is not necessarily contained in the other., Thanks, [~vanzin]. I guess the question is the difference between the follow two (both defined in Hive):
1. hive.spark.client.connect.timeout
2. hive.spark.client.server.connect.timeout

The second question is: what's the timeout value that spark-submit uses in case of no available containers?, * hive.spark.client.connect.timeout

That's the socket connect timeout when the driver connects to the RSC server. Equivalent to this:
http://docs.oracle.com/javase/7/docs/api/java/net/Socket.html#connect(java.net.SocketAddress,%20int)

* hive.spark.client.server.connect.timeout

That's the timeout explained in my previous comment.

* what's the timeout value that spark-submit uses in case of no available containers?

I don't believe there is one., Thanks, [~vanzin].

If there is no timeout in spark-submit (wait indefinitely), I'm wondering what happens if the cluster is busy. Here is my speculation. Hive will time out first (also corresponding to Rui's observation), but spark-submit will continue to run. If a container becomes available, Spark AM will start and connect to Hive. Hive of course refuses. Then, AM will error out.

I'm not sure if this what the user experienced. It would be good if we can cancel the submit. However, it doesn't look too bad even if we decide to live with it.

Unless [~JoyoungZhang@gmail.com] can provide more info, it doesn't seem we can do much here., bq. Regarding your last question, I tried submitting application when no container is available. Spark-submit will wait until timeout (90s).
Sorry this comment is misleading. Actually I mean hive will timeout after 90s. But after this, we'll interrupt the driver thread:
{code}
    try {
      // The RPC server will take care of timeouts here.
      this.driverRpc = rpcServer.registerClient(clientId, secret, protocol).get();
    } catch (Throwable e) {
      LOG.warn("Error while waiting for client to connect.", e);
      driverThread.interrupt();
      try {
        driverThread.join();
      } catch (InterruptedException ie) {
        // Give up.
        LOG.debug("Interrupted before driver thread was finished.");
      }
      throw Throwables.propagate(e);
    }
{code}
which in turn will destroy the SparkSubmit process:
{code}
        public void run() {
          try {
            int exitCode = child.waitFor();
            if (exitCode != 0) {
              rpcServer.cancelClient(clientId, "Child process exited before connecting back");
              LOG.warn("Child process exited with code {}.", exitCode);
            }
          } catch (InterruptedException ie) {
            LOG.warn("Waiting thread interrupted, killing child process.");
            Thread.interrupted();
            child.destroy();
          } catch (Exception e) {
            LOG.warn("Exception while waiting for child process.", e);
          }
        }
{code}
So on my machine, after the timeout, SparkSubmit is terminated.
I think the {{Client closed before SASL negotiation finished.}} exception is worth investigating and should be root cause here., Hi [~lirui], thanks for the info. It's good that spark-submit is killed when Hive times out. Now the user's problem seems more interesting, though we cannot do much unless we have more information.

"Client closed before SASL negotiation finished" could be caused by the fact that AM tries to connect back to Hive, but Hive has already timed out. While Spark-submit is killed, is possible that YARN RM still has the request which will be eventually served?
, Thanks Xuefu. Yeah I tried again and found the application is served (AM launched) and failed eventually, even after SparkSubmit is killed. Although I didn't get the AM log due to some env issue., I see. I think that's what the [~JoyoungZhang@gmail.com] experienced as well. Killing spark-submit doesn't cancel AM request. When AM is finally launched, it tries to connect back to Hive and gets refused. As a result, it quickly errors out. (However, on spark side, the message, saying "spark context initialization times out in xxx seconds", is very confusing.) I'm not sure if we can do anything here.

Nevertheless, it seems spark.yarn.am.waitTime isn't relevant after all., Hi all,
I'm sorrry reply you so late.

Yes
hive.spark.client.server.connect.timeout and spark.yarn.am.waitTime does not have any relations.
hive.spark.client.server.connect.timeout is the timeout between RPC server and client handshake.When no container is available, hive cient  will exit after hive.spark.client.server.connect.timeout.
spark.yarn.am.waitTime is the time the Spark AM waits for the SparkContext to be created after the AM has been launched.

There are two types of error log
1.Client closed before SASL negotiation finished was happened on resubmitted. See https://issues.apache.org/jira/browse/HIVE-12649.
2.Connection refused: /hiveclientip:port was happend when am tries to connect back to Hive.

Container: container_1448873753366_113453_01_000001 on 10.247.169.134_8041
============================================================================
LogType: stderr
LogLength: 3302
Log Contents:
Please use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future
Please use CMSClassUnloadingEnabled in place of CMSPermGenSweepingEnabled in the future
15/12/09 02:11:48 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]
15/12/09 02:11:48 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1448873753366_113453_000001
15/12/09 02:11:49 INFO spark.SecurityManager: Changing view acls to: mqq
15/12/09 02:11:49 INFO spark.SecurityManager: Changing modify acls to: mqq
15/12/09 02:11:49 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mqq); users with modify permissions: Set(mqq)
15/12/09 02:11:49 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
15/12/09 02:11:49 INFO yarn.ApplicationMaster: Waiting for spark context initialization
15/12/09 02:11:49 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
15/12/09 02:11:49 INFO client.RemoteDriver: Connecting to: 10.179.12.140:58013
15/12/09 02:11:49 ERROR yarn.ApplicationMaster: User class threw exception: java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013
java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)
        at org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:156)
        at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:556)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:483)
Caused by: java.net.ConnectException: Connection refused: /10.179.12.140:58013
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
        at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
        at java.lang.Thread.run(Thread.java:745)
15/12/09 02:11:49 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.util.concurrent.ExecutionException: java.net.ConnectException: Connection refused: /10.179.12.140:58013)
15/12/09 02:11:59 ERROR yarn.ApplicationMaster: SparkContext did not initialize after waiting for 150000 ms. Please check earlier log output for errors. Failing the application.
15/12/09 02:11:59 INFO util.Utils: Shutdown hook called, Encountered the issue based on Apache Hive 2.0.0.

BTW, suggest to change the issue title to a new one to avoid confusion. E.g. "Spark-submit is killed when Hive times out.  Killing spark-submit doesn't cancel AM request. When AM is finally launched, it tries to connect back to Hive and gets refused.". Thanks., The timeout is necessary in case the RSC crashes due to some errors. But the issue here shows that it could also because the RSC is just waiting for resources from a busy cluster. I think we need a way to distinguish these two scenarios and don't timeout on the latter., Here're my findings so far (for yarn-client mode).

# If the cluster has no resources available, the {{RemoteDriver}} is blocked at creating the SparkContext. Rpc has been set up at this point. So {{SparkClientImpl}} believes that driver is up.
# There're 2 points where hive can get timeout. If we enable container pre-warm, we'll timeout at {{RemoteHiveSparkClient#createRemoteClient#getExecutorCount}}. If pre-warm is off, we'll timeout at {{RemoteSparkJobMonitor#startMonitor}}. Both are because SparkContext is not created and RemoteDriver can't respond to requests. For the latter, the job status remains {{SENT}} until timeout. Ideally it should be {{QUEUE}} instead. My understanding is that the Rpc handler is blocked at RemoteDriver for {{ADD JAR/FILE}} calls, which we submitted before the real job.
# Currently YARN doesn't timeout a starving application, which means the app will eventually get served. Refer to YARN-3813 and YARN-2266.

Based on these findings, I think we have to decide whether hive should timeout in this situation. Waiting is reasonable for busy clusters. But on the other hand, it seems difficult to tell whether we're blocked for lack of resources. I'm not sure if spark has such facilities for it. For yarn-cluster mode, this may be even more difficult because RemoteDriver is not running in that case and we'll have less information.
What do you guys think?, Since Hive cannot really differentiate the scenarios, I'm not sure if there is anything we can do better except for better error message and documentation. RPC timeout is necessary due to network.

On a side note, Yarn queues are more appropriate for solving the starvation problem. The problem here seems more like an uncommon scenario., Regarding better error message, do you think we can throw a timeout exception if SparkContext is not up after certain amount of time? Otherwise user only gets a timeout on the future and doesn't know the cause. On the other hand, this means adding another property and I think it only works for yarn-client., I was thinking of just improving the current message, maybe by naming different possibilities when a timeout occurs. The new timeout you mentioned doesn't seem very helpful as yarn-cluster is what we recommend., I think the difficult part is that we really don't know the possible reasons. Anyway all we get is a timeout, it could be due to network issue, exceptions, or the RSC is just busy.

Another possible refinement is that we can make the behavior more consistent. Like I said, there're now 2 paths that can lead to timeout/failure and user will see different error messages. How about remove the timeout at {{RemoteHiveSparkClient#createRemoteClient#getExecutorCount}}? I mean after certain amount of time, we can give up the pre-warm and eventually fail the job at job monitor., Thanks, Rui. I think it's fine to list all possible causes in an error message when we don't actually know the exact one. We can also suggest user where to look further (such as yarn logs).

I understand that prewarming containers complicates the things a bit, but I'm not sure of your proposal. Could you provide a patch showing the changes you have in mind?, Assigned this to me and upload a patch.
The main change in the patch is that we don't error out when timeout pre-warming. I think it makes sense because we already allow timeout in pre-warm so it shouldn't be a fatal error.
The patch also adds some explanations in error messages so it should be more user friendly.

One thing I noticed when I tested the patch is that, with yarn-client mode, we may end up with hanging spark AM trying to connect to the driver that has already timed out. But I think we have to live with that and the AM will eventually give up and exit., Update patch to also improve messages in yarn-cluster mode. Here's the summary of behaviors under these two modes.
||   ||Error users will see||Will spark-submit be killed after timeout||
|yarn-cluster|Failed to create spark client|Y|
|yarn-client|Job hasn't been submitted|N|

I think the bottom line here is that when the starving app gets served, the aborted query won't be executed so that resources won't be wasted., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12795584/HIVE-12650.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 9888 tests executed
*Failed tests:*
{noformat}
TestSparkCliDriver-groupby3_map.q-sample2.q-auto_join14.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-groupby_map_ppr_multi_distinct.q-table_access_keys_stats.q-groupby4_noskew.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-join_rc.q-insert1.q-vectorized_rcfile_columnar.q-and-12-more - did not produce a TEST-*.xml file
TestSparkCliDriver-ppd_join4.q-join9.q-ppd_join3.q-and-12-more - did not produce a TEST-*.xml file
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7402/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/7402/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-7402/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12795584 - PreCommit-HIVE-TRUNK-Build, I tried several failed tests locally and they were not reproduced.
[~xuefuz] would you mind take a look at the patch when you have time? Thanks., +1. Patch looks good to me. Thanks for working on this, Rui!, Committed to master. Thanks Xuefu for the review and guys for the discussions.]