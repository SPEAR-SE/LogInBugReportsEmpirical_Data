[Hi [~cabot],
{code}
replication = (short) Math.max(minReplication, numOfPartitions);
{code}
{{numOfPartitions}} is {{fs.getDefaultReplication(path)}}, and {{minReplication}} is no more than {{dfs.replication.max}}. So why it exceeds the maximum replication? And according to the [hadoop doc|https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml], {{dfs.replication}} is the default replication, not {{dfs.replication.max}} right?, @[~ruili] The point is that the current code is ignoring {{dfs.replication}} (if value is <10 = magic number) and the only known workaround is to set {{dfs.replication.max}} 
(which is not obvious until you spend a day with solving this problem...), [~cabot], the code intends to distribute the hash table to more nodes so that following tasks are more likely to get the data from local DN. In that sense, it's intended to be bigger than {{dfs.replication}}. That's why we chose the magic number 10 (not an ideal solution I agree).
However, since {{minReplication = Math.min(minReplication, dfsMaxReplication)}}, I still don't understand how the replication factor exceeds {{dfs.replication.max}} (by default 512)?, [~lirui] The 10 exceeds {{dfs.replication}} value, not {{dfs.replication.max}}.

Here is the stacktrace
{code:java}
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Requested replication factor of 10 exceeds maximum of 3 for /tmp/hive/xxxx/5db022b0-42b8-44c1-b303-823ed6c1f133/hive_2017-07-27_10-40-52_061_7060239276893628864-1/-mr-10003/HashTable-Stage-1/MapJoin-mapfile161--.hashtable/HASHTABLESINK_218-552787044 from x.x.x.x
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:1027)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2630)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2589)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:595)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:112)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:395)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy12.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy13.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1965)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1663)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:405)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:401)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:401)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:344)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:920)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:901)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:827)
	at org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.flushToFile(SparkHashTableSinkOperator.java:156)
	at org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.closeOp(SparkHashTableSinkOperator.java:97)
	... 18 more

{code}
, Hi [~cabot], that seems strange. The exception should be thrown only if the replication [exceeds|https://github.com/apache/hadoop/blob/release-2.8.0-RC3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L1125] the [maximum|https://github.com/apache/hadoop/blob/release-2.8.0-RC3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L370]. Could you provide the versions of your Hive and Hadoop?, [~lirui] We are using Cloudera distribution - 2.6.0-cdh5.8.0
According to these sources: 
https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.8.0/hadoop-hdfs-2.6.0-cdh5.8.0-sources.jar
the code looks same as in the code you linked. , [~cabot], is it possible the configurations used by the Hive job and HDFS are different? I can't think of other reasons why this may happen.
[~stakiar], [~spena] could you take a look as it happens with CDH?, {{Requested replication factor of 10 exceeds maximum of 3 for ...}} should only be thrown if the value of {{dfs.replication.max}} exceeds 10.

Hive shouldn't be setting {{dfs.replication.max}}. If you are using CM, you can check the HDFS service to see what the value for {{dfs.replication.max}} is.

You can also open a Hive session via BeeLine and run {{set dfs.replication.max;}} to see what Hive thinks the value of {{dfs.replication.max}} is., Our current Cloudera- hdfs settings, [~stakiar] [~ruili]
Please see
!dfs.replication-settings.png|HDFS Settings!

And the output of Beeline:

{code}
set dfs.replication.max;
+--------------------------+--+
|           set            |
+--------------------------+--+
| dfs.replication.max=512  |
+--------------------------+--+
{code}

, [~cabot], that confirms the value of {{dfs.replication.max}} is different for Hive and HDFS. Can you check whether your HS2 is using different configuration files? Another possible reason is the HS2 is not restarted after {{dfs.replication.max}} has changed in CM.]