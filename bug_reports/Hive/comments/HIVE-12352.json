[[~alangates] could you review please, Why are thrift and DB changes done in separate JIRA? , Can you post it on RB?, bq. Why are thrift and DB changes done in separate JIRA?
To make it easier for Eugene and I to collaborate without stomping on each other's toes., CompactionTxnHandler, line 347:
{code}
if (info.partName != null) {
  s += " and ctc_partition = '" + info.partName + "'" +
  (info.highestTxnId == null ? "" : " and ctc_txnid <= " + info.highestTxnId);
}
{code}
Shouldn't that "and ctc_txnid <=... go outside this if?  Don't you want it to apply for tables and partitions?

----------------

I don't understand this code, also in CompactionTxnHandler
{code}
638	    highWater = minOpenTxn == Long.MAX_VALUE ? highWater : minOpenTxn - 1;
639	    return new ValidCompactorTxnList(exceptions, -1, highWater);
{code}
Why is this change necessary?, yes, you are right, the HWM should apply to all tables

wrt 2nd comment:
Compactions find the smallest open txn in the working set and make sure to only compact up to that txn (exclusive). so if there are any files that include txn ids between minOpenTxn and ValidCompactorTxnList.highWatermark they would be ignored.  This txn ID is also the HWM mark for the issue in this bug.  (Longer term ValidCompactorTxnList can be refactored w/o minOpenTxn but in the short run this is the simplest way to pass compaction HWM to Worker)

, +1 for patch 2., [~ekoifman] can you make a combined patch with thrift changes and attach it for HiveQA to run? This is the last blocker for Hive 2.0 so it would be nice to commit it quickly, [~sershe] Alan will commit HIVE-12832 later today and I can commit this right after.
I would really like to get HIVE-12353 into 2.0 as well - I should have a patch tomorrow, How about a HiveQA run for this?, I'll rebase and and get it going
, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12782361/HIVE-12352.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10019 tests executed
*Failed tests:*
{noformat}
TestHWISessionManager - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_tez_union
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_import
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestSSL.testSSLVersion
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6627/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/6627/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-6627/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12782361 - PreCommit-HIVE-TRUNK-Build, committed to master https://github.com/apache/hive/commit/4935cfda78577bd63f1c4ae04a26dc307e640b6f, committed to 1.3 and 2.0]