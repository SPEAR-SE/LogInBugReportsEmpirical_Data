[Message from: "Yongqiang He" <heyongqiangict@gmail.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/598/
-----------------------------------------------------------

Review request for Hive Developers.


Summary
-------

archive is not working when multiple partitions inside one table are archived.


This addresses bug hive-1515.
    http://issues.apache.org/jira/browse/hive-1515


Diffs
-----

  trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java 982490 
  trunk/ql/src/test/queries/clientpositive/archive_2.q PRE-CREATION 
  trunk/ql/src/test/results/clientpositive/archive_2.q.out PRE-CREATION 

Diff: http://review.cloudera.org/r/598/diff


Testing
-------


Thanks,

Yongqiang


, See comments on reviewboard., Message from: "Paul Yang" <pyang@facebook.com>

-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
http://review.cloudera.org/r/598/#review853
-----------------------------------------------------------


Talked to Yongqiang offline about this one. The way that this patch attempts to fix the caching issue is to append some path information to the host so that we create a new HAR filesystem instance for different HAR files. The way that this is implemented now, a "-" and path information in added to the host e.g. har://hdfs-localhost-user--warehouse--mytable:50030... if the original were har://hdfs-localhost:50030. However, the HAR filesystem does not ignore the stuff after the second "-" and so has errors when trying to connect to the underlying filesystem. A possible fix would be to modify HiveHarFileSystem to extend the initialize() method so that the characters after the second "-" is ignored.

- Paul



, Attache a possible fix.

Talked with Namit and Paul this afternoon about this issue. Actually there is config which can disable FileSystem cache: fs.%s.impl.disable.cache . where %s is the filesystem schema, for archive, it's har.

So if you set "fs.har.impl.disable.cache" to false, the archive will automatically work. This should be the clean way to fix this issue.
In order to do this, you need to apply https://issues.apache.org/jira/browse/HADOOP-6231 if your hadoop does not include the code to disable FileSystem cache.]