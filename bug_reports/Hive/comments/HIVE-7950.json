[Going to break the bigger issue down into more manageable pieces to fix., I thought Tez 0.5 had APIs to add jars to existing session. Have we upgraded to 0.5 yet?, Nope, 0.4.1-incubating. Being able to add more resources to an existing session would certainly be preferable though.., Ah, the tez branch is on 0.5.0 (trunk is on 0.4.1). What's the lifecycle on the tez branch, is it occasionally merged into trunk?, I took a look at the tez branch to see if I could add more resources to an existing session as you described, [~sershe]. Looking at the javadoc, I feel like this patch should work, but the query still errors out when the map inside the dag fails due to missing classes.

I can see that the dag does get the extra jars localized:
{noformat}
2014-09-08 23:20:34,823 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.DAGImpl: Added additional resources : [[file:/usr/local/lib/hadoop-2.6.0-SNAPSHOT/yarn/nm-tmp/usercache/jelser/appcache/application_1410243497503_0001/container_1410243497503_0001_01_000001/accumulo-fate-1.6.0.jar, file:/usr/local/lib/hadoop-2.6.0-SNAPSHOT/yarn/nm-tmp/usercache/jelser/appcache/application_1410243497503_0001/container_1410243497503_0001_01_000001/accumulo-core-1.6.0.jar, file:/usr/local/lib/hadoop-2.6.0-SNAPSHOT/yarn/nm-tmp/usercache/jelser/appcache/application_1410243497503_0001/container_1410243497503_0001_01_000001/accumulo-trace-1.6.0.jar, file:/usr/local/lib/hadoop-2.6.0-SNAPSHOT/yarn/nm-tmp/usercache/jelser/appcache/application_1410243497503_0001/container_1410243497503_0001_01_000001/accumulo-start-1.6.0.jar, file:/usr/local/lib/hadoop-2.6.0-SNAPSHOT/yarn/nm-tmp/usercache/jelser/appcache/application_1410243497503_0001/container_1410243497503_0001_01_000001/zookeeper-3.4.6.jar]] to classpath
{noformat}

But I'm still getting a NoClassDefFoundException on a class which is in accumulo-core.jar:
{noformat}
Serialization trace:
inputFileFormatClass (org.apache.hadoop.hive.ql.plan.TableDesc)
tableDesc (org.apache.hadoop.hive.ql.plan.PartitionDesc)
aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:183)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:180)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:172)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:167)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat
Serialization trace:
inputFileFormatClass (org.apache.hadoop.hive.ql.plan.TableDesc)
tableDesc (org.apache.hadoop.hive.ql.plan.PartitionDesc)
aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork)
	at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:384)
	at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:281)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.<init>(MapRecordProcessor.java:73)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:134)
	... 12 more
Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat
Serialization trace:
inputFileFormatClass (org.apache.hadoop.hive.ql.plan.TableDesc)
tableDesc (org.apache.hadoop.hive.ql.plan.PartitionDesc)
aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672)
	at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:963)
	at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:871)
	at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:885)
	at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:352)
	... 15 more
Caused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45)
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:26)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:343)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:336)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:56)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:476)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:148)
	at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:238)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:226)
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:745)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113)
	... 30 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:32)
	... 42 more
Caused by: java.lang.NoClassDefFoundError: org/apache/accumulo/core/client/security/tokens/AuthenticationToken
	at java.lang.Class.getDeclaredFields0(Native Method)
	at java.lang.Class.privateGetDeclaredFields(Class.java:2397)
	at java.lang.Class.getDeclaredFields(Class.java:1806)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:150)
	at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.<init>(FieldSerializer.java:109)
	... 47 more
Caused by: java.lang.ClassNotFoundException: org.apache.accumulo.core.client.security.tokens.AuthenticationToken
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 52 more
{noformat}

Any idea if I'm missing some extra step here? , Ok, I figured a bit more out here. I believe that the AM *is* correctly getting the extra jars from the storage handler as expected. The subsequent errors are coming from the containers that are started to actually run the DAG (rather than the coordination from the tez AM).

The interesting part is that the patch (HIVE-7950-1.diff) which starts a brand new Session will result in a successful query. It seems like maybe Tez isn't passing along the extra resources we added to the running session (AM) in Hive along to the DAG containers to actually run the query. I have no idea at this point if this is a problem in how hive is using tez or if it's a bug in tez itself..., Can you post the sequence of things you are doing?

As in, how is the JARs getting added - is it an explicit "ADD JAR"?

Tez-0.5.0-rc1 had an issue we tackled with an API change to ship JARs differently between the AM and tasks., Sure thing, [~gopalv].

I don't actually have to do any extra {{ADD JAR}} commands. The AccumuloStorageHandler constructs a list of jars that need to be passed along to the "execution engine" (via tmpjars in the Hadoop configuration). With the 'yarn' execution.engine, this works just fine -- the resources are localized and added to the Map/Reduce containers and things are great.

When I try to run with 'tez', there are a few issues. The first is that, if there is already a TezSessionState that was already open'ed (e.g. like what is done when I just open the hive shell), it will have been started without those extra 'tmpjars' resources from the StorageHandler and the query will fail because we need those jars.

Sergey mentioned that Tez 0.5.0 had a new method that would allow more resources to be added to an already started TezClient ({{TezClient#addAppMasterLocalFiles(Map<String, LocalResource>)}}). Implementing this (in the hive-7950-tez-WIP.diff attachment), appears to have successfully added the extra jars from the StorageHandler to the DAGAppMaster, but the containers started to actually run the query are missing those extra jars.

Does that make sense?, This is related to TEZ-1469, which should provide guidance.

For reference, take a look at some of my example code

https://github.com/t3rmin4t0r/tez-broadcast-example/blob/master/src/main/java/org/notmysock/tez/BroadcastTest.java#L203

Lines 203 and 195., You're the man. That was exactly what I needed. I completely missed that I needed to add the resources to the DAG as well.

I'll clean up my changes and post and updated patch here later today after I poke/prod it some more., I have found one corner-case which I'm still trying to maneuver around. The DAG code fails if local resources are added that already exist in the "state" of extra resources that are already going to be added. The problem is that you can't get find out what resources are already set to be localized for a DAG.

I can call {{DAG#addTaskLocalFiles(Map<String, LocalResource>)}} to add resources but that will fail if any of them happen to be already loaded. This seems to be lacking WRT to Vertex which also has a {{getTaskLocalFiles()}} method. That's a Tez nit -- I can open a JIRA over there if you think that's necessary (or not already fixed upstream).

This is the actual stack I'm trying to work around:
{noformat}
org.apache.tez.dag.api.TezUncheckedException: Attempting to add duplicate resource: accumulo-fate-1.6.0.jar
	at org.apache.tez.common.TezCommonUtils.addAdditionalLocalResources(TezCommonUtils.java:307)
	at org.apache.tez.dag.api.Vertex.addTaskLocalFiles(Vertex.java:256)
	at org.apache.tez.dag.api.DAG.createDag(DAG.java:643)
	at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:372)
	at org.apache.tez.client.TezClient.submitDAG(TezClient.java:342)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:385)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:209)
        ...
{noformat}

Essentially, we have a DAG, we tried to submit it to the Session we have, but the underlying application was dead (for testing purposes, because I {{kill}}'ed it, but this happens if you just wait long enough). The code gets a {{SessionNotRunning}} exception, tries to {{closeAndOpen}} , Ugh, published too soon:

The code gets a {{SessionNotRunning}} exception, tries to {{closeAndOpen}} session, and then ultimately fails when it goes to submit the DAG. I believe I need to figure out a way to ensure the DAG doesn't have the extra local resources (from the StorageHandler) in the case where we start up a new Session, and then DAG would get the resources from that new Session (as opposed to the old session which didn't have the extra resources to begin with), but I'm not 100% sure yet., I think I finally got to the bottom of this, and it is broken with Tez-0.5.0.

TezTask needs to be altered (as described by the previous discussion) to add the necessary StorageHandler resources to the DAG using

{code}
dag.addTaskLocalFiles(localResources);
{code}

For the case of the AccumuloStorageHandler, this adds the jars necessary to connect to Accumulo to {{commonTaskLocalFiles}} in {{DAG}}. Then, {{TezTask}} will proceed to eventually submit the DAG to be run.

{code}
    try {
      // ready to start execution on the cluster
      sessionState.getSession().addAppMasterLocalFiles(resourceMap);
      dagClient = sessionState.getSession().submitDAG(dag);
    } catch (SessionNotRunning nr) {
      console.printInfo("Tez session was closed. Reopening...");

      // close the old one, but keep the tmp files around
      TezSessionPoolManager.getInstance().closeAndOpen(sessionState, this.conf);
      console.printInfo("Session re-established.");

      dagClient = sessionState.getSession().submitDAG(dag);
    }
{code}

Consider the case where we had a Session already created for the user, but the underlying application has exited, say due to a timeout. In the try block, we try to submit our DAG to run. In doing so, TezClient creates a DAGPlan from the DAG

{code}
DAGPlan dagPlan = dag.createDag(amConfig.getTezConfiguration());
{code}

When we create a {{DAGPlan}} from the {{DAG}}, we modify the {{DAG}} instance, adding the local resources to each {{Vertex}} in the {{DAG}}. Then, we identify that the underlying application has already died, and that we need to {{closeAndOpen}} a new Session. So, we get the {{SessionNotRunning}} exception, pop out to the catch block, and end up creating another {{DAGPlan}} from the {{DAG}} _that was already altered by the last attempt to submit it_.

As I'm looking at it, I don't think there's anything I can do at the Hive level to fix this because {{TezClient}} will always try to add duplicate resources to the {{Vertex}}'s in a {{DAG}} which throws an Exception and tanks the query., The basic part of the problem seems to have been solved, this is a second issue on top of the other.

That looks like a rather corner case scenario of duplicate resource handling - can you file a Tez JIRA about this?, Yeah, you're totally right. I was getting hung up on this edge case and forgot and the first issue to fix. I'll open a tez jira for the above., Patch against the tez branch which attempts to ensure that the Tez AM and the DAG both have the necessary extra local resources as required by a StorageHandler. Tried to add some tests which ensure modifications to TezTask work as expected., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12667581/HIVE-7950.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6194 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/719/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/719/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-719/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12667581, Updated patch. Needed to make a small change after getting past the Tez bug. Added some more unit tests and tried to clean things up., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12667831/HIVE-7950.3.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6200 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.miniHS2.TestHiveServer2.testConnection
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.testImpersonation
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/732/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/732/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-732/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12667831, Test failures appear unrelated to me. Can anyone give this a review for me?, Can you post and RB? Mostly nits like if arrays are null there's no need to copy, Sure thing. RB is linked., Updated patch with feedback from Sergey on RB., +1, one nit on RB, can be fixed on commit., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12669758/HIVE-7950.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6300 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.pig.TestOrcHCatLoader.testReadDataPrimitiveTypes
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/868/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/868/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-868/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12669758, Fixed your nit, Sergey. Thanks for taking the time to review -- much appreciated., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12670172/HIVE-7950.5.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6307 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/900/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/900/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-900/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12670172, committed to trunk, Thanks for your help, [~sershe]!, This has been fixed in 0.14 release. Please open new jira if you see any issues.
]