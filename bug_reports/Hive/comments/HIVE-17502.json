[There may be a bigger problem here.
I think the logic behind this exception is related to the giant try-finally in TezTask after SessionState.get() and then getTezSession is called.
In the finally, returnSession is called, which will ignore a non-pool session, but for a pool session it would put it back into the pool and reset SessionState's Tez session to null. This is not necessarily the best logic, cause maybe pool session reuse could also be allowed, but for obvious reasons (limits of the pool) holding a session while there may be other users waiting is undesirable.
However it's beside the point as, unless I'm missing something, the only way for getTezSession before the try block to get an existing _pool_  session from the SessionState, is the case where the latter used for two Hive queries in parallel (i.e. SessionState.get.getTezSession is called from one thread while another is inside of TezTask try-finally block, running some query). Let me know if it's a different scenario.
This pattern would probably break all this session setting and unsetting, which would need to be fixed with additional checks; however I think it also has other similar state-related issues and is not supported.
[~thejas] do you know if we allow parallel queries with the same SessionState?, [~sershe] Thanks for the prompt reply. I see that the SessionState.get keeps a thread's static SessionState object which gets set/unset every time a session is returned to the pool. I'm not that familiar with Hive/TezPool/TezTask session code base, but given that it is thread local, it shouldn't impact other thread's and/or client, correct?

Also, to help us understand my use case a little bit better, I will try to elaborate what I'm trying to do a little bit better.

Let's say a client C1 requests for a new session, it is handled by thread T1 in HS2, which yields session S1 from the pool. The same client C1 wants to make a parallel request and reuses S1's in the request. Now, since S1 is still being used, the session pool should skip S1 and returns S2 from the pool, given that S2 is unused. 

Since, S2 should be created by another thread T2 since T1 was busy executing the TezTask in the first request in HS2. Given that SessionState.get is thread's local to only T1, T2 should gets a different SessionState.get, which yield a different SessionState. But I think that is not the case, SessionState.get will always yield the same currently used SessionState of the client C1, which will be unset and that is problem, correct?

If that's the case, what's the best course of actions that I can take to make parallel query execution of the same client in a session pool happen?, The problem are methods in HiveSessionImpl that call  SessionState.setCurrentSessionState(sessionState), setting the same sessionState object to different threads when this pattern is used (client requesting the same Hive session from different threads).
So, the SessionState object is getting reused even though it's accessed thru a threadlocal.
My understanding was that this was not supported, so the client should open separate sessions for parallel requests. However, I'm not sure about that.
[~thejas] [~vgumashta] can you clarify on Hive session usage w/HS2?

If the same SessionState can be reused otherwise, it needs to be changed to support multiple Tez sessions., The SessionState has state for the session/jdbc connection. So its meant to be used for everything in that session.
However, it is not a class that we have worked on to ensure thread safety.

I had created a patch to disabled parallel use of single session because of safety issues, but forgot to commit it - HIVE-14247 .

[~thai.bui]
What is the purpose of sharing the same connection for different queries at same time ? 
Its not something that api's like JDBC easily allow and not something usually recommended in the RDBMS world. Sharing the session won't give you any significant advantages in terms of resource utilization in LLAP mode AFAIK.

, [~thejas] I'm not sure if we are on the same page, but my intention was to not share the same session concurrently (meaning a session being used by 2 distinct query at the same time). My intention was to allow skipping of a default session and return an unused default session when a client has made a request that contains a currently used session.

For my use case, it is Hue4 that is making the concurrent requests from the same session. In Hue4, each user can work on a notebook-like environment. Each notebook could have multiple snippets (text, or Hive query), with this model, a user could execute multiple Hive snippets in the same notebook. Thus, there's a need for this patch. Without the patch, HS2 will complain because the currently used session should have been returned to the pool before the second request was made., [~thai.bui] there's a little bit of confusion because there are 2 separate sessions, and 3 objects, involved.
There's Hive (HS2) session, related to the JDBC connection and represented by  HiveSession(Impl) and  SessionState (separate for historical reasons that are probably not relevant anymore), and then there's TezClient, that represents the Tez session and AM.
The patch addresses the reuse of the latter correctly; it skips the one in use and returns a new one instead. However, it still uses the same Hive session (incl. SessionState) in parallel, which is also not supported. 
Is it possible to change Hue to only issue one query at a time per JDBC/Hive session? It might be possible to add behavior to Hive to catch such behavior and to use/open a separate Hive session (starting with HiveSessionImpl..., etc.), but I'm not sure how practical that is and it's doesn't seen to me like a good solution. , I see what you mean. So without HiveSession(Impl) and SessionState to support reusing of sessions from the same user, the patch is not complete. Interestingly, we have been using this patch for a couple of weeks with 3-5 concurrent users each making and reusing sessions according to this logics and things were fine. I did observe a couple of weird problems but I don't thing they are related.

Anyhow, I'm happy to contribute and/or solidify the HiveSession(Impl) and/or SessionState to make them immutable and/or stateless more suitable to be reused by multiple sessions. For example, the current SessionState is bind to a thread-local static object, if it's bind to a unique SessionID locked in a database (hive metadata store?), or Zookeeper, or HDFS files, things could have been different.

It is also possible to change Hue to only issue 1 query at a time per user but the point was to go beyond that to allow a much better user experience using Hive 2 w/ LLAP + Hue 4.

Let me know what you guys think, we (our big data & analytics group at Bazaarvoice) are happy to contribute. Currently we have to build a custom hive-exec jar with this logic and deploy this jar specifically in Ambari for this to work. Both worlds are fine but I would prefer to push the patch upstream to make it official and potentially solidify HiveSession implementation incremental. For example, I think making a new HS2 config option `hive.sessions.default-session.reuse=false` could work for this patch. If the option is false (by default), the logics stay the same and an exception is thrown, if true, then the new patch logics apply, allowing avid users to have multiple sessions per user. Understandably, having too many options is confusing. If that's the case we'll just close this ticket but thanks for the discussion either way!, The config option sounds good to me, esp. if we can limit it to HS2 pool sessions that are not ever directly reused anyway. [~thejas] wdyt?
Also, do we have a list or a notion of why sessionstate/hivesessionimpl object couldn't used in parallel?
ThreadLocal is not an obstacle in itself but rather an artifact on not having a good dependency injection-type logic for most of Hive compile, similar to other globals., [~sershe] Based on our previous discussions, I've made a patch to add an option to allow default sessions reuse. Please review when you have time and let me know if there's anything I need to change. Thanks!, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12890993/HIVE-17502.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 11191 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_predicate_pushdown] (batchId=231)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_single_sourced_multi_insert] (batchId=231)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[optimize_nullscan] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_explainuser_1] (batchId=171)
org.apache.hadoop.hive.cli.TestTezPerfCliDriver.testCliDriver[query14] (batchId=239)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/7186/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/7186/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-7186/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12890993 - PreCommit-HIVE-Build, Those test failures are unrelated to my changes and it looks like all the current builds are failing with the same error., Hmm.. SessionState::getTezSession and all related methods will become invalid now, right? I wonder if they need additional correctness checks for this case.
[~thejas] any other input?, Sure, I'm happy to add additional checks, although nothing should be affected unless the new flag is turned on. I'm not sure what you mean SessionState::getTezSession and all related methods will become invalid, could you clarify?, Because currently each SessionState (Hive session) can only have one Tez session. With this patch it's possible to reuse the Hive session, utilizing several Tez sessions. So the code that gets/sets/closes/etc. the session by getting it from SessionState will become invalid I assume.

Just having a config is not good enough to allow known buggy code... 

Btw, we were wondering in some discussion, why cannot Hue just not reuse the session this way?, [~thai.bui]
Do you know why Hue uses same HiveServer2 session for multiple queries ?
I fear this is mostly kicking can down the road as we don't seem to have all cases covered here or any ongoing investment to cover those cases, that I am aware of.
In general, with JDBC,  recommendations people have seem to be to not run simultaneous queries within same jdbc connection. (Also, jdbc statement calls are blocking, so its not easy to do without multiple threads).
Is it possible to change Hue to use different sessions for each query ?
We can still get this patch in, but I just want to get the conversation going on the long term solution for this.
, 
bq. Because currently each SessionState (Hive session) can only have one Tez session. With this patch it's possible to reuse the Hive session, utilizing several Tez sessions. So the code that gets/sets/closes/etc. the session by getting it from SessionState will become invalid I assume.

Yes, it's possible to reuse the Hive session, *if and only if that session has been returned to the pool*. So in effect, 1 session is used by 1 client at a time and so SessionState only handles 1 Tez session.

What this patch does is to remove the exception being thrown when a client is making a second request using the same session (while it's still being used). In that case, the first session in the request will be *skipped*, and a new session from the pool will be returned. Thus, allowing the same client to use multiple sessions (handled by distinct threads). Since all session states are thread-local, each session request is handled by a different thread and things are fine, there's no bug AFAIK.

[~thejas] I totally understand the concern about long term support. My company has a big investment in this and we do want to work with the community to find the best solution using Hive 2 w/ LLAP + Tez. This is one of the biggest requirements for my team since it's frustrating to use Hue 4 in one-query-at-time mode. To change Hue 4 to use a different session per query is very invasive whereas it makes more sense for HiveServer2 to be more permissive and not throw an exception.

Also, I think we need to clarity the intention of this patch. It is not to use the same session for multiple queries in parallel. It will use a session for a query at a time. However, when the client tries to reuse the session (while it's still being used), a new session from the pool will be returned.

The only downside to this patch is that there will be quite a bit of orphaned sessions (since users keep requesting new sessions and leave existing sessions). However, that is taken care of easily by setting HS2 session expiration times more aggressively., Deferring this to 3.1.0 since the branch for 3.0.0 has been cut off. Please update the JIRA if you would like to get your patch in 3.0.0., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12890993/HIVE-17502.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/10109/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/10109/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-10109/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2018-04-10 07:28:05.554
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-10109/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2018-04-10 07:28:05.557
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at dcd9b59 HIVE-19146 : Delete dangling q.out
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at dcd9b59 HIVE-19146 : Delete dangling q.out
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2018-04-10 07:28:06.110
+ rm -rf ../yetus_PreCommit-HIVE-Build-10109
+ mkdir ../yetus_PreCommit-HIVE-Build-10109
+ git gc
+ cp -R . ../yetus_PreCommit-HIVE-Build-10109
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-10109/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: patch failed: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java:2425
Falling back to three-way merge...
Applied patch to 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java' with conflicts.
Going to apply patch with: git apply -p0
error: patch failed: common/src/java/org/apache/hadoop/hive/conf/HiveConf.java:2425
Falling back to three-way merge...
Applied patch to 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java' with conflicts.
U common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12890993 - PreCommit-HIVE-Build, Hi [~vgarg], I would definitely want to get this patch onto 3.0.0 if possible. At Bazaarvoice, we are currently maintain a separate branch to apply this patch and pushing this upstream would remove that effort, we are also currently testing using 3.0 in productionÂ since early this year (with Hadoop 3.0 but soon to upgrade to 3.1). If you guys need help testing the branch, we are that the perfect stage for that.

Please let me know what I need to do to get this approved & merged to 3.0.0. Thanks!

cc: [~thejas], Hi [~thai.bui]. For now we have only cut branch for hive 3.0 release and we have yet to finalize the release date. So you have at least two or more weeks to get this patch in 3.0. Once the patch is ready and pushed to master you can push it to branch-3 to make it in 3.0.

As for testing you can use branch-3 for testing in your production and report any issues you find., | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  1s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 50s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 47s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 34s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 59s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 22s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 11s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 19s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 14s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 19m 21s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-10144/dev-support/hive-personality.sh |
| git revision | master / b3fe652 |
| Default Java | 1.8.0_111 |
| modules | C: common ql U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-10144/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12918499/HIVE-17502.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 87 failed/errored test(s), 13625 tests executed
*Failed tests:*
{noformat}
TestBeeLineDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=253)
TestCopyUtils - did not produce a TEST-*.xml file (likely timed out) (batchId=231)
TestDbNotificationListener - did not produce a TEST-*.xml file (likely timed out) (batchId=247)
TestDummy - did not produce a TEST-*.xml file (likely timed out) (batchId=253)
TestHCatHiveCompatibility - did not produce a TEST-*.xml file (likely timed out) (batchId=247)
TestMiniDruidCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=253)
TestMiniDruidKafkaCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=253)
TestNegativeCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=96)
	[udf_invalid.q,authorization_uri_export.q,default_constraint_complex_default_value.q,druid_datasource2.q,check_constraint_max_length.q,view_update.q,default_partition_name.q,authorization_public_create.q,load_wrong_fileformat_rc_seq.q,default_constraint_invalid_type.q,altern1.q,describe_xpath1.q,drop_view_failure2.q,temp_table_rename.q,invalid_select_column_with_subquery.q,udf_trunc_error1.q,insert_view_failure.q,dbtxnmgr_nodbunlock.q,authorization_show_columns.q,cte_recursion.q,merge_constraint_notnull.q,load_part_nospec.q,clusterbyorderby.q,orc_type_promotion2.q,ctas_noperm_loc.q,udf_min.q,udf_instr_wrong_args_len.q,invalid_create_tbl2.q,part_col_complex_type.q,authorization_drop_db_empty.q,smb_mapjoin_14.q,subquery_scalar_multi_rows.q,alter_partition_coltype_2columns.q,subquery_corr_in_agg.q,insert_overwrite_notnull_constraint.q,authorization_show_grant_otheruser_wtab.q,regex_col_groupby.q,ptf_negative_DuplicateWindowAlias.q,exim_22_export_authfail.q,authorization_insert_noinspriv.q,udf_likeany_wrong1.q,groupby_key.q,ambiguous_col.q,groupby3_multi_distinct.q,authorization_alter_drop_ptn.q,invalid_cast_from_binary_5.q,show_create_table_does_not_exist.q,invalid_select_column.q,exim_20_managed_location_over_existing.q,interval_3.q,authorization_compile.q,join35.q,udf_concat_ws_wrong3.q,create_or_replace_view8.q,create_external_with_notnull_constraint.q,split_sample_out_of_range.q,materialized_view_no_transactional_rewrite.q,authorization_show_grant_otherrole.q,create_with_constraints_duplicate_name.q,invalid_stddev_samp_syntax.q,authorization_view_disable_cbo_7.q,autolocal1.q,avro_non_nullable_union.q,load_orc_negative_part.q,drop_view_failure1.q,columnstats_partlvl_invalid_values_autogather.q,exim_13_nonnative_import.q,alter_table_wrong_regex.q,udf_next_day_error_2.q,authorization_select.q,udf_trunc_error2.q,authorization_view_7.q,udf_format_number_wrong5.q,touch2.q,orc_type_promotion1.q,lateral_view_alias.q,show_tables_bad_db1.q,unset_table_property.q,alter_non_native.q,nvl_mismatch_type.q,load_orc_negative3.q,authorization_create_role_no_admin.q,invalid_distinct1.q,authorization_grant_server.q,orc_type_promotion3_acid.q,hms_using_serde_alter_table_update_columns.q,show_tables_bad1.q,macro_unused_parameter.q,drop_invalid_constraint3.q,drop_partition_filter_failure.q,char_pad_convert_fail3.q,exim_23_import_exist_authfail.q,drop_invalid_constraint4.q,authorization_create_macro1.q,archive1.q,subquery_multiple_cols_in_select.q,change_hive_hdfs_session_path.q,udf_trunc_error3.q,invalid_variance_syntax.q,authorization_truncate_2.q,invalid_avg_syntax.q,invalid_select_column_with_tablename.q,mm_truncate_cols.q,groupby_grouping_sets1.q,druid_location.q,groupby2_multi_distinct.q,authorization_sba_drop_table.q,dynamic_partitions_with_whitelist.q,compare_string_bigint_2.q,udf_greatest_error_2.q,authorization_view_6.q,show_tablestatus.q,duplicate_alias_in_transform_schema.q,create_with_fk_uk_same_tab.q,udtf_not_supported3.q,alter_table_constraint_invalid_fk_col2.q,udtf_not_supported1.q,dbtxnmgr_notableunlock.q,ptf_negative_InvalidValueBoundary.q,alter_table_constraint_duplicate_pk.q,udf_printf_wrong4.q,create_view_failure9.q,udf_elt_wrong_type.q,selectDistinctStarNeg_1.q,invalid_mapjoin1.q,load_stored_as_dirs.q,input1.q,udf_sort_array_wrong1.q,invalid_distinct2.q,invalid_select_fn.q,authorization_role_grant_otherrole.q,archive4.q,load_nonpart_authfail.q,recursive_view.q,authorization_view_disable_cbo_1.q,desc_failure4.q,create_not_acid.q,udf_sort_array_wrong3.q,char_pad_convert_fail0.q,udf_map_values_arg_type.q,alter_view_failure6_2.q,alter_partition_change_col_nonexist.q,update_non_acid_table.q,authorization_view_disable_cbo_5.q,ct_noperm_loc.q,interval_1.q,authorization_show_grant_otheruser_all.q,authorization_view_2.q,show_tables_bad2.q,groupby_rollup2.q,truncate_column_seqfile.q,create_view_failure5.q,authorization_create_view.q,ptf_window_boundaries.q,ctasnullcol.q,input_part0_neg_2.q,create_or_replace_view1.q,udf_max.q,exim_01_nonpart_over_loaded.q,msck_repair_1.q,orc_change_fileformat_acid.q,udf_nonexistent_resource.q,msck_repair_3.q,exim_19_external_over_existing.q,serde_regex2.q,msck_repair_2.q,exim_06_nonpart_noncompat_storage.q,illegal_partition_type4.q,udf_sort_array_by_wrong1.q,windowing_leadlag_in_udaf.q,avro_decimal.q,materialized_view_update.q,illegal_partition_type2.q,invalid_varchar_length_1.q,authorization_view_5.q,nested_complex_neg.q,lockneg_try_drop_locked_db.q,constraint_partition_columns.q,authorization_insertpart_noinspriv.q,avro_add_column_extschema.q,udf_sort_array_wrong2.q,drop_database_cascade.q,archive3.q,alter_view_failure5.q,load_orc_negative1.q,create_external_acid.q,check_constraint_temporary_udf.q,file_with_header_footer_negative.q,alter_view_failure6.q,create_view_failure6.q,char_pad_convert_fail1.q,invalid_var_samp_syntax.q,update_partition_col.q,database_create_already_exists.q,union2.q,windowing_invalid_udaf.q,authorization_public_drop.q,truncate_table_failure5.q,alter_view_failure2.q,udf_reflect_neg.q,interval_2.q,column_rename2.q,set_hiveconf_validation0.q,materialized_view_drop2.q,repl_dump_requires_admin.q,alter_view_failure7.q,alter_view_failure4.q,alter_view_failure3.q,udf_map_keys_arg_type.q,alter_partition_with_whitelist.q,authorization_show_role_principals_no_admin.q,table_nonprintable_negative.q,clusterbydistributeby.q,authorization_rolehierarchy_privs.q,alter_notnull_constraint_violation.q,check_constraint_aggregate.q,script_broken_pipe3.q,column_rename3.q,authorization_fail_create_db.q,compute_stats_long.q,sortmerge_mapjoin_mismatch_1.q,insert_into6.q,select_charliteral.q,fs_default_name2.q,check_constraint_qual_name.q,archive_multi5.q,input4.q,create_udaf_failure.q,create_table_failure1.q,regex_col_1.q,materialized_view_authorization_no_select_perm.q,decimal_precision_1.q,columnstats_partlvl_multiple_part_clause.q,udf_if_not_bool.q,materialized_view_replace_with_view.q,invalid_cast_to_binary_6.q,lockneg5.q,database_drop_not_empty.q,smb_bucketmapjoin.q,alter_rename_partition_failure2.q,invalid_cast_to_binary_4.q,decimal_precision.q,create_view_failure10.q,invalid_cast_to_binary_3.q,groupby_invalid_position.q,load_wrong_fileformat_txt_seq.q,exchange_partition_neg_partition_exists.q,authorization_not_owner_alter_tab_serdeprop.q,alter_rename_partition_failure.q,merge_negative_5.q,authorization_invalid_priv_v2.q,bucket_mapjoin_wrong_table_metadata_1.q,lockneg2.q,join28.q,exchange_partition_neg_test.q,lockneg_try_lock_db_in_use.q,udf_assert_true.q,udf_coalesce.q,join29.q,archive_partspec4.q,exim_24_import_part_authfail.q,exim_08_nonpart_noncompat_serde.q,udtf_explode_not_supported4.q,create_view_failure4.q,default_constraint_invalid_default_value_type.q,invalid_char_length_1.q,udf_in.q,create_view_failure3.q,strict_pruning_2.q,insertexternal1.q,alter_partition_change_col_dup_col.q,authorization_ctas2.q,uniquejoin.q,authorization_role_grant_nosuchrole.q,column_rename4.q,create_with_fk_wrong_ref2.q,authorization_role_case.q,udf_if_wrong_args_len.q,create_skewed_table_col_name_value_no_mismatch.q,compare_string_bigint.q,archive_insert2.q,authorization_grant_table_fail1.q,exim_02_all_part_over_overlap.q,archive_multi1.q,subquery_in_groupby.q,authorization_role_grant.q,insert_into_with_schema3.q,create_with_fk_constraint.q,materialized_view_insert.q,orc_replace_columns1.q,updateBasicStats.q,udf_likeall_wrong1.q,udf_map_keys_arg_num.q,subquery_scalar_corr_multi_rows.q,exchange_partition_neg_partition_exists2.q,udf_test_error.q,distinct_windowing_failure2.q,authorization_caseinsensitivity.q,update_bucket_col.q,create_or_replace_view3.q,lockneg_query_tbl_in_locked_db.q,authorization_select_view.q,create_or_replace_view4.q,disallow_incompatible_type_change_on2.q,ptf_negative_DistributeByOrderBy.q,desc_failure3.q,create_insert_outputformat.q,alter_table_wrong_location2.q,groupby_grouping_sets5.q,authorization_uri_create_table_ext.q,invalid_t_create2.q,archive_corrupt.q,groupby_grouping_sets6.q,subquery_corr_grandparent.q,archive_insert3.q,authorization_alter_table_exchange_partition_fail2.q,invalid_cast_from_binary_2.q,authorize_grant_public.q,authorization_fail_drop_db.q,insert_into_with_schema4.q,parquet_alter_part_table_drop_columns.q,unionSortBy.q,invalid_char_length_2.q,insert_multi_into_notnull.q,groupby_grouping_id1.q,strict_orderby_2.q,udf_next_day_error_1.q,authorize_revoke_public.q,load_part_authfail.q,drop_default_partition_filter.q,orc_replace_columns2.q,alter_file_format.q,authorization_alter_db_owner_default.q,authorization_uri_createdb.q,authorization_import_ptn.q,selectDistinctWithoutAggr.q,split_sample_wrong_format.q,authorization_fail_8.q,regex_col_2.q,ptf_negative_PartitionBySortBy.q,stats_aggregator_error_2.q,drop_table_failure1.q,stats_aggregator_error_1.q,udf_concat_ws_wrong1.q,exim_12_nonnative_export.q,clustern4.q,strict_orderby.q,authorization_insertoverwrite_nodel.q,clustern3.q,exim_18_part_spec_missing.q,invalid_t_transform.q,columnstats_tbllvl_complex_type.q,authorization_grant_table_allpriv.q,msck_repair_4.q,set_hiveconf_validation1.q,authorization_alter_table_exchange_partition_fail.q,ctas_noemptyfolder.q,clustern2.q,set_hiveconf_validation2.q,bucket_mapjoin_mismatch1.q,mm_convert.q,truncate_bucketed_column.q,druid_datasource.q,udf_assert_true2.q,strict_join_2.q,udf_format_number_wrong3.q,default_constraint_invalid_default_value_length.q,masking_acid_update.q,materialized_view_no_transactional_rewrite_2.q,materialized_view_authorization_drop_other.q,dyn_part1.q,default_constraint_invalid_default_value2.q,select_star_suffix.q,subquery_select_distinct.q,authorization_uri_create_table1.q,special_character_in_tabnames_1.q,archive_partspec2.q,analyze_non_existent_tbl.q,create_with_pk_constraints_enforced.q,orderby_position_unsupported.q,subquery_subquery_chain_exists.q,orc_reorder_columns1_acid.q,subquery_notin_implicit_gby.q,notable_alias4.q,semijoin2.q,desc_failure1.q,ptf_negative_HavingLeadWithPTF.q,alter_tableprops_external_with_default_constraint.q,create_unknown_udf_udaf.q,materialized_view_authorization_create_no_grant.q,orc_change_serde.q,uniquejoin3.q,stats_publisher_error_1.q,authorization_part.q,alter_table_constraint_invalid_fk_tbl1.q,fileformat_void_input.q,truncate_table_failure3.q,alter_table_constraint_invalid_fk_tbl2.q,mismatch_columns_insertion.q,repl_load_requires_admin.q]
TestNonCatCallsWithCatalog - did not produce a TEST-*.xml file (likely timed out) (batchId=217)
TestReplicationScenariosAcidTables - did not produce a TEST-*.xml file (likely timed out) (batchId=231)
TestReplicationScenariosAcrossInstances - did not produce a TEST-*.xml file (likely timed out) (batchId=231)
TestSequenceFileReadWrite - did not produce a TEST-*.xml file (likely timed out) (batchId=247)
TestTezPerfCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=253)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[explainuser_2] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[groupby_groupingset_bug] (batchId=174)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=163)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_1] (batchId=171)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_main] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[update_access_time_non_current_db] (batchId=172)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorized_dynamic_semijoin_reduction] (batchId=155)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=105)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.org.apache.hadoop.hive.cli.TestNegativeCliDriver (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_into_acid_notnull] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_into_notnull_constraint] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_publisher_error_2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_in_implicit_gby] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_column_list_bucketing] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_nonexistant_column] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_partition_column2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_partition_column] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_table_failure6] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udaf_invalid_place] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_add_months_error_1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_add_months_error_2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_array_contains_wrong1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_concat_ws_wrong2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_elt_wrong_args_len] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_field_wrong_args_len] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_field_wrong_type] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_format_number_wrong1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_format_number_wrong2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_format_number_wrong4] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_format_number_wrong7] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_greatest_error_1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_in_2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_instr_wrong_type] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_last_day_error_1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_last_day_error_2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_locate_wrong_args_len] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_locate_wrong_type] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_map_values_arg_num] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_printf_wrong1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_printf_wrong2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_printf_wrong3] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_qualified_name] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_size_wrong_args_len] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_sort_array_by_wrong2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_sort_array_by_wrong3] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_test_error_reduce] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_when_type_wrong] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_explode_not_supported1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_explode_not_supported2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_explode_not_supported3] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_invalid_place] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[union22] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionClusterBy] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionDistributeBy] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionLimit] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionOrderBy] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[uniquejoin2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unset_view_property] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_no_such_table] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_not_acid] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_notnull_constraint] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[windowing_after_orderby] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[windowing_ll_no_neg] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[windowing_ll_no_over] (batchId=95)
org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=255)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.org.apache.hadoop.hive.cli.TestSparkPerfCliDriver (batchId=255)
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testMetastoreVersion (batchId=227)
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMatching (batchId=227)
org.apache.hadoop.hive.ql.TestAcidOnTez.testGetSplitsLocks (batchId=228)
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1 (batchId=232)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.org.apache.hive.jdbc.TestJdbcWithMiniHS2 (batchId=242)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.testLlapInputFormatEndToEnd (batchId=238)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/10144/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/10144/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-10144/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 87 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12918499 - PreCommit-HIVE-Build, Deferring this to 3.2.0 since the branch for 3.1.0 has been cut off., | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 36s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 21s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 19s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 55s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m 34s{color} | {color:blue} common in master has 64 extant Findbugs warnings. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  4m  0s{color} | {color:blue} ql in master has 2287 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 11s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 26s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 13s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 27m 27s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-12250/dev-support/hive-personality.sh |
| git revision | master / 35cec21 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| modules | C: common ql U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-12250/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12918499/HIVE-17502.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 14633 tests passed

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/12250/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/12250/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-12250/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12918499 - PreCommit-HIVE-Build]