[Modified HiveMetaStoreChecker to load partitions directly and not through the PartitionPruner.
Technically one could modify the PartitionPruner to not check hive.mapred.mode when called from HMSChecker, but I think this solution is cleaner even if is at the cost of a bit of code duplication., Thanks [~zsombor.klara] for the patch. Took a quick look at the patch. Is the issue due to {{hive.strict.checks.large.query}} instead of {{hive.mapred.mode}}? Looks like the following code in PartitionPruner is throwing the exception which checks for a {{hive.strict.checks.large.query}} instead of the config in description.

{noformat}
if (!hasColumnExpr(prunerExpr)) {
      // If the "strict" mode is on, we have to provide partition pruner for each table.
      String error = StrictChecks.checkNoPartitionFilter(conf);
      if (error != null) {
        throw new SemanticException(error + " No partition predicate for Alias \""
            + alias + "\" Table \"" + tab.getTableName() + "\"");
      }
    }
{noformat}

Also, Can you use {{PartitionPruner.getAllPartitions}} method instead of {{hive.getAllPartitionsOf(table)}}. It uses some performance logging which can be helpful. The other thing I noticed is that with the patch method will throw a HiveException instead of SemanticException at line. May not be a issue but worth investigating I think., sorry my bad. Didn't realize {{PartitionPruner.getAllPartitions}} is a private method. May be adding some perf logging instead would help in that case., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12854226/HIVE-16024.01.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10241 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=140)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=130)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=211)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3733/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3733/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3733/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12854226 - PreCommit-HIVE-Build, Thanks for taking a look at the patch [~vihangk1].
The hive.strict.checks.large.query is driven by the setting of hive.mapred.mode. If the mapred mode is not strict then the strict checks are not performed. So we could say that both are causing the exception.
I have added the performance logging, you are right that this should be wrapping the metastore call.
I don't think however that the change in the thrown exception type is relevant. The original code before this regression was also throwing HiveException, we did not have a release with the SemanticException version and I don't see a benefit in wrapping the HiveException since I cannot provide any useful, context related information.
I am tempted a bit to make PartitionPruner#getAllPartitions public and to reuse that to reduce the code duplication, but I don't want to expose an API in the pruner that isn't pruning... what do you think?, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12854451/HIVE-16024.02.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 3 failed/errored test(s), 10258 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=140)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3758/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3758/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3758/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12854451 - PreCommit-HIVE-Build, All 3 failures are well known flaky/failing tests:
- query14: HIVE-15744
- vector_if_expr: HIVE-15776
- TestDerbyConnector: HIVE-15702, I think the second version of the code looks fine. I prefer not to expose the PartitionPruner#getAllPartitions, since the perflog wrapped Hive#getAllPartitions does the same, and it is not that much code duplication.

LGTM (non-binding), Wouldn't there OOM errors if MSCK fetches a large number of partitions? This is a typical problem in Hive when attempting to read all partitions from the Metastore. The strict mode was added for that. Should the MSCK have a restriction on that as well?

Should we use the {{PartitionIterable}} class to get all partitions in batches to avoid OOM instead?, We could do something similar to what the {{DDLTask.dropTable}} method does?

{code}
    int partitionBatchSize = HiveConf.getIntVar(conf,
        ConfVars.METASTORE_BATCH_RETRIEVE_TABLE_PARTITION_MAX);

    // We should check that all the partitions of the table can be dropped
    if (tbl != null && tbl.isPartitioned()) {
      List<String> partitionNames = db.getPartitionNames(tbl.getDbName(), tbl.getTableName(), (short)-1);

      for(int i=0; i < partitionNames.size(); i+= partitionBatchSize) {
        List<String> partNames = partitionNames.subList(i, Math.min(i+partitionBatchSize,
            partitionNames.size()));
        List<Partition> listPartitions = db.getPartitionsByNames(tbl, partNames);
        for (Partition p: listPartitions) {
          if (!p.canDrop()) {
            throw new HiveException("Table " + tbl.getTableName() +
                " Partition" + p.getName() +
                " is protected from being dropped");
          }
        }
      }
    }
{code}, As per [~spena]'s and [~stakiar]'s comments I've refactored the code to use the PartitionIterable., Looks good.
+1, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12855351/HIVE-16024.03.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 10323 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_table] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=140)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=224)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=212)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3870/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3870/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3870/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12855351 - PreCommit-HIVE-Build, 2 failures are well known flaky/failing tests:
- query14: HIVE-15744
- vector_if_expr: HIVE-15776

schema_evol_text_vec_table has been failing for 50+ builds and testQueryProgressParallel is passing locally., Thanks [~zsombor.klara] for the patch. I can understand that tuning the performance (memory/runtime) of {{msck repair table}} was not the original intent of this patch and we should probably take this up in a separate JIRA. I wonder if this patch is regressing the performance of {{MSCK REPAIR TABLE}} queries esp. on S3. The original implementation was checking for the strict mode config and not allowing {{MSCK ..}} to scan all the partitions if strict mode is set. But this implementation now switches to using {{PartitionIterable}} for all the msck queries irrespective of whether it would have caused a OOM or not. According to documentation of {{PartitionIterable}} 

bq. It is very likely that any calls to PartitionIterable are going to result in a large number of calls, so use sparingly only when the memory cost of fetching all the partitions in one shot is too prohibitive.

If we want to enable msck repair even for strict mode then it might be a good idea to invoke PartitionIterable only when user is setting the strict mode like original use-case. That would be a good middle ground I believe without causing performance regressions and causing OOM errors in pursuit of higher performance. What do you think?, I think that HIVE-13788 is actually a backwards incompatible change. Before HIVE-13788 users could run msck without thinking about whether strict mode is on or off. After the patch, they could only run it if strict was is set to nonstrict. I'm not sure if that was done intentionally in HIVE-13788 or not., Probably to avoid OOM issues when fetching a large number of partitions. But using PartitionIterable should help because it fetches only names, then it fetches all partition information in batches., Refactored based on [~vihangk1]'s comments. If strict mode is set, then we will lazily load the partitions, if strict mode is not not set then we load everything with the PartitionsPruner.
I think this is the best compromise we can achieve between safety and performance, while still enabling msck repair in every case., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12855587/HIVE-16024.04.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 10283 tests executed
*Failed tests:*
{noformat}
TestCommandProcessorFactory - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestDbTxnManager - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestDummyTxnManager - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestHiveInputSplitComparator - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestIndexType - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
TestSplitFilter - did not produce a TEST-*.xml file (likely timed out) (batchId=272)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[escape_comments] (batchId=229)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_table] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=140)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=224)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_between_in] (batchId=119)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3891/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3891/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3891/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12855587 - PreCommit-HIVE-Build, 2 failures are well known flaky/failing tests:
- query23: HIVE-15744
- vector_if_expr: HIVE-15776
the rest have been failing for several builds:
- schema_evol_text_vec_table - failing for 4 builds
- escape_comments - failing for 5 builds
- vector_between_in - failing for 16 builds, Looks like the failures are not related. Thanks for the change [~zsombor.klara] Looks good to me. +1 (non-binding), [~zsombor.klara] Could we add a comment on why we're using a different approach when HIVEMAPREDMODE is in strict mode? Developers would get confused, and they would want to use the PartitionIterable instead., [~zsombor.klara]

I took a look at the code again, and I think there might be a OOM problem even if we fetch all partitions in batches (using PartitionIterable) when the strict mode is used. Here's the piece of code:

{noformat}
void checkTable(Table table, PartitionIterable parts,
      boolean findUnknownPartitions, CheckResult result) throws IOException,
      HiveException {
...
Set<Path> partPaths = new HashSet<Path>();
...
for (Partition partition : parts) {
...
     if (!fs.exists(partPath)) {
        PartitionResult pr = new PartitionResult();
        pr.setPartitionName(partition.getName());
        pr.setTableName(partition.getTable().getTableName());
        result.getPartitionsNotOnFs().add(pr);
      }

      for (int i = 0; i < partition.getSpec().size(); i++) {
        partPaths.add(partPath.makeQualified(fs));
        partPath = partPath.getParent();
      }
}
...
{noformat}

My concern is that when running MSCK with million of partitions (fetched in batches), and none of the partitions exist on the filesystem, then the above code will add each partition name on the CheckResult object and partition locations on the partPaths temporary. There's no statistics, but still a concern about OOM. Should we refactor that code instead for handling partitions in batches on MSCK better?, [~spena] Thats a good point. But I think each PartitionResult object is just a container of 2 String objects (PartitionName and TableName) which is probably more light-weight than Partition object. It will only be used when metastore knows about it and it not present in FS. That may be an uncommon case (reverse cases where partition is on FS and not present in Metastore might be more common). Another idea would be to use {{StringInternUtils.internUriStringsInPath(partPath)}} like it was done by Misha in HIVE-15882 so that lots of duplicate strings are eliminated. These path objects might already have been interned when they were discovered or added the first time but doesn't hurt doing them again here. We should perhaps do some experiments to see when OOM happens for msck., As [~vihangk1] mentioned, seems this regression issue was introduced unintentionally by HIVE-13788. [~hsubramaniyan] Do you know if HIVE-13788 was attempting to prevent a user to use MSCK when hive.mapred.mode was in strict mode? If we use strict mode, then users won't be allowed to recover unknown partitions unless they mention the partition, but they're unknown., I think even if the strict mode check was introduced unintentionally, [~spena] is right in that we should keep guarding against OOMs. But maybe we could do it in a separate Jira and fix the regression in this one. As the author of the original Jira would you agree [~hsubramaniyan]?, As suggested 
- internalised the path strings
- used fixed size collections for the results and added a negative qtest to verify it
- confirmed that we are already using direct sql instead of DataNucleus when returning the partitions, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12857304/HIVE-16024.05.patch

{color:green}SUCCESS:{color} +1 due to 3 test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 10344 tests passed

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/4068/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/4068/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-4068/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12857304 - PreCommit-HIVE-Build, Updated the patch based on review board comments., Reuploading patch because the pre-commit tests did not run., Updated patch after RB comments. (Changed qtest), 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12859516/HIVE-16024.07.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 10449 tests executed
*Failed tests:*
{noformat}
TestCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=12)
	[auto_join18.q,partition_coltype_literals.q,input1_limit.q,load_dyn_part3.q,autoColumnStats_4.q,correlationoptimizer8.q,auto_sortmerge_join_14.q,udf_array_contains.q,bucket_map_join_tez2.q,sample_islocalmode_hook.q,literal_decimal.q,constprog2.q,parquet_external_time.q,mapjoin_hook.q,schema_evol_orc_nonvec_table.q,cbo_rp_subq_in.q,authorization_view_disable_cbo_4.q,list_bucket_dml_2.q,input20.q,smb_join_partition_key.q,union_remove_14.q,non_ascii_literal2.q,udf_if.q,input38.q,load_fs_overwrite.q,input21.q,join_reorder.q,groupby_cube_multi_gby.q,bucketmapjoin8.q,union34.q]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[comments] (batchId=35)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/4244/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/4244/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-4244/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12859516 - PreCommit-HIVE-Build, The test "comments" has been failing for 5 builds., Thanks [~zsombor.klara].

+1

Are those failing flaky tests already reported on another JIRA?, TestCliDriver[comments] didn't have a Jira yet. I raised HIVE-16256 to cover it but based on the diff I'm not sure if this is the fault of the test itself. I checked locally and I cannot reproduce it so it's not just that someone forgot to update the baseline., I committed this patch to master.
Thanks [~zsombor.klara] for your contribution. I see the tests are not related, and a JIRA is reported., Does this need any documentation in the wiki?  It looks like a simple bug fix, but here are the relevant doc links just in case:

* [Recover Partitions (MSCK REPAIR TABLE) | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RecoverPartitions(MSCKREPAIRTABLE)]
* [Configuration Properties -- hive.mapred.mode | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.mapred.mode], Thanks for reminding me [~leftylev], but I think that this patch didn't change anything really worth documenting., Okay, thanks.]