[Is the {{SharedWorkOptimizer}} different from HoS's {{CombineEquivalentWorkResolver}}?, [~stakiar]: thanks for interesting on it.  I guess this optimization have following effect.
{code}
set hive.strict.checks.cartesian.product=false;
set hive.join.emit.interval=2;
set hive.auto.convert.join=false;

explain SELECT *
FROM (
  SELECT test1.key AS key1, test1.value AS value1, test1.col_1 AS col_1,
         test2.key AS key2, test2.value AS value2, test2.col_2 AS col_2
  FROM test1 RIGHT OUTER JOIN test2
  ON (test1.value=test2.value
    AND (test1.key between 100 and 102
      OR test2.key between 100 and 102))
  ) sq1
FULL OUTER JOIN (
  SELECT test1.key AS key3, test1.value AS value3, test1.col_1 AS col_3,
         test2.key AS key4, test2.value AS value4, test2.col_2 AS col_4
  FROM test1 LEFT OUTER JOIN test2
  ON (test1.value=test2.value
    AND (test1.key between 100 and 102
      OR test2.key between 100 and 102))
  ) sq2
ON (sq1.value1 is null or sq2.value4 is null and sq2.value3 != sq1.value2);

{code}

the spark explain
{code}
 STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 12), Map 4 (PARTITION-LEVEL SORT, 12)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1), Reducer 6 (PARTITION-LEVEL SORT, 1)
        Reducer 6 <- Map 5 (PARTITION-LEVEL SORT, 12), Map 7 (PARTITION-LEVEL SORT, 12)
      DagName: root_20170911043433_e314705a-beca-41a0-b28a-c85c5f811a67:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: test1
                  Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: int), col_1 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: int), _col2 (type: string)
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: test2
                  Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: int), col_2 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: int), _col2 (type: string)
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: test1
                  Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: int), col_1 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: int), _col2 (type: string)
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: test2
                  Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: int), col_2 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: int), _col2 (type: string)
        Reducer 2 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Right Outer Join 0 to 1
                keys:
                  0 _col1 (type: int)
                  1 _col1 (type: int)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                residual filter predicates: {(_col0 BETWEEN 100 AND 102 or _col3 BETWEEN 100 AND 102)}
                Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: int), _col4 (type: int), _col5 (type: string)
        Reducer 3 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Outer Join 0 to 1
                keys:
                  0 
                  1 
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                residual filter predicates: {(_col1 is null or (_col10 is null and (_col7 <> _col4)))}
                Statistics: Num rows: 36 Data size: 768 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 36 Data size: 768 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Left Outer Join 0 to 1
                keys:
                  0 _col1 (type: int)
                  1 _col1 (type: int)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                residual filter predicates: {(_col0 BETWEEN 100 AND 102 or _col3 BETWEEN 100 AND 102)}
                Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: int), _col4 (type: int), _col5 (type: string)

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

{code}


the explain tez:
{code}
STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 5 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE), Reducer 4 (CUSTOM_SIMPLE_EDGE)
        Reducer 4 <- Map 1 (SIMPLE_EDGE), Map 5 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: test1
                  Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: int), col_1 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: int), _col2 (type: string)
                  Select Operator
                    expressions: key (type: int), value (type: int), col_1 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 6 Data size: 56 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: int), _col2 (type: string)
            Execution mode: llap
            LLAP IO: no inputs
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: test2
                  Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: int), value (type: int), col_2 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: int), _col2 (type: string)
                  Select Operator
                    expressions: key (type: int), value (type: int), col_2 (type: string)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col1 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col1 (type: int)
                      Statistics: Num rows: 4 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col0 (type: int), _col2 (type: string)
            Execution mode: llap
            LLAP IO: no inputs
        Reducer 2 
            Execution mode: llap
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Right Outer Join0 to 1
                keys:
                  0 _col1 (type: int)
                  1 _col1 (type: int)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                residual filter predicates: {(_col0 BETWEEN 100 AND 102 or _col3 BETWEEN 100 AND 102)}
                Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: int), _col4 (type: int), _col5 (type: string)
        Reducer 3 
            Execution mode: llap
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Outer Join 0 to 1
                keys:
                  0 
                  1 
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
                residual filter predicates: {(_col1 is null or (_col10 is null and (_col7 <> _col4)))}
                Statistics: Num rows: 36 Data size: 768 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 36 Data size: 768 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 4 
            Execution mode: llap
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                keys:
                  0 _col1 (type: int)
                  1 _col1 (type: int)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                residual filter predicates: {(_col0 BETWEEN 100 AND 102 or _col3 BETWEEN 100 AND 102)}
                Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 6 Data size: 61 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: int), _col4 (type: int), _col5 (type: string)

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink
{code}
From the explain, hive on spark loads test1, test2 twice while hive on tez only loads once. If my understanding is wrong, tell me,thanks!

, hmm interesting, I'm curious why {{CombineEquivalentWorkResolver}} doesn't combine the scans, as far as I can tell {{Map 1}} is the same as {{Map 5}} and {{Map 4}} is the same as {{Map 7}}., Could be a bug in {{CombineEquivalentWorkResolver}}.

{{SharedWorkOptimizer}} has some other interesting optimization though. According to the javadocs it can combine more than just scans, it can also combine RS and Joins, not sure if {{CombineEquivalentWorkResolver}} can do that., the reason why CombineEquivalentWorkResolver does not think Map1 is same as Map5, Map4 is same as Map7 is:
when comparing Map4 and Map7
Map4
{code}
TS[2]-SEL[3]-RS[13]
{code}
Map7 
{code}
TS[6]-SEL[7]-RS[9]
{code}

It returns not equal when comparing RS\[13\] and RS\[9\] at [ExprNodeColumnDesc#isSame|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeColumnDesc.java#L181]. {code}
if ( tabAlias != null && dest.tabAlias != null ) {
      if ( !tabAlias.equals(dest.tabAlias) ) {
        return false;
      }
    }
{code}

here {{tabAlias}} is {{$hdt$_1}} while dest.tabAlias is {{$hdt$_3}}, actually {{$hdt$_1}} and {{$hdt$_3}} points to table {{test2}}., mapjoin.q
{code}
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join=true;
explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from 
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from 
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key
{code}

before shared work optimization, the physical plan in Tez
{code}
TS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[11]-GBY[12]-MAPJOIN[47]-SEL[31]-FS[32]
TS[3]-FIL[42]-SEL[5]-RS[7]-MAPJOIN[45]
TS[14]-FIL[43]-SEL[16]-MAPJOIN[46]-GBY[24]-RS[25]-GBY[26]-RS[29]-MAPJOIN[47]
TS[17]-FIL[44]-SEL[19]-RS[21]-MAPJOIN[46]
{code}

after the optimization
{code}

TS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[11]-GBY[12]-MAPJOIN[47]-SEL[31]-FS[32]
                                        -RS[25]-GBY[26]-RS[29]-MAPJOIN[47]
TS[3]-FIL[42]-SEL[5]-RS[7]-MAPJOIN[45]
{code}

so the tez explain 
before
{code}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      DagId: root_20171027044107_4977290a-6856-41c5-b0e3-24b4ec32c59e:1
      Edges:
        Map 1 <- Map 3 (BROADCAST_EDGE)
        Map 4 <- Map 6 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 5 (BROADCAST_EDGE)
        Reducer 5 <- Map 4 (SIMPLE_EDGE)
      DagName: root_20171027044107_4977290a-6856-41c5-b0e3-24b4ec32c59e:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0
                        input vertices:
                          1 Map 3
                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                        HybridGraceHashJoin: true
                        Group By Operator
                          aggregations: count()
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: int)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: int)
                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint)
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0
                        input vertices:
                          1 Map 6
                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                        HybridGraceHashJoin: true
                        Group By Operator
                          aggregations: count()
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: int)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: int)
                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint)
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col0 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col0, _col1, _col3
                  input vertices:
                    1 Reducer 5
                  Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                  HybridGraceHashJoin: true
                  Select Operator
                    expressions: _col0 (type: int), _col3 (type: bigint), _col1 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 5 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint)

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

{code}

After
{code}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      DagId: root_20171027043345_8893c266-bde2-4f23-9914-0dfccc8be5ea:1
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 3 (BROADCAST_EDGE)
        Reducer 3 <- Map 1 (SIMPLE_EDGE)
      DagName: root_20171027043345_8893c266-bde2-4f23-9914-0dfccc8be5ea:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0
                        input vertices:
                          1 Map 4
                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                        HybridGraceHashJoin: true
                        Group By Operator
                          aggregations: count()
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: int)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: int)
                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint)
                          Reduce Output Operator
                            key expressions: _col0 (type: int)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: int)
                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint)
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col0 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col0, _col1, _col3
                  input vertices:
                    1 Reducer 3
                  Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                  HybridGraceHashJoin: true
                  Select Operator
                    expressions: _col0 (type: int), _col3 (type: bigint), _col1 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 3 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: int)
                  Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint)

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

{code}

We can see that there are only 2 Maps(Map1 and Map4) in the after explain (in before explain, there are 4 Maps).
When i tried to change the physical plan for HOS like what HOT does. I found change of the optimization  
before
{code}
STAGE DEPENDENCIES:
  Stage-3 is a root stage
  Stage-2 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-2
  Stage-1 depends on stages: Stage-4
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-3
    Spark
      DagName: root_20171027045349_0dc8e597-ffad-4da7-bbfc-c2b7533d9b58:4
      Vertices:
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-2
    Spark
      Edges:
        Reducer 5 <- Map 4 (GROUP, 1)
      DagName: root_20171027045349_0dc8e597-ffad-4da7-bbfc-c2b7533d9b58:2
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0
                        input vertices:
                          1 Map 6
                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          aggregations: count()
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: int)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: int)
                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint)
            Local Work:
              Map Reduce Local Work
        Reducer 5 
            Local Work:
              Map Reduce Local Work
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                Spark HashTable Sink Operator
                  keys:
                    0 _col0 (type: int)
                    1 _col0 (type: int)

  Stage: Stage-4
    Spark
      DagName: root_20171027045349_0dc8e597-ffad-4da7-bbfc-c2b7533d9b58:3
      Vertices:
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (GROUP, 1)
      DagName: root_20171027045349_0dc8e597-ffad-4da7-bbfc-c2b7533d9b58:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0
                        input vertices:
                          1 Map 3
                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          aggregations: count()
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: int)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: int)
                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint)
            Local Work:
              Map Reduce Local Work
        Reducer 2 
            Local Work:
              Map Reduce Local Work
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col0 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col0, _col1, _col3
                  input vertices:
                    1 Reducer 5
                  Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col0 (type: int), _col3 (type: bigint), _col1 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink


{code}

After
{code}
STAGE DEPENDENCIES:
  Stage-3 is a root stage
  Stage-2 depends on stages: Stage-3
  Stage-4 depends on stages: Stage-2
  Stage-1 depends on stages: Stage-4
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-3
    Spark
      DagName: root_20171027045324_b7f21ae7-9bcc-4077-8577-e6c6747dfcff:3
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-2
    Spark
      Edges:
        Reducer 3 <- Map 6 (GROUP, 1)
      DagName: root_20171027045324_b7f21ae7-9bcc-4077-8577-e6c6747dfcff:2
      Vertices:
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0
                        input vertices:
                          1 Map 4
                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          aggregations: count()
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: int)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: int)
                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint)
            Local Work:
              Map Reduce Local Work
        Reducer 3 
            Local Work:
              Map Reduce Local Work
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                Spark HashTable Sink Operator
                  keys:
                    0 _col0 (type: int)
                    1 _col0 (type: int)

  Stage: Stage-4
    Spark
      DagName: root_20171027045324_b7f21ae7-9bcc-4077-8577-e6c6747dfcff:4
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: b
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Spark HashTable Sink Operator
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 5 (GROUP, 1)
      DagName: root_20171027045324_b7f21ae7-9bcc-4077-8577-e6c6747dfcff:1
      Vertices:
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: a
                  Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0
                        input vertices:
                          1 Map 4
                        Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                        Group By Operator
                          aggregations: count()
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0, _col1
                          Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                          Reduce Output Operator
                            key expressions: _col0 (type: int)
                            sort order: +
                            Map-reduce partition columns: _col0 (type: int)
                            Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                            value expressions: _col1 (type: bigint)
            Local Work:
              Map Reduce Local Work
        Reducer 2 
            Local Work:
              Map Reduce Local Work
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 5 Data size: 35 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col0 (type: int)
                    1 _col0 (type: int)
                  outputColumnNames: _col0, _col1, _col3
                  input vertices:
                    1 Reducer 3
                  Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col0 (type: int), _col3 (type: bigint), _col1 (type: bigint)
                    outputColumnNames: _col0, _col1, _col2
                    Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 5 Data size: 38 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

{code}
only the Map about table {{b}} are merged to 1 Map, there are still two Maps about table {{a}}(Map1,Map4).

The reason causes this is because [GenSparkWork|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java#L421] will split following physical operator tree once encounting RS
{code}
TS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[11]-GBY[12]-MAPJOIN[47]-SEL[31]-FS[32]
                                        -RS[25]-GBY[26]-RS[29]-MAPJOIN[47]
TS[3]-FIL[42]-SEL[5]-RS[7]-MAPJOIN[45]
{code}  to 
{code}
Map1:TS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[11]
Reduce2:GBY[12]-MAPJOIN[47]-SEL[31]-FS[32]
Map3:TS[0]-FIL[41]-SEL[2]-MAPJOIN[45]-GBY[10]-RS[25]
Reduce4:GBY[26]-RS[29]
Map5:TS[3]-FIL[42]-SEL[5]-RS[7]
{code}

, In tez, Map can be connected 2 Reducers while in spark, we can not do this. In above example, 
{code}
        Map 1 <- Map 4 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Reducer 3 (BROADCAST_EDGE)
        Reducer 3 <- Map 1 (SIMPLE_EDGE)
{code}
There are 2 edges from Map1 to other vertexes( Map1->Reducer3, Map1->Reducer2), Now HoS does not support multiple edge between two vertex. Let's give an example to show this.
TPC-DS/[query28.sql|https://github.com/kellyzly/hive-testbench/blob/hive14/sample-queries-tpcds/query28.sql].   Before scan shared optimization(HIVE-16602). the tez explain is  [scanshare.before.svg|https://issues.apache.org/jira/secure/attachment/12895148/scanshare.before.svg]. After scan shared optimization(HIVE-16602), the tez explain is [scanshare.after.svg|https://issues.apache.org/jira/secure/attachment/12895149/scanshare.after.svg]. We can see that after optimization, there is only 1 map(before there are 6 maps). But later the only map Map1 connects other 6 reducers by 6 edges. This is because tez supports mulitple edges between two vertexes(TEZ-1190). Now i am working on enabling this feature on HoS. But in HoS, it does not support " mulitple edges between two vertexes". So even i change the physical plan as what HoT does, it may not reduce the number of Map. [~lirui],[~xuefuz],can you help to see the problem., Hi [~kellyzly], I think your observation is correct. Spark has certain limitations. In fact, the edge theory doesn't even apply to Spark. Spark uses RDD model. Internally Hive translates the DAG to RDD operations (transformations and actions). In the example of ( Map1->Reducer3, Map1->Reducer2), Hive on Spark actually has a plan like (map12 - > reduce2, map13 ->reduce3) with map12 = map13. This way, there will be two spark jobs. In the second job, the cached result is used instead of loading the data again. BTW, this is a multi-insert example.

Multiple edges between two vertices are even less thinkable. You might be able to turn this optimization for Spark, but Spark might not be able to run it. I'm not sure if there is any case that this optimization might help Spark. My gut feeling is that this needs to be combined with Spark RDD caching or HIve's materialized view., [~xuefuz]:

{quote}
 My gut feeling is that this needs to be combined with Spark RDD caching or Hive's materialized view.
{quote}
 About the optimization, I found that Hive on Tez can get indeed improvement(20%+) in TPC-DS/query28,88,90 on not excellent hw or in table scan with huge data. So I want to implement it on the Hive on Spark.  
 I agree that we need to combine Spark RDD caching with the optimization to reduce the table scan. As you described, the multi-insert case  benefits from the Spark RDD caching because map12=map13. But more complex cases can not. Use DS/query28.sql as an example.
 The physical plan:
 {code}
TS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]
TS[7]-FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]
TS[14]-FIL[54]-SEL[16]-GBY[17]-RS[18]-GBY[19]-RS[44]-JOIN[48]
TS[21]-FIL[55]-SEL[23]-GBY[24]-RS[25]-GBY[26]-RS[45]-JOIN[48]
TS[28]-FIL[56]-SEL[30]-GBY[31]-RS[32]-GBY[33]-RS[46]-JOIN[48]
TS[35]-FIL[57]-SEL[37]-GBY[38]-RS[39]-GBY[40]-RS[47]-JOIN[48]
{code}

After the scan share optimization, the phyiscal plan
{code}
TS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]
     -FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]
     -FIL[54]-SEL[16]-GBY[17]-RS[18]-GBY[19]-RS[44]-JOIN[48]
     -FIL[55]-SEL[23]-GBY[24]-RS[25]-GBY[26]-RS[45]-JOIN[48]
     -FIL[56]-SEL[30]-GBY[31]-RS[32]-GBY[33]-RS[46]-JOIN[48]
     -FIL[57]-SEL[37]-GBY[38]-RS[39]-GBY[40]-RS[47]-JOIN[48]

{code}

HoS will split operators trees when encounting {{RS}}.
{code}
Map1: TS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]
Map2: TS[0]-FIL[53]-SEL[9]-GBY[10]-RS[11]
Map3: TS[0]-FIL[54]-SEL[16]-GBY[17]-RS[18]
Map4: TS[0]-FIL[55]-SEL[23]-GBY[24]-RS[25]
Map5: TS[0] -FIL[56]-SEL[30]-GBY[31]-RS[32]
Map6: TS[0]-FIL[57]-SEL[37]-GBY[38]-RS[39]
{code}

We can not combine Map1,..., Map6 because the {{FIL}}(FIL\[52\], FIL\[53\],...,FIL\[57\]) are not same.
So what i think about can we directly extract TS from MapTask and put the TS to a single Map
{code}
Map0: TS[0]
Map1: FIL[52]-SEL[2]-GBY[3]-RS[4]
Map2: FIL[53]-SEL[9]-GBY[10]-RS[11]
Map3: FIL[54]-SEL[16]-GBY[17]-RS[18]
Map4: FIL[55]-SEL[23]-GBY[24]-RS[25]
Map5: FIL[56]-SEL[30]-GBY[31]-RS[32]
Map6: FIL[57]-SEL[37]-GBY[38]-RS[39]
{code}
There is only TS\[0\] in the Map0 and connect Map0 to Map1,...,Map6.  Appreciate to get some suggestion from you!
, Hi [~kellyzly],
bq. In tez, Map can be connected 2 Reducers while in spark, we can not do this.
My understanding is HoS also supports one Map connecting to multiple Reducers - that's what {{CombineEquivalentWorkResolver}} is intended for and there're some example queries in {{dynamic_rdd_cache.q}}. The problem here is HoS doesn't merge equivalent works as aggressively as HoT does. Is that right?, [~lirui]:
{quote}
My understanding is HoS also supports one Map connecting to multiple Reducers 
{quote}
There is only 1 RS in Map in HoS. It is true that there are cases that 1 Map is used by two Reducers in HoS. But in HoT, 2 RS are allowed in 1 Map, the different 2 RS in the 1 Map can transfer different data to 2 different Reducers. 
{quote}
The problem here is HoS doesn't merge equivalent works as aggressively as HoT does. 
{quote}
yes, [~lirui]: what i want to ask is there any possiblity to change current structure in the SparkTask in HoS
{code}
M->R 
{code}
to 
{code}
M->M->R
{code}, [~kellyzly] I think M->M->R is possible. It's just that the current planner doesn't do this, but in theory it can be done. Currently the assumption is that a Map task is always followed by a Reduce task. , I also think that's possible in theory. But I guess it will require lots of work. E.g. we may need to modify MapOperator to accommodate the new M->M->R scheme. I'm wondering whether it'll be easier to find a way to cache equivalent MapInput. I noted the caching was disabled by HIVE-8920. [~xuefuz] do you still remember the reason why it was disabled?, [~lirui] The reason was briefly given at https://issues.apache.org/jira/browse/HIVE-8920?focusedCommentId=14260846&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14260846. I was dealing with the IOContext initialization issue., [~lirui]:
{quote}
I also think that's possible in theory. But I guess it will require lots of work. E.g. we may need to modify MapOperator to accommodate the new M->M->R scheme
{quote}
  now i am working on changing from {{M->R}} to {{M->M->R}} schema. Not very clear about the modification on MapOperator. If you know, please say more detailed. I think at first need change {{GenSparkWork}} to split the physical operator trees once en-counting one TS has more than 1 child.  For example 
physical plan
{code}
TS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]
        -FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]

{code}
As TS\[0\] has two children(FIL\[52\], FIL\[53\]). First split at TS\[0\] and bring it to Map1, then split following operator trees when en counting RS.  So the final operator tree will be
{code}
Map1: TS[0]
Map2:FIL[52]-SEL[2]-GBY[3]-RS[4]
Map3:FIL[53]-SEL[9]-GBY[10]-RS[11]
Reducer1:GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]
Reducer2:GBY[12]-RS[43]
{code}
This is very initial thinking. If have suggestion, please tell me, thanks!, I set the flag {{hive.spark.optimize.shared.work}} to enable the SharedWorkOptimizer in Hive on Spark.  The attach explain.28.scan.share.true is the explain when enabling the flag and explain.28.scan.share.false is the explain when disabling the flag for [DS/query28.sql|https://github.com/kellyzly/hive-testbench/blob/hive14/sample-queries-tpcds/query28.sql], explain.28.scan.share.true

we can see that there is only operator (TS) in Map1, and the child of TS to the RS are belongs to another Map(Map12,Map15,Map18,Map2,Map6,Map9). So  change current {{M-R}} in 1 SparkTask to {{M-M-R}}
{code}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Spark
      Edges:
        Map 12 <- Map 1 (NONE, 1000)
        Map 15 <- Map 1 (NONE, 1000)
        Map 18 <- Map 1 (NONE, 1000)
        Map 2 <- Map 1 (NONE, 1000)
        Map 6 <- Map 1 (NONE, 1000)
        Map 9 <- Map 1 (NONE, 1000)
        Reducer 10 <- Map 9 (GROUP PARTITION-LEVEL SORT, 1)
        Reducer 13 <- Map 12 (GROUP PARTITION-LEVEL SORT, 1)
        Reducer 16 <- Map 15 (GROUP PARTITION-LEVEL SORT, 1)
        Reducer 19 <- Map 18 (GROUP PARTITION-LEVEL SORT, 1)
        Reducer 3 <- Map 2 (GROUP PARTITION-LEVEL SORT, 1)
        Reducer 4 <- Reducer 10 (PARTITION-LEVEL SORT, 1), Reducer 13 (PARTITION-LEVEL SORT, 1), Reducer 16 (PARTITION-LEVEL SORT, 1), Reducer 19 (PARTITION-LEVEL SORT, 1), Reducer 3 (PARTITION-LEVEL SORT, 1), Reducer 7 (PARTITION-LEVEL SORT, 1)
        Reducer 7 <- Map 6 (GROUP PARTITION-LEVEL SORT, 1)
      DagName: root_20171204042631_0435ff7e-3f10-4c84-a5fc-dc5b607497ba:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: store_sales
                  filterExpr: ((ss_quantity BETWEEN 0 AND 5 and (ss_list_price BETWEEN 11 AND 21 or ss_coupon_amt BETWEEN 460 AND 1460 or ss_wholesale_cost BETWEEN 14 AND 34)) or (ss_quantity BETWEEN 6 AND 10 and (ss_list_price BETWEEN 91 AND 101 or ss_coupon_amt BETWEEN 1430 AND 2430 or ss_wholesale_cost BETWEEN 32 AND 52)) or (ss_quantity BETWEEN 11 AND 15 and (ss_list_price BETWEEN 66 AND 76 or ss_coupon_amt BETWEEN 920 AND 1920 or ss_wholesale_cost BETWEEN 4 AND 24)) or (ss_quantity BETWEEN 16 AND 20 and (ss_list_price BETWEEN 142 AND 152 or ss_coupon_amt BETWEEN 3054 AND 4054 or ss_wholesale_cost BETWEEN 80 AND 100)) or (ss_quantity BETWEEN 21 AND 25 and (ss_list_price BETWEEN 135 AND 145 or ss_coupon_amt BETWEEN 14180 AND 15180 or ss_wholesale_cost BETWEEN 38 AND 58)) or (ss_quantity BETWEEN 26 AND 30 and (ss_list_price BETWEEN 28 AND 38 or ss_coupon_amt BETWEEN 2513 AND 3513 or ss_wholesale_cost BETWEEN 42 AND 62))) (type: boolean)
                  Statistics: Num rows: 28800991 Data size: 4751513940 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Map 12 
            Map Operator Tree:
                Filter Operator
                  predicate: (ss_quantity BETWEEN 16 AND 20 and (ss_list_price BETWEEN 142 AND 152 or ss_coupon_amt BETWEEN 3054 AND 4054 or ss_wholesale_cost BETWEEN 80 AND 100)) (type: boolean)
                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ss_list_price (type: double)
                    outputColumnNames: ss_list_price
                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)
                      keys: ss_list_price (type: double)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        sort order: +
                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)
        Map 15 
            Map Operator Tree:
                Filter Operator
                  predicate: (ss_quantity BETWEEN 21 AND 25 and (ss_list_price BETWEEN 135 AND 145 or ss_coupon_amt BETWEEN 14180 AND 15180 or ss_wholesale_cost BETWEEN 38 AND 58)) (type: boolean)
                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ss_list_price (type: double)
                    outputColumnNames: ss_list_price
                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)
                      keys: ss_list_price (type: double)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        sort order: +
                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)
        Map 18 
            Map Operator Tree:
                Filter Operator
                  predicate: (ss_quantity BETWEEN 26 AND 30 and (ss_list_price BETWEEN 28 AND 38 or ss_coupon_amt BETWEEN 2513 AND 3513 or ss_wholesale_cost BETWEEN 42 AND 62)) (type: boolean)
                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ss_list_price (type: double)
                    outputColumnNames: ss_list_price
                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)
                      keys: ss_list_price (type: double)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        sort order: +
                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)
        Map 2 
            Map Operator Tree:
                Filter Operator
                  predicate: (ss_quantity BETWEEN 0 AND 5 and (ss_list_price BETWEEN 11 AND 21 or ss_coupon_amt BETWEEN 460 AND 1460 or ss_wholesale_cost BETWEEN 14 AND 34)) (type: boolean)
                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ss_list_price (type: double)
                    outputColumnNames: ss_list_price
                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)
                      keys: ss_list_price (type: double)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        sort order: +
                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)
        Map 6 
            Map Operator Tree:
                Filter Operator
                  predicate: (ss_quantity BETWEEN 6 AND 10 and (ss_list_price BETWEEN 91 AND 101 or ss_coupon_amt BETWEEN 1430 AND 2430 or ss_wholesale_cost BETWEEN 32 AND 52)) (type: boolean)
                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ss_list_price (type: double)
                    outputColumnNames: ss_list_price
                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)
                      keys: ss_list_price (type: double)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        sort order: +
                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)
        Map 9 
            Map Operator Tree:
                Filter Operator
                  predicate: (ss_quantity BETWEEN 11 AND 15 and (ss_list_price BETWEEN 66 AND 76 or ss_coupon_amt BETWEEN 920 AND 1920 or ss_wholesale_cost BETWEEN 4 AND 24)) (type: boolean)
                  Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ss_list_price (type: double)
                    outputColumnNames: ss_list_price
                    Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: avg(ss_list_price), count(ss_list_price), count(DISTINCT ss_list_price)
                      keys: ss_list_price (type: double)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: double)
                        sort order: +
                        Statistics: Num rows: 1066701 Data size: 175981606 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,input:double>), _col2 (type: bigint)
        Reducer 10 
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)
        Reducer 13 
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)
        Reducer 16 
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)
        Reducer 19 
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)
        Reducer 3 
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)
        Reducer 4 
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                     Inner Join 0 to 2
                     Inner Join 0 to 3
                     Inner Join 0 to 4
                     Inner Join 0 to 5
                keys:
                  0 
                  1 
                  2 
                  3 
                  4 
                  5 
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17
                Statistics: Num rows: 1 Data size: 625 Basic stats: COMPLETE Column stats: NONE
                Limit
                  Number of rows: 100
                  Statistics: Num rows: 1 Data size: 625 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 625 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 7 
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), count(VALUE._col1), count(DISTINCT KEY._col0:0._col0)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: double), _col1 (type: bigint), _col2 (type: bigint)

  Stage: Stage-0
    Fetch Operator
      limit: 100
      Processor Tree:
        ListSink


{code}, will upload patch later and describe the problems i encountered later., Here record the problems currently I met
1. I want to change the M->R to M->M->R and split the operator tree when encountering TS. I create [SparkRuleDispatcher| https://github.com/kellyzly/hive/blob/HIVE-17486.3/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkRuleDispatcher.java] to apply rules to the operator tree, the reason why i don't use DefaultRuleDispatcher is because there already a rule called [Handle Analyze Command|https://github.com/kellyzly/hive/blob/jdk9-trial/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java#L432] to split operator trees once encountering TS. Original [SparkCompiler#opRules|https://github.com/kellyzly/hive/blob/jdk9-trial/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java#L417] is a linkedHashMap which stores one key with one value. It can not deal with the case where one key with two values. So current solution is to modify SparkCompile#opRules to a Multimap and create SparkRuleDispatcher . But I am afraid once encountering TS, only 1 rule will be applied  for {{SparkRuleDispatcher#dispatch}}

SparkRuleDispatcher#dispatch
{code}

@Override
  public Object dispatch(Node nd, Stack<Node> ndStack, Object... nodeOutputs)
      throws SemanticException {

    // find the firing rule
    // find the rule from the stack specified
    Rule rule = null;
    int minCost = Integer.MAX_VALUE;
    for (Rule r : procRules.keySet()) {
      int cost = r.cost(ndStack);
      if ((cost >= 0) && (cost < minCost)) {
        minCost = cost;
        // Here I am afraid there is only 1 rule will be applied even there are two rules for TS
        rule = r;
      }
    }

    Collection<NodeProcessor> procSet;

    if (rule == null) {
      procSet = defaultProcSet;
    } else {
      procSet = procRules.get(rule);
    }

    // Do nothing in case proc is null
    Object ret = null;
    for (NodeProcessor proc : procSet) {
      if (proc != null) {
        // Call the process function
        ret = proc.process(nd, ndStack, procCtx, nodeOutputs);
      }
    }
    return ret;
  }
{code}

I can change above code like following but don't know return the result of which rule if there are more than 1 rule for TS.
{code}
  @Override
  public Object dispatch(Node nd, Stack<Node> ndStack, Object... nodeOutputs)
      throws SemanticException {

    // find the firing rule
    // find the rule from the stack specified
    ArrayList ruleList =new ArrayList();
    int minCost = Integer.MAX_VALUE;
    for (Rule r : procRules.keySet()) {
      int cost = r.cost(ndStack);
      if ((cost >= 0) && (cost < minCost)) {
        minCost = cost;
        ruleList.add(r);
      }
    }

    Collection<NodeProcessor> procSet;

    if (ruleList.size() == 0) {
      procSet = defaultProcSet;
    } else {
      for(Rule r: ruleList) {
        // Question: Here I don't know which rule I should use if there is more than 1 rule in the ruleList
        procSet = procRules.get(r);
      }
    }

    // Do nothing in case proc is null
    Object ret = null;
    for (NodeProcessor proc : procSet) {
      if (proc != null) {
        // Call the process function
        ret = proc.process(nd, ndStack, procCtx, nodeOutputs);
      }
    }
    return ret;

  }
}

{code}
[~lirui], [~xuefuz] can you give your suggestions about the problem?
, [~lirui], [~xuefuz]: can you spend some time to view the problem i met about  DefaultRuleDispatcher? thanks!, Hi [~kellyzly], I think this thread is getting a little bit long and the problem doesn't seem trivial. Could you please create a doc that describes the problem or feature we are addressing and your proposal? That's probably easier to communicate. Thanks. , [~xuefuz]: ok, will upload doc soon., the document includes the introduction, problems  and limitations of this optimization., [~xuefuz]: have uploaded the [design doc|https://docs.google.com/document/d/1f4f0oMhN2vKSTCtXbnd3FBYOV02H4QflX1BbkglnC30/edit?usp=sharing]. I described the problems i met in the [Problem Section|https://docs.google.com/document/d/1f4f0oMhN2vKSTCtXbnd3FBYOV02H4QflX1BbkglnC30/edit#heading=h.d0ptagvbv8k3], please help view the problem if have time, thanks!, [~xuefuz] and [~lirui]: the second problem I found  is the modification of MapOperator after changing from M->R to M->M->R.  
MapOperator  is reponsible for deserializing and  [initObjectInspector| https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java#L170]. For the second M in M->M->R,  deserializing is not necessary and initObjectInspector is necessary. Currently I am investigating how to make this work. If there is some wrong in my understanding, please tell me!, [~lirui] and [~xuefuz]: Please help view the [First Problem|https://docs.google.com/document/d/1f4f0oMhN2vKSTCtXbnd3FBYOV02H4QflX1BbkglnC30/edit#heading=h.d0ptagvbv8k3] i mentioned in the design doc, thanks!, Hi [~kellyzly], could you provide more design and implementation details in your doc?
For the first problem, can the new feature be done in a separate pass, so that you don't need to deal with existing rules or graph walker?, [~kellyzly], Thanks for working on this. I'm not sure if we should just look at TS to determine whether to generate M-M-R. It seems that we can do so whenever an TS is connected to multiple RSs. The split point should happen at the fork. I'm not sure what's the best way to apply the optimization rules, but if you look at SparkProcessAnalyzeTable, it has an if statement to check if it's an analyze table command. If not, it doesn't do anything. Thus, you can have a super rule that covers both analyze table and the new rule you're adding., [~lirui]:
{quote}
could you provide more design and implementation details in your doc?
{quote}
ok, will add more in the design doc.
{quote}
For the first problem, can the new feature be done in a separate pass, so that you don't need to deal with existing rules or graph walker?
{quote}
thanks for your suggestion, I will try to avoid current problem., [~xuefuz]
{quote}  It seems that we can do so whenever an TS is connected to multiple RSs. The split point should happen at the fork. {quote}  not very understand about this. Currently the split is on the TS
for example
{code}
TS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]
        -FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]
{code}

->
{code}		
Map1: TS[0]
Map2:FIL[52]-SEL[2]-GBY[3]-RS[4]
Map3:FIL[53]-SEL[9]-GBY[10]-RS[11]
Reducer1:GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]
Reducer2:GBY[12]-RS[43]
{code}, I meant, if FIL[52] and FIL[53] is the same in your example, then we should break after the filter op for M-M split. Looking forward to your complete design doc for this. Thanks., [~lirui] and [~xuefuz]:  I have updated  HIVE-17486.2.patch and designed again . A similar case like DS/query28.sql is run successfully.
This proved that current design(M-M-R) can use RDD cache to reduce the table scan.
In the latest design doc, there is a simple case.  Currently, although i have added the simple case(spark_optimize_shared_work.q), there is some exception when running qtest in my local env. Once fix the problem, will trigger Hive QA to test., [~xuefuz]: before you mentioned that the reason to disable caching for MapInput because of [IOContext initialization problem|https://issues.apache.org/jira/browse/HIVE-8920?focusedCommentId=14260846&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14260846]. And reading HIVE-9041, it shows an example to show IOContext initialization problem
{code}
I just found another bug regarding IOContext, when caching is turned on.
Taking the sample query above as example, right now I have this result plan:

   MW 1 (table0)   MW 2 (table1)   MW 3 (table0)   MW 4 (table1)
      \            /                 \             /
       \          /                   \           /
        \        /                     \         /
         \      /                       \       /
           RW 1                           RW 2
Suppose MapWorks are executed from left to right, also suppose we are just running with a single thread.
Then, the following will happen:
1. executing MW 1: since this is the first time we access table0, initialize IOContext and make input path point to table0;
2. executing MW 2: since this is the first time we access table1, initialize IOContext and make input path point to table1;
3. executing MW 3: since this is the second time access table0, do not initialize IOContext, and use the copy saved in step 2), which is table1.

Step 3 will then fail.
 how to make MW 3 know that it needs to get the saved IOContext from MW 1, but not MW 2
{code}
If the problem exists in the MapInput RDD cache because of IOContext is a static variable which will be stored in cache and the IOContext will be updated in different Maps. Why only disable in [MapInput rdd cache|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java#L202]? 
This should be disabled in all MapTrans.  Please explain more if have time., add spark_optimize_shared_work.q.out and update HIVE-17486.3.patch. Trigger QA tests to see whether patch influences current code or not., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12902412/HIVE-17486.3.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/8276/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/8276/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8276/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-12-15 22:03:28.091
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-8276/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-12-15 22:03:28.094
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at f52e8b4 HIVE-18258: Vectorization: Reduce-Side GROUP BY MERGEPARTIAL with duplicate columns is broken (Matt McCline, reviewed by Teddy Choi)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at f52e8b4 HIVE-18258: Vectorization: Reduce-Side GROUP BY MERGEPARTIAL with duplicate columns is broken (Matt McCline, reviewed by Teddy Choi)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-12-15 22:03:33.108
+ rm -rf ../yetus
+ mkdir ../yetus
+ cp -R . ../yetus
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-8276/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java: does not exist in index
error: a/itests/src/test/resources/testconfiguration.properties: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkMapRecordHandler.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SharedWorkOptimizer.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkUtils.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/GenSparkWork.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java: does not exist in index
error: a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java: does not exist in index
Going to apply patch with: git apply -p1
/data/hiveptest/working/scratch/build.patch:1448: trailing whitespace.
        Map 1 
/data/hiveptest/working/scratch/build.patch:1463: trailing whitespace.
                        sort order: 
/data/hiveptest/working/scratch/build.patch:1466: trailing whitespace.
        Map 4 
/data/hiveptest/working/scratch/build.patch:1481: trailing whitespace.
                        sort order: 
/data/hiveptest/working/scratch/build.patch:1484: trailing whitespace.
        Reducer 2 
warning: squelched 18 whitespace errors
warning: 23 lines add whitespace errors.
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
protoc-jar: protoc version: 250, detected platform: linux/amd64
protoc-jar: executing: [/tmp/protoc1701815878933627184.exe, -I/data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore, --java_out=/data/hiveptest/working/apache-github-source-source/standalone-metastore/target/generated-sources, /data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore/metastore.proto]
ANTLR Parser Generator  Version 3.5.2
Output file /data/hiveptest/working/apache-github-source-source/standalone-metastore/target/generated-sources/org/apache/hadoop/hive/metastore/parser/FilterParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/parser/Filter.g
org/apache/hadoop/hive/metastore/parser/Filter.g
log4j:WARN No appenders could be found for logger (DataNucleus.General).
log4j:WARN Please initialize the log4j system properly.
DataNucleus Enhancer (version 4.1.17) for API "JDO"
DataNucleus Enhancer : Classpath
>>  /usr/share/maven/boot/plexus-classworlds-2.x.jar
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDatabase
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFieldSchema
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MType
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MConstraint
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MOrder
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStringList
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartition
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MIndex
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRole
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRoleMap
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMasterKey
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDelegationToken
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MVersionTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMetastoreDBProperties
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MResourceUri
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFunction
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationLog
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationNextId
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMResourcePlan
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMPool
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMTrigger
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMMapping
DataNucleus Enhancer completed with success for 35 classes. Timings : input=173 ms, enhance=222 ms, total=395 ms. Consult the log for full details
ANTLR Parser Generator  Version 3.5.2
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveLexer.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
org/apache/hadoop/hive/ql/parse/HiveLexer.g
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
org/apache/hadoop/hive/ql/parse/HiveParser.g
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HintParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HintParser.g
org/apache/hadoop/hive/ql/parse/HintParser.g
Generating vector expression code
Generating vector expression test code
[ERROR] COMPILATION ERROR : 
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[437,7] cannot find symbol
  symbol:   variable opRules
  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[440,5] cannot find symbol
  symbol:   variable opRules
  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[443,5] cannot find symbol
  symbol:   variable opRules
  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[446,5] cannot find symbol
  symbol:   variable opRules
  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[448,5] cannot find symbol
  symbol:   variable opRules
  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[454,5] cannot find symbol
  symbol:   variable opRules
  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[483,5] cannot find symbol
  symbol:   variable opRules
  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[512,53] cannot find symbol
  symbol:   variable opRules
  location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[437,7] cannot find symbol
[ERROR] symbol:   variable opRules
[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[440,5] cannot find symbol
[ERROR] symbol:   variable opRules
[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[443,5] cannot find symbol
[ERROR] symbol:   variable opRules
[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[446,5] cannot find symbol
[ERROR] symbol:   variable opRules
[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[448,5] cannot find symbol
[ERROR] symbol:   variable opRules
[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[454,5] cannot find symbol
[ERROR] symbol:   variable opRules
[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[483,5] cannot find symbol
[ERROR] symbol:   variable opRules
[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java:[512,53] cannot find symbol
[ERROR] symbol:   variable opRules
[ERROR] location: class org.apache.hadoop.hive.ql.parse.spark.SparkCompiler
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-exec
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12902412 - PreCommit-HIVE-Build, update HIVE-17486.4.patch to fix the compilation error., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 33s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 34s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 16s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 53s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  5s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 22s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 18s{color} | {color:red} common: The patch generated 2 new + 931 unchanged - 0 fixed = 933 total (was 931) {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 34s{color} | {color:red} ql: The patch generated 82 new + 269 unchanged - 21 fixed = 351 total (was 290) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 10s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 12s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 16m 20s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |
| git revision | master / f52e8b4 |
| Default Java | 1.8.0_111 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8291/yetus/diff-checkstyle-common.txt |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8291/yetus/diff-checkstyle-ql.txt |
| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-8291/yetus/patch-asflicense-problems.txt |
| modules | C: common ql itests U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8291/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12902486/HIVE-17486.4.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 19 failed/errored test(s), 11529 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join25] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=48)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=35)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucketsortoptimize_insert_2] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[hybridgrace_hashjoin_2] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[quotedid_smb] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucketizedhiveinputformat] (batchId=178)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[authorization_part] (batchId=93)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_aggregator_error_1] (batchId=93)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_sortmerge_join_10] (batchId=138)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[bucketsortoptimize_insert_7] (batchId=128)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=120)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=113)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=209)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=226)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/8291/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/8291/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8291/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 19 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12902486 - PreCommit-HIVE-Build, [~kellyzly] {quote} what i want to ask is there any possiblity to change current structure in the SparkTask in HoS {quote}

What exactly is the advantage of introducing a M -> M edge? I know Tez does it, but I think they mainly use it to broadcast data from one set of map tasks to another (which is useful for map-joins)., [~stakiar]:
the original purpose to change M->R to M->M->R is to let CombineEquivalentWorkResolver combine same Maps. Like
logical plan
{code}
TS[0]-FIL[52]-SEL[2]-GBY[3]-RS[4]-GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]
TS[1] -FIL[53]-SEL[9]-GBY[10]-RS[11]-GBY[12]-RS[43]-JOIN[48]
{code}	
physical plan
{code}	
Map1:TS[0]
Map2:TS[1]
Map3:FIL[52]-SEL[2]-GBY[3]-RS[4]
Map4:FIL[53]-SEL[9]-GBY[10]-RS[11]
Reducer1:GBY[5]-RS[42]-JOIN[48]-SEL[49]-LIM[50]-FS[51]
Reducer2:GBY[12]-RS[43]
{code}
For {{CombineEquivalentWorkResolver}}, it will combine same Maps. In above case, Map2 will be removed because TS\[0\] is same as TS\[1\].  

But when I finished the code, I found that there is no necessary to use this way to combine TS\[0\] and TS\[1\]. {{MapInput}} is responsible for TS and I only need generate same MapInput for TS\[0\] and TS\[1\]. More detail see HIVE-17486.5.patch.
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m 11s{color} | {color:red} /data/hiveptest/logs/PreCommit-HIVE-Build-8411/patches/PreCommit-HIVE-Build-8411.patch does not apply to master. Rebase required? Wrong Branch? See http://cwiki.apache.org/confluence/display/Hive/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8411/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12904307/HIVE-17486.5.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 67 failed/errored test(s), 10834 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join25] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_hook] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=35)
org.apache.hadoop.hive.cli.TestLocalSparkCliDriver.org.apache.hadoop.hive.cli.TestLocalSparkCliDriver (batchId=247)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucketsortoptimize_insert_2] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[hybridgrace_hashjoin_2] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=164)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=176)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=177)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=178)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=179)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[authorization_part] (batchId=93)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[materialized_view_authorization_create_no_grant] (batchId=93)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_publisher_error_1] (batchId=93)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=104)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=105)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=106)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=108)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=109)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=111)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=112)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=113)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=114)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=115)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=118)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=120)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=122)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=124)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=125)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=127)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=128)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=129)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=130)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=131)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=132)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=134)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=135)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=136)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=137)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=138)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=139)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=140)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=141)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=142)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=143)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=144)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=145)
org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=247)
org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testTransactionalValidation (batchId=213)
org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.testWrite (batchId=253)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=225)
org.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=231)
org.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=231)
org.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=231)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/8411/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/8411/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8411/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 67 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12904307 - PreCommit-HIVE-Build]