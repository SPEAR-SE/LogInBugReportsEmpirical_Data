[I can take a look at this .. Does it work if you use fs.s3a.secret.key and fs.s3a.access.key?, [~vihangk1] no it doesn't work with fs.s3a.secret.key and fs.s3a.access.key., you should switch to using s3a:// URLs in things based on Hadoop 2.7.+, which the latest CDH versions are. 

Set up securiy as per : https://hortonworks.github.io/hdp-aws/s3-security/index.html
Then test on the command line before worrying about Hive: https://hortonworks.github.io/hdp-aws/s3-s3aclient/index.html

setting up core-site.xml & testing via the hdfs fs commands will let you get up and running faster
, Hi [~kalexin] I tried this on CDH 5.8 onwards until 5.10. I could not make it work using the {{set fs.s3a.secret.key=<secret>}} and {{set fs.s3a.access.key=<access>}}. Can you tell me the exact version of CDH where it works? Are you using BeeLine or HiveCLI? As far as I know you have to add the keys to the core-site.xml to make it work., [~vihangk1] We are using hive CLI. 

Have you tried including the key and access in the Hive query like this: `LOCATION 's3n://AWS_ACCESS_KEY:AWS_SECRET_KEY@<path to S3 file>`? 

Is it possible to set multiple pairs of S3 key and secret in core-site.xml?

Thanks for the heads up about s3a [~steve_l], # try with s3a URS and the fs.s3a secret and access keys
# do not put secrets in your URIs, it's a security leak waiting to be discovered. That's why you get told off about it. See HADOOP-3733]