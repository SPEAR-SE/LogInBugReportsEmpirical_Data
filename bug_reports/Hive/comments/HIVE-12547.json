[It's probably time to use multiple arrays. , [~mmccline] is there a reason why 3 longs are used per key in the Fast hashtable? Full hashcode and valueref are in separate longs, I wonder if combining them to have last few bits of the hash will improve perf overall for the cases where HT is big enough to fit with 2 longs but not 3? The only loss will be some extra lookups when last hash bits match but the rest don't., At the time I wrote it I was using long to hold the hash code but at some point I realized only an int was being used by Murmur, etc.  The newer code in HIVE-12369 "Faster Vector GroupBy" I change it to pass around an int hashCode instead of a long -- but still store the hashCode as a long in the 3 long hash table.

I could look at being more compact.

As to this JIRA -- how big a hash table is really effective?



, (HIVE-12492  MapJoin: 4 million unique integers seems to be a probe plateau), Well, that's a separate problem. For this JIRA, I think I'll just change both to use optional extra arrays, or something like that., No I don't think we should do that.
, Why? Otherwise, we get query failures and the only workaround is to reduce the conversion thhreshold, which doesn't map to anything in an obvious way, so it's easy to disable mapjoin for queries that would benefit from it., 2 parallel arrays means a double CPU Data Cache miss., Beats query failures in my book :P Also, these would be sequential arrays that would only trigger on overflow.
I meant the write-buffer-like approach after resize failure, I want to make sure the 99% normal case when we aren't creating 1 billion entry hash tables uses the double/triple entry single long array and has good CPU data cache behavior., Actually, I wonder how we get into this situation. It's very easy to fix, but 1B (2B will actually go into negative) / 3 (longs per key in the fast case) * 0.75 (load factor) that's 250M keys. I would really like to see hashtable stats for that case cause it seems like it would hit HIVE-12492, WIP patch for BytesBytes (easier to modify as proof of concept, only one impl).. It also requires 64-bit hash function to actually address the larger-than-Java-array hashtable, which also wastes extra 4 bytes per key.

Maybe we should just reduce the array size by going to 2 longs from 3, and throw a better exception instead...
, [~mmccline] what do you think? I looked at the code, it looks like it should be possible to compress value reference and hashcode into one (or key+hash+value into two?)
Is there a point of storing the small value length in value ref? We never compare values, just retrieve them, so there are no cases where we avoid going to WriteBuffer-s, like there is with keys., That makes sense., [~jdere] can you comment about the distributed hashjoin that would avoid large tables? This issue may not be important because such large hashtables are not expected in practice., HIVE-15892]