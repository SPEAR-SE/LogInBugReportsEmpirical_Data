[[~xuefuz], [~lirui], could you review the code to see if the change makes sense? thanks., [~ctang.ma], you might want to take a look at HIVE-15671. I'll also have another look at these timeouts., [~ctang.ma], thanks for looking into this. It seems that your patch basically reverts the change made in HIVE-15671. There is a long discussion in that JIRA, and you might want to visit that. The short summary is that hive.spark.client.connect.timeout is for SASL handshake, which happens after the remote driver is launched. This timeout doesn't need to be too long. On the other hand, hive.spark.client.server.connect.timeout is the timeout for Hive to wait for the remote driver to connect back. Because launching the remote driver takes time, not to mention there could be resource constraint, it takes much longer. I understand this is a frequent confusion that bothers many of us, so it's not surprising that this comes back again.

The particular problem you saw might not be attributed to a bug. SASL usually takes little time to establish. If the default value is too short for your network, you might consider to increase the timeout value a bit.

CC: [~vanzin], 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12855287/HIVE-16071.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 10298 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_table] (batchId=147)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3859/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3859/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3859/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12855287 - PreCommit-HIVE-Build, My understanding is the timeout on SparkClient side is longer because it needs to wait for the RemoteDriver to launch. The timeout on the RemoteDriver side should be shorter because the SparkClient is already running when RemoteDriver starts - and it usually won't take long to just connect back and finish SASL handshake. Although the default 1000ms may be a little too short.

Looking at the stack trace in description, we detect the channel is closed and eventually get a {{SaslException}} instead of a {{TimeoutException}}. I wonder why the channel is closed before the handshake finishes. [~ctang.ma], is it possible that your HS2 runs into some issue?

Another question (may be irrelevant to this JIRA) to [~vanzin]: we use the server side timeout in two places:
# [Constructing RpcServer|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L108]
# [Registering client|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L162]

I understand 2 needs the long timeout because it includes the time to launch the RemoteDriver. But does 1 also need that timeout? I think 1 only needs to take care of the SASL handshake, which should take much less time., re: the timeout used in 2 places, it's been a while since I closely looked at that code, but I think in the end only one of them matters (i.e. the second one ends up being pre-empted by the first one firing). The idea is that once you start a Spark app, you have that much time before the app starts and registers (i.e. completes the handshake)., [~xuefuz], [~lirui], and [~vanzin], thanks for the clarification. I did not notice HIVE-15671 before.
If so, will it be more reasonable that the timeout used in Constructing RpcServer should match that used by RemoteDriver for its RPC handshaking (hive.spark.client.connect.timeout)? Or we might not need it since the handshaking timeout has been controlled by that used in RemoteDriver?, I think we still need both. server.connect.timeout covers the waiting time for remote driver to come up, which is absolutely necessary (unless we let it wait forever). client.connect.timeout seems also needed for remote driver establish a connection with Hive. Too small value may cause declaring connection failure too soon and large value can cause remote driver to take too long a time to detect abnormality of Hive, for instance, Hive exiting right before the connection is made.

Having said that, however, a reasonable default value for client.connect.timeout may eliminate the need for a configuration exposed.

In our environment, the default value seems working fine., Hi [~ctang.ma], for the issue mentioned in this JIRA, I think you can try increasing the default value of client timeout. I also suggest you check whether HS2 has any problems.

Regarding the timeout in RpcServer constructor, I tried making it use the client side timeout, and successfully run some simple queries. Not sure if we can remove it. Looking at the code, that timeout task is responsible for closing the RPC, thus closing the underlying channel. We might as well keep it, but with the client timeout., For the error I mentioned in this JIRA description, it was caused by the [timeout|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L176] set in HS2 registerClient. It happened when SaslServerHandler and SaslClientHandler were undergoing handshaking. The timeout caused the registerClient in SparkClientImpl to throw out an error which interrupted the process calling spark-submit, therefore ending the channel between HS2 and RemoteClient. The channel termination was detected by SaslClientHandler.channelInactive at RemoteDriver side, which in term invoked SaslClientHandler.dispose(). Therefore we saw the SASLException with msg "SaslException: Client closed before SASL negotiation finished."I have managed to reproduce this error by adjusting the hive.spark.client.server.connect.timeout value to make the HS2 timeout happen during SASL negotiation but RemoteDriver has not reached its own timeout.

Looking more into the code, I think that the [cancelTask|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L100] is not used at all in the code. It never has a chance to effect because it has the same timeout value as that used for [registerClient|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L170], and the latter one always kicks in before it.
Timeout at RemoteDriver side could happen at two places. If it happens when driver [connects|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/Rpc.java#L110] back to HS2, HS2 could not detect this timeout error at driver site and has to wait until its own timeout set in  [registerClient|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L170] effects. If the [timeout|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/Rpc.java#L122] happens during SASL handshaking, the RemoteDriver main will exit abnormally. The SaslServerHandler.channelInactive at HS2 side could detect this channel termination and invokes the SaslServerHandler.dispose, which in term cancels this cancelTask (not be used again). Depending on the stage where HS2 is at (see following code snippet) 
{code}
    protected void onError(Throwable error) {
      cancelTask.cancel(true);
      if (client != null) {
        client.timeoutFuture.cancel(true);
        if (!client.promise.isDone()) {
          client.promise.setFailure(error);
        }
      }
    }
{code}
HS2 should either wait until its hive.spark.client.server.connect.timeout when clientInfo is null, or terminates the process immediately.
So the [cancelTask|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L100] is currently useless in the code unless its timeout could be set with a different (shorter) value than that set for registerClient (hive.spark.client.server.connect.timeout). Or we can consider removing it though its existence in current code does not do any harm either.
, Thanks [~ctang.ma] for the detailed analysis!
bq. it was caused by the timeout set in HS2 registerClient
Then increasing the timeout on RemoteDriver side won't help right? You have to increase {{hive.spark.client.server.connect.timeout}} to avoid it.

My only concern of remove the cancelTask is it's responsible for closing the channel, in case the SASL handshake doesn't finish in time. But seems the concern is not valid according to your analysis:
bq. If the timeout happens during SASL handshaking, the RemoteDriver main will exit abnormally. The SaslServerHandler.channelInactive at HS2 side could detect this channel termination
If that's the case, I'm fine to remove it., Yes, [~lirui]. Increasing hive.spark.client.server.connect.timeout (instead of the hive.spark.client.connect.timeout) could help in my case.
The cancelTask could effect and close the channel only when its timeout is set to a value shorter than current hive.spark.client.server.connect.timeout. So for this cancelTask, we can do:
1. remove it to make code more understandable; or
2. leave it as is since it is not be executed anyway; or
3. Use a different HoS timeout configuration (either hive.spark.client.connect.timeout or a new one) so that we have more and finer control to the waiting time at HS2 side. Adding a new timeout config may not be desirable since we already have many such configurations.
[~xuefuz], [~lirui], [~vanzin], what do you think?, Hi [~ctang.ma], Thanks for your findings and analysis. Would you think it's better if we set the timeout for cancelTask to client.connect.timeout instead, which is used on remote driver side anyway (with HIVE-15671)? I think the purpose of cancelTask is to make sure that Sasl connection is established within a given timeframe when connection request comes from remote driver. However, server.connect.timeout is usually much longer (1hr in our cluster), and we don't expect that Sasl would take such long to establish. This way, we are consistent on both server side and client side.

On the other hand, if we user server.connect.timeout for both registerClient and cancelTask, then what can happens is that if Sasl handshake happens right after registerClient but failed to establish and if the connection isn't actually broken at that point, then the server will detect a connection loss and so will not react to this until registerClient timeout, which can be significantly delayed (1hr in our case rather just a few seconds).

Thus, what your found seems to be a bug, however rarely it happens., Hi [~xuefuz] and [~ctang.ma], I'd prefer removing the cancelTask. The main reason is we already have a [timeout task|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L170] in {{RpcServer::registerClient}}, which takes {{hive.spark.client.server.connect.timeout}}. And this timeout task covers both connection establishment and SASL handshake - it's canceled [when the SaslServerHandler completes|https://github.com/apache/hive/blob/master/spark-client/src/main/java/org/apache/hive/spark/client/rpc/RpcServer.java#L313]. Therefore I don't think the cancelTask is needed.
On the other hand, if we remove it, we'll have one timeout task on server and client side respectively. Each such task takes care of timeout in connection establishment and SASL handshake, with different default value because server needs to wait longer for client to start. Thus it's more consistent and cleaner. Thoughts?, [~lirui], did you see above my hypothetic example where we may need cancelTask? I'm not convinced that we should remove cancelTask. server.connect.timeout is usually much larger than client.connect.timeout. In our case, 1hr for former and 1s for the latter. If the cluster resource is available, while the network is super busy at the moment, then Sasl handshake may have issues. In such case, we don't want HiveServer wait for 1 hour before declaring a failure. The same thing on the remote driver side. In HIVE-15671 we fixed the problem on driver side. Of course we may rely on remove driver to disconnect and Hive detects the connection loss. However, this is again unreliable in a busy network. It's good for the server side to has its own Sasl timeout regardless.

My proposal is to change the timeout value for cancelTask to client.connect.timeout. Please share your concern about this.

Also, ping [~vanzin] for comments., I agree with  [~xuefuz] that we need a timeout for SASL handshaking at RPC server site for the case he raised. This timeout should be shorter than client.server.connect.timeout used by RegisterClient, but ideally I think it should be a little longer than the client.connect.timeout used by RemoteDriver handshaking so that we can try to avoid the handshaking timeout initiated by the server given that starting a remoteDriver is quite expensive. If so, I would suggest we can introduce a new configuration like hive.spark.rpc.handshake.server.timeout, and rename   hive.spark.client.connect.timeout to hive.spark.rpc.handshake.client.timeout (though it is also used as the socket connect timeout at RemoteDriver side like now). Also the hive.spark.client.server.connect.timeout could be renamed to something like hive.spark.register.remote.driver.timeout if necessary. What do you guys think about it?, Hi [~xuefuz], in your example, if the SASL handshake doesn't finish in time, the client side will exit after 1s. Even if netty can't detect the disconnection immediately, I don't think it takes 1h to detect it. Besides, the cancelTask only closes the channel, it doesn't set failure to the Future. Therefore we can't really rely on the cancelTask to stop the waiting. My proposal is:
# We need to reliably detect disconnection. I think netty is good enough for this (maybe with some reasonable delay). But I'm also OK to keep the cancelTask to close the channel ourselves.
# We need to reliably cancel the Future when disconnection is detected. This can be done in the SaslHandler which monitors the channel inactive event.

I also did some tests to verify. I modified the client code so that it makes the connection but doesn't finish SASL handshake. I tried two ways to do this, one is the client never sends the SaslMessage, the other is the client sends the SaslMessage and then just exits. The test is done in yarn-cluster mode.
# If no SaslMessage is sent, Hive will still wait for {{hive.spark.client.server.connect.timeout}}, even if cancelTask closes the channel after 1s.
# If SaslMessage is sent, SaslHandler will detect the disconnection and cancel the Future, no matter whether the cancelTask fires or not. Of course, this requires netty to detect the disconnection., Hi [~lirui], thank for your input and experiment. I think we are making some progress in drawing a conclusion.
{quote}
If no SaslMessage is sent, Hive will still wait for hive.spark.client.server.connect.timeout, even if cancelTask closes the channel after 1s.
{quote}
I'm particularly concern on cases where Hive takes more than it needs to detect a problem and return the error to the user. In this case, Hive should know in 1s that Sasl handshake doesn't complete. It doesn't make sense to let user know the failure until after 1 hr. (1 hr is set to accommodate the resource availability, not connection establishment.)

{quote}
the cancelTask only closes the channel, it doesn't set failure to the Future.
{quote}
This is a good observation. Is this another bug that we should fix? That is, let cancelTask fail the Future so that Hive stops waiting until server.connect.timeout elapses.

Any further thoughts?

[~ctang.ma], To answer your question, I don't think we need another property. We should use client.connect.timeout as it's also used on driver side. If the default value is too low, we can bump it up.
, Hi [~xuefuz], let me summarise my point: here we're talking about two issues - detecting disconnection and react to the disconnection. I think the root cause of your example is we don't react properly (i.e. we don't fail the future) on disconnection.
Regarding detecting the disconnection, I suppose we can rely on netty. The cancelTask is kind of a further insurance in case netty fails (or takes too long) to detect it.
bq. let cancelTask fail the Future so that Hive stops waiting
Like I mentioned in my proposal, I think SaslHandler is in a better place to do this. SaslHandler is intended for the SASL handshake, and it removes itself from the pipeline once the handshake finishes. Therefore, if SaslHandler detects disconnection, it means the channel is closed before the handshake finishes. And thus we should fail the Future. Do you think it makes sense to open another JIRA for this?, I looked more into the code and it turned out to be trickier than I thought. In case the client doesn't send SaslMessage (due to busy network or failure), the SaslHandler can detect disconnection, but it won't be able to know which Future to fail because it hasn't received the client ID (neither can cancelTask). The situation is we may have multiple pending clients/futures in RpcServer, and all we know now is one channel closed unexpectedly. I can't think of how to deal with it., Hi [~lirui], thank you very much for your further investigation. Based on what you described and my understanding of the code, I have the following thoughts to share:

1. If a network problem happens before client sends its id, I don't think we can fail the future, as you said we don't know which one to fail. This is fine and understandable. However, in this case, we still want to close the channel (which is what cancelTask does).
1. If SaslServerHandle detects any problem, I'm hoping that SaslServerHandle.onError() is called.  onError() seems doing the right thing (if client is known at that point), except missing of cancelling the rpc channel.
{code}
    @Override
    protected void onError(Throwable error) {
      cancelTask.cancel(true);
      if (client != null) {
        client.timeoutFuture.cancel(true);
        if (!client.promise.isDone()) {
          client.promise.setFailure(error);
        }
      }
    }
{code}

Thus, I'm thinking of two work items:
1. Fix the cancelTask timeout value.
2. Fix about #2 by closing the server channel.
These are to make sure that the channel is closed in both cases, though I'm not sure how significant it is.

What do you think?, Hi [~xuefuz], your understanding is inline with mine. Not sure if SaslHandler.onError has to close the channel. If it's called via channelInactive, it means the channel is already closed. If it's called via exceptionCaught, the method itself will close the channel.
One improvement may be to call onError unconditionally in channelInactive - current code depends on dispose to call onError:
{code}
  @Override
  public void channelInactive(ChannelHandlerContext ctx) throws Exception {
    dispose();
    super.channelInactive(ctx);
  }
{code}, {quote}
One improvement may be to call onError unconditionally in channelInactive - current code depends on dispose to call onError:
{quote}
I think this makes sense. However, given that we don't really have a concrete case to justify, we probably just need to keep an eye on this.

What seems more certain is that we can fix cancelTask timeout value using client.connnect.timeout. If the default value of 1s is inadequate, we can pop up it as well., Yeah that makes sense to me., So we reached the consensus that hive.spark.client.server.connect.timeout should not be used for cancelTask at RPCServer side. The value proposed could be hive.spark.client.connect.timeout.
[~xuefuz] The reason that I previously suggested we could consider another timeout for cancelTask (a little longer than hive.spark.client.connect.timeout.) is to give RemoteDriver a little more time to timeout the handshaking than RPCServer. If the timeout at both sides are set to exactly same value, we might see the situations quite often where the terminations of SASL handshaking are initiated by cancelTask at RpcServer side because the timeout at RemoteDriver side might be slightly later for whatever reasons. During this short window, the handshake could still have a chance to succeed if it is not terminated by cancelTask.
To my understanding, to shorten cancelTask timeout is mainly for RpcServer to detect the handshake timeout (fired by RemoteDriver) sooner, we still want RemoteDriver to mainly control the SASL handshake timeout, and most handshake timeout should be fired from remoteDriver, right?, [~ctang.ma], Sasl handshake happens after the connection between Hive and remote driver has already established. The handshake usually happens pretty faster. I don't see a need of different timeout values on each side. On the other hand, 1s is generally enough and user can pop the value up if otherwise.

In essence, with this change, client.connect.timeout is more like sasl.handshake.timeout, and server.connect.timeout description seems also needing some rewording., Change to use the hive.spark.client.connect.timeout value for sasl handshake timeout  (cancelTask) at RpcServer side. [~xuefuz], [~lirui], could you review it? Thanks.
In addition, I am thinking about renaming the property hive.spark.client.connect.timeout and 
hive.spark.client.server.connect.timeout to hive.spark.rpc.sasl.handshake.timeout and hive.spark.remote.driver.register.timeout respectively. The rename should be tracked in a separate JIRA since we need consider property name backward compatibility issue. Comments?, +1, Precommit tests were run but for some reasons the result was not reported (https://builds.apache.org/job/PreCommit-HIVE-Build/4203/testReport/). There is a spark failure but I do not think it is related to this patch. Reattach the patch to kick off tests to see if it could be reproducible., Two tests failed but none of them are related to this patch (See https://builds.apache.org/job/PreCommit-HIVE-Build/4215/testReport/) , Committed to 2.2.0. Thanks [~xuefuz], [~lirui] for review.]