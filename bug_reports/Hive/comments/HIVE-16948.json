[HoS? Hive on Spark?, [~pxiong]: i found it in HoS, will modify the description soon., thanks. :), the reason why Map4 does not exist in the explain is because of [CombineEquivalentWorkResolver|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/CombineEquivalentWorkResolver.java]

before CombineEquivalentWorkResolver optimization is enabled,Map4 exists in the explain, after CombineEquivalentWorkResolver which will find and combine equivalent works.  Map4 is deleted because Map4 equals Map1. So we need to remove the spark dynamic pruning sink branch in Reducer 11 and Reducer 13 in Stage-2.

[~lirui], [~stakiar], [~csun] please help review, thanks!
, One thing need to be mentioned here:
 why remove dynamic partition pruning sink operator branch directly in CombineEquivalentWorkResolver#dispatch?
{code}
   @Override
    public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs) throws SemanticException {
      if (nd instanceof SparkTask) {
        SparkTask sparkTask = (SparkTask) nd;
        SparkWork sparkWork = sparkTask.getWork();
        Set<BaseWork> roots = sparkWork.getRoots();
        compareWorksRecursively(roots, sparkWork);
        +removeDynamicPartitionPruningSinkBranch(removedMapWorkNames, sparkWork);
      }
      return null;
    }
{code}
 We found that Map4 equals to Map1 in Stage-1 and remove the dynamic partition pruning sink operator branch in Reducer 11 and Reducer 13 in Stage-2. Stage-1 is the child task of Stage-2. And the traverse order is definite, first Stage-1 then Stage-2 because [TaskGraphWalker|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/lib/TaskGraphWalker.java#L184] will first put children task in the front of task queue., [~lirui], [~stakiar], [~csun] please help review, thanks!, Overall, the approach LGTM. You may need to re-attach the patch, seem Hive QA hasn't run yet., upload HIVE-16948_1.patch to trigger HiveQA., Is it possible that the DPP work doesn't contain branches, and therefore when the target work is gone, the whole DPP work/task should be removed?, [~lirui]: 
{quote}
Is it possible that the DPP work doesn't contain branches, and therefore when the target work is gone, the whole DPP work/task should be removed?
{quote}
 There are 3 places to remove spark dynamic partition pruning sink before CombineEquivalentWorkResolver 
1. SparkRemoveDynamicPruningBySize 
2. SparkCompiler#runCycleAnalysisForPartitionPruning
3. SparkMapJoinOptimizer(HIVE-17087)

In this jira, there are two dynamic partition pruning sink operators which are target to two same Maps. If we need remove dynamic partition pruning operators in above three conditions. These 2 dynamic partition pruning sink operators will be removed together. In theory, there will not happen the DPP work doesn't contain branches( remove 1 and remain another).

if my understanding is not correct, please tell me., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12879093/HIVE-16948_1.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6145/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6145/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6145/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-07-27 08:10:44.402
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-6145/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-07-27 08:10:44.405
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 0f7c33d HIVE-17088: HS2 WebUI throws a NullPointerException when opened (Sergio Pena, reviewed by Aihua Xu)
+ git clean -f -d
Removing ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfVectorExpression.java
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 0f7c33d HIVE-17088: HS2 WebUI throws a NullPointerException when opened (Sergio Pena, reviewed by Aihua Xu)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-07-27 08:10:51.019
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/CombineEquivalentWorkResolver.java: No such file or directory
error: a/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning.q.out: No such file or directory
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12879093 - PreCommit-HIVE-Build, [~kellyzly], my point is, in your example, both Reducer11 and Reducer13 contain two DPP sinks, and we need to remove one of them in each Reducer. Is it possible the reduce works only contain one DPP sink?
More specifically, you use {{OperatorUtils.removeBranch(pruneSinkOp)}} to remove the DPP sink, which only works if the DPP sink is in a branch. The other 3 places you mentioned can use {{OperatorUtils.removeBranch}} because DPP sinks are always in a branch in logical plan. But in physical plan (after {{SplitOpTreeForDPP}} has split the tree), I'm not sure whether the assumption will hold., [~lirui]: 
{quote}

 Is it possible the reduce works only contain one DPP sink?
{quote}
there are 3 conditions to remove dpp sink:
1. SparkRemoveDynamicPruningBySize 
2. SparkCompiler#runCycleAnalysisForPartitionPruning
3. SparkMapJoinOptimizer(HIVE-17087)

If i use 1 condition to remove dpp sink, can you give one example to show to remove 1 and remain another?, [~kellyzly], I'm not talking about the 3 places. Here's an example:
{noformat}
set hive.cbo.enable=false;
explain select * from (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.key) a join (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.key) b on a.key=b.key;

STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
      DagName: lirui_20170728110559_4c2bc0ba-ab9a-428b-bf09-23f1b19b068f:16
      Vertices:
        Map 8
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          partition key expr: ds
                          Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                          target column name: ds
                          target work: Map 1
            Execution mode: vectorized
        Map 9
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          partition key expr: ds
                          Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                          target column name: ds
                          target work: Map 5
            Execution mode: vectorized

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 1), Map 4 (PARTITION-LEVEL SORT, 1)
        Reducer 3 <- Reducer 2 (PARTITION-LEVEL SORT, 1), Reducer 6 (PARTITION-LEVEL SORT, 1)
        Reducer 6 <- Map 1 (PARTITION-LEVEL SORT, 1), Map 4 (PARTITION-LEVEL SORT, 1)
      DagName: lirui_20170728110559_4c2bc0ba-ab9a-428b-bf09-23f1b19b068f:15
      Vertices:
        Map 1
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: ds (type: string)
                      sort order: +
                      Map-reduce partition columns: ds (type: string)
                      Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
                      value expressions: key (type: string)
            Execution mode: vectorized
        Map 4
            Map Operator Tree:
                TableScan
                  alias: src
                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: key is not null (type: boolean)
                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: key (type: string)
                      sort order: +
                      Map-reduce partition columns: key (type: string)
                      Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
            Execution mode: vectorized
        Reducer 2
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 ds (type: string)
                  1 key (type: string)
                outputColumnNames: _col0, _col2
                Statistics: Num rows: 2200 Data size: 23372 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: string), _col0 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 2200 Data size: 23372 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col1 (type: string)
                    sort order: +
                    Map-reduce partition columns: _col1 (type: string)
                    Statistics: Num rows: 2200 Data size: 23372 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col0 (type: string)
        Reducer 3
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 _col1 (type: string)
                  1 _col1 (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 2420 Data size: 25709 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 2420 Data size: 25709 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
        Reducer 6
            Reduce Operator Tree:
              Join Operator
                condition map:
                     Inner Join 0 to 1
                keys:
                  0 ds (type: string)
                  1 key (type: string)
                outputColumnNames: _col0, _col2
                Statistics: Num rows: 2200 Data size: 23372 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col2 (type: string), _col0 (type: string)
                  outputColumnNames: _col0, _col1
                  Statistics: Num rows: 2200 Data size: 23372 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col1 (type: string)
                    sort order: +
                    Map-reduce partition columns: _col1 (type: string)
                    Statistics: Num rows: 2200 Data size: 23372 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col0 (type: string)
{noformat}
Map1 and Map5 are equivalent and Map5 is removed. Therefore the whole Map9 should be removed. But I think your patch won't work because Map9 doesn't have branches., Thinking more about this, I find a bug in combing equivalent works. If 2 map works contain same operators, but will be pruned by different DPP sinks, then they can't be combined. E.g., let's slightly change the above example into:
{code}
explain select * from (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.key) a join (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.value) b on a.key=b.key;
{code}
The two map works for {{srcpart}} still get combined. However, they need to be pruned by different values: {{src.key}} and {{src.value}} respectively. In the current implementation we'll probably have incorrect results., [~lirui]:  thanks for your catch, it needs to remove the whole map work if there is no branch in map work contain spark pruning sink operator.  update HIVE-16948.2.patch., {quote}

Thinking more about this, I find a bug in combing equivalent works. If 2 map works contain same operators, but will be pruned by different DPP sinks, then they can't be combined. E.g.,
{quote}

CombineEquivalentWorkResolver.EquivalentWorkMatcher#compareCurrentOperator only compare the self property of operator. Will not compare the relationship with other operators. Suggest to have a configuration to enable/disable combine equivalent work so that if users disable combine equivalent to cross above issue., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12879315/HIVE-16948.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 11013 tests executed
*Failed tests:*
{noformat}
TestPerfCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=240)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[materialized_view_create_rewrite] (batchId=240)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=100)
org.apache.hadoop.hive.metastore.TestHiveMetaStoreStatsMerge.testStatsMerge (batchId=206)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=179)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=179)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=179)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6176/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6176/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6176/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12879315 - PreCommit-HIVE-Build, changes:
1.	remove the empty sparkWork and sparkTask
2.	add test in TestSparkTask to test removeEmptySparkTask. Directly copy part code of CombineEquivalentWorkResolver.EquivalentWorkMatcher#removeEmptySparkTask to TestSparkTask to test. It is better to mock some situation to test EquivalentWorkMatcher#removeEmptySparkTask even this situation is rarely occurred. If this is unnecessary, i can remove them.
, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12880794/HIVE-16948.5.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6294/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6294/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6294/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-08-08 09:46:15.376
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-6294/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-08-08 09:46:15.379
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 6b03a9c HIVE-17247: HoS DPP: UDFs on the partition column side does not evaluate correctly (Sahil Takiar, reviewed by Rui Li)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 6b03a9c HIVE-17247: HoS DPP: UDFs on the partition column side does not evaluate correctly (Sahil Takiar, reviewed by Rui Li)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-08-08 09:46:21.472
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/CombineEquivalentWorkResolver.java: No such file or directory
error: a/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkTask.java: No such file or directory
error: a/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning.q.out: No such file or directory
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12880794 - PreCommit-HIVE-Build, [~lirui]: attached is HIVE-16948.6.patch. Update code with latest master.  Can you spend some time to review, thanks!, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12881698/HIVE-16948.6.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 11005 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[materialized_view_create_rewrite] (batchId=240)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[insert_overwrite_dynamic_partitions_merge_move] (batchId=243)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[insert_overwrite_dynamic_partitions_merge_only] (batchId=243)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[insert_overwrite_dynamic_partitions_move_only] (batchId=243)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_mapjoin_only] (batchId=170)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=235)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=235)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=180)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testJoinThriftSerializeInTasks (batchId=228)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6381/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6381/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6381/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12881698 - PreCommit-HIVE-Build, [~lirui]: Can you spend some time to review HIVE-16948.6.patch, thanks!, [~kellyzly], sorry about the delay. I've left some comments on RB., I do not find my comment on the review board, so I leave a comment here too:
I think {{spark_dynamic_partition_pruning.q.out}} changes are caused by HIVE-17148 - "Incorrect result for Hive join query with COALESCE in WHERE condition". Created HIVE-17346 to track the progress there, [~pvary]: I saw your comment on review board.  Thanks for your tracking HIVE-17346., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12882506/HIVE-16948.7.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10977 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=235)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=235)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=180)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=180)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6451/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6451/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6451/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12882506 - PreCommit-HIVE-Build, +1. Thanks for the update [~kellyzly], thanks for your review, [~lirui],[~stakiar],[~pvary]!, Pushed to master. Thanks [~kellyzly] for the work., Hive 3.0.0 has been released so closing this jira.]