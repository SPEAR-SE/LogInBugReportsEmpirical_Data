[I tried to create table with decimal(30) column, load data, select data. Everything works fine.
{code}
hive> create table dec_test (a decimal(30));
OK
Time taken: 0.853 seconds
{code}
{code}
hive> desc dec_test;
OK
a                   	decimal(30,0)       	                    
Time taken: 0.467 seconds, Fetched: 1 row(s)
{code}
{code}
hive> insert into dec_test values (123456789012345);
Query ID = apivovarov_20150514181058_b8de7b66-d691-4a1a-9afa-4a605b2d296a
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1431652081178_0001, Tracking URL = http://c11.example.com:8088/proxy/application_1431652081178_0001/
Kill Command = /usr/lib/hadoop-2.6.0/bin/hadoop job  -kill job_1431652081178_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2015-05-14 18:11:10,098 Stage-1 map = 0%,  reduce = 0%
2015-05-14 18:11:17,666 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.99 sec
MapReduce Total cumulative CPU time: 1 seconds 990 msec
Ended Job = job_1431652081178_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://localhost/apps/apivovarov/warehouse/dec_test/.hive-staging_hive_2015-05-14_18-10-58_355_1605293869887130934-1/-ext-10000
Loading data to table default.dec_test
Table default.dec_test stats: [numFiles=1, numRows=1, totalSize=16, rawDataSize=15]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.99 sec   HDFS Read: 3446 HDFS Write: 88 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 990 msec
OK
Time taken: 21.828 seconds
{code}
{code}
hive> select * from dec_test;
OK
123456789012345
Time taken: 0.144 seconds, Fetched: 1 row(s)
{code}
{code}
hive> select max(*) from dec_test;
FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
hive> select max(a) from dec_test;
Query ID = apivovarov_20150514181227_144b5d8c-c656-4e4e-ae59-696979eef94a
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1431652081178_0002, Tracking URL = http://c11.example.com:8088/proxy/application_1431652081178_0002/
Kill Command = /usr/lib/hadoop-2.6.0/bin/hadoop job  -kill job_1431652081178_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2015-05-14 18:12:35,733 Stage-1 map = 0%,  reduce = 0%
2015-05-14 18:12:42,277 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.39 sec
2015-05-14 18:12:49,692 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.11 sec
MapReduce Total cumulative CPU time: 3 seconds 110 msec
Ended Job = job_1431652081178_0002
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.11 sec   HDFS Read: 6866 HDFS Write: 16 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 110 msec
OK
123456789012345
Time taken: 23.982 seconds, Fetched: 1 row(s)
{code}
{code}
hive> create table dec_test2 like dec_test;
OK
Time taken: 0.117 seconds
{code}
{code}
hive> desc  dec_test2;
OK
a                   	decimal(30,0)       	                    
Time taken: 0.101 seconds, Fetched: 1 row(s)
{code}
{code}
hive> desc formatted dec_test2;
OK
# col_name            	data_type           	comment             
	 	 
a                   	decimal(30,0)       	                    
	 	 
# Detailed Table Information	 	 
Database:           	default             	 
Owner:              	apivovarov          	 
CreateTime:         	Thu May 14 18:13:04 PDT 2015	 
LastAccessTime:     	UNKNOWN             	 
Protect Mode:       	None                	 
Retention:          	0                   	 
Location:           	hdfs://localhost/apps/apivovarov/warehouse/dec_test2	 
Table Type:         	MANAGED_TABLE       	 
Table Parameters:	 	 
	transient_lastDdlTime	1431652384          
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	serialization.format	1                   
Time taken: 0.072 seconds, Fetched: 26 row(s)
{code}, hive-1.3.0, [~tfriedr] can you answer [~apivovarov] question?, Agreed with Alex, I don't think there is any issue. Metadata always comes with two parameters, even if user omits some in which case default is filled in. As noted in the comments, the only chance that you might no parameters at all is in the metadata migrated from that prior to decimal precision/scale support where no parameters are stored. I believe that it's impossible to have a case where there is only one parameter (precision) stored in the metadata.

Please provide a repro case otherwise., The CLI has no issue, that's why I opened the defect against the Serializers/Deserializers component.
We are using the TypeInfoUtils class (package org.apache.hadoop.hive.serde2.typeinfo) in one of our own serdes to parse the datatype and call the static method getTypeInfoFromTypeString which should treat the decimal data type according to the spec.
However, if you pass in a string for the decimal type with just one argument, the precision is always 10 and the scale is 0 because the parseType method doesn't handle the one argument decimal type.

You can run a simple test like this:
TypeInfo t = TypeInfoUtils.getTypeInfoFromTypeString("decimal(20)");      
DecimalTypeInfo dti = (DecimalTypeInfo)t;
System.out.println(dti.getPrecision());  // prints 10
System.out.println(dti.getScale());       // prints 0


, [~tfriedr], thanks for the info. I believe you're using these classes as if they were public APIs. Since these classes are not meant for this consumption, I'm not convinced that there is a need for a fix on Hive side., The TypeInfoUtils class is documented as part of the public APIs https://hive.apache.org/javadocs/r1.2.1/api/index.html. I don't see where the class is marked as internal. 
The ParseUtils class, method getDecimalTypeTypeInfo handles the case correctly:
      int precision = HiveDecimal.USER_DEFAULT_PRECISION;
      int scale = HiveDecimal.USER_DEFAULT_SCALE;

      if (node.getChildCount() >= 1) {
        String precStr = node.getChild(0).getText();
        precision = Integer.valueOf(precStr);
      }

      if (node.getChildCount() == 2) {
        String scaleStr = node.getChild(1).getText();
        scale = Integer.valueOf(scaleStr);
      }

Why can't the TypeInfoUtils have the same behavior instead of returning the wrong values., A bit late to the dance here, but the behavior here is definitely odd, should be either
1) correctly parse decimal(precision)
2) raise an error because decimal(precision, scale) is required
Returning decimal(10,0) in this case is just wrong - if we do make a change, might as well use [~tfriedr]'s patch. Do you mind adding a testcase to TestTypeInfoUtils for this?, Added test case for patch. [~hagleitn], can you take a look?, +1, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12875317/HIVE-10616.2.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10825 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[skewjoin_union_remove_1] (batchId=81)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[columnstats_part_coltype] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=239)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5860/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5860/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5860/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12875317 - PreCommit-HIVE-Build, Failures do not look related - [~jdere]?, Failures do not look related. I've committed to master., Hive 3.0.0 has been released so closing this jira.]