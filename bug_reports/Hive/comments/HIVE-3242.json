[if dfs -ls file, when the file does not exist, hadoop0.20 returns exit
code -1, and hadoop0.23 returns exit code 1. Is this correct?
For example, in hive, for the following dfs command:
dfs -ls ../build/ql/tmp/hbase/hbase_table_0;
If the file does not exist, get exit code -1 on hadoop0.20, and exit
code 1 on hadoop0.23.
A brief read of hadoop code, on hadoop 0.20(FsShell.java):
private int ls(String srcf, boolean recursive) throws IOException {
Path srcPath = new Path(srcf);
FileSystem srcFs = srcPath.getFileSystem(this.getConf());
FileStatus[] srcs = srcFs.globStatus(srcPath);
if (srcs==null || srcs.length==0) { throw new FileNotFoundException("Cannot access " + srcf + ": No such file or directory."); }
boolean printHeader = (srcs.length == 1) ? true: false;
int numOfErrors = 0;
for(int i=0; i<srcs.length; i++) { numOfErrors += ls(srcs[i], srcFs, recursive, printHeader); }
return numOfErrors == 0 ? 0 : -1;
}
So, it returns -1 on non-exist file.
While, on hadoop0.23(Command.java):
public int run(String...argv) {
LinkedList<String> args = new LinkedList<String>(Arrays.asList(argv));
try {
if (isDeprecated()) { displayWarning( "DEPRECATED: Please use '"+ getReplacementCommand() + "' instead."); }
processOptions(args);
processRawArguments(args);
} catch (IOException e) { displayError(e); }
return (numErrors == 0) ? exitCode : exitCodeForError();
}
/**
The exit code to be returned if any errors occur during execution.
This method is needed to account for the inconsistency in the exit
codes returned by various commands.
@return a non-zero exit code
*/
protected int exitCodeForError() { return 1; }
So, it returns 1 on non-exist file.
The return exit code is always non-zero on non-exist file, while, it
is 1 on hadoop0.23, and -1 on hadoop0.20.

Plan to have separate qfile and expected output for hadoop0.20 and hadoop0.23, review request submitted:
https://reviews.facebook.net/D4047, updated review request submitted:
https://reviews.facebook.net/D4047, @Zhenxiao - if possible, can you please update "TestHBaseCliDriver.vm" with the same changes to keep all VM files in sync for better maintenance. Thanks, @Kanna: Thanks a lot. My patch is updated, and re-submitted at:
https://reviews.facebook.net/D4047, Thanks Zhenxiao. The changes look good to me but I am not 100% sure with forking of .q files for different Hadoop versions. I will leave it for experts to comment.
, New patch submitted for review at:
https://reviews.facebook.net/D4077, +1. Will commit if tests pass., [~zhenxiao] Patch doesnt apply cleanly anymore. Can you rebase it?, updated patch submitted at:
https://reviews.facebook.net/D4455, +1 Running tests., [~zhenxiao] See my comments on phabricator., @Ashutosh: thanks a lot for the comments. updated patch submitted at:
https://reviews.facebook.net/D4455, [~zhenxiao] I think include/exclude hadoop version combination is still not correct. For 1.x line, exit code = -1, so cascade_dbdrop.q will fail for it and cascade_dbdrop20.q will not run on 1.x, @Ashutosh:

My idea is:
For 1.x line(hadoop20), cascade_dbdrop.q does not run, so it EXCLUDE 0.20, cascade_dbdrop_hadoop20.q will be running, so it INCLUDE 0.20.
And, cascade_dbdrop_hadoop20.q.out is having exit code -1.

For 2.x line(hadoop23), cascade_dbdrop.q will be running. (it EXCLUDE 0.20, so will be running for 0.23), cascade_dbdrop_hadoop20.q does not run(it only INCLUDE 0.20, not running for 0.23).
And, cascade_dbdrop.q.out is having exit code 1.

INCLUDE only runs the test if the hadoop version is included in the INCLUDE macro
EXCLUDE only runs the test if the hadoop version is not excluded in the EXCLUDE macro

Is my understanding correct?
Any comments are appreciated.

Thanks,
Zhenxiao, @Zhenxiao: Yes, your understanding is correct, but Ashutosh's point is that this test doesn't work for Hadoop 1.x. Please update the patch so that it passes by adding "1.0" to one of the macros., @Ashutosh and Carl:
Get it. Thanks for your comments. I updated my patch and submitted for review at:
https://reviews.facebook.net/D4545

Thanks,
Zhenxiao, +1. Will commit if tests pass., Committed to trunk. Thanks Zhenxiao!, Integrated in Hive-trunk-h0.21 #1619 (See [https://builds.apache.org/job/Hive-trunk-h0.21/1619/])
    HIVE-3242. Fix cascade_dbdrop.q when building hive on hadoop0.23 (Zhenxiao Luo via cws) (Revision 1375313)

     Result = FAILURE
cws : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375313
Files : 
* /hive/trunk/hbase-handler/src/test/queries/negative/cascade_dbdrop.q
* /hive/trunk/hbase-handler/src/test/queries/negative/cascade_dbdrop_hadoop20.q
* /hive/trunk/hbase-handler/src/test/results/negative/cascade_dbdrop.q.out
* /hive/trunk/hbase-handler/src/test/results/negative/cascade_dbdrop_hadoop20.q.out
* /hive/trunk/hbase-handler/src/test/templates/TestHBaseCliDriver.vm
* /hive/trunk/hbase-handler/src/test/templates/TestHBaseNegativeCliDriver.vm
, Integrated in Hive-trunk-hadoop2 #54 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/54/])
    HIVE-3242. Fix cascade_dbdrop.q when building hive on hadoop0.23 (Zhenxiao Luo via cws) (Revision 1375313)

     Result = ABORTED
cws : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375313
Files : 
* /hive/trunk/hbase-handler/src/test/queries/negative/cascade_dbdrop.q
* /hive/trunk/hbase-handler/src/test/queries/negative/cascade_dbdrop_hadoop20.q
* /hive/trunk/hbase-handler/src/test/results/negative/cascade_dbdrop.q.out
* /hive/trunk/hbase-handler/src/test/results/negative/cascade_dbdrop_hadoop20.q.out
* /hive/trunk/hbase-handler/src/test/templates/TestHBaseCliDriver.vm
* /hive/trunk/hbase-handler/src/test/templates/TestHBaseNegativeCliDriver.vm
, This issue is fixed and released as part of 0.10.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.]