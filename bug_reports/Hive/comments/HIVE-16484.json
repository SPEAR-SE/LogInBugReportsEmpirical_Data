[[~lirui], [~xuefuz] not sure if this was something that the team looked into when developing HoS, but it looks like it could be a useful improvement?, Hi [~stakiar], thanks for bringing it up. I think it would be very useful, especially when replacing the option of starting a spark application in Hive when SPARK_HOME isn't available, using org.apache.spark.deploy.SparkSubmit, which is never intended for serious use.

Though, I'm not sure of production readiness of SparkLauncher. Maybe [~vanzin] can shed some lights on this., {{SparkLauncher}} is just a wrapper around spark-submit, currently. It adds some nice APIs on top of it, but it still requires a SPARK_HOME and everything else. Once SPARK-11035 is fixed then you can avoid the dependency on spark-submit (the shell script), but you'd still need most of the other things you already need (like a proper configuration)., Thanks Marcelo.

Attached a WIP patch to trigger a run of Hive QA. It sounds like {{SparkLauncher}} may be a better way of submitting the Spark application going forward. Until SPARK-11035 is completed, the {{SparkLauncher}} will still run {{bin/spark-submit}} but at least in its current state the {{SparkLauncher}} handles all the complexity of running {{bin/spark-submit}} and re-directing the stdout and stderr.

If stability is a concern, we can target this change towards the Hive 3.x branch, which won't be released for a while. Migrating earlier will help us do more testing of the {{SparkLauncher}} so we can report any bugs., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12864554/HIVE-16484.2.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/4830/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/4830/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-4830/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2017-04-21 22:32:37.242
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-4830/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2017-04-21 22:32:37.245
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 6566065 HIVE-15982 : Support the width_bucket function (Sahil Takiar via Ashutosh Chauhan)
+ git clean -f -d
Removing ql/src/java/org/apache/hadoop/hive/ql/QueryInfo.java
Removing service/src/java/org/apache/hive/service/cli/operation/QueryInfoCache.java
Removing service/src/test/org/apache/hive/service/cli/operation/TestQueryLifeTimeHooksWithSQLOperation.java
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 6566065 HIVE-15982 : Support the width_bucket function (Sahil Takiar via Ashutosh Chauhan)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2017-04-21 22:32:38.403
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
Going to apply patch with: patch -p1
patching file common/src/java/org/apache/hadoop/hive/common/ProcessRunner.java
patching file spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java
patching file spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriverLocalRunner.java
patching file spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
[ERROR] COMPILATION ERROR : 
[ERROR] /data/hiveptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java:[80,12] unreported exception java.lang.InterruptedException; must be caught or declared to be thrown
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile (default-compile) on project spark-client: Compilation failure
[ERROR] /data/hiveptest/working/apache-github-source-source/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java:[80,12] unreported exception java.lang.InterruptedException; must be caught or declared to be thrown
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :spark-client
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12864554 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12864605/HIVE-16484.3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10626 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_index] (batchId=225)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[zero_rows_blobstore] (batchId=237)
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver[hbase_joins] (batchId=94)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=143)
org.apache.hive.spark.client.TestSparkClient.testRemoteClient (batchId=280)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/4842/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/4842/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-4842/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12864605 - PreCommit-HIVE-Build, Only related failure is {{TestSparkClient.testRemoteClient}} 

The issue is what happens if {{SPARK_HOME}} is not set. The {{SparkClientImpl}} has some code to handle the case where {{SPARK_HOME}} is not set; if it isn't set, then it runs {{bin/java org.apache.spark.deploy.SparkSubmit}}.

This patch deleted the code in {{SparkClientImpl}} that calls {{bin/java org.apache.spark.deploy.SparkSubmit}} since {{SparkLauncher}} is used for all Spark job submissions. There was only one test that actually invoked that code path ({{TestSparkClient.testRemoteClient}})

{{SparkLauncher}} requires {{SPARK_HOME}} to be set since it calls {{bin/spark-submit}}, it doesn't attempt to call {{org.apache.spark.deploy.SparkSubmit}} if {{SPARK_HOME}} is not present.

So we could (1) modify {{SparkLauncher}} to not require {{sparkHome}} to be set, (2) modify this test so that {{SPARK_HOME}} is set, or (3) refactor the code so that it can still directly invoke {{bin/java org.apache.spark.deploy.SparkSubmit}} if {{SPARK_HOME}} isn't set.

I'm leaning towards approach 2. [~vanzin] the code to run {{bin/java org.apache.spark.deploy.SparkSubmit}} if {{SPARK_HOME}} isn't set was added in HIVE-8528 - is there a use case for launching Spark jobs without {{SPARK_HOME}} being set, or was it just added for testing?, It was mostly added for testing., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12865465/HIVE-16484.6.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 10635 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_index] (batchId=225)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[columnstats_part_coltype] (batchId=155)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=143)
org.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles (batchId=280)
org.apache.hive.spark.client.TestSparkClient.testCounters (batchId=280)
org.apache.hive.spark.client.TestSparkClient.testErrorJob (batchId=280)
org.apache.hive.spark.client.TestSparkClient.testJobSubmission (batchId=280)
org.apache.hive.spark.client.TestSparkClient.testMetricsCollection (batchId=280)
org.apache.hive.spark.client.TestSparkClient.testRemoteClient (batchId=280)
org.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob (batchId=280)
org.apache.hive.spark.client.TestSparkClient.testSyncRpc (batchId=280)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/4925/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/4925/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-4925/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12865465 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12865595/HIVE-16484.7.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 10635 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_index] (batchId=225)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[escape_comments] (batchId=234)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/4944/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/4944/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-4944/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12865595 - PreCommit-HIVE-Build, [~xuefuz] could you take a look? https://reviews.apache.org/r/58684/, [~vanzin] it looks like SPARK-11035 has been complete. Will the {{InProcessLauncher}} work for HoS, at least in {{yarn-client}} mode?

It looks like in-process launcher is targeted for Spark 2.3.0, so we might have to wait to get this into Hive, or we can use spark-2.3.0-rc0 which looks like it will be released soon - http://apache-spark-developers-list.1001551.n3.nabble.com/Branch-2-3-is-cut-td23072.html, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12865595/HIVE-16484.7.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/8398/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/8398/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8398/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2018-01-02 17:27:43.925
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-8398/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2018-01-02 17:27:43.929
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 4ecf2a7 HIVE-18149: addendum; update missed TestAcidOnTez
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 4ecf2a7 HIVE-18149: addendum; update missed TestAcidOnTez
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2018-01-02 17:27:44.476
+ rm -rf ../yetus
+ mkdir ../yetus
+ cp -R . ../yetus
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-8398/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: a/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java: does not exist in index
error: a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java: does not exist in index
error: a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java: does not exist in index
error: a/spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java: does not exist in index
error: patch failed: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java:75
Falling back to three-way merge...
Applied patch to 'spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java' with conflicts.
error: patch failed: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java:24
Falling back to three-way merge...
Applied patch to 'spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java' with conflicts.
Going to apply patch with: git apply -p1
error: patch failed: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java:75
Falling back to three-way merge...
Applied patch to 'spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java' with conflicts.
error: patch failed: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java:24
Falling back to three-way merge...
Applied patch to 'spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java' with conflicts.
U spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java
U spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12865595 - PreCommit-HIVE-Build, {{InProcessLauncher}} works for client mode, but I wouldn't recommend it. It will only allow one Spark application at a time, and has some other side effects (like the system properties being polluted by the application's configuration, and affecting applications launched afterwards).

That should be ok for unit tests, though.

For actual HoS deployments, I don't see a lot of advantages in supporting client mode, so you could use that API to launch everything in cluster mode., Thanks [~vanzin]. I think I was confused about Spark's deploy-mode option. To clarify, you don't recommend using {{InProcessLauncher}} in client mode (but unit tests should be ok), but it should be fine in cluster mode?

I think that should work for HoS, most HoS deployments should be using cluster mode already; we could also just change the code so that {{InProcessLauncher}} is only used when deploy mode = cluster., That's correct. Using it only with deploy mode == cluster sounds good too, although in that case the user still needs to deploy Spark separately somehow if they want to use client mode., Attaching updated patch. Basically, just rebased the old patch and resolved all conflicts.

I'll keep this JIRA focused on migrating to {{SparkLauncher}} and work on integrating with {{InProcessLauncher}} in a sub-task., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12904293/HIVE-16484.8.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/8424/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/8424/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8424/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2018-01-03 21:09:26.686
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-8424/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2018-01-03 21:09:26.690
+ cd apache-github-source-source
+ git fetch origin
From https://github.com/apache/hive
   d015656..006d69d  master     -> origin/master
+ git reset --hard HEAD
HEAD is now at d015656 HIVE-17929: Use sessionId for HoS Remote Driver Client id (Sahil Takiar, reviewed by Rui Li)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is behind 'origin/master' by 1 commit, and can be fast-forwarded.
  (use "git pull" to update your local branch)
+ git reset --hard origin/master
HEAD is now at 006d69d HIVE-18360 : NPE in TezSessionState (Sergey Shelukhin, reviewed by Jason Dere)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2018-01-03 21:09:30.024
+ rm -rf ../yetus
+ mkdir ../yetus
+ cp -R . ../yetus
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-8424/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: a/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java: does not exist in index
error: a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java: does not exist in index
error: a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java: does not exist in index
error: a/spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java: does not exist in index
error: patch failed: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java:84
Falling back to three-way merge...
Applied patch to 'spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java' with conflicts.
error: patch failed: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java:82
Falling back to three-way merge...
Applied patch to 'spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java' with conflicts.
error: patch failed: spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java:307
Falling back to three-way merge...
Applied patch to 'spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java' with conflicts.
Going to apply patch with: git apply -p1
error: patch failed: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java:84
Falling back to three-way merge...
Applied patch to 'spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java' with conflicts.
error: patch failed: spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java:82
Falling back to three-way merge...
Applied patch to 'spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java' with conflicts.
error: patch failed: spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java:307
Falling back to three-way merge...
Applied patch to 'spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java' with conflicts.
U spark-client/src/main/java/org/apache/hive/spark/client/SparkClientFactory.java
U spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
U spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12904293 - PreCommit-HIVE-Build, Re-based patch., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 30s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 39s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 19s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 20s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 28s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 10s{color} | {color:red} common: The patch generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m  9s{color} | {color:red} spark-client: The patch generated 20 new + 22 unchanged - 20 fixed = 42 total (was 42) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 11s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 10m 48s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |
| git revision | master / eca6b89 |
| Default Java | 1.8.0_111 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8433/yetus/diff-checkstyle-common.txt |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8433/yetus/diff-checkstyle-spark-client.txt |
| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-8433/yetus/patch-asflicense-problems.txt |
| modules | C: common spark-client U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8433/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12904474/HIVE-16484.9.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 17 failed/errored test(s), 11542 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join25] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_hook] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=35)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucketsortoptimize_insert_2] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[hybridgrace_hashjoin_2] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=164)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=159)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[authorization_part] (batchId=93)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=120)
org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testTransactionalValidation (batchId=213)
org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.testWrite (batchId=253)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=225)
org.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=231)
org.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=231)
org.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=231)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/8433/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/8433/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8433/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 17 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12904474 - PreCommit-HIVE-Build, Test failures are un-related. I updated the RB and added a few notes to explain what the code is doing - https://reviews.apache.org/r/58684/ [~xuefuz], [~lirui] can you review?, Thanks [~stakiar] for the work! I had a quick glance at the code and left some comments. Will try to find some time and look into more details tomorrow., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  1s{color} | {color:blue} Findbugs executables are not available. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 21s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 14s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 26s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 18s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 20s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 26s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m  9s{color} | {color:red} common: The patch generated 4 new + 5 unchanged - 0 fixed = 9 total (was 5) {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m  8s{color} | {color:red} spark-client: The patch generated 11 new + 22 unchanged - 20 fixed = 33 total (was 42) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 10s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 10m  5s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |
| git revision | master / a6b88d9 |
| Default Java | 1.8.0_111 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8482/yetus/diff-checkstyle-common.txt |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-8482/yetus/diff-checkstyle-spark-client.txt |
| modules | C: common spark-client U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8482/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12904861/HIVE-16484.10.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 18 failed/errored test(s), 11549 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=48)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_hook] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=35)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucketsortoptimize_insert_2] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[hybridgrace_hashjoin_2] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=164)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=159)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[authorization_part] (batchId=93)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[stats_aggregator_error_1] (batchId=93)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=120)
org.apache.hadoop.hive.metastore.TestEmbeddedHiveMetaStore.testTransactionalValidation (batchId=213)
org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.testWrite (batchId=253)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=225)
org.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=231)
org.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=231)
org.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=231)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/8482/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/8482/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8482/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 18 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12904861 - PreCommit-HIVE-Build, Hi [~stakiar], got a higher level question: what's the main advantage of moving to SparkLauncher if it still requires spark-submit? One advantage I see is finer grained app state change, e.g. SUBMITTED.
On the other hand, the LauncherServer and LauncherBackend looks similar to our RPC framework and some functions are overlapping, e.g. we already have the EndSession to shut down the Spark app, not sure whether we still need {{SparkAppHandle::stop/kill}}., Hey [~lirui], thanks for taking a look. You bring up a good point about the {{LauncherServer}} and {{LauncherBackend}}, we probably don't need those and using them would double the # of connections required for HoS sessions.

[~vanzin] would it be possible to avoid using the {{LauncherServer}} / {{LauncherBackend}} for the {{InProcessLauncher}} when in {{yarn-cluster}} mode? If not, maybe we could just do the same thing that {{InProcessLauncher}} is doing? It looks like it invokes {{SparkSubmit#main}} directly., {{InProcessLauncher}} also uses the launcher server. So that won't save you anything.

The most you can do here is call "handle.disconnect()" to close the connection without stopping the app. You'll still need the launcher server for a little bit of time, though, so you won't completely get rid of it.

Supporting server-less handles with {{InProcessLauncher}} should be possible, but it hasn't been implemented yet., [~vanzin] thanks for the info. What about just directly calling {{SparkSubmit#main}}? It seems that is what {{InProcessLauncher}} is doing internally. We can special-case it so {{SparkSubmit#main}} is only used in {{cluster}} deploy-mode., Yes, that's what the launcher does internally, but also, it's an internal API, sort of. You can try it and it will probably work, but I'd stick with the public API unless the extra fds are a real issue (one extra fd per session doesn't sound too crazy?)., I'd echo with [~lirui], wondering the benefits the proposal brings. While I only gave a brief look on the patch, but from the conversations I found that SparkLauncher doesn't really offer all the advantages that are listed in the description. Rather, it brings uncertainty and possible stability issues in Hive.

We have been using HoS using spark-submit for our production. While it bears some imperfection (like launching a dummy process), it works for us. I'd feel nervous in completely different code path which is so critical. Moreover, security related stuff will need more testing at least.

Having said that, I'd suggest we keep existing implementation of Spark job submission. If we want to test out SparkLauncher, I think we can use it to replace the other code path where class {{org.apache.spark.deploy.SparkSubmit}} is directly invoked(, if that makes sense at all).

When SparkLauncher becomes mature and capable of replacing {{bin/spark-submit}} with the promised benefits, we can make a switch in later releases, which hopefully brings no impact to Hive on Spark users., [~xuefuz] thanks for voicing your concern, I see a few benefits to doing this:

* The main benefit is the usage of {{InProcessLauncher}} which was added in SPARK-11035
** I didn't add the integration with {{InProcessLauncher}} to this patch mainly because I didn't want the diff to get too big; I plan to add integration with {{InProcessLauncher}} in another JIRA
** The {{InProcessLauncher}} avoids running {{bin/spark-submit}}, it calls {{SparkSubmit#main}} directly, which decreases the amount of time it takes to start a HoS session; a separate process doesn't need to be launched to start the Spark app
** It also makes HoS easier to debug because everything is run in a single process, we don't have to rely on re-directing stdout / stderr output streams, etc.
* The API is much cleaner than building up command line arguments for {{bin/spark-submit}}

Some other thoughts:

{quote} Moreover, security related stuff will need more testing at least. {quote} I'm not that familiar with the security aspects of HoS, but I can add some tests with {{MiniHiveKdc}} / doAs to check if things are still good. 

{quote} I'd feel nervous in completely different code path which is so critical {quote} Valid point, but the code path isn't that different, at the end of the day everything is going through {{SparkSubmit.scala}}.

{quote} we can make a switch in later releases {quote} I don't think we have plans to release Hive 3.0.0 anytime soon, so we can fix any issues with {{SparkLauncher}} before the release.

Let me know your thoughts., Just amending my previous response: if the concern with {{SparkLauncher}} is the number of file descriptors because of the extra one used by the launcher server connection, using {{InProcessLauncher}} will probably end up decreasing the number of fds being used. Instead of potentially 3 fds for a child process (pipes for stdin / stdout / stderr), you have one for the socket connection. For the normal, child process case, then yes, you just get one extra file descriptor. (Or maybe you get even, because I think {{SparkLauncher}} will merge stdout and stderr in that case.)

As for why this is better, I think the main advantage will come by using {{InProcessLauncher}} eventually, since Hive wouldn't need a separate Spark installation to be able to launch Spark apps. It could ship with everything ready to run HoS out of the box.

Security can probably become simpler; instead of having to run kinit before starting a Spark child process, HS2 could potentially just instantiate {{InProcessLauncher}} inside a {{proxyUser.doAs}} call. I haven't actually tried that, but that's the general idea of how to use it in a secure env.
, [~stakiar], I'm not denying the potential benefits we might get, for which I'm totally up for them. However, I wouldn't feel comfortable to replace a critical code path that's proven working with something that's completely new. For this, a fallback is much better than a sheer replacement., [~xuefuz] makes sense. In that case, maybe making this change configurable (and turned off by default) makes sense? I can create a separate instance of {{SparkClient}} so that there aren't many changes required to {{SparkClientImpl}}., bq. Hive wouldn't need a separate Spark installation to be able to launch Spark apps. It could ship with everything ready to run HoS out of the box.
Yeah I also believe that's the main benefit. But if SparkLauncher cannot give us that, why don't we just use {{InProcessLauncher}}?

Regarding the extra connection, I'm not sure how it impacts us performance-wise. My main concern is it brings extra chance of issues while the benefits are not quite clear. For example, we had several connection timeout issues with the RPC framework. And seems {{LauncherServer}}/{{LauncherBackend}} have very similar configs to tweak, like {{spark.launcher.childConnectionTimeout}}.

Regarding debug, I assume it's mainly for yarn-client mode right? Because the process we launched in yarn-cluster mode is only a light-weight client talking to RM. And by deault it exits once the app starts running(HIVE-13895). I agree it makes debugging easier, but again that require InProcessLauncher.

So my suggestion is we wait until InProcessLauncher is released and implement another SparkClient using it. We can decide whether to get rid of the current SparkClientImpl when InProcessLauncher is mature. Does that make sense?

BTW, is there any docs about the SparkLauncher implementation? I just want to have a better understanding about it., Sounds good to me. I think release for Spark 2.3.0 is just starting - http://apache-spark-developers-list.1001551.n3.nabble.com/Branch-2-3-is-cut-td23072.html 

Once that gets started I'll work on using {{InProcessLauncher}} in a new {{SparkClient}}.

I'm going to close this JIRA for now, the comments and patch history are already pretty long. The original intention was just an investigation, which I think is complete since we have a plan going forward. I'll work on the {{InProcessLauncher}} integration in another JIRA., Cool. 
{quote}
Once that gets started I'll work on using InProcessLauncher in a new SparkClient.
{quote}
Another option is to use InProcessLauncher to replace what is current in SparkClientImpl that invokes SparkSubmit class directly, which isn't used much anyway. We can organize whatever a way to make code cleaner. ]