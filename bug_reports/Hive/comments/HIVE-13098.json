[This is especially problematic for implicit conversions..., The epic WIP patch... still need to take care of some paths.

Propagating config to all decimals is difficult thanks to massive static use in Hive..., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12830622/HIVE-13098.WIP.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1325/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1325/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1325/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/custom-udfs/udf-vectorized-badexample/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ udf-vectorized-badexample ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ udf-vectorized-badexample ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ udf-vectorized-badexample ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/custom-udfs/udf-vectorized-badexample/target/udf-vectorized-badexample-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ udf-vectorized-badexample ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ udf-vectorized-badexample ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/custom-udfs/udf-vectorized-badexample/target/udf-vectorized-badexample-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-custom-udfs/udf-vectorized-badexample/2.2.0-SNAPSHOT/udf-vectorized-badexample-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/custom-udfs/udf-vectorized-badexample/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-custom-udfs/udf-vectorized-badexample/2.2.0-SNAPSHOT/udf-vectorized-badexample-2.2.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - HCatalog Unit Tests 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hcatalog-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---
[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/test-classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-hcatalog-it-unit ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-jar-plugin:2.4:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-util ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 66 source files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Recompile with -Xlint:deprecation for details.
[WARNING] Some messages have been simplified; recompile with -Xdiags:verbose to get full output
[INFO] 3 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,60] no suitable method found for create(java.lang.String)
    method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable
      (argument mismatch; java.lang.String cannot be converted to int)
    method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable
      (argument mismatch; java.lang.String cannot be converted to long)
[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,91] no suitable method found for create(java.lang.String)
    method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable
      (argument mismatch; java.lang.String cannot be converted to int)
    method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable
      (argument mismatch; java.lang.String cannot be converted to long)
[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,122] no suitable method found for create(java.lang.String)
    method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable
      (argument mismatch; java.lang.String cannot be converted to int)
    method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable
      (argument mismatch; java.lang.String cannot be converted to long)
[INFO] 3 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [1.672s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [3.930s]
[INFO] Hive Integration - Custom udfs .................... SUCCESS [0.921s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-util  SUCCESS [0.796s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf1  SUCCESS [0.864s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf2  SUCCESS [0.801s]
[INFO] Hive Integration - Custom UDFs - udf-vectorized-badexample  SUCCESS [0.717s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [4.335s]
[INFO] Hive Integration - Testing Utilities .............. FAILURE [4.740s]
[INFO] Hive Integration - Unit Tests ..................... SKIPPED
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] Hive Integration - QFile Accumulo Tests ........... SKIPPED
[INFO] JMH benchmark: Hive ............................... SKIPPED
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] Hive Integration - QFile Spark Tests .............. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 19.516s
[INFO] Finished at: Wed Sep 28 04:54:48 UTC 2016
[INFO] Final Memory: 87M/716M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-it-util: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,60] no suitable method found for create(java.lang.String)
[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable
[ERROR] (argument mismatch; java.lang.String cannot be converted to int)
[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable
[ERROR] (argument mismatch; java.lang.String cannot be converted to long)
[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,91] no suitable method found for create(java.lang.String)
[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable
[ERROR] (argument mismatch; java.lang.String cannot be converted to int)
[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable
[ERROR] (argument mismatch; java.lang.String cannot be converted to long)
[ERROR] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/accumulo/AccumuloTestSetup.java:[91,122] no suitable method found for create(java.lang.String)
[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(int) is not applicable
[ERROR] (argument mismatch; java.lang.String cannot be converted to int)
[ERROR] method org.apache.hadoop.hive.common.type.HiveDecimal.create(long) is not applicable
[ERROR] (argument mismatch; java.lang.String cannot be converted to long)
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-util
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12830622 - PreCommit-HIVE-Build, This patch handles many more paths.
I think it's close to being ready to review/commit..
The main question is how to handle OIs, cause OI static giant global methods are just too general. I almost wonder if I should give up and add a threadlocal just for that case in the static OI class that would read config once per thread... however, that will make the config global for that case. We can document that config is HS2-wide and cannot be changed per query and do a followup.
As for write paths, these would need to be handled on case by case basis cause their initialization is obscure. I started on the ORC write path, but put the config in the wrong place for master (it's the right place for branch-1). I will finish that.

[~jdere] [~ashutoshc] do you want to take a look? 95% of the changes are just propagating the stupid config. Main changes are in HiveDecimal and HiveDecimalOverflow classes., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12830837/HIVE-13098.WIP2.patch

{color:green}SUCCESS:{color} +1 due to 57 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 97 failed/errored test(s), 10645 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[annotate_stats_select]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_1]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_2]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_5]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_precision]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_skewjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_stats]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_ppd_decimal]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_ppd_decimal]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_format_number]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_greatest]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_least]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_to_byte]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_to_long]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_to_short]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_aggregate_9]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_between_in]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_cast_constant]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_1]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_2]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_3]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_aggregate]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_precision]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_decimal_udf]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_struct_in]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_0]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_13]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_17]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorization_short_regress]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[tez_union_decimal]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[tez_vector_dynpart_hashjoin_1]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_aggregate_9]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_between_in]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_cast_constant]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_char_mapjoin1]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_2]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_3]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_aggregate]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_precision]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_decimal_udf]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_inner_join]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_interval_mapjoin]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_join_filters]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_left_outer_join2]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_left_outer_join]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_leftsemi_mapjoin]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_mapjoin_reduce]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join0]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join1]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join2]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join3]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join4]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join5]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_outer_join6]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vector_varchar_mapjoin1]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorization_0]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorization_13]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorization_17]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorization_short_regress]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorized_context]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorized_mapjoin]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[vectorized_nested_mapjoin]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_inner_join]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join0]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join1]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join2]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join3]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join4]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join5]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query21]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_between_in]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_cast_constant]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_decimal_aggregate]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_left_outer_join]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_0]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_13]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_17]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_short_regress]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_mapjoin]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_nested_mapjoin]
org.apache.hadoop.hive.common.type.TestHiveDecimal.testMultiply
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgDecimal
org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testAvgDecimalNegative
org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumDecimal
org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.testSumDecimalHive6508
org.apache.hadoop.hive.ql.exec.vector.TestVectorSerDeRow.testVectorSerDeRow
org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.testCastTimestampToDecimal
org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testByteDivideShort
org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testDecimalDivideDecimal
org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.testLongDivideDecimal
org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPrintf.testDecimalArgs
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1345/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1345/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1345/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 97 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12830837 - PreCommit-HIVE-Build, Upon looking further on OI path I don't think it's possible to propagate it there without major changes, in fact OI-related parts of this patch are not valid, since OIs are assumed to be stateless and are cached process-wide, ditto for TypeInfo-s. There are lots of static method paths accessing those...
I think I might scrape a lot of the patch and add a globally accessible static that would have to be initialize on CLI/HS2/task startup.. The only exception would be write path that happens outside of Hive services... 

This will reduce size of the patch a lot (but also make it a global setting not modifiable per query...)

Update: another alternative would be a (TADA!) threadlocal.
We could set it at compile time and change the patch to have only compile paths use it, whereas runtime paths would use the fields in OIs and fns that compile populates. As much as I hate threadlocals, I think that's the best approach as it will make patch smaller (right now 700kb of code changes is not even everything, OI changes would be massive), also allow one to set it per query and remove the requirement to initialize it for everyone using Hive libs, since APIs would not use it beyond compilation.

[~ashutoshc] [~hagleitn] [~jdere] opinions?, I will stop working on this for now cause it;s a giant annoying time sink. If there are no objections to threadlocal I will go with that., What if we added some function(s) to help people explore their data?

What about a function that takes a column value or expression and a target data type and reports on how that conversion would go.

For example, for string to int, it could report:
   string doesn't parse to a number,
   string has decimal digits that would be thrown away,
   number parses but would overflow an int

For string to decimal, it could report:
   (parse errors)
   Integer digits will not fit in decimal precision
   Decimal digits would require rounding given the precision.

We could even go further and have function(s) that examine a string column and speculate on good possible data types that would be appropriate for a conversion.

We could borrow ideas from the schema discovery folks (Drill?).

, [~hagleitn] Perhaps a different approach., [~mmccline] the main concern here is automated nulls, where people get them after they import a large amount of data. If the ETL runs every day they cannot be expected to look at the data every day (arguably it should be cleaned up by someone else before Hive in this case, but people make mistakes and there are bugs in other code...)

One way to handle this for most cases would be to break the existing behavior to always throw, and add a separate UDF ("trycast"?) for people who don't care. However that would only work in queries, not for automated pipelines/writers.
, There are other industry solutions.  E.g. Greenplum added an ERROR TABLE feature a long time ago for saving rejected rows so they could be cleaned and add later.  Also, see Teradata., Well, the crux of the matter is, whatever solution we do in Hive would be super unwieldy code-wise, because decimals, decimal OIs, etc. are created in 1000000 places in giant static methods. I was going to add a global for compilation (thread-local since that is single threaded) to be able to populate it  everywhere. At runtime (or during import) it can be used from the fields in runtime objects (see DecimalUdf interface in the patch and its usage). That would reduce the impact a lot compared to 700Kb of code changes...
Then we can choose what to do with the config once all the requisite code has it.
The question is whether to do it at all, esp. since as Gopal noted other types are also converted to null on overflow (unless they are bugged like decimal-to-int cast ;)), so the proper fix that covers all the cases uniformly would be very um, impactful in the codebase.

It is much easier to make the solution that requires user to actively participate (e.g. trycast or functions to explore data), but it doesn't cover the main case of the automated nullification., bq. However that would only work in queries, not for automated pipelines/writers.

There's already a config for this problem for SQOOP, right? {{sqoop.bigdecimal.format.string}}?, Not sure how that config is relevant? Different columns could have different widths, etc. Sqoop could have a config that would ensure that data fits in each column, but I don't think it does.]