[*seams to only affects ORC format*

More investigation on last trunk (26/09/2014):
{noformat}
0: jdbc:hive2://nc-h04:10000/casino> drop table if exists foo6;
No rows affected (0.121 seconds)
0: jdbc:hive2://nc-h04:10000/casino> create table foo6 (id int);
No rows affected (0.08 seconds)
0: jdbc:hive2://nc-h04:10000/casino> insert into table foo6 VALUES(1);
No rows affected (2.823 seconds)
0: jdbc:hive2://nc-h04:10000/casino> select * from foo6;
+----------+--+
| foo6.id  |
+----------+--+
| 1        |
+----------+--+
1 row selected (0.079 seconds)
0: jdbc:hive2://nc-h04:10000/casino>
0: jdbc:hive2://nc-h04:10000/casino>
0: jdbc:hive2://nc-h04:10000/casino> drop table if exists foo7;
No rows affected (0.127 seconds)
0: jdbc:hive2://nc-h04:10000/casino> create table foo7 (id int) STORED AS ORC;
No rows affected (0.059 seconds)
0: jdbc:hive2://nc-h04:10000/casino> insert into table foo7 VALUES(1);
No rows affected (1.707 seconds)
0: jdbc:hive2://nc-h04:10000/casino> select * from foo7;
+----------+--+
| foo7.id  |
+----------+--+
+----------+--+
No rows selected (0.084 seconds)
{noformat}, I see in log :
{noformat}
2014-09-26 15:32:18,483 ERROR [Thread-8]: compactor.Initiator (Initiator.java:run(111)) - Caught exception while trying to determine if we should compact testsimon.values__tmp__table__11.  Marking clean to avoid repeated failures, java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:88)

2014-09-26 15:32:18,484 ERROR [Thread-8]: txn.CompactionTxnHandler (CompactionTxnHandler.java:markCleaned(355)) - Unable to delete compaction record
{noformat}

I wonder if there is a problem with the tables that store "values".

Every "values" table stay in database :
{noformat}
0: jdbc:hive2://nc-h04:10000/casino> show tables;
+-----------------------------------+--+
|             tab_name              |
+-----------------------------------+--+
| classification_compte             |
| dim_hotesse                       |
...
| societe                           |
| testsimon__dim_lieu_sorted_dls__  |
| values__tmp__table__10            |
| values__tmp__table__11            |
| values__tmp__table__12            |
| values__tmp__table__2             |
| values__tmp__table__3             |
| values__tmp__table__4             |
| values__tmp__table__5             |
| values__tmp__table__6             |
| values__tmp__table__7             |
| values__tmp__table__8             |
| values__tmp__table__9             |
+-----------------------------------+--+
47 rows selected (0.061 seconds)
0: jdbc:hive2://nc-h04:10000/casino>
{noformat}, The values table should vanish as soon as you close the session that did the insert, since they are temp tables., [~damien.carol], based on your output above it looks like you are reading the data via HS2 (since it says jdbc in your command line).  There was a definite bug in using ACID via HS2.  I'm curious if you still see this after HIVE-8203.  I'll have that one checked in as soon as the 24 hours after Eugene's +1 passes., bq. "... based on your output above it looks like you are reading the data via HS2."
[~alangates] yes, I'm using HS2. I saw your comment on HIVE-8203 and I made the link. I'm waiting the fix of HIVE-8203 and I will re-test on real data.
The behaviour of "values" tables is still confusing., [~alangates] I made lot of tests in WE. It seems that INSERT/DELETE/UPDATE doesn't work at all with concurrency enabled.

If I deactivate ACID with :
{noformat}
<!-- concurrency -->
<property>
  <name>hive.support.concurrency</name>
  <value>false</value>
</property>

<!-- compactor org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager / org.apache.hadoop.hive.ql.lockmgr.DbTxnManager -->
<property>
  <name>hive.txn.manager</name>
  <value>org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager</value>
</property>
{noformat}
then everything is ok., To be more precise, this commands works :
{code}
drop table if exists foo6;
create table foo6 (id int) clustered by (id) into 1 buckets;
insert into table foo6 VALUES(1);
select * from foo6;

drop table if exists foo7;
create table foo7 (id int) STORED AS ORC;
insert into table foo7 VALUES(1);
select * from foo7;
{code}, This bug is still here even with HIVE-8203 committed., I think I can reproduce the same bug with 2 command line sessions doing things in the following order:

# Start session 1
# in session 1 insert into table
# start session 2
# in session 2 select * see all rows
# in session 1, delete some rows
# in session 1 selec *, see less rows
# in session 2 select * , see all rows

If I stop and restart session 2 after this, than it sees the appropriate number of rows.  So either it isn't getting new transaction information for each query in the session, or the results are being cached somewhere on it.

Does this match the behavior you're seeing?, I need to test [~alangates] use case.
I will also try another test:  stop and restart session between each query.

Few details on my config:
* We use Tez session
* We use container re-use, At the moment I'm testing with MR.

Also, I see that if I do a query that causes an MR job (for instance {{select count (*))}}) then it gets the right result.

In looking through the logs the select * on session two (step 7 above) actually sees the wrong number of delta files in the directory.  I'm working on debugging whether this is because it is caching the file listing (which I doubt) or because it is caching the transaction information (more likely) and not seeing the newer files because it believes it shouldn't.  I have confirmed that Driver.runInternal (which encodes the transaction information) is encoding the right information, but that may not be being properly propagated to the FileSinkOperator reading the data. , Restarting HDFS AND Yarn AND remote Metastore AND Hiveserver2 didn't helped.
I think the bug comes because there are no "base" dir.
When I'm doing major compaction. The base is created and I can see the new rows.

, I don't think it's a cache issue. Doing stop/run of ALL daemons of the cluster change nothing., Another strange result, when I'm doing this query:
{code}
select ROW__ID, INPUT__FILE__NAME, * from foo7;
{code}
I'm having this result :
{noformat}
+-----------------------------------------------+-----------------------------------------------------------------------------+----------+--+
|                    row__id                    |                              input__file__name                              | foo7.id  |
+-----------------------------------------------+-----------------------------------------------------------------------------+----------+--+
| {"transactionid":536,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":537,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":538,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":539,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":540,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":541,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":542,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":544,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":545,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
| {"transactionid":546,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000542/bucket_00000  | 1        |
+-----------------------------------------------+-----------------------------------------------------------------------------+----------+--+
10 rows selected (0.168 seconds)
{noformat}

Which is wrong.

See here :
{noformat}
0: jdbc:hive2://nc-h04:10000/casino> dfs -ls /user/hive/warehouse/casino.db/foo7;
+-------------------------------------------------------------------------------------------------------------------------+--+
|                                                       DFS Output                                                        |
+-------------------------------------------------------------------------------------------------------------------------+--+
| Found 4 items                                                                                                           |
| drwxr-xr-x   - hduser supergroup          0 2014-09-30 11:29 /user/hive/warehouse/casino.db/foo7/base_0000542           |
| drwxr-xr-x   - hduser supergroup          0 2014-09-30 11:30 /user/hive/warehouse/casino.db/foo7/delta_0000544_0000544  |
| drwxr-xr-x   - hduser supergroup          0 2014-09-30 11:30 /user/hive/warehouse/casino.db/foo7/delta_0000545_0000545  |
| drwxr-xr-x   - hduser supergroup          0 2014-09-30 11:30 /user/hive/warehouse/casino.db/foo7/delta_0000546_0000546  |
+-------------------------------------------------------------------------------------------------------------------------+--+
5 rows selected (0.025 seconds)
{noformat}
, With block offset VC :
{noformat}
+-----------------------------------------------+-----------------------------------------------------------------------------+------------------------------+----------+--+
|                    row__id                    |                              input__file__name                              | block__offset__inside__file  | foo7.id  |
+-----------------------------------------------+-----------------------------------------------------------------------------+------------------------------+----------+--+
| {"transactionid":536,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 61                           | 1        |
| {"transactionid":537,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 122                          | 1        |
| {"transactionid":538,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 183                          | 1        |
| {"transactionid":539,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 244                          | 1        |
| {"transactionid":540,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 306                          | 1        |
| {"transactionid":541,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 367                          | 1        |
| {"transactionid":542,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 428                          | 1        |
| {"transactionid":544,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 489                          | 1        |
| {"transactionid":545,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 550                          | 1        |
| {"transactionid":546,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 612                          | 1        |
| {"transactionid":547,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 612                          | 1        |
| {"transactionid":548,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 612                          | 1        |
| {"transactionid":549,"bucketid":0,"rowid":0}  | hdfs://nc-h04/user/hive/warehouse/casino.db/foo7/base_0000546/bucket_00000  | 612                          | 1        |
+-----------------------------------------------+-----------------------------------------------------------------------------+------------------------------+----------+--+
13 rows selected (0.162 seconds)
{noformat}

Column {{input\_\_file\_\_name}} and {{block\_\_offset\_\_inside\_\_file}} have wrong values for the last 3 rows. These rows are in delta directories., Ok, I'm not sure if we're chasing the same bug or not.  But I'll keep chasing the one I see and if we get lucky it will turn out to have the same root cause.

Could you turn on debug level logging on your hive client and HiveServer2 instance, then do the insert and select that reproduces the error and attach both logs.  That would help me have an idea where things may be going wrong., I mean JDBC client, not hive client., I definitely think we are seeing separate issues.  I have a filed a new issue HIVE-8311 for what I am seeing., Attached few files.

Use case :
1. drop table if exists foo7 (no log)
2. create table foo7 (id int) STORED AS ORC (no log)
3. insert into table foo7 VALUES(1) (log a_hiveserver2 and a_beeline)
4. select * from foo7  (log b_hiveserver2 and b_beeline), [~alangates] I agree because every query in my cluster use the SAME Tez Session and reuse the same containers.
Feel free to help me on this one once HIVE-8311 is out :), 5. select distinct id from foo7 (log c_beeline and c_hiveserver2), Damien, I'm definitely still trying to debug this.  But I'm juggling it with several other bugs, so it may be a day or until until I figure anything out., [~alangates] HIVE-8290 solve this issue.

As HIVE-8290 is commited, HIVE don't try to use {{foo7}} as bucketed table., This has been fixed in 0.14 release. Please open new jira if you see any issues.
]