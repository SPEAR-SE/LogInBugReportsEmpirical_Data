[The result of division has the maximum precision. The following multiplication makes a result with a greater precision than the maximum precision, so it causes an error. https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27838462#LanguageManualTypes-FloatingPointTypes provides more information.

Please use ROUND() to reduce precisions or use DOUBLE to be tolerant of precision issues., I don't know how other database handles this issue. If it works differently from others, please tell me. It may be a bug., To avoid this problem, I changed HiveDecimal#multiply as following. It produced a non-null result, with loss of precision. I'm not sure whether it reflects the original purpose or not.

{code}
  public HiveDecimal multiply(HiveDecimal dec) {
    return new HiveDecimal(bd.multiply(dec.bd), true);
  }
{code}, The answer cannot be anymore precise than the most precise operand.  If this were a scientific calculation, then it would be the least precise operand that would dictate the precision.  I have not actually looked at how other databases handle the situation, but they certainly do not throw nulls.  I would use the most precise operand as my guide just to be safe.  As you said in your note, people can always round up if they want fewer significant digits.

Thank!, It seems like that multiplication is not the only one makes this error. Multiple additions, subtractions, and a power after a division can make this error, too. So I will update the patch., Review request on https://reviews.apache.org/r/13553/, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12597894/HIVE-5022.2.patch.txt

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 2856 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_precision
org.apache.hadoop.hive.ql.io.orc.TestOrcFile.testUnionAndTimestamp
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_decimal_udf
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_infer_bucket_sort_reducers_power_two
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/434/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/434/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests failed with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated., It affected some arithmetic results. I'll update it. :), I could not reproduce the error on infer_bucket_sort_reducers_power_two.q. It may be a temporal error caused by parallel tests. I fixed other errors., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12598711/HIVE-5022.3.patch.txt

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 2885 tests executed
*Failed tests:*
{noformat}
org.apache.hcatalog.listener.TestNotificationListener.testAMQListener
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/483/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/483/console

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests failed with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated., Just want to point out that there is also HIVE-3976 which will eventually allow a user to specify decimal precision/scale.  This may help a bit, if there are rules about the precision/scale of a result based an arithmetic operation between values of known precision/scale. Also want to make sure that whatever is done here makes sense with what is being done with HIVE-3976.  Does anyone know what the SQL reference says about how this should be handled? I'll try to take a look at what the reference says., [~jdere] I expect HIVE-3976 will make sense with HIVE-5022. It looks great.

I'll check whether they work together well and meet SQL specifications in next week., I tried to keep the data type standard compliant when I first implemented the HiveDecimal. I believe that the proposed patch isn't in accordance with the standard though. I might have misinterpreted it - happy to be corrected.

From what I remember:

- Precision is always implementation specific
- Scale is *only* implementation specific for division

If an exact numeric value cannot be represented without rounding (in mult/add/sub) you're supposed to raise an error. I didn't do that because that seems bad in hive, so I returned "null" from the operation. Rounding implicitly for mult/add/sub seems to sort of defeat the purpose of having an exact numeric type.

For division you can round or truncate but not loose any leading significant digits.

(side note: div by zero is also supposed to error out, but in hive it returns null)

So, one way to fix this problem would be to more aggressively round or truncate division. That way you will have more "room" for subsequent operations. Better yet, with HIVE-3976 we could have a default that's much smaller than the max and just round to the current scale of the dividend. This won't resolve this problem, but make it much less likely to occur.

I also think that [~jdere] makes a good point. If we change the behavior in a non backwards compatible way, it would be really great to do that only once (i.e.: have HIVE-5022 and HIVE-3976 in the same release). In the meantime the "round manually" workaround should help.
, There's the actual SQL reference rules regarding exact precision arithmetic (6.12 if you're looking at SQL92, and it looks like later references look the same):

         1) If the data type of both operands of a dyadic arithmetic opera-
            tor is exact numeric, then the data type of the result is exact
            numeric, with precision and scale determined as follows:

            a) Let S1 and S2 be the scale of the first and second operands
              respectively.

            b) The precision of the result of addition and subtraction is
              implementation-defined, and the scale is the maximum of S1
              and S2.

            c) The precision of the result of multiplication is implementation-
              defined, and the scale is S1 + S2.

            d) The precision and scale of the result of division is
              implementation-defined.


I'd agree with what hagleitn said about the resulting precision/scale of division operations, Hive is allowed to define what precision/scale it returns on division, and it probably should not be allowed to take up the entire precision. Tinkering with MySQL a bit, it looks like it follows the multiplication scale rules until it hits the max scale of 30, and then any further multiplications continue to have scale 30.  Not quite sure what rules it's using for division scale, but it also does not exceed their max scale of 30., Since I have been working on HIVE-3976 and its relatives and since this is closely related to decimal precision/scale handling, I take this issue going forward. Hopefully, this will be fixed when HIVE-3976 and the related are completed., This is fixed via HIVE-3976 and its related tasks (HIVE-5356, HIVE-5866, etc.). Now the output:
{code}
hive> select (cast (4.53 as decimal) / cast(25.86 as decimal)) * cast(0.087 as decimal) from decimal_udf limit 1;
OK
0
hive> select (cast (4.53 as decimal(3,2)) / cast(25.86 as decimal(4,2))) * cast(0.087 as decimal(3,3)) from decimal_udf limit 1;
OK
0.015240138
hive> select cast (4.53 as decimal) / cast(25.86 as decimal) * cast(0.087 as decimal) from decimal_udf limit 1;  
OK
0
hive> select cast (4.53 as decimal(3,2)) / cast(25.86 as decimal(4,2)) * cast(0.087 as decimal(3,3)) from decimal_udf limit 1;
OK
0.015240138
{code}]