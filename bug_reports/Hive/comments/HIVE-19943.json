[I didn't find this bug,but  my version is hive-1.1.0 on cdh5.11.2,I'll use it later 2.1.0 version test., I didn't find 2.1.0. Can you tell me where you downloaded it? Or 2.1.0 is not the official version, then you download it, if not the official version, or try not to use it., We did not download the Hive version our self's. We are running it on Azure HDInsight (more information [Here|[https://azure.microsoft.com/en-us/services/hdinsight/])]. We are using the Hive interactive query option as well. I am talking about Hive2 just to be on the same line, Ask for help, If it's HDInsight, I'm sorry, I can't solve this problem., I think it should probably work the same way there as the regular hive :D
I've a hunch that probably the stats optimizer have got the wrong value; and on hdi that thing is turned on...
Could you please upload the textfile with the appropriate contents; and give the commands being used - to be sure that we are on the same page here?, That is what i thought as well.

okay will have a look at how that works, Do i disable it just for testing purposes?

updated my question with what you requested., thanks; however I was also able to reproduce the issue
something seems to be really off here...doing a group by id brings the header in..., It seems to me that somehow the reader somehow picks it up...I guess that some optimization is replacing the reader with a more specialized one - but that one doesn't handle the header skip....
disabling llap or vectorization restores good behaviour for me.
{{set hive.vectorized.execution.enabled=false}}, okay, can i ask what what does this do exactly? just to know what impact it can have for the project if we start working with big datasets., I just tested your solution and it seems to work.

However we need this enabled for Interactive query and to run faster queries on bigger data sets so this is not a real solution for us at this moment.

For now we are going to remove the headers but this loses readability if somebody gets the file and looks into it without a header. Is there maybe another solution as well?

Thanks in advance, it will be slower; I'm not sure how much...
I think it would be good to:

* disable vectorization
* create a new orc table from the input
* enable vectorization

and use the orc table version after that...
, we are already working on orc tables with gzip compressed data.

I tried what you suggested but i still get the header back. Also we are using external tables and those cannot be put into orc because we get the data into csv files. And we also have the same problem there.

Can this be a problem by HDInsight that they need to fix? If this is the case i can raise a ticket by them to look into this because it is kind of a basic  option in my opinion., I think your input table is in text form because {{STORED AS TEXTFILE}} is not an orc table...

Altought it's not specific to hdi...I think reporting to hdi might probably also help - as you will need this fix on the hdi hive to be usable for you...
, That is correct but that is for the external table only because we cannot do it in orc because we import data from SAP to ADLS (Azure data lake store) and this is in a csv file.

Okay will update my question there with your remarks if that is okay for you? (will link to this ticket).

Is it then a problem with Hive itself that needs to be fixed? Because i guesse this is no expected behavior right?

 , I might have misunderstood...but in that case I think you may only need to disable vectorization while you are loading the data from that external table into your orc tables :)

I think referencing this ticket might help., no problem, However we will run this job (moving from external table to hive table) every day and we have now around 218 tables (still counting) So for us this does not seem like a very valid solution.

Is there any way this could be reported to Hive to get fixed? Because to me it looks like a basic question.

I linked the ticket and explained what the problem is. I hope to hear from them soon., {quote}Is there any way this could be reported to Hive to get fixed?{quote}

This ticket is exactly that :P 

I think if you could live with it: right now it would be the best option to disable vectorization - and when the problem is better known there might be other options to workaround the issue - or there might be a fix..., okay guesse we will wait then ;)

We chose to remove the headers all together because we really want to make use of Interactive query for the loads of data we have. Once there might be a fix or a better work around we can add the headers again.

Thanks for your time in investigating this with me!, background: these properties "skip.header.line.count" and it cousine are stored as tableproperties; which makes it somewhat fragile; because its handled directly in the reader - I think if it would be an option to the inputformat; it would "just" work....currently what happens is that the data is "crossing" the reader in vectorized form...and HiveContextAwareRecordReader doesn't really expect that - so the header/footer left in there :D, I'm not sure how this supposed to be fixed; exploring to add these as inputformat args is a dead end because the actual reader is some kind of "linereader" from hadoop...
I feel that this "HiveRecordReader" should somehow be pushed under the llaprecordreader somehow...but that seems like a hard thing to do (and probably not the right move)...

[~sershe] do you have any suggestion?

To reproduce, patching an "existing test" which by mistake only tested the local mode...so it missed this issue all along... (and run it with TestMiniLlapCliDriver)
{code}
diff --git ql/src/test/queries/clientpositive/file_with_header_footer.q ql/src/test/queries/clientpositive/file_with_header_footer.q
index 8913e54ad0..5dddcaba2a 100644
--- ql/src/test/queries/clientpositive/file_with_header_footer.q
+++ ql/src/test/queries/clientpositive/file_with_header_footer.q
@@ -11,6 +11,10 @@ CREATE EXTERNAL TABLE header_footer_table_1 (name string, message string, id int
 
 SELECT * FROM header_footer_table_1;
 
+explain
+SELECT count(distinct name) FROM header_footer_table_1;
+SELECT assert_true(count(distinct name)=11) FROM header_footer_table_1;
+
 SELECT * FROM header_footer_table_1 WHERE id < 50;
 
 CREATE EXTERNAL TABLE header_footer_table_2 (name string, message string, id int) PARTITIONED BY (year int, month int, day int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' tblproperties ("skip.header.line.count"="1", "skip.footer.line.count"="2");
{code}

, I think an easy way to mitigate for now is to not use cache is these properties are present on the table.
They are obscure and generally not recommended... there's typically no reason to have them on any realistically sized dataset, since headers are for humans and humans typically won't be reading files that are larger than few Mb, so they may not be worth caching. The standard recommendation is to clean up the data during ETL.
]