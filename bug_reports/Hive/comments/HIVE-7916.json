[Hi [~xuefuz], I tried on my cluster but cannot reproduce the problem. I removed the spark jars from local maven repo before building hive, so that the jars are downloaded from the AWS server we maintain. After hive is built, I linked the spark-assembly jar to {{lib}} of the hive home directory. The spark-assembly jar is built with {{mvn -Pyarn -Phadoop-2.4 -DskipTests clean package}} of the spark 1.1 branch.
Could you provide more info about your environment, e.g. the spark jars you used or if the table is snappy compressed?, I noted this may be related to SPARK-2881. Snappy-java is bumped to 1.0.5.3 in the 1.1 branch and to 1.1.1.3 in the master branch. Hadoop-2.4.0 seems to use snappy-java-1.0.4.1.
While the snappy-java version is different, I don't see any conflicts on my side.
[~xuefuz], I found the following in the description of SPARK-2881:
{quote}
The issue was that someone else had run with snappy and it created /tmp/snappy-*.so but it had restrictive permissions so I was not able to use it or remove it. This caused my spark job to not start.
{quote}
Could you check if this is the case in your environment?, [~lirui] Thanks for looking into this. I had noticed the version difference, and thought for a moment it was the cause. However, I don't see any /tmp/snappy*.so file in my machine. I also built spark-assembly myself, but with slightly different params:
{code}
mvn -Dhadoop.version=2.3.0-cdh5.0.1 -Phadoop-2.3 -DskipTests clean install.
{code}

Aslo, I'm not sure if my machine has ever run snappy before. Is there anything that needs to be done in order for it to work?

I can try again to reproduce the problem.


, Hi [~xuefuz], do you use the latest code of spark 1.1 branch? SPARK-2881 is resolved with [#1999|https://github.com/apache/spark/pull/1999] for branch-1.1. You can check the pom file of your spark to verify that, latest code uses snappy-java-1.0.5.3., [~lirui], I tried both branch 1.1 and master. Spark 1.1 branch doesn't have the guava lib fix, right? So, using that branch, I hit the guava lib conflict. Using master gives the snappy error.

I'm thinking if you can take a look at the guide at https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started#HiveonSpark:GettingStarted-HiveonSpark:GettingStarted and update it if necessary. Then, I can try to follow the guide to see if I still get the snappy problem or guava problem.

Thanks!, I also hit the following snappy lib exceptions - I am using snappy snappy-java-1.0.5.jar. Let me try upgrading to snappy 1.1.1.3

2014-10-17 16:18:01,977 ERROR [Executor task launch worker-0]: executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 0.0 (TID 0)
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
	at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
	at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
	at org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)
	at org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:125)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1083)
	at org.apache.spark.storage.BlockManager$$anonfun$7.apply(BlockManager.scala:579)
	at org.apache.spark.storage.BlockManager$$anonfun$7.apply(BlockManager.scala:579)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:126)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:192)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4$$anonfun$apply$2.apply(ExternalSorter.scala:732)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4$$anonfun$apply$2.apply(ExternalSorter.scala:731)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:789)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4.apply(ExternalSorter.scala:731)
	at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4.apply(ExternalSorter.scala:727)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:727)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

----------
2014-10-17 16:18:02,021 INFO  [main]: scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 0 failed: foreach at SparkPlan.java:80, took 3.389683 s
2014-10-17 16:18:02,021 ERROR [main]: spark.SparkClient (SparkClient.java:execute(166)) - Error executing Spark Plan
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
        org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        org.xerial.snappy.SnappyOutputStream.<init>(SnappyOutputStream.java:79)
        org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:125)
        org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1083)
        org.apache.spark.storage.BlockManager$$anonfun$7.apply(BlockManager.scala:579)
        org.apache.spark.storage.BlockManager$$anonfun$7.apply(BlockManager.scala:579)
        org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:126)
        org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:192)
        org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4$$anonfun$apply$2.apply(ExternalSorter.scala:732)
        org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4$$anonfun$apply$2.apply(ExternalSorter.scala:731)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:789)
        org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4.apply(ExternalSorter.scala:731)
        org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$4.apply(ExternalSorter.scala:727)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:727)
        org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:70)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1180)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1179)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1179)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:694)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:694)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:694)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1397)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

, Hitting the same problem with snappy 1.1.1.3 as well. Using hive tar ball as of today (fri, oct 17, 2014) with spark.master=local
, With latest Spark-Hive integration, the problem seems disappeared. , Not sure what solved it for you, but setting this seems to work for me on a Mac OS X -
export HADOOP_OPTS="-Dorg.xerial.snappy.tempdir=/tmp -Dorg.xerial.snappy.lib.name=libsnappyjava.jnilib $HADOOP_OPTS"
, Yeah. I'm on Ubuntu, and I didn't set up anything to HADOOP_OPTS. It just magically works now. :)]