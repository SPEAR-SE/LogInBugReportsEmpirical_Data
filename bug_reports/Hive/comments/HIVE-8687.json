[The above description is created as a clone of HIVE-4329.

While HIVE-4329 attempted to rewire HCatalog to use Hive's IO system to achieve that goal, this jira tracks the changes to HCatalog, AvroContainerOutputFormat and AvroSerdeUtils to add support to use Avro from within HCatalog.

We may still eventually want to do what HIVE-4329 attempted to do, but as that is a riskier change, it's not a change that should be attempted close to a release. Thus, for 0.14, we can proceed with this route., (Hrm, I did my development against branch 0.14, and this patch is for that - but 0.15 has since moved on, and this patch will not cleanly apply against that - is there a way to run pre-commit tests for 0.14 for this?)

I can come up with a 0.15 version of this patch as well, but wanted to upload this and have tests run on it in the meanwhile., From the hcat side these changes look fine to me.  I think someone who knows the avro serde better should look at that side though, [~brocknoland] or [~xuefuz] maybe?, Looks good to me and thanks for the cleanup. The only question I had was one the following:

{noformat}
if(path.getFileSystem(jobConf).isDirectory(path)){
   path = new Path(path,"datafile");
{noformat}
shouldn't that be a part-.. file?, Uploading trunk version of the patch., Wow, thanks for the quick turnaround on the reviews, guys!

@Brock : yeah, that part was something I had to look at for a while to see if I could do a better job of. It will be a part-0000 file when invoked from within DefaultFileRecordWriterContainer. And at that time, the isDirectory() call will fail, and thus, will be treated as-is.

The problem that's trying to solve is that pig winds up doing some dummy initialization calls in the beginning  of the job, and at that time, the result of mapred.output.dir will be a scratch dir, not the eventual file being written to. That file should be a temporary only file, which gets removed on cleanup., Since the pre-commit tests will likely run only on the trunk version of the patch, I ran hcat tests with the 0.14 branch, and had the following results locally:

{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Hive HCatalog ..................................... SUCCESS [  2.747 s]
[INFO] Hive HCatalog Core ................................ SUCCESS [17:22 min]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [14:11 min]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [ 20.902 s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [ 32.510 s]
[INFO] Hive HCatalog Webhcat ............................. SUCCESS [ 12.214 s]
[INFO] Hive HCatalog Streaming ........................... SUCCESS [03:39 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 36:22 min
[INFO] Finished at: 2014-10-31T14:59:08-08:00
[INFO] Final Memory: 46M/135M
{noformat}

The important bits, though, are as follows:

{noformat}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hive.hcatalog.pig.TestE2EScenarios
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.69 sec - in org.apache.hive.hcatalog.pig.TestE2EScenarios
Running org.apache.hive.hcatalog.pig.TestHCatLoader
Tests run: 60, Failures: 0, Errors: 0, Skipped: 3, Time elapsed: 216.043 sec - in org.apache.hive.hcatalog.pig.TestHCatLoader
Running org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema
Tests run: 18, Failures: 0, Errors: 0, Skipped: 6, Time elapsed: 84.396 sec - in org.apache.hive.hcatalog.pig.TestHCatLoaderComplexSchema
Running org.apache.hive.hcatalog.pig.TestHCatLoaderStorer
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.12 sec - in org.apache.hive.hcatalog.pig.TestHCatLoaderStorer
Running org.apache.hive.hcatalog.pig.TestHCatStorer
Tests run: 162, Failures: 0, Errors: 0, Skipped: 28, Time elapsed: 482.723 sec - in org.apache.hive.hcatalog.pig.TestHCatStorer
Running org.apache.hive.hcatalog.pig.TestHCatStorerMulti
Tests run: 18, Failures: 0, Errors: 0, Skipped: 6, Time elapsed: 28.469 sec - in org.apache.hive.hcatalog.pig.TestHCatStorerMulti
Running org.apache.hive.hcatalog.pig.TestHCatStorerWrapper
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.437 sec - in org.apache.hive.hcatalog.pig.TestHCatStorerWrapper
Running org.apache.hive.hcatalog.pig.TestPigHCatUtil
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.439 sec - in org.apache.hive.hcatalog.pig.TestPigHCatUtil
Running org.apache.hive.hcatalog.pig.TestUtil
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.052 sec - in org.apache.hive.hcatalog.pig.TestUtil

Results :

Tests run: 266, Failures: 0, Errors: 0, Skipped: 43
{noformat}

The above is after removing Avro from test exclusions for HCatLoader and HCatStorer. Note that HCatStorer still has some tests disabled on Avro for the following reasons:
   * testDateCharTypes : incorrect precision returned (returned "5.20" instead of "5.2" )
   * testWriteDecimalXY : incorrect precision returned (returned "1.20" instead of "1.2" )
   * testWriteSmallInt : Avro does not have a small, and uses an int instead, and thus returned a value of 32768 instead of a null from an overflow
   * testWriteTinyInt : Avro does not have a "tiny", and uses an int instead, and thus returned a value of 300 isntead of a null from an overflow
   * testWriteTimestamp : Avro does not support the timestamp datatype

Since the above are avro feature support issues, and not avro-hcatalog integration issues, I kept these tests disabled for now, and we can re-enable them later as and when we make those work., +1 then, pending test results., Cancelling patch - I dug a little more into my earlier assertion into Brock's question - hcat does generate the part file name, but this patch ignores that. The fix is easy though, doing so and re-uploading., Updated trunk patch uses the constructed filename passed in instead of looking at mapred.output.dir., Uploading .2.patch for branch-0.14 as well., One more update - enabling some more tests that were previously disabled, And trunk version of the latest patch with the other test changes., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12678649/HIVE-8687.3.patch

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 6634 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column2
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_add_column3
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_charvarchar
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_date
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_decimal_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_partitioned_native
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_avro_schema_evolution_native
org.apache.hive.hcatalog.streaming.TestStreaming.testInterleavedTransactionBatchCommits
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1591/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1591/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1591/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12678649 - PreCommit-HIVE-TRUNK-Build, Attaching updated version of trunk patch to fix above issue (branch-0.14 version 3 of the patch was good for branch-0.14), 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12678690/HIVE-8687.4.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 6637 tests executed
*Failed tests:*
{noformat}
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1596/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1596/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1596/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12678690 - PreCommit-HIVE-TRUNK-Build, +1 for hive .14, Committed to branch and trunk. Thanks [~sushanth], Just a bug fix, or does this need documentation?, [~leftylev] : Hmm.. not a bugfix, this is a feature add - but basically the "feature" here is that Hive Avro tables, which have so far not been accessible(writable) through HCatalog, now are. Is there a section that could go under?, Doc location candidates:

* [HCatalog StorageFormats | https://cwiki.apache.org/confluence/display/Hive/HCatalog+StorageFormats]
* [Avro SerDe | https://cwiki.apache.org/confluence/display/Hive/AvroSerDe]

The HCat Storage Formats doc implies that Avro works with HCat ... that's my error, sorry, it should be clarified now., This has been fixed in 0.14 release. Please open new jira if you see any issues.
]