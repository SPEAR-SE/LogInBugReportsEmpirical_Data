[Currently working on testing the fix for this issue., The Null Pointer Exception occurs due to a mismatch on the hashcode computed from a row in ReduceSinkOperator and FileSinkOperator. ReduceSinkOperator's hashcode is used by the partitioner to distribute rows to reducers. The FileSinkOperator's hashcode is used to compute the row's bucket number at the reducer. If the hashcodes mismatch, the bucket number computed is not one of the expected bucket numbers for that reducer. This causes a Null Pointer Exception.

ReduceSinkOperator was computing a different hashcode because its bucketNumber field was initialized to valid bucket number, 0. The attached patch initializes this bucketNumber field to an invalid number, -1. This fixes ReduceSinkOperator to compute the same hashcode as FileSinkOperator.

The NPE can be reproduced in a simpler query which is included as a qtest in the patch.
{code}
set hive.enforce.bucketing = true;
set mapred.reduce.tasks = 16;

create table bucket_many(key int, value string) clustered by (key) into 256 buckets;

insert overwrite table bucket_many
select * from src;
{code}, [~petersla]: Is the fix really down to initializing a transient correctly?, Yes. Here is an explanation that refers to how this transient is used.

The transient is used to compute the row's hash in [ReduceSinkOperator.java#L368|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java#L368].
{code}
        hashCode = computeHashCode(row, bucketNumber);
{code}
If the given the bucket number is valid (which would always be as the transient is initialized to a valid number) the computed hashcode would be always multiplied by 31, see [ReduceSinkOperator.java#L488|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java#L488]:
{code}
  ...
  private int computeHashCode(Object row, int buckNum) throws HiveException {
  ...
    } else {
      for (int i = 0; i < partitionEval.length; i++) {
        Object o = partitionEval[i].evaluate(row);
        keyHashCode = keyHashCode * 31
            + ObjectInspectorUtils.hashCode(o, partitionObjectInspectors[i]);
      }
    }
    int hashCode = buckNum < 0 ? keyHashCode : keyHashCode * 31 + buckNum;
    ...
    return hashCode;
  }
{code}
FileSinkOperator recomputes the hashcode at findWriterOffset(), but won't multiple by 31. This causes a different bucket number to be computed than expected. bucketMap only contains mappings for the bucket numbers that is expected for the current reducer to receive. From [FileSinkOperator.java#L811|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L811]:
{code}
  private int findWriterOffset(Object row) throws HiveException {
  ...
      for (int i = 0; i < partitionEval.length; i++) {
        Object o = partitionEval[i].evaluate(row);
        keyHashCode = keyHashCode * 31
            + ObjectInspectorUtils.hashCode(o, partitionObjectInspectors[i]);
      }
      key.setHashCode(keyHashCode);
      int bucketNum = prtner.getBucket(key, null, totalFiles);
      return bucketMap.get(bucketNum);
  }
{code}
The transient was introduced in [HIVE-8151] which refactored the bucket number from a local variable to a transient field. Initially, the local variable was initialized to -1. The refactor changed the code so that the transient field was used instead., [~petersla]: that's a very clear explanation - +1 tests pending.

[~prasanth_j]: do we need to pull this in for 1.2.0?, [~petersla] Thanks for the patch!

[~gopalv] Yes. I think we should pull this for 1.2.0 as it is critical and no workaround., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12730336/HIVE-10538.1.patch

{color:red}ERROR:{color} -1 due to 33 failed/errored test(s), 8901 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_authorization_parts
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_insert_partition_dynamic
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_insert_partition_static
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_join_unencrypted_tbl
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_join_with_different_encryption_keys
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_load_data_to_encrypted_tables
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver_encryption_select_read_only_encrypted_tbl
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_disallow_transform
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_droppartition
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_sba_drop_table
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testNegativeCliDriver_authorization_uri_alterpart_loc
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_gby
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_cbo_udf_udaf
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_groupby_complex_types_multi_single_reducer
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_lateral_view_explode2
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_remove_25
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_top_level
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_cast_constant
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vectorized_timestamp_funcs
org.apache.hadoop.hive.ql.security.TestStorageBasedClientSideAuthorizationProvider.testSimplePrivileges
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropDatabase
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropPartition
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropTable
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.testDropView
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProvider.testSimplePrivileges
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.testSimplePrivileges
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadDbFailure
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadDbSuccess
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadTableFailure
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadTableSuccess
org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.testConfigProcessing
org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.testConfigProcessingCustomSetWhitelistAppend
org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3737/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3737/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3737/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 33 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12730336 - PreCommit-HIVE-TRUNK-Build, [~petersla]: the Spark driver failures look related, can you take a look (looks like rows are being reordered only in Spark?)., Will do., The Spark driver failures are caused by this change. This would be expected if a row's hashcode affected its ordering in Spark. This patch makes it so that HiveKey's hashcode outputted from ReduceSinkOperator is no longer always multiplied by 31 (as explained previously).

Also, for at least those failed qtests, the row ordering/output in the expected output differs across MapRed, Tez, and Spark. So, execution engine affects ordering.

From [spark/groupby_complex_types_multi_single_reducer.q.out#L221|https://github.com/apache/hive/blob/master/ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out#L221]
{code}
POSTHOOK: query: SELECT DEST2.* FROM DEST2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest2
#### A masked pattern was here ####
{"120":"val_120"}	2
{"129":"val_129"}	2
{"160":"val_160"}	1
{"26":"val_26"}	2
{"27":"val_27"}	1
{"288":"val_288"}	2
{"298":"val_298"}	3
{"30":"val_30"}	1
{"311":"val_311"}	3
{"74":"val_74"}	1
{code}
From [groupby_complex_types_multi_single_reducer.q.out#L240|https://github.com/apache/hive/blob/master/ql/src/test/results/clientpositive/groupby_complex_types_multi_single_reducer.q.out#L240]
{code}
POSTHOOK: query: SELECT DEST2.* FROM DEST2
POSTHOOK: type: QUERY
POSTHOOK: Input: default@dest2
#### A masked pattern was here ####
{"0":"val_0"}	3
{"10":"val_10"}	1
{"100":"val_100"}	2
{"103":"val_103"}	2
{"104":"val_104"}	2
{"105":"val_105"}	1
{"11":"val_11"}	1
{"111":"val_111"}	1
{"113":"val_113"}	2
{"114":"val_114"}	1
{code}, The result difference seems to be an expected change because of hashcode difference. [~petersla] Can you put an updated patch by running the tests again with "-Dtest.output.overwrite=true" option? This will overwrite the q.out files., Great, I've been working on just that. I'll be able to posted an updated patch tomorrow., I've attached the second revision of the patch which updates failed Spark qtests., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12730695/HIVE-10538.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 8907 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_vector_cast_constant
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3796/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3796/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3796/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12730695 - PreCommit-HIVE-TRUNK-Build, [~petersla] What JDK version were you using when you re-generated the out files? I am using 1.7.0_55 and I am getting the same output as reported by the precommit test., Precommit tests used jdk1.7.0_45., vector_cast_constant.q test case is updated. It now shows result similar to precommit run., Committed the lastest patch to master and branch-1.2. I re-ran the test using JDK 7 and it produces the same result for me as precommit test run.  
Thanks [~petersla] for the patch and [~gopalv] for the review!
, [~prasanth_j], Thanks for resolving that last failed test and getting this fix in.

I was initially using Oracle's JDK 1.7.0_60, but didn't have luck with 1.7.0_45 either. I also tried the OpenJDK version I already had installed, 1.7.0_79., This issue has been fixed and released as part of the 1.2.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.]