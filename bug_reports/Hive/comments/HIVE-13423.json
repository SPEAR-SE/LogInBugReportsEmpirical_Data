[Patch-1: simple change that when the sum is out of decimal range, then give a warning and return null as the result. , 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12833448/HIVE-13423.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10564 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_globallimit]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[order_null]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_fast_stats]
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0]
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0]
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1]
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testBlackListedUdfUsage
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1577/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1577/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1577/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12833448 - PreCommit-HIVE-Build, [~xuefuz] and [~ctang.ma] Initially I remember we had issues with GroupBy on this decimal data type, but I couldn't see such issue any more (seems it has been fixed by HIVE-6459).

But we still have a small issue that when the sum overflows, it will produce corrupted intermediate file and give ArrayIndexOutOfBoundsException. 

Can you help take a look at the simple fix or do you have a better idea?, [~aihuaxu] The patch looks good. The issue might also exist in all other arithmetic functions or operations like plus(+), multiplication(*) etc I believe. I wonder if we need truncate the scale like SQLServer does to fit the intermediate data to the precision as discussed in HIVE-14281. [~xuefuz], you have more insights into the decimal and what is your thought?, [~aihuaxu], thanks for looking into this. Do we know the cause of ArrayIndexOutOfBoundsException?

Giving a warning message is fine, though that may not help all cases. I think the right behavior is to return NULL when result overflows but to provide a strict mode in which error will be thrown instead. This should be considered for all such cases.

One thing to find out is to sum integer columns. In such case, overflowing can also occur. I expect that NULL will be returned. For decimal, we should do the same until a general strict mode is implemented.

, Yes. ArrayIndexOutOfBoundsException is because of the overflow and with this patch we does return NULL when the overflow occurs.

When there is an overflow, we are writing a 'non-null' flag to the file since the data indeed is a non-null, while later when we try to interpret the data, it resolves to null since it overflows. That causes to generate a corrupted file.



, Does Oracle truncate? Seems hive follows more on Oracle (which follows more on sql standard). , [~ctang.ma]/[~aihuaxu], as to sacrificing scale for bigger integer part, it doesn't seem to be a viable option. The precision/scale of the result type of sum udf is determined statically as result metadata. That is, the result type of sum(decimal(p, s)) is decimal(p+10, s), which is decided before seeing any actual data. Thus, at run time when the data is actually processed, we cannot return the result of decimal( p+10+d, s-d) because the data (result) doesn't conform to the metadata (type decimal(p+10, s).

Please feel free to check standards or what other dbs are dong. As far as I know, there is no standard that permits this., Doesn't seem to have standard for that.

I feel we can just return null if it's out of the range as well. Seems clean and also match what we have for other datatypes in Hive.

The attached patch is to return null if the sum is out of range. Can we commit this change?, This sounds good to me. I'd +1 on this unless [~ctang.ma] has some objections., This sounds good to me. I'd +1 on this unless [~ctang.ma] has some objections., Sound good to me as well., +1, Pushed to master. Thanks Xuefu and Chaoyu for reviewing.]