[A version of CopyTask that closes the source and destination FileSystem after opening them., This issue still occurring when loading large number of files via Hive CLI.

The file handlers that are being opened and remain so are of the following type:

java      15930     hadoop 3438u     IPv6            2677372        0t0        TCP ip-10-34-139-215.ec2.internal:52216->ip-10-244-182-97.ec2.internal:9200 (ESTABLISHED)

Just a sample, note this is taken from EMR with running Hive during the process of running hive -f loadfiles.q command, where loadfiles.q contained about 60 thousand lines of LOAD DATA commands. The number of open handlers such as example above was continiously growing through the process, until it failed with the exception in CopyTask., If there is a problem it is in the copyutil. Hadoop filesystems should not he explicitly closed. I will take a look., @Edward - I fixed one resource leak in Hadoop sequence file constructor (HADOOP-8486). It may be related to this issue. Thanks
, It could be but the copy utils do not use sequence file constructor., I noticed this problem with MoveTask::execute (Not with Copy Task) at the following code. While checking the file format, we are attempting to create the SequenceFile to validate the file format. If the given file is not a valid sequence file then it is resulting into a resource leak with Hadoop v1.x or lower.

if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {
            // Check if the file format of the file matches that of the table.
            boolean flag = HiveFileFormatUtils.checkInputFormat(

Thanks, if HIVECHECKFILEFORMAT is the problem for a hot fix this problem can be disabled in hive-site.xml, If HIVECHECKFILEFORMAT is the issue.It seems like the correct way to handle this is to create a shim. Hadoop 20 versions could simply not validate, not check or we could package a back ported sequenceFile under a new name to do the checking.]