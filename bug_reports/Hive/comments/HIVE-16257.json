[[~xuefuz] [~szehon] Any clues on where this could be originating? When the problem does occur, the incorrect column value always seems to match a value from another row like show above. 
Ruled out any beeline display issue with output because it is reproducible from CLI too.
Although this is not reproducible with spark-shell, I have not ruled out to be a spark issue because the set of transformations used by spark-shell could be different from the transformations used by Hive.

What code should we instrument to confirm or eliminate hive as a source of the problem? Any help appreciated. Thank you, Now THAT is how to file a bug report! Tried it 8 different ways!, Thanks [~appodictic] I try to add as many details as possible. It helps others understand the issue as well. Heck, even myself. A couple of months from now, I would be wondering what this issue was. :), [~ngangam], This does sound like a concurrency issue. If you are able to reproduce the issue, it would be helpful to get the yarn logs. Ideally, you can get logs for one run that doesn't produce the problem. Thanks., Managed to reproduced this in-house (just on one particular cluster though). 
I am attaching the yarn logs from one particular run when I could reproduce the issue the first time(@17/03/29 09:21:36) I ran the query, but the next run (@17/03/29 09:23:23) immediately after the first worked fine.

Query plan looks identical in both cases and I do not spot anything that looks suspicious. [~xuefuz] Could you take a quick look to see if you can spot something? Any help much appreciated. Thanks a bunch., Hi [~ngangam], thanks for providing the log. Unfortunately, the log does't tell much. I also tried the case many times but failed to reproduce. I'm using cdh 5.7.2.

BTW, when you reproduced it, did you run the query in the same user session or different ones?, [~xuefuz] Thanks for taking a look at this.  I can reproduce it by re-running the query within the same beeline session. Also by setting {{spark.executor.cores=1}} the issue no longer occurs. So serializing the tasks within spark somehow doesnt cause this issue. In discussion with the spark team, they think it is an indication of a race condition on HoS side.
From the spark plan, we have 2 stages, the root stage with 4 tasks. The second stage has just one task.
We only have a single input split in this case, so I am guessing that the number of tasks are determined by spark internals based on number of spark partitions.
{code}
17/03/29 09:21:51 INFO input.FileInputFormat: Total input paths to process : 1
17/03/29 09:21:51 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 3, size left: 0
17/03/29 09:21:51 INFO io.CombineHiveInputFormat: number of splits 1
17/03/29 09:21:51 INFO io.CombineHiveInputFormat: Number of all splits 1
{code}

If a single executor running all these tasks can resolve the issue, to me it suggests there might be an issue in Spark. But I am not sure how to conclude this either way. Any tips? Thanks, It's a little strange that you have 4 tasks in the first stage, given that there are only 5 rows. I have only one task, which could explain why I couldn't reproduce the problem. Can you check how many source files you have for the table?

I think it's helpful if you can find out why you have 4 tasks, which might give some hints where it might go wrong., It seems like HIVE-12768 is the root cause., [~xuefuz] In my case, it is always coming out to 4 tasks (in the first stage) for some reason. There is a single input file on hdfs. However, the default {{spark.executor.cores}} for me is 4. I dont know if they are related. After adding some instrumentation, the data being passed to reducers was found to be correct indicating the problem was prior to this phase. Seems like we use serde to parse decimal types which pointed to HIVE-12768. It does not occur with HoMR because each mapper is run in a seperate process. It also does not occur when using a single executor.cores because it uses single thread to process it.

With the fix for HIVE-12768, we are currently unable to reproduce the issue after several dozens of retries. As far as it being an intermittent issue, I think the size of the data matters. The larger the dataset, the higher the chance of reproducing it. In this case, the data was very small and hence was hard to reproduce. Closing this jira. Thanks for all your help [~xuefuz], [~ngangam], Yes, our build has HIVE-12768, which explains why we cannot reproduce the issue. Nevertheless, you should check why you have 4 tasks for a 5-row file. I also have spark.executor.core=4, and I only see one task. Also, I was using text file. That can also relate to minSplitSize, etc. 

Besides HIVE-12768, we have also fixed many concurrency issues that can impact HoS.

Thanks for looking to this problem.]