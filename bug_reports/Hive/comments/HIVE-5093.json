[Quick test-results 

{code}
hive> set min.num.spills.for.combine=0;            
hive> set hive.exec.combiner=false;

hive> select cd_credit_rating, cd_education_status from customer_demographics order by cd_credit_rating limit 10;

Job 0: Map: 1  Reduce: 1   Cumulative CPU: 14.92 sec   HDFS Read: 18709 HDFS Write: 130 SUCCESS

Reduce input groups 	4
Reduce input records 	480200 
{code}

With the optimizer on

{code}
hive> set hive.exec.combiner=true;

hive> select cd_credit_rating, cd_education_status from customer_demographics order by cd_credit_rating limit 10;

Job 0: Map: 1  Reduce: 1   Cumulative CPU: 9.39 sec   HDFS Read: 18709 HDFS Write: 130 SUCCESS

Reduce input groups 	1
Reduce input records 	10 

{code}

That is a massive reduction in reduce records for minimal complexity (the spills per-combiner is set to zero, to always run it).

On query27, we went from 816036 bytes shuffled to 9006 bytes shuffled over-all.

, Rebased to trunk WIP.

This does mix up a mapred.Reducer ref into MapWork, which is not cleanly split out.

Would like some advice on how to do that with Tez in mind., In my experience with pig and combiners, combiners can be significantly expensive because the records are serialized and deserialized between the map and combiner boundary. Ie, if the use of combiner does not result in data size reduction beyond some threshold, it will actually slow the query.
Can you also try "limit 480200;" just to get an idea about what the worst case performance for this can be ?
, [~thejas] You are right, the systems slows down & adds another 3 seconds when the combiner is not actually doing anything useful (i.e limit > row count).

But the use-case of LIMIT <human-number> queries used in analytics is common enough for this to be worthwhile.

If this is saving time, the next step is to use a O(n + k*lg(k)) quick-select with a limit-k instead of quick-sort as the map sort class & get away from sorting it as well.

Particularly if k is so much smaller than n, which is an easy decision at the partition phase in the indexed sorter., I am thinking we should not do this. Hive uses map side aggregation as an alternative to combiners. Also I feel like designing and maintaining tons of code around limit optimizations is a waste. Who will benefit from this and how often? If you have a problem like this better not to use hive., [~gopalv] I agree this is going to be very useful with <human-number> limits.

[~appodictic] I think limit queries are fairly common, for analytical queries as well as when people are iteratively trying our their queries and want quickly check if the queries are working as expected. This optimization can lead to significant performance boost for such queries and the code change required is not significant as demonstrated by attached WIP patch. 
I agree that we should look at different ways of adding this optimization. 

As Gopal suggested, using a different sort function for map is one option for the order-by queries. 

The fact that hive uses map-side aggregation is something to consider for optimizing the group-by case. One option would be to push the limit into the map-side aggregation operator, that will also reduce its memory requirements. But that is probably little more complicated than this change which is more factored out in a separate combiner code.
, [~appodictic]: HIVE-3562 is actually doing this in the map-side aggregation with a java.util.SortedMap. 

I wrote the combiner version for memory efficiency, which is relatively simple as well., I have to think this over a little bit. If my understanding is correct hive does not use a combiner anywhere. I do not understand the win of using a combiner in limit query situations then. How much optimization should we do for limit queries?, HIVE-3562 made this redundant.]