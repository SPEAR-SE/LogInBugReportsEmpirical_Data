[I must mention: This breaks only in the case of a multi-query. 

I've a silly question: Doesn't Hive support a multi-query equivalent? For instance:

{code:sql}
FROM myth_table
INSERT OVERWRITE target_table1 SELECT * WHERE ...
INSERT OVERWRITE target_table2 SELECT ...
{code}

Isn't this the Hive equivalent of a Pig multi-output case? How does Hive handle the problem of 2-phase commit?, Tentative fix using a singleton to track OutputCommitter calls for TaskAttempt commits/aborts.
I tried solving this using the configuration, and by storing state in the OutputFormat. It can't be done. OutputFormats are constructed afresh using Reflection, and hence have no state. Similarly, Pig clones the configurations (and TaskAttemptContext instances) when calling RecordWriter functions, to prevent pollution.
I even tried storing state in the UDFContext, by modifying {{HCatStorer::setLocation()}}, and then transferring the state into the JobConf. No dice., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12674344/HIVE-8394.1.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 4137 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_tez_smb_1
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1236/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1236/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1236/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12674344, The test-failure is unrelated, IMO. Could I please bother someone for a review?, Ok, it looks like this might not work in case the TaskAttempt involves 2 HCatStorers. Since the Singleton uses TaskAttemptID as a key, it's possible that some OutputJobInfos might be committed twice (because Pig will call commitTask() once per Storer)., (Just recording what we discussed in review:)

1. We can get this to work if instead of keying on the TaskAttemptID, we use <attemptId>.<db>.<table> as the key. This will keep it unique and unambiguous.
2. We'll also need to add a {{finally}} block to discard any cleanup-handler, in {{abortTask()}} and {{commitTask()}}.

This should do the trick for now. But it does feel like a workaround., I'll admit to the same distaste to using a Singleton to store state like this - we've had similar problems with HCatContext in the past, but I agree with your assertion that that seems to be the only real way to handle this issue.

Now, that said, the attached file HIVE-8394.1.patch includes a TaskCommitterContextRegistry.discardCleanupFor and does not ever call it. I assume that's what you mean with your comment on needing to add a finally block? Also, yes, the patch in the current form has an issue with multiple HCatStorers - do you have an updated patch with both these issues resolved?
, (Also, slight warning - I'll likely be applying your patch post HIVE-4329, and that might wind up requiring a rebase - the two should be compatible, I think, but I wanted to point it out if you think it makes a difference), Yes, actually, I do have to update the patch. Will do shortly., Actually, re-reading HIVE-7803, there is a problem - HIVE-4329 and HIVE-7803 will definitely not merge cleanly, so the pre-requisite for this is itself not going to merge., Here's the updated patch. It handles the singleton-cleanup more completely. And instead of keying on only the TaskAttemptID, the solution now uses the commit-path as part of the key. This should handle multiple Pig outputs for the same attempt. (Thanks, [~cdrome].), 

{color:red}Overall{color}: -1 no tests executed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12678359/HIVE-8394.2.patch

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1573/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1573/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1573/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/contrib/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-contrib/0.15.0-SNAPSHOT/hive-contrib-0.15.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive HBase Handler 0.15.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hbase-handler ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hbase-handler ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hbase-handler ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hbase-handler ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hbase-handler ---
[INFO] Compiling 37 source files to /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/classes
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDe.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDeParameters.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseSerDeParameters.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- avro-maven-plugin:1.7.6:protocol (default) @ hive-hbase-handler ---
[INFO] 
[INFO] --- build-helper-maven-plugin:1.7:add-test-source (add-test-sources) @ hive-hbase-handler ---
[INFO] Test Source directory: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/gen/avro/gen-java added.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hbase-handler ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hbase-handler ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/tmp/conf
     [copy] Copying 8 files to /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hbase-handler ---
[INFO] Compiling 16 source files to /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/test-classes
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseKeyFactory2.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestHBaseKeyFactory2.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/org/apache/hadoop/hive/hbase/avro/ContactInfo.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/org/apache/hadoop/hive/hbase/avro/ContactInfo.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ hive-hbase-handler ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.2:jar (default-jar) @ hive-hbase-handler ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/hive-hbase-handler-0.15.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hbase-handler ---
[INFO] 
[INFO] --- maven-jar-plugin:2.2:test-jar (default) @ hive-hbase-handler ---
[INFO] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/hive-hbase-handler-0.15.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hbase-handler ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/hive-hbase-handler-0.15.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hbase-handler/0.15.0-SNAPSHOT/hive-hbase-handler-0.15.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hbase-handler/0.15.0-SNAPSHOT/hive-hbase-handler-0.15.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/target/hive-hbase-handler-0.15.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hbase-handler/0.15.0-SNAPSHOT/hive-hbase-handler-0.15.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive HCatalog 0.15.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hcatalog ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/hcatalog (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/target/tmp/conf
     [copy] Copying 8 files to /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog ---
[INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hcatalog/hive-hcatalog/0.15.0-SNAPSHOT/hive-hcatalog-0.15.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive HCatalog Core 0.15.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-hcatalog-core ---
[INFO] Deleting /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-core ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-core ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-core ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-core ---
[INFO] Compiling 79 source files to /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/target/classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatBaseOutputFormat.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatBaseOutputFormat.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java:[104,44] method register in class org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry cannot be applied to given types;
  required: org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry.TaskCommitterProxy
  found: org.apache.hadoop.mapreduce.TaskAttemptID,<anonymous org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry.TaskCommitterProxy>
  reason: actual argument org.apache.hadoop.mapreduce.TaskAttemptID cannot be converted to org.apache.hadoop.mapreduce.TaskAttemptContext by method invocation conversion
[INFO] 1 error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [12.650s]
[INFO] Hive Shims Common ................................. SUCCESS [7.257s]
[INFO] Hive Shims 0.20 ................................... SUCCESS [3.670s]
[INFO] Hive Shims Secure Common .......................... SUCCESS [5.452s]
[INFO] Hive Shims 0.20S .................................. SUCCESS [2.454s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [8.050s]
[INFO] Hive Shims ........................................ SUCCESS [1.852s]
[INFO] Hive Common ....................................... SUCCESS [26.256s]
[INFO] Hive Serde ........................................ SUCCESS [21.837s]
[INFO] Hive Metastore .................................... SUCCESS [37.088s]
[INFO] Hive Ant Utilities ................................ SUCCESS [1.887s]
[INFO] Hive Query Language ............................... SUCCESS [1:37.469s]
[INFO] Hive Service ...................................... SUCCESS [12.555s]
[INFO] Hive Accumulo Handler ............................. SUCCESS [6.831s]
[INFO] Hive JDBC ......................................... SUCCESS [1:28.444s]
[INFO] Hive Beeline ...................................... SUCCESS [1.530s]
[INFO] Hive CLI .......................................... SUCCESS [1.709s]
[INFO] Hive Contrib ...................................... SUCCESS [1.603s]
[INFO] Hive HBase Handler ................................ SUCCESS [8.545s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.600s]
[INFO] Hive HCatalog Core ................................ FAILURE [2.134s]
[INFO] Hive HCatalog Pig Adapter ......................... SKIPPED
[INFO] Hive HCatalog Server Extensions ................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client ................. SKIPPED
[INFO] Hive HCatalog Webhcat ............................. SKIPPED
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive ODBC ......................................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 5:52.804s
[INFO] Finished at: Fri Oct 31 09:00:12 EDT 2014
[INFO] Final Memory: 125M/832M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hive-hcatalog-core: Compilation failure
[ERROR] /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java:[104,44] method register in class org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry cannot be applied to given types;
[ERROR] required: org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry.TaskCommitterProxy
[ERROR] found: org.apache.hadoop.mapreduce.TaskAttemptID,<anonymous org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry.TaskCommitterProxy>
[ERROR] reason: actual argument org.apache.hadoop.mapreduce.TaskAttemptID cannot be converted to org.apache.hadoop.mapreduce.TaskAttemptContext by method invocation conversion
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-hcatalog-core
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12678359 - PreCommit-HIVE-TRUNK-Build, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12678513/HIVE-8394.2.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6608 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_schemeAuthority
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1581/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1581/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1581/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12678513 - PreCommit-HIVE-TRUNK-Build, Ok, HIVE-8394.2.patch assumes FileOutputCommitters. Must switch to using the  {{baseDynamicCommitters}} list instead., Updated patch., Minor logging adjustment., Now with more logging, and ASF header., Okay, this .4.patch version looks good to me, +1.

Awaiting test results now before committing.

[~hagleitn], could we get a +1 on this for 0.14 inclusion as well?, Precommit test link when it gets there : http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1606/, 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12678738/HIVE-8394.4.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6668 tests executed
*Failed tests:*
{noformat}
org.apache.hive.hcatalog.streaming.TestStreaming.testTransactionBatchEmptyCommit
org.apache.hive.minikdc.TestJdbcWithMiniKdc.testNegativeTokenAuth
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1606/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1606/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1606/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12678738 - PreCommit-HIVE-TRUNK-Build, +1 for hive .14, [~mithun] - this looks ready, but I hear you're still testing. Is that complete?, Just completed the test with the erstwhile failing case. Success. We're good to go. 

Sorry for the delay. The commit-code in the end was a little different from what we were using internally, and I needed to be sure this fix was good.

Thanks for holding on, Gunther., Good to know - I'll go ahead and commit it in both branches., Committed in trunk and in 0.14.

Thanks, Mithun!, [~leftylev] : No worries on documentation here, this is a bugfix. :), Thanks for saving me a bit of sleuthing, [~sushanth]!  Maybe you'll start a trend...., This has been fixed in 0.14 release. Please open new jira if you see any issues.
]