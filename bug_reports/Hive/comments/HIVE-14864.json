[Does srcFS.getContentSummary(src).getLength() return total number of files in the directory ? 

I think this whole condition needs to be re-thought. Because AFAIK, DistCp speeds up copies when multiple
files are involved. There is no advantage in DistCp'ing a single file, no matter how big that file. Which means
HIVE_EXEC_COPYFILE_MAXSIZE does not make sense even for a file.

IOW, we need to look into DistCp'ing directories only, and possibly ones which contain more than a threshold
number of files., Unfortunately, the documentation of ContentSummary.getLength() says ... returns the length :)

This is the implementation of getContentSummary() in FileSystem.java which suggests getLength is actually returning the sum of lengths of all the files within that directory.

{noformat}
  public ContentSummary getContentSummary(Path f) throws IOException {
    FileStatus status = getFileStatus(f);
    if (status.isFile()) {
      // f is a file
      return new ContentSummary(status.getLen(), 1, 0);
    }
    // f is a directory
    long[] summary = {0, 0, 1};
    for(FileStatus s : listStatus(f)) {
      ContentSummary c = s.isDirectory() ? getContentSummary(s.getPath()) :
                                     new ContentSummary(s.getLen(), 1, 0);
      summary[0] += c.getLength();
      summary[1] += c.getFileCount();
      summary[2] += c.getDirectoryCount();
    }
    return new ContentSummary(summary[0], summary[1], summary[2]);
  }
{noformat}

These are the revelant constructors for ContentSummary.
{noformat}
  /** Constructor */
  public ContentSummary(long length, long fileCount, long directoryCount) {
    this(length, fileCount, directoryCount, -1L, length, -1L);
  }

  public ContentSummary(
      long length, long fileCount, long directoryCount, long quota,
      long spaceConsumed, long spaceQuota) {
    this.length = length;
    this.fileCount = fileCount;
    this.directoryCount = directoryCount;
    this.quota = quota;
    this.spaceConsumed = spaceConsumed;
    this.spaceQuota = spaceQuota;
  }
{noformat}
, I looked into distcp documentation and I think you are right. It creates a list of files to be copied if src is a directory and this list is divided among a bunch of CopyMappers to do the actual copy. Do you have any ideas as to how should we evaluate the threshold of number of files when distcp is beneficial?, Adding [~spena] since he has worked on this before., I run some tests with directories, and distcp is indeed faster than copying files one at a time. I don't know the threshold, we should investigate with different files to see it., Attaching a pre-lim patch just for reference. The patch is still a WIP.

Changes are pretty simple, {{FileUtils.copy}} uses {{getContentSummary}} to get the number of files under the folder. It triggers a Distcp job based on the size of files under the folder + the number of files under the folder.

If only a single file needs to be copied, the {{ContentSummary}} length will be the size of that file, and the number of files under it will be 1.

For now the logic is pretty simple, if the number of files exceeds a threshold set by {{hive.exec.copyfile.maxnumfiles}} (which defaults to 1) and the size of the files exceeds a threshold set by hive.exec.copyfile.maxsize (which defaults to 32 MB), the Distcp job will be triggered.

So basically any folder that contains more than 1 file and whose total contents is greater than 32 MB., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12833234/HIVE-14864.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 89 failed/errored test(s), 10534 tests executed
*Failed tests:*
{noformat}
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[autoColumnStats_7]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[crtseltbl_serdeprops]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cte_3]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cte_4]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cte_mat_4]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cte_mat_5]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_00_nonpart_empty]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_01_nonpart]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_02_00_part_empty]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_02_part]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_03_nonpart_over_compat]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_04_all_part]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_04_evolved_parts]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_05_some_part]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_06_one_part]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_07_all_part_over_nonoverlap]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_08_nonpart_rename]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_09_part_spec_nonoverlap]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_10_external_managed]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_11_managed_external]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_12_external_location]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_13_managed_location]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_14_managed_location_over_existing]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_15_external_part]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_16_part_external]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_17_part_managed]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_18_part_external]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_19_00_part_external_location]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_19_part_external_location]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_20_part_managed_location]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_21_export_authsuccess]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_22_import_exist_authsuccess]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_23_import_part_authsuccess]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_24_import_nonexist_authsuccess]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_25_export_parentpath_has_inaccessible_children]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[exim_hidden_files]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[foldts]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[masking_9]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[materialized_view_describe]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[reloadJar]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[repl_2_exim_basic]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[repl_3_exim_metadata]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[stats_partial_size]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[temp_table]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[temp_table_gb1]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[temp_table_join1]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[temp_table_subquery1]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_tablesample_rows]
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_load_data_to_encrypted_tables]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[import_exported_table]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[import_exported_table]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[authorization_import]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[ctas_noemptyfolder]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_01_nonpart_over_loaded]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_02_all_part_over_overlap]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_03_nonpart_noncompat_colschema]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_04_nonpart_noncompat_colnumber]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_05_nonpart_noncompat_coltype]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_06_nonpart_noncompat_storage]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_07_nonpart_noncompat_ifof]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_08_nonpart_noncompat_serde]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_09_nonpart_noncompat_serdeparam]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_10_nonpart_noncompat_bucketing]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_11_nonpart_noncompat_sorting]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_13_nonnative_import]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_14_nonpart_part]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_15_part_nonpart]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_16_part_noncompat_schema]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_17_part_spec_underspec]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_18_part_spec_missing]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_19_external_over_existing]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_20_managed_location_over_existing]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_21_part_managed_external]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_23_import_exist_authfail]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_24_import_part_authfail]
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[exim_25_import_nonexist_authfail]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[temp_table]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[temp_table_gb1]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[temp_table_join1]
org.apache.hadoop.hive.ql.history.TestHiveHistory.testHiveHistoryConfigDisabled
org.apache.hadoop.hive.ql.history.TestHiveHistory.testHiveHistoryConfigEnabled
org.apache.hadoop.hive.ql.history.TestHiveHistory.testQueryloglocParentDirNotExist
org.apache.hadoop.hive.ql.history.TestHiveHistory.testSimpleQuery
org.apache.hadoop.hive.ql.security.TestExtendedAcls.testExim
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testExim
org.apache.hive.hcatalog.api.repl.commands.TestCommands.testBasicReplEximCommands
org.apache.hive.hcatalog.api.repl.commands.TestCommands.testMetadataReplEximCommands
org.apache.hive.hcatalog.api.repl.commands.TestCommands.testNoopReplEximCommands
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1546/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1546/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1546/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 89 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12833234 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12833308/HIVE-14864.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 20 failed/errored test(s), 10522 tests executed
*Failed tests:*
{noformat}
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_globallimit]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[order_null]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[reloadJar]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_fast_stats]
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_load_data_to_encrypted_tables]
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver[hbase_bulk]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[import_exported_table]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[import_exported_table]
org.apache.hadoop.hive.ql.security.TestExtendedAcls.testExim
org.apache.hadoop.hive.ql.security.TestFolderPermissions.testExim
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0]
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0]
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1]
org.apache.hive.jdbc.TestNoSaslAuth.org.apache.hive.jdbc.TestNoSaslAuth
org.apache.hive.jdbc.authorization.TestHS2AuthzContext.org.apache.hive.jdbc.authorization.TestHS2AuthzContext
org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext.org.apache.hive.jdbc.authorization.TestHS2AuthzSessionContext
org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth.org.apache.hive.jdbc.authorization.TestJdbcMetadataApiAuth
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthUDFBlacklist.testBlackListedUdfUsage
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1563/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1563/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1563/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 20 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12833308 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12833472/HIVE-14864.2.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10564 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_globallimit]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[order_null]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_fast_stats]
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver[hbase_bulk]
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0]
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0]
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1]
org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.testBlackListedUdfUsage
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1581/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1581/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1581/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12833472 - PreCommit-HIVE-Build, A couple of comments:

* Will srcFS.getContentSummary(src) cause extra time when source is on S3? If so, maybe we want to put this line inside the if() statement.

* Once we're here, could you fix the message from HIVE_EXEC_COPYFILE_MAXSIZE to say (in Bytes) instead of (in Mb) ?, [~spena] comments addressed, rebased the patch. While writes to S3 won't hit this code path, its still useful for other destination filesystems., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12854547/HIVE-14864.3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 10259 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=140)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3768/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3768/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3768/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12854547 - PreCommit-HIVE-Build, Can we add q-tests for this new variable?, [~spena] am trying to write some qtests for this, do you know which CliDrivers run against HDFS filesystems? When I run the {{TestCliDriver}} all the URIs have schemes of either {{file}} or {{pfile}}. This code path only gets triggered if the scheme is {{hdfs}}., Didn't the mini HDFS accepts hdfs?  Check all the encryption tests, such as encryption_ctas.q. They usually check for the hdfs scheme., {{FileSystem.getContentSummary()}} does a recursive treewalk, so is pathologically bad on a blobstore which has to mock directories through many., many HTTP requests.

If you need to use it, could you actually supply a patch (+ FS contract tests) for the method so that it uses listFiles(path, recursive=true)? That does the same treewalk against HDFS, but blobstores can do it as an O(1) listing call instead. If you can get that patch in, then enumerating the size of a blobstore tree will be fast, [~spena] I worked on this some more, and think a unit test may be better suited for this patch rather than a qtest. There are a number of different queries that could invoke this method (e.g. IMPORT queries use this method too), and more may be added in the future. I added some integration tests that run against a mini HDFS cluster, and some unit tests that just rely on mocking.

[~stevel@apache.org] I agree, calling {{getContentSummary}} on S3 will be very slow. I've thought about filing a JIRA for the optimization you mentioned a few times, but never got around to doing it. Fortunately, this specific code won't be hit for S3, only for HDFS., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12856936/HIVE-14864.4.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 10337 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery (batchId=219)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/4047/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/4047/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-4047/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12856936 - PreCommit-HIVE-Build, The patch looks good [~stakiar]
+1, Thanks [~stakiar]. I committed the patch to upstream., Doc note:  This adds *hive.exec.copyfile.maxnumfiles* to HiveConf.java and corrects the description of *hive.exec.copyfile.maxsize* (added in 1.1.0 by HIVE-8750 but not documented yet) so they need to be documented in the wiki.

* [Configuration Properties -- Query and DDL Execution | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-QueryandDDLExecution]

Added a TODOC2.2 label., I updated the documentation for this here: https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-QueryandDDLExecution]