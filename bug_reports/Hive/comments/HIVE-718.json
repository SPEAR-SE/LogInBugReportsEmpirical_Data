[patch.  
I will check and test it late tonight., the patch that passed tests in my local.
Added the code to undo the copy operation when error occures (this might not help in most situations, but we should try out best)., I can confirm this bug on trunk - it's a regression since 0.3.0.

However, I"m not sure about the patch - if one of the later rename fails, we should undo the previous ones, but in this patch it looks like it's actually deleting the previous ones. Why not attempt to move it back to its original location?

Also, it seems worth it to add some LOG.error() statements in these cases., Here's a new revision of the patch which addresses my concerns above.

Is there a way to write a unit test for this? We should have a test to prevent another regression., can't this be done the same way replaceFiles() does? that is use a tmpdir as a staging directory and do a rename() which is atomic., What if the partition already exists? In that case we couldn't rename the staging directory since the destination name would already exist, right? Or can we just make another subdirectory of the partition with some unique name?, another subdir should work fine. there is no rule that there can only be files in a partition dir, OK. How do we come up with a unique name? Just use timestamp?

Does the metastore need to know this name somehow? Or is the change just localized to that method? (Also, should we rename that method? It's called copyFiles, but does nothing of the sort), metastore doesn't need to know the subdir name. any random enough name should be fine. feel free to rename it.
, In looking through this code, I've found a few more issues:

- In isolation, it looks like copyFiles/replaceFiles are supposed to be able to handle a srcf like "/foo/*" with a directory layout like:

/foo/subdir1/part-00000
/foo/subdir2/part-00000

I'm assuming this because it first does fs.globStatus on srcf, and then for each of the results of the glob, it calls fs.listStatus (implying that they are directories).

However, given the example above, this would actually fail, since both files are named part-00000 and the could would attempt to rename both to tmpdir/part-00000.

- In fact, using the tmpdir like this is consistent from the view of an outside observer, but not atomic. If the renamer crashes in the middle of the operation, the files will have been moved out of the original location and into the tmpdir, but the tmpdir has not been renamed into the destination. Is this OK? I feel like the solution would be to make dstdir/_staging_<timestamp>, move the files one-by-one into there, and then rename _staging_<timestamp> to the destination. This way if there is a failure in the middle, the client can at least determine where their files went without looking through a temporary directory., I think it's not acceptable for a failed "insert" to corrupt the original data of the table.

We never have a table with sub directories (instead of files) inside. We will need some testing to make sure it actually works.
For unique name, maybe we can just prepend the job id.
, bq. I think it's not acceptable for a failed "insert" to corrupt the original data of the table. 

then we definitely have to move an entire directory of files in at once - otherwise we can have an insert partially succeed

bq. We never have a table with sub directories (instead of files) inside. We will need some testing to make sure it actually works.

This is going to be a necessity to do non-overwrite loads into a table/partition, right?

bq. For unique name, maybe we can just prepend the job id.

This isn't always available (eg running LOAD DATA from the cli). I think we're stuck with java.util.UUID, as ugly as it may be.

I've spent the last hour or so trying to figure out any other way of generating a unique name inside a subdirectory. Because of the semantics of FileSystem.mkdirs and FileSystem.rename, I don't believe there's any way of doing this. mkdirs doesn't return false in the case that the directory already exists, and if you rename(src, dst), and dst already exists as a directory, it will move src *inside* of dst., bq. We never have a table with sub directories (instead of files) inside. We will need some testing to make sure it actually works.

Zheng, aren't buckets are separate subdirs? they work so sub-dirs should be fine., bq. Zheng, aren't buckets are separate subdirs? they work so sub-dirs should be fine.

I tried to add a directory into a table, and then run this. Apparently hadoop file format does not like the sub directory:
Buckets are files not directories.

{code}
> select * from zshao_tt;
OK
Failed with exception java.io.IOException:Not a file: hdfs://dfs1.data.facebook.com:9000/user/facebook/warehouse/zshao_tt/a
09/08/04 14:49:38 ERROR exec.FetchTask: Failed with exception java.io.IOException:Not a file: hdfs://dfs1.data.facebook.com:9000/user/facebook/warehouse/zshao_tt/a
java.io.IOException: Not a file: hdfs://dfs1.data.facebook.com:9000/user/facebook/warehouse/zshao_tt/a
        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:231)
        at org.apache.hadoop.hive.ql.exec.FetchTask.getRecordReader(FetchTask.java:236)
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:291)
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:368)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:216)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:306)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:166)
        at org.apache.hadoop.mapred.JobShell.run(JobShell.java:194)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.mapred.JobShell.main(JobShell.java:220)
{code}

I discussed with Ashish offline on this. I think we still want the atomic property of insert - as a result, we may need to manually expand the input directory into a bunch of files, and feed the files into the map/reduce jobs (instead of the directories).  That code is in ExecDriver.java and MapRedTask.java when we set the JobConf.

What do you think?
, Todd's patch looks good (at least better than mine).  so +1.
I assumed the srcfs is a tmp dir, and did not recover them when error occurs. , I think we should go ahead with my patch for now and then open another JIRA for fixing the atomicity issue., I think this should be merged into 0.4 release., lacking atomicity can lead to silent bugs which are unacceptable in production systems., Prasad: this is true, but the atomicity issue was already there - we just discovered it while looking at this section of the code. My guess is that there are similar bugs elsewhere in Hive - there's no safe way to have multiple threads compete for a directory name, so really any movement of files has to be coordinated by the metastore to be safe. The hadoop-side issues are:

- mkdirs on an existing directory will return the exact same thing as mkdirs on one that didn't previously exist
- rename of one directory to a new name will not give an error if the new name exists - rather, it will move your directory inside that one
- checking if (not exists) { do something to a location } is obviously racy, Let's get a consensus on this issue soon.

Atomicity is definitely wanted (either now or some time in the future). It's not just the move of all files need to be atomic - if the files are moved, the job should always return successful - otherwise most probably the client will try again and end up insert the data twice. It seems it's impossible to me with what we have.

Maybe we should not support "append" - user can always load into a new partition every time. Thoughts?
, 'overwrite' path has less of an issue in the sense that only one of two competing statements will win out. the resulting directory will not contain some files from first statement and some from the second statement. (this assuming probability of two statements creating same random tmp directory is very less)

my concern in this case is that, it is possible to corrupt the existing partition with only a part of new files and overwrite some of the old files and user has no way of knowing that such a thing has happened and it may not possible to recover the data.

but if you guys think the current patch is no worse than the existing solution, i  do not have a problem., bq. my concern in this case is that, it is possible to corrupt the existing partition with only a part of new files and overwrite some of the old files and user has no way of knowing that such a thing has happened and it may not possible to recover the data.

Can you explain the order of events that causes this? I think even with the current patch the operation will not fail silently and should not cause unrecoverable loss., Two statements issue load into same partition around the same time. For one, dirExisted will be true and false for the other. Suppose the 'false' stmt copies a file named 'a1' first and then 'true' stmt will fail if it copies the same file. so it will try to undo the previous copies and then delete the dir. But the 'false' stmt keeps copying the files blissfully and succeed but the files 'a1' and others that were copied before the 'true' stmt deleted the directory will not be there but there won't be an error for the 'false' stmt.

hoping my writing is understandable enough :), Thanks for the explanation. I'll try to block out an hour to look through the code again this afternoon or evening to see if I can fix this issue somehow or another.

There is one ugly solution whereby you can use a file (not a directory) as a lock. That is to say, open a file "foo_load_lock". If it's already being created by another writer, the NN will throw an IOException to the second guy, and we can enforce exclusive access. The problem here is what to do with the failure scenario which might leave a lock hanging around for perpetuity :(, continuing the ugly part, you can do some sort of leases instead of locks using files. create a file uniquely named with the current hour so lease will be held for an hour atmost. there might be lot of files hanging around but they can be created in /tmp which will get cleaned.

another option is, ashish is implementing session level temporary tables in metastore. you can use them as locks without worrying them existing after the session., Here's a proposal: How would you guys feel about an addition to the metastore API that is as simple as:

string get_unique_id()

The metastore would simply keep an autoincrement field to hand these out. We can then use these safely to allocate race-free names to clients. Should be straightforward and a lot more simple than any kind of lock/lease based mechanism., Can the unique name problem be solved at the file system level via a mktemp like method? Then you wouldnt have to depend on the metastore to reliably maintain unique ids (for example, what happens when the metastore uses an in-memory db and is restarted)., I hadn't considered the case of an in-memory metastore. A mktemp-like method would be great, but o.a.h.FileSystem gives you nothing of the sort :(, bq. A mktemp-like method would be great, but o.a.h.FileSystem gives you nothing of the sort

That's fine. We could just implement such a method in hive. The logic could be the same as unix mktemp. See http://src.opensolaris.org/source/xref/onnv/onnv-gate/usr/src/lib/libast/common/path/pathtemp.c . Specifically, the for loop starting 245., Not sure how that actually helps - if we use an algorithm like:

{code}
for each file to be moved:
  while not successful:
    come up with a random name
    try to move src file to the random name
    if it fails due to dst already existing, try again with a new random name
{code}

then we'd lose the atomicity/isolation - readers would see a partial load during the middle of the operation.

We can't use that algorithm with atomic directory renames, since Hadoop has the wacky behavior that move("srcdir", "dstdir") will create "dstdir/srcdir" if dstdir already exists, @todd
dhruba said he would be willing to provide a 'mkdir' method that would mimic the POSIX mkdir() fn. But in the meantime i think the algorithm with atomic directory renames would be sufficient. we can try to generate a pseudo random number and check for that directory to exist before creating it. Since we are generating pseudo random numbers, the probability that two different loads into same directory will be generating same random number and checking for existence at the same time is pretty low. , Apologies on following this earlier. It caught my attention as Todd brought up whether we should get this into 0.4.0 release as this is a regression when compared to 0.3.0. I checked the code on 0.3.0 and it seems to be the same as that in 0.4.0. So I am not sure if this is a regression. If this is not a regression then potentially we can go out with 0.4.0 without this and document this?

As is evident by this discussion LOAD INTO and its cousin INSERT INTO (when we have it) are very tricky. Almost all our code has been written with the overwrite semantics. Appending new data to an existing partition would need more work to get right and I feel we should punt it and document that insert into is not reliable - I think it has never been reliable.

In order to safely implement the INSERT INTO and LOAD INTO semantics one approach is to introduce a notion of versions on the DML commands which is encoded in the directory structure i.e.

instead of storing things as 

xyz/part-0000

we store the files as

xyz/v1/part-0000

and so on so forth. We store the latest created version in the metastore entry for that table. When a reader comes in it first looks at this entry and then finds a version corresponding to that in the table. The versions themselves could be garbage collected by deleting version directories that are older than say some configurable duration old and this could either be done lazily by the writer on the table or by an active garbage collector in the background. These are of course somewhat involved changes and would solve the isolation and atomicity problems. The later becase v1 is a directory so moving data to that directory would be a rename and hence atomic. Thoughts?
, Not sure if people are already following the discussion on HADOOP-6240, but it's worth checking out -- discussions regarding rename() semantics on HDFS., INSERT INTO is not supported - it will get a parse error.

LOAD INTO is the only scenario we have to worry about, from HADOOP-6240, it appears that atomic rename() might not be supported on all FileSystems. Given this, I think we may need to avoid depending on HDFS for atomicity or locking.

There was another discussion on hive-users@ about providing some kind of locking so that two different jobs can do conflicting things to a directory (if there is one writer and a reader). So if we go down that route, this problem can be solved by acquiring write locks (or leases). Ofcourse, this won't work if one of the processes is a non-Hive process. Even ashish's solution will not work since the external process needs to handle the version numbers correctly.
, @Todd, I dont think it is a regression, and there is no easy fix for this.

Do you think we should hold the release for this ?, @prasad, can you explain your comment about the external process stuff?
, It is possible for users now to add new files to the table/partition directory without going through Hive to load files. Hive will pick it up these for the future queries. Currently the directory structure is easily inferred and so external processes can write and read data independent of Hive. if we start having versions then they have to read the correct version from Hive to interact with the data. Doesn't this goes against the philosophy of openness? :), Namit: here's a trace from a session on hive 0.3.0:

{noformat}
todd@todd-laptop:~$ cat /tmp/insert.txt 
a
b
c
d
todd@todd-laptop:~$ cat /tmp/insert2.txt 
e
f
g
h
todd@todd-laptop:~$ hive
Hive history file=/tmp/todd/hive_job_log_todd_200909111603_978288634.txt
hive> create table tmp_insert_test_p (value string) partitioned by (ds string);
OK
Time taken: 3.865 seconds
hive> load data local inpath '/tmp/insert.txt' into table tmp_insert_test_p partition (ds = '2009-08-01');
Copying data from file:/tmp/insert.txt
Loading data to table tmp_insert_test_p partition {ds=2009-08-01}
OK
Time taken: 0.672 seconds
hive> select * from tmp_insert_test_p where ds = '2009-08-01';
OK
a       2009-08-01
b       2009-08-01
c       2009-08-01
d       2009-08-01
Time taken: 0.374 seconds
hive> load data local inpath '/tmp/insert2.txt' into table tmp_insert_test_p partition (ds = '2009-08-01');
Copying data from file:/tmp/insert2.txt
Loading data to table tmp_insert_test_p partition {ds=2009-08-01}
OK
Time taken: 0.261 seconds
hive> select * from tmp_insert_test_p where ds = '2009-08-01';
OK
a       2009-08-01
b       2009-08-01
c       2009-08-01
d       2009-08-01
e       2009-08-01
f       2009-08-01
g       2009-08-01
h       2009-08-01
Time taken: 0.14 seconds
{noformat}

The same session fails on the 0.4 branch:

{noformat}
hive> create table tmp_insert_test_p (value string) partitioned by (ds string);
OK
Time taken: 0.068 seconds
hive> load data local inpath '/tmp/insert.txt' into table tmp_insert_test_p partition (ds = '2009-08-01');
Copying data from file:/tmp/insert.txt
Loading data to table tmp_insert_test_p partition {ds=2009-08-01}
OK
Time taken: 0.315 seconds
hive> select * from tmp_insert_test_p where ds = '2009-08-01';
OK
Time taken: 0.523 seconds
{noformat}, hmm.... that is strange. Given this we should definitely figure out the regression and get it fixed., After discussing with Ashish offline, we think it is due to fact that Prasad changed the order of directory move and metastore.

Unfortunately, users can wait on both hdfs partition creation and partition creation, and we need to do perform 2 in some order while doing a load into.

Prasad might have some ideas on this, the only change i made was not to create the partition/table until Hive.copyFiles() returns. i,e the partition/table directory was created (if it did not exist) before copyFiles() was called in 0.3. It could be the reason for the discrepancy between 0.3 and 0.4 but I am not sure.

We can't do the create the directory if we want to support correct semantics (i.e. the partition directory does not exist until the data has been copied completely). This  is needed for both COPY or REPLACE without which down stream data get corrupted/incomplete data.

But if you want to keep 0.3 semantics (which we might want to since COPY otherwise is quite unusable), we just need to create destf directory in Hive.copyFiles(). 
, Even in the case of LOAD...OVERWRITE, we currently lack atomicity. The old directory is deleted prior to the new directory being moved in. There's a small window of time in which neither old nor new data is present. Sure, this is probably on the order of a half second, but it is still not correct. There's also the general case of a query which has already computed input splits being affected by a concurrent LOAD DATA OVERWRITE. The versioning solution I think gets us partly there, but will be very tricky to implement correctly while maintaining performance since there's no way to do a copy-on-write directory snapshot built in to HDFS. So, I think a significant amount of work will have to be done in the metastore and we'll definitely have to drop the "external process load" ability that currently exists.

Should we open a new JIRA for the general concurrency control/locking issues we're discussing here? It seems like this ticket should be used for the 0.3->0.4 regression, even if it's just a temporary fix. We can then put a more general correct solution on the roadmap for 0.5 or later, since it's looking like it will be a complicated project., +1 to todd's suggestion. let's fix the regression by just creating the directory in copyFiles() and let's open a separate JIRA for concurrency and atomicity issues.

, In 0.3, if we insert the same file twice, we get a error 

2009-09-14 13:15:16,422 ERROR exec.MoveTask (SessionState.java:printError(279)) - Failed with exception checkPaths: /data/users/njain/hive6/hive6/build/ql/test/data/warehouse/tmp_insert_test_p/ds=2009-08-01/kv1.txt already exists
org.apache.hadoop.hive.ql.metadata.HiveException: checkPaths: /data/users/njain/hive6/hive6/build/ql/test/data/warehouse/tmp_insert_test_p/ds=2009-08-01/kv1.txt already exists
	at org.apache.hadoop.hive.ql.metadata.Hive.checkPaths(Hive.java:703)
	at org.apache.hadoop.hive.ql.metadata.Hive.copyFiles(Hive.java:726)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:522)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:145)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:245)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:176)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:211)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:411)
	at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_input40(TestCliDriver.java:57)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)


I will stick with the above error for now for 0.4 also.
, Namit: that error is tracked by HIVE-307 marked linked above. I think it's OK behavior for 0.4 - we should verify, though, that the file that fails to load remains safely in place., @Todd, @Prasad, @Raghu - the current patch is very simple - creates the directory if it does not exist - does not account for concurrency etc.
It should be compatible with 0.3. 

Please review, if it is OK, please merge and I will cut a new release candidate, +1 from my side. We don't want to delay 0.4 release because of this.
, +1

Looks good to me.
, @Todd, I am assuming you are OK with this - If yes, we will try to commit it in a hour or so, so that we can get another release candidate , +1 lgtm. Thanks for getting this in, guys!, Committed. Thanks Namit!]