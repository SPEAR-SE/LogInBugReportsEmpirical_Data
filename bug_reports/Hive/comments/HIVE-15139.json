[I think we can only store the stageId in the map, because that's what we need later.
The patch also does some simple refactor.
[~xuefuz], [~chinnalalam], [~chengxiang li] could you take a look? Thanks., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12837714/HIVE-15139.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 4 failed/errored test(s), 10628 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=91)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=90)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1998/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1998/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1998/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12837714 - PreCommit-HIVE-Build, Thanks [~lirui] for the patch, it looks good to me. I ran into the exactly same issue when playing with the HoS statistics, and worked it around by passing 0 instead of Integer.parseInt(stageId) to metricsCollection.addMetrics just like the taskId. I thought stageId would not be used anyway when we get all metrics from metricsCollection (metricsCollection.getAllMetrics()) in getSparkStatistics. My change is like following:
{code}
@@ -143,7 +143,7 @@ public SparkStatistics getSparkStatistics() {
       List<TaskMetrics> taskMetrics = jobMetric.get(stageId);
       for (TaskMetrics taskMetric : taskMetrics) {
         Metrics metrics = new Metrics(taskMetric);
-        metricsCollection.addMetrics(jobId, Integer.parseInt(stageId), 0, metrics);
+        metricsCollection.addMetrics(jobId, 0, 0, metrics);
       }
     }
     SparkJobUtils sparkJobUtils = new SparkJobUtils();
{code}
Your patch removes the stageAttemptId from jobMetrics, I wonder if it might still be useful for some other metrics (e.g. average times a stage attempted)? It is my first time to look into HoS, I wonder if my question makes sense. Thanks., Hi [~ctang.ma], thanks for the review. Yeah your solution also works, like you said, the stageId is not used when aggregating all the metrics. But it's also no harm to pass the actual stageId which we have already stored in the map.
Why I remove the attempt number is because what we expect is just the stageId down the road, e.g. in {{TaskInfo}}, {{MetricsCollection}} etc. That means currently we don't differentiate multiple attempts for a single stage, in terms of metrics.
In remote mode, we also only collect the stageId. You can refer to the {{ClientListener}} in RemoteDriver. So this will make local mode consistent with remote mode., Hi [~lirui], thanks for working on this. The patch looks good. Just curious, the problem happens just now because of recent changes (on insertion error on task attemps)?, Thanks [~lirui] for the explanation. Yeah, the ClientLister collects both the stageId and taskId.
LGTM, +1., [~xuefuz], no it's actually related to HIVE-12205. It only happens with local mode. So I guess it's not reported because local mode is rarely used. And I happened to hit it when I'm working on HIVE-14825 :), BTW, what the mode we usually use when we develop something on HoS in local env? Is there a better way other than local mode?, [~ctang.ma], usually we use yarn mode (either yarn-client or yarn-cluster). You can setup a single-node cluster in your local env for development. Local mode is kept only for the purpose of debugging. And it's been broken for a while: HIVE-13301. But seems I can have it work again with Spark 2.0., Great, many thanks., [~ctang.ma] - no problem.
[~xuefuz], I'll commit this if you have no further comments., +1. Thanks for fixing this., Committed to master. Thanks [~ctang.ma] and [~xuefuz] for the review!]