[cc [~xuefuz], Maybe we'll also get newer Hadoop version in the damn tgz file so we can actually upgrade that too!, Hi [~spena] do you know how to update the tgz file http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-${spark.version}-bin-hadoop2-without-hive.tgz with a newly built version? , [~Ferd] I will investigate that. However, I see that spark 2.0 is a preview release, and it is not stable yet. Should we want to upgrade to this now? I read on the website that preview releases may contain critical bugs or documentation errors, so I think we should wait until it is officially released as GA., OK, let us wait for GA release., I think we can resume the work here since Spark 2.0 has released., Let's wait until HIVE-14240 is resolved. The current spark assembly used for all itests uses spark 1.6 so this patch won't work. Also, I've heard that spark 2.0 won't use spark assembly anymore, so we need to depend on spark maven dependencies to run the tests., Yeah it'd be great if we can get rid of that tar, or at least make it smaller - we currently package the example jar into it which shouldn't be necessary., This patch is used to test whether qtest is passed., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829075/HIVE-14029.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1225/testReport
Console output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1225/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1225/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2016-09-18 01:52:20.390
+ [[ -n /usr/java/jdk1.8.0_25 ]]
+ export JAVA_HOME=/usr/java/jdk1.8.0_25
+ JAVA_HOME=/usr/java/jdk1.8.0_25
+ export PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/jenkins-PreCommit-HIVE-Build-1225/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2016-09-18 01:52:20.392
+ cd apache-github-source-source
+ git fetch origin
From https://github.com/apache/hive
   05e2510..c90eed2  master     -> origin/master
+ git reset --hard HEAD
warning: unable to access '/home/sseth/.config/git/attributes': Permission denied
HEAD is now at 05e2510 HIVE-14767: Migrate slow MiniMr tests to faster options (Prasanth Jayachandran reviewed by Siddharth Seth)
+ git clean -f -d
warning: unable to access '/home/sseth/.config/git/ignore': Permission denied
+ git checkout master
warning: unable to access '/home/sseth/.config/git/attributes': Permission denied
warning: unable to access '/home/sseth/.config/git/ignore': Permission denied
Already on 'master'
Your branch is behind 'origin/master' by 1 commit, and can be fast-forwarded.
  (use "git pull" to update your local branch)
+ git reset --hard origin/master
warning: unable to access '/home/sseth/.config/git/attributes': Permission denied
HEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2016-09-18 01:52:22.343
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
warning: unable to access '/home/sseth/.config/git/attributes': Permission denied
error: a/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java: No such file or directory
error: a/itests/pom.xml: No such file or directory
error: a/pom.xml: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveBaseFunctionResultList.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SortByShuffler.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java: No such file or directory
error: a/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestHiveKVResultCache.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/MetricsCollection.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/InputMetrics.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/Metrics.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleReadMetrics.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/metrics/ShuffleWriteMetrics.java: No such file or directory
error: a/spark-client/src/test/java/org/apache/hive/spark/client/TestMetricsCollection.java: No such file or directory
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829075 - jenkins-PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829079/HIVE-14029.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1226/testReport
Console output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1226/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1226/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2016-09-18 02:31:43.131
+ [[ -n /usr/java/jdk1.8.0_25 ]]
+ export JAVA_HOME=/usr/java/jdk1.8.0_25
+ JAVA_HOME=/usr/java/jdk1.8.0_25
+ export PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ PATH=/usr/java/jdk1.8.0_25/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ M2_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/jenkins-PreCommit-HIVE-Build-1226/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2016-09-18 02:31:43.133
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
warning: unable to access '/home/sseth/.config/git/attributes': Permission denied
warning: unable to access '/home/sseth/.config/git/attributes': Permission denied
HEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)
+ git clean -f -d
warning: unable to access '/home/sseth/.config/git/ignore': Permission denied
+ git checkout master
warning: unable to access '/home/sseth/.config/git/ignore': Permission denied
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at c90eed2 HIVE-14734: Detect ptest profile and submit to ptest-server from jenkins-execute-build.sh (Sergio Pena, reviewed by Siddarth Seth)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2016-09-18 02:31:44.018
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
warning: unable to access '/home/sseth/.config/git/attributes': Permission denied
error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java:76
error: ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java: patch does not apply
The patch does not appear to apply with p0, p1, or p2
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829079 - jenkins-PreCommit-HIVE-Build, Rebase patch, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829082/HIVE-14029.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1227/testReport
Console output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1227/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1227/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/ConcurrentHashMap.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/FileNotFoundException.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URISyntaxException.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Integer.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/IMetaStoreClient.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/metastore/target/hive-metastore-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/metastore/api/MetaException.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/Text.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Credentials.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/token/Token.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/core/target/hive-hcatalog-core-2.2.0-SNAPSHOT.jar(org/apache/hive/hcatalog/common/HCatUtil.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/thrift/libthrift/0.9.3/libthrift-0.9.3.jar(org/apache/thrift/TException.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/security/Groups.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/HashSet.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Set.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Date.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceAudience$Private.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/BufferedReader.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/InputStream.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/InputStreamReader.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/PrintWriter.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Map$Entry.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/Semaphore.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/CommandLine.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/DefaultExecutor.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/ExecuteWatchdog.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/commons/commons-exec/1.1/commons-exec-1.1.jar(org/apache/commons/exec/PumpStreamHandler.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Shell.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Thread.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Runnable.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/DataInput.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/DataOutput.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputSplit.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/io/OutputStreamWriter.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFramework.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/io/NullWritable.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/RecordReader.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/TaskAttemptContext.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/CreateMode.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/KeeperException.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar(org/apache/zookeeper/ZooDefs$Ids.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/InputFormat.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobContext.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/classification/InterfaceStability$Evolving.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URLConnection.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/net/URLDecoder.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Enumeration.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Properties.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/StringTokenizer.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriBuilder.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/LogUtils.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Class.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/StringBuilder.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/JobID.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/Iterator.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/LinkedList.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/ExecutorService.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/Executors.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/util/concurrent/TimeUnit.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Process.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper$Context.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFrameworkFactory.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar(org/apache/curator/retry/ExponentialBackoffRetry.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configured.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobClient.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/JobConf.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Job.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenIdentifier.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Tool.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configurable.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/ClassNotFoundException.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapred/RunningJob.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience$LimitedPrivate.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Annotation.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/SuppressWarnings.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Retention.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/RetentionPolicy.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/Target.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/annotation/ElementType.class)]]
[loading ZipFileIndexFileObject[/data/hive-ptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/HttpMethod.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(java/lang/Override.class)]]
[loading ZipFileIndexFileObject[/usr/java/jdk1.8.0_25/jre/lib/rt.jar(sun/misc/Contended.class)]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$3.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatDelegator$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/LauncherDelegator$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/Server$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatException$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$2.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/LogRetriever$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/HDFSStorage$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/ZooKeeperStorage$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonUtils$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1.class]]
[loading RegularFileObject[/data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1$1.class]]
[done in 2216 ms]
[WARNING] Javadoc Warnings
[WARNING] Sep 18, 2016 3:24:28 AM com.sun.jersey.wadl.resourcedoc.ResourceDoclet start
[WARNING] INFO: Wrote /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/resourcedoc.xml
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-webhcat ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-webhcat ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-webhcat ---
[INFO] Compiling 9 source files to /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/test-classes
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Recompile with -Xlint:deprecation for details.
[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Some input files use unchecked or unsafe operations.
[WARNING] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/TestWebHCatE2e.java: Recompile with -Xlint:unchecked for details.
[INFO] 4 warnings 
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[67,3] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[72,3] method does not override or implement a method from a supertype
[INFO] 2 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive .............................................. SUCCESS [1.987s]
[INFO] Hive Shims Common ................................. SUCCESS [3.928s]
[INFO] Hive Shims 0.23 ................................... SUCCESS [2.028s]
[INFO] Hive Shims Scheduler .............................. SUCCESS [0.789s]
[INFO] Hive Shims ........................................ SUCCESS [0.493s]
[INFO] Hive Storage API .................................. SUCCESS [1.208s]
[INFO] Hive ORC .......................................... SUCCESS [4.733s]
[INFO] Hive Common ....................................... SUCCESS [4.729s]
[INFO] Hive Service RPC .................................. SUCCESS [3.410s]
[INFO] Hive Serde ........................................ SUCCESS [3.448s]
[INFO] Hive Metastore .................................... SUCCESS [19.085s]
[INFO] Hive Ant Utilities ................................ SUCCESS [0.267s]
[INFO] Hive Llap Common .................................. SUCCESS [2.802s]
[INFO] Hive Llap Client .................................. SUCCESS [1.071s]
[INFO] Hive Llap Tez ..................................... SUCCESS [1.214s]
[INFO] Spark Remote Client ............................... SUCCESS [23.836s]
[INFO] Hive Query Language ............................... SUCCESS [53.022s]
[INFO] Hive Llap Server .................................. SUCCESS [3.353s]
[INFO] Hive Service ...................................... SUCCESS [3.861s]
[INFO] Hive Accumulo Handler ............................. SUCCESS [2.300s]
[INFO] Hive JDBC ......................................... SUCCESS [9.583s]
[INFO] Hive Beeline ...................................... SUCCESS [1.997s]
[INFO] Hive CLI .......................................... SUCCESS [1.501s]
[INFO] Hive Contrib ...................................... SUCCESS [0.808s]
[INFO] Hive Druid Handler ................................ SUCCESS [3.230s]
[INFO] Hive HBase Handler ................................ SUCCESS [2.421s]
[INFO] Hive HCatalog ..................................... SUCCESS [0.202s]
[INFO] Hive HCatalog Core ................................ SUCCESS [2.674s]
[INFO] Hive HCatalog Pig Adapter ......................... SUCCESS [2.686s]
[INFO] Hive HCatalog Server Extensions ................... SUCCESS [1.478s]
[INFO] Hive HCatalog Webhcat Java Client ................. SUCCESS [1.310s]
[INFO] Hive HCatalog Webhcat ............................. FAILURE [4.897s]
[INFO] Hive HCatalog Streaming ........................... SKIPPED
[INFO] Hive HPL/SQL ...................................... SKIPPED
[INFO] Hive HWI .......................................... SKIPPED
[INFO] Hive Llap External Client ......................... SKIPPED
[INFO] Hive Shims Aggregator ............................. SKIPPED
[INFO] Hive TestUtils .................................... SKIPPED
[INFO] Hive Packaging .................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2:51.263s
[INFO] Finished at: Sun Sep 18 03:24:28 UTC 2016
[INFO] Final Memory: 254M/935M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-webhcat: Compilation failure: Compilation failure:
[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[67,3] method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-github-source-source/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java:[72,3] method does not override or implement a method from a supertype
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-webhcat
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829082 - jenkins-PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829095/HIVE-14029.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1228/testReport
Console output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1228/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1228/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---
[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/test-classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-hcatalog-it-unit ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-jar-plugin:2.4:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-util ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 66 source files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-it-util ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-it-util ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-unit ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-unit ---
[INFO] Executing tasks

main:
     [exec] + /bin/pwd
     [exec] /data/hive-ptest/working/apache-github-source-source/itests/hive-unit
     [exec] + BASE_DIR=./target
     [exec] + HIVE_ROOT=./target/../../../
     [exec] + DOWNLOAD_DIR=./../thirdparty
     [exec] + mkdir -p ./../thirdparty
     [exec] + download 'https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0' spark
     [exec] + url='https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0'
     [exec] + finalName=spark
     [exec] ++ basename 'https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0'
     [exec] + tarName='spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0'
     [exec] + rm -rf ./target/spark
     [exec] + [[ ! -f ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0 ]]
     [exec] + curl -Sso './../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0' 'https://www.dropbox.com/s/cv47dca5f2hqefn/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0'
     [exec] + tar -zxf './../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0' -C ./target
     [exec] tar (child): ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz?dl=0: Cannot open: No such file or directory
     [exec] tar (child): Error is not recoverable: exiting now
     [exec] tar: Child returned status 2
     [exec] tar: Error is not recoverable: exiting now
     [exec] + mv ./target/spark-2.0.0-preview-bin-hadoop2-without-hive ./target/spark
     [exec] mv: cannot stat ?./target/spark-2.0.0-preview-bin-hadoop2-without-hive?: No such file or directory
     [exec] + cp -f ./target/../../..//data/conf/spark/log4j2.properties ./target/spark/conf/
     [exec] cp: cannot create regular file ?./target/spark/conf/?: No such file or directory
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [1.708s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [4.071s]
[INFO] Hive Integration - Custom udfs .................... SUCCESS [0.714s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-util  SUCCESS [0.696s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf1  SUCCESS [0.711s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf2  SUCCESS [0.623s]
[INFO] Hive Integration - Custom UDFs - udf-vectorized-badexample  SUCCESS [0.894s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [4.463s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [4.189s]
[INFO] Hive Integration - Unit Tests ..................... FAILURE [1.870s]
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] Hive Integration - QFile Accumulo Tests ........... SKIPPED
[INFO] JMH benchmark: Hive ............................... SKIPPED
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] Hive Integration - QFile Spark Tests .............. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 20.754s
[INFO] Finished at: Sun Sep 18 08:04:50 UTC 2016
[INFO] Final Memory: 92M/697M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (download-spark) on project hive-it-unit: An Ant BuildException has occured: exec returned: 1
[ERROR] around Ant part ...<exec failonerror="true" dir="/data/hive-ptest/working/apache-github-source-source/itests/hive-unit" executable="bash">... @ 4:122 in /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target/antrun/build-main.xml
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829095 - jenkins-PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829099/HIVE-14029.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1229/testReport
Console output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1229/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1229/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-hcatalog-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-hcatalog-it-unit ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-hcatalog-it-unit ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-hcatalog-it-unit ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-hcatalog-it-unit ---
[INFO] Compiling 8 source files to /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/test-classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/mapreduce/TestHCatHiveThriftCompatibility.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-hcatalog-it-unit ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-hcatalog-it-unit ---
[INFO] 
[INFO] --- maven-jar-plugin:2.4:test-jar (default) @ hive-hcatalog-it-unit ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-hcatalog-it-unit ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT.pom
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/hcatalog-unit/target/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-hcatalog-it-unit/2.2.0-SNAPSHOT/hive-hcatalog-it-unit-2.2.0-SNAPSHOT-tests.jar
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Testing Utilities 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-util ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/util (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-util ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @ hive-it-util ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (define-classpath) @ hive-it-util ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hive-it-util ---
[INFO] Compiling 66 source files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/classes
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Some input files use or override a deprecated API.
[WARNING] /data/hive-ptest/working/apache-github-source-source/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/VerifyPartitionIsSubdirectoryOfTableHook.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hive-it-util ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /data/hive-ptest/working/apache-github-source-source/itests/util/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (setup-test-dirs) @ hive-it-util ---
[INFO] Executing tasks

main:
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/warehouse
    [mkdir] Created dir: /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf
     [copy] Copying 15 files to /data/hive-ptest/working/apache-github-source-source/itests/util/target/tmp/conf
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hive-it-util ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hive-it-util ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hive-it-util ---
[INFO] Building jar: /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar
[INFO] 
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @ hive-it-util ---
[INFO] 
[INFO] --- maven-install-plugin:2.4:install (default-install) @ hive-it-util ---
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/target/hive-it-util-2.2.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.jar
[INFO] Installing /data/hive-ptest/working/apache-github-source-source/itests/util/pom.xml to /data/hive-ptest/working/maven/org/apache/hive/hive-it-util/2.2.0-SNAPSHOT/hive-it-util-2.2.0-SNAPSHOT.pom
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Hive Integration - Unit Tests 2.2.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ hive-it-unit ---
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target
[INFO] Deleting /data/hive-ptest/working/apache-github-source-source/itests/hive-unit (includes = [datanucleus.log, derby.log], excludes = [])
[INFO] 
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-no-snapshots) @ hive-it-unit ---
[INFO] 
[INFO] --- maven-antrun-plugin:1.7:run (download-spark) @ hive-it-unit ---
[INFO] Executing tasks

main:
     [exec] + /bin/pwd
     [exec] + BASE_DIR=./target
     [exec] + HIVE_ROOT=./target/../../../
     [exec] + DOWNLOAD_DIR=./../thirdparty
     [exec] + mkdir -p ./../thirdparty
     [exec] /data/hive-ptest/working/apache-github-source-source/itests/hive-unit
     [exec] + download http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz spark
     [exec] + url=http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz
     [exec] + finalName=spark
     [exec] ++ basename http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz
     [exec] + tarName=spark-2.0.0-bin-hadoop2-without-hive.tgz
     [exec] + rm -rf ./target/spark
     [exec] + [[ ! -f ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz ]]
     [exec] + curl -Sso ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz
     [exec] + tar -zxf ./../thirdparty/spark-2.0.0-bin-hadoop2-without-hive.tgz -C ./target
     [exec] + mv ./target/spark-2.0.0-preview-bin-hadoop2-without-hive ./target/spark
     [exec] mv: cannot stat ?./target/spark-2.0.0-preview-bin-hadoop2-without-hive?: No such file or directory
     [exec] + cp -f ./target/../../..//data/conf/spark/log4j2.properties ./target/spark/conf/
     [exec] cp: cannot create regular file ?./target/spark/conf/?: No such file or directory
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hive Integration - Parent ......................... SUCCESS [1.573s]
[INFO] Hive Integration - Custom Serde ................... SUCCESS [3.578s]
[INFO] Hive Integration - Custom udfs .................... SUCCESS [0.591s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-util  SUCCESS [1.068s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf1  SUCCESS [0.834s]
[INFO] Hive Integration - Custom UDFs - udf-classloader-udf2  SUCCESS [0.644s]
[INFO] Hive Integration - Custom UDFs - udf-vectorized-badexample  SUCCESS [0.615s]
[INFO] Hive Integration - HCatalog Unit Tests ............ SUCCESS [4.321s]
[INFO] Hive Integration - Testing Utilities .............. SUCCESS [4.851s]
[INFO] Hive Integration - Unit Tests ..................... FAILURE [9.328s]
[INFO] Hive Integration - Test Serde ..................... SKIPPED
[INFO] Hive Integration - QFile Tests .................... SKIPPED
[INFO] Hive Integration - QFile Accumulo Tests ........... SKIPPED
[INFO] JMH benchmark: Hive ............................... SKIPPED
[INFO] Hive Integration - Unit Tests - Hadoop 2 .......... SKIPPED
[INFO] Hive Integration - Unit Tests with miniKdc ........ SKIPPED
[INFO] Hive Integration - QFile Spark Tests .............. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 28.170s
[INFO] Finished at: Sun Sep 18 08:34:08 UTC 2016
[INFO] Final Memory: 90M/757M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (download-spark) on project hive-it-unit: An Ant BuildException has occured: exec returned: 1
[ERROR] around Ant part ...<exec failonerror="true" dir="/data/hive-ptest/working/apache-github-source-source/itests/hive-unit" executable="bash">... @ 4:122 in /data/hive-ptest/working/apache-github-source-source/itests/hive-unit/target/antrun/build-main.xml
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-it-unit
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829099 - jenkins-PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829100/HIVE-14029.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10497 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles
org.apache.hive.spark.client.TestSparkClient.testCounters
org.apache.hive.spark.client.TestSparkClient.testMetricsCollection
org.apache.hive.spark.client.TestSparkClient.testRemoteClient
org.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob
{noformat}

Test results: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1230/testReport
Console output: https://builds.apache.org/job/jenkins-PreCommit-HIVE-Build/1230/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/jenkins-PreCommit-HIVE-Build-1230/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829100 - jenkins-PreCommit-HIVE-Build, Fix some dependencies issues, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829333/HIVE-14029.1.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10498 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1237/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1237/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1237/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829333 - PreCommit-HIVE-Build, Do we need to finish HIVE-14240 to get this unblocked? [~spena]  It doesn't look like to me.

[~Ferd]  There are several baselines which got updated by moving {{"BASIC_STATS":"true"}}. Do you know what causes it? , [~Ferd], could you also explain why we need to upgrade the dependencies, and why DataReadMethod is removed from InputMetrics?, Hi [~aihuaxu], HIVE-14240 is trying to replacing current tgz file. This patch can bypass it by using a tmp file. It doesn't block HIVE-14240. 
I am not quite sure why the order changed. It looks strange that the context is the same expect the displaying order., Hi [~lirui], some APIs are changed in Spark side and the updates for dependencies are required since Spark use newer version which will lead inconsistent errors for HoS., https://builds.apache.org/job/PreCommit-HIVE-Build/1239 was failed. Retest the patch., [~Ferd] I think we should try to fix HIVE-14240 first to avoid dependencies issues when Spark and Hive are running in the same machine. I talked with the Spark team a few times, and they think this assembly tar.gz will cause issues due to other Hive libraries Spark depends, such as Hive 1.2 metastore and Hive 1.2 serde.

Would you like to start working on HIVE-14240? You can ask [~stakiar] if you're interested., Hi [~spena], I am not quite sure why assembly tar.gz  will cause issues for Hive since it's included in Hive itest only. Could you explain a little bit more?  BTW, I will take a look at how to remove it from itest., Sure. [~stakiar] Let me know if the below statements are correct, and feel free to correct me.

- Spark2 uses a fork of Hive 1.2 due to issues with Apache Hive. They called this project {{spark-hive}}. Spark only uses Hive 1.2 metastore/serde/udf jars form this forked project.
  They download this from https://mvnrepository.com/artifact/org.apache.spark/spark-hive_2.10 

- Spark2 assembly without hive will be built without any of the above dependencies.

- Hive2 itests will use Spark2 assembly to run Hive2 tests. This means Hive2 might not test Spark2 correctly due to the lack of Hive 1.2 libraries in it., bq. some APIs are changed in Spark side
Is there any other way we can track the read method? If not, guess we can just remove the class from Hive side.
bq. Hive2 itests will use Spark2 assembly to run Hive2 tests. This means Hive2 might not test Spark2 correctly due to the lack of Hive 1.2 libraries in it.
I'm not sure what problem spark has without hive libraries. We have been requiring that spark is built without hive. Otherwise we'll have different hive libraries in our classpath which causes conflicts.
I don't think HIVE-14240 blocks this one. Actually HIVE-14240 should be implemented for Spark 2.0 right?, My understanding is spark needs hive libraries only for SparkSQL, which is not needed for HoS., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829398/HIVE-14029.1.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 23 failed/errored test(s), 10556 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucket4]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucket5]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[disable_merge_for_bucketing]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[list_bucket_dml_10]
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[reduce_deduplicate]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[bucket4]
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[disable_merge_for_bucketing]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles
org.apache.hive.spark.client.TestSparkClient.testCounters
org.apache.hive.spark.client.TestSparkClient.testErrorJob
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
org.apache.hive.spark.client.TestSparkClient.testMetricsCollection
org.apache.hive.spark.client.TestSparkClient.testRemoteClient
org.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1242/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1242/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1242/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 23 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829398 - PreCommit-HIVE-Build, [~Ferd] how was the http://blog.sundp.me/spark/spark-2.0.0-bin-hadoop2-without-hive.tgz built?

I don't think HIVE-14240 is a blocker for this assuming the tar-ball was built in a supported way, but I'm trying to contact some Spark committers to see if they have any input., Hi [~stakiar], the tgz was built via the following commands:
{code}
sh ./dev/make-distribution.sh  --name hadoop2-without-hive --tgz -Phadoop-2.7 -Pyarn -Pparquet-provided -Dhadoop.version=2.7.3
{code}
[~dapengsun], can you confirm it please?, Hi [~spena], I think we should move it forwards since HIVE-14240 needs further discussions and it doesn't block this ticket. We can upload the tgz into a stable location to upgrade the Spark version and once we fixed HIVE-14240, we can easily remove this tgz. [~lirui] [~stakiar] [~aihuaxu] any thoughts?, [~Ferd]
Yes, I used this command, I agree to move this forward. HIVE-14240 can be done in parallel, if it doesn't depend on this one :), Hi [~lirui] 
bq. Is there any other way we can track the read method? If not, guess we can just remove the class from Hive side.

I will investigate this in a separate JIRA. Thank you for pointing this out., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829489/HIVE-14029.2.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10556 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1249/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1249/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1249/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829489 - PreCommit-HIVE-Build, Hi folks, the failed cases are not related. [~spena], do you have comments for the patch? I am going to commit it if no further comments., +1 to the latest patch.
[~Ferd], is there anything needs to be updated in our getting started [wiki|https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started]? I think at least we need to update how to integrate spark since we no longer have the assembly jar., Hi [~lirui], WIKI needs to be updated because for Spark 2.0.0 or above, there is no assembly jar built. Before updating the wiki, I need to verify it locally., OK. AFAIK, HoS only needs spark-core. So we can try adding spark-core and all its dependencies to hive's classpath., Thanks [~Ferd]. I uploaded the spark 2.0 assembly jar to the stable location. Could you upload a new patch without the URL spark assembly change? To test that it will work.

I will review the patch today., Do you mean the tgz file? What's the new address for it or the same name as before? , It is the same as before., Attached as HIVE-14029.3.patch., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829601/HIVE-14029.3.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10556 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1254/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1254/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1254/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829601 - PreCommit-HIVE-Build, [~Ferd] overall this looks good to me. It would be nice if you could update the description to list out the high-level changes that needed to be made to Hive to add support for Spark 2.0.0. For example, dependency updates, which APIs changed (change from Iterable to Iterator, InputMetrics constructor change)., [~Ferd] The patch looks good. +1

I just found a variable that is not used anymore.
{noformat}
MetricsCollection.java
 - Should we remove 'DataReadMethod readMethod = null'? is not used anymore.
{noformat}

[~xuefuz] Do you think this patch is ready to go to start supporting spark 2.0?
[~Ferd] Have we run any other tests in an environment with spark 2.0 and hive 2.1? I think we should do that if you haven't yet before commit the patch. Just to confirm we don't have issues with the classpath., Hi guys, thanks for working/reviewing this. The patch looks good. I understand that there is a pending discussion about removing spark tarball from the test. However, in this long thread there seems a confusion of this with the spark's assembly jar which is part of spark build as of 1.6. [~Ferd], do we have a clear picture of that for 2.0? If there is any change, we do want to update the doc. For instance, I used to get the assembly.jar from spark build and copy it to hive's /lib directory and I'm ready to run Hive on Spark.

Sorry I'm a little behind Spark 2.0. I will try to figure it out on my end as well., I made a build of spark 2.0 and indeed spark-assembly.jar is missing., Hi [~xuefuz], Spark assembly was removed since Spark 2.0.0. They don't provide an assembly jar considering some dependency conflicts. I find some comments in the root pom file for Spark. To support 2.0.0, we have to copy all Spark related jars under the hive/lib AFAIK., Hi [~spena], do we need to support in Hive 2.1? I do some smoke tests in current upstream and Spark 2.0 and it passed if you set SPARK_HOME correctly and copy all lib jars of Spark into hive/lib folder. This needed to be updated in Hive On Spark WIKI., [~Ferd], the classpath is just for HS2/CLI, so I don't think we need all the spark jars. Please find a minimum set of required jars. You can start with spark-core., Thanks [~stakiar] for your review. Description is updated., Hi [~lirui], 
bq. I don't think we need all the spark jars.

Agree. It's not required for all Spark jars. From Hive pom.xml file, we see that it depends only on Spark_core. You can find the dependency for Spark_core. All of them should be included into HIVE_CLASSPATH. Do we really need to filter those jars? It isn't very user friendly because users have to find them one by one in Spark jars and add it to HIVE classpath. I think we can simple add the whole folder to HIVE classpath when running Hive on Spark. Any thoughts?

{code}
core]# mvn dependency:tree
[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Core 2.0.0
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-dependency-plugin:2.10:tree (default-cli) @ spark-core_2.11 ---
[INFO] org.apache.spark:spark-core_2.11:jar:2.0.0
[INFO] +- org.apache.avro:avro-mapred:jar:hadoop2:1.7.7:compile
[INFO] |  +- org.apache.avro:avro-ipc:jar:1.7.7:compile
[INFO] |  |  \- org.apache.avro:avro:jar:1.7.7:compile
[INFO] |  +- org.apache.avro:avro-ipc:jar:tests:1.7.7:test
[INFO] |  +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile
[INFO] |  \- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile
[INFO] +- com.google.guava:guava:jar:14.0.1:provided
[INFO] +- com.twitter:chill_2.11:jar:0.8.0:compile
[INFO] |  \- com.esotericsoftware:kryo-shaded:jar:3.0.3:compile
[INFO] |     \- com.esotericsoftware:minlog:jar:1.3.0:compile
[INFO] +- com.twitter:chill-java:jar:0.8.0:compile
[INFO] +- org.apache.xbean:xbean-asm5-shaded:jar:4.4:compile
[INFO] +- org.apache.hadoop:hadoop-client:jar:2.2.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-common:jar:2.2.0:compile
[INFO] |  |  +- commons-cli:commons-cli:jar:1.2:compile
[INFO] |  |  +- xmlenc:xmlenc:jar:0.52:compile
[INFO] |  |  +- commons-io:commons-io:jar:2.4:compile
[INFO] |  |  +- commons-lang:commons-lang:jar:2.6:compile
[INFO] |  |  +- commons-configuration:commons-configuration:jar:1.6:compile
[INFO] |  |  |  +- commons-digester:commons-digester:jar:1.8:compile
[INFO] |  |  |  |  \- commons-beanutils:commons-beanutils:jar:1.7.0:compile
[INFO] |  |  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO] |  |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile
[INFO] |  |  +- org.apache.hadoop:hadoop-auth:jar:2.2.0:compile
[INFO] |  |  \- org.apache.commons:commons-compress:jar:1.4.1:compile
[INFO] |  |     \- org.tukaani:xz:jar:1.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-hdfs:jar:2.2.0:compile
[INFO] |  |  \- org.mortbay.jetty:jetty-util:jar:6.1.26:compile
[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-app:jar:2.2.0:compile
[INFO] |  |  +- org.apache.hadoop:hadoop-mapreduce-client-common:jar:2.2.0:compile
[INFO] |  |  |  +- org.apache.hadoop:hadoop-yarn-client:jar:2.2.0:compile
[INFO] |  |  |  |  \- com.google.inject:guice:jar:3.0:compile
[INFO] |  |  |  |     +- javax.inject:javax.inject:jar:1:compile
[INFO] |  |  |  |     \- aopalliance:aopalliance:jar:1.0:compile
[INFO] |  |  |  \- org.apache.hadoop:hadoop-yarn-server-common:jar:2.2.0:compile
[INFO] |  |  \- org.apache.hadoop:hadoop-mapreduce-client-shuffle:jar:2.2.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-yarn-api:jar:2.2.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.2.0:compile
[INFO] |  |  \- org.apache.hadoop:hadoop-yarn-common:jar:2.2.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-mapreduce-client-jobclient:jar:2.2.0:compile
[INFO] |  \- org.apache.hadoop:hadoop-annotations:jar:2.2.0:compile
[INFO] +- org.apache.spark:spark-launcher_2.11:jar:2.0.0:compile
[INFO] +- org.apache.spark:spark-network-common_2.11:jar:2.0.0:compile
[INFO] +- org.apache.spark:spark-network-shuffle_2.11:jar:2.0.0:compile
[INFO] |  +- org.fusesource.leveldbjni:leveldbjni-all:jar:1.8:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-annotations:jar:2.6.5:compile
[INFO] +- org.apache.spark:spark-unsafe_2.11:jar:2.0.0:compile
[INFO] +- net.java.dev.jets3t:jets3t:jar:0.7.1:compile
[INFO] |  +- commons-codec:commons-codec:jar:1.10:compile
[INFO] |  \- commons-httpclient:commons-httpclient:jar:3.1:compile
[INFO] +- org.apache.curator:curator-recipes:jar:2.4.0:compile
[INFO] |  +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO] |  |  \- org.apache.curator:curator-client:jar:2.4.0:compile
[INFO] |  \- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO] +- org.eclipse.jetty:jetty-plus:jar:9.2.16.v20160414:compile
[INFO] |  +- org.eclipse.jetty:jetty-webapp:jar:9.2.16.v20160414:compile
[INFO] |  |  \- org.eclipse.jetty:jetty-xml:jar:9.2.16.v20160414:compile
[INFO] |  \- org.eclipse.jetty:jetty-jndi:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-security:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-util:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-server:jar:9.2.16.v20160414:compile
[INFO] |  \- org.eclipse.jetty:jetty-io:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-http:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-continuation:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-servlet:jar:9.2.16.v20160414:compile
[INFO] +- org.eclipse.jetty:jetty-servlets:jar:9.2.16.v20160414:compile
[INFO] +- javax.servlet:javax.servlet-api:jar:3.1.0:compile
[INFO] +- org.apache.commons:commons-lang3:jar:3.3.2:compile
[INFO] +- org.apache.commons:commons-math3:jar:3.4.1:compile
[INFO] +- com.google.code.findbugs:jsr305:jar:1.3.9:compile
[INFO] +- org.slf4j:slf4j-api:jar:1.7.16:compile
[INFO] +- org.slf4j:jul-to-slf4j:jar:1.7.16:compile
[INFO] +- org.slf4j:jcl-over-slf4j:jar:1.7.16:compile
[INFO] +- log4j:log4j:jar:1.2.17:compile
[INFO] +- org.slf4j:slf4j-log4j12:jar:1.7.16:compile
[INFO] +- com.ning:compress-lzf:jar:1.0.3:compile
[INFO] +- org.xerial.snappy:snappy-java:jar:1.1.2.4:compile
[INFO] +- net.jpountz.lz4:lz4:jar:1.3.0:compile
[INFO] +- org.roaringbitmap:RoaringBitmap:jar:0.5.11:compile
[INFO] +- commons-net:commons-net:jar:2.2:compile
[INFO] +- org.scala-lang:scala-library:jar:2.11.8:compile
[INFO] +- org.json4s:json4s-jackson_2.11:jar:3.2.11:compile
[INFO] |  \- org.json4s:json4s-core_2.11:jar:3.2.11:compile
[INFO] |     +- org.json4s:json4s-ast_2.11:jar:3.2.11:compile
[INFO] |     +- com.thoughtworks.paranamer:paranamer:jar:2.6:compile
[INFO] |     \- org.scala-lang:scalap:jar:2.11.8:compile
[INFO] |        \- org.scala-lang:scala-compiler:jar:2.11.8:compile
[INFO] |           \- org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.0.4:compile
[INFO] +- org.glassfish.jersey.core:jersey-client:jar:2.22.2:compile
[INFO] |  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile
[INFO] |  +- org.glassfish.hk2:hk2-api:jar:2.4.0-b34:compile
[INFO] |  |  +- org.glassfish.hk2:hk2-utils:jar:2.4.0-b34:compile
[INFO] |  |  \- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34:compile
[INFO] |  +- org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34:compile
[INFO] |  \- org.glassfish.hk2:hk2-locator:jar:2.4.0-b34:compile
[INFO] +- org.glassfish.jersey.core:jersey-common:jar:2.22.2:compile
[INFO] |  +- javax.annotation:javax.annotation-api:jar:1.2:compile
[INFO] |  +- org.glassfish.jersey.bundles.repackaged:jersey-guava:jar:2.22.2:compile
[INFO] |  \- org.glassfish.hk2:osgi-resource-locator:jar:1.0.1:compile
[INFO] +- org.glassfish.jersey.core:jersey-server:jar:2.22.2:compile
[INFO] |  +- org.glassfish.jersey.media:jersey-media-jaxb:jar:2.22.2:compile
[INFO] |  \- javax.validation:validation-api:jar:1.1.0.Final:compile
[INFO] +- org.glassfish.jersey.containers:jersey-container-servlet:jar:2.22.2:compile
[INFO] +- org.glassfish.jersey.containers:jersey-container-servlet-core:jar:2.22.2:compile
[INFO] +- org.apache.mesos:mesos:jar:shaded-protobuf:0.21.1:compile
[INFO] +- io.netty:netty-all:jar:4.0.29.Final:compile
[INFO] +- io.netty:netty:jar:3.8.0.Final:compile
[INFO] +- com.clearspring.analytics:stream:jar:2.7.0:compile
[INFO] +- io.dropwizard.metrics:metrics-core:jar:3.1.2:compile
[INFO] +- io.dropwizard.metrics:metrics-jvm:jar:3.1.2:compile
[INFO] +- io.dropwizard.metrics:metrics-json:jar:3.1.2:compile
[INFO] +- io.dropwizard.metrics:metrics-graphite:jar:3.1.2:compile
[INFO] +- com.fasterxml.jackson.core:jackson-databind:jar:2.6.5:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-core:jar:2.6.5:compile
[INFO] +- com.fasterxml.jackson.module:jackson-module-scala_2.11:jar:2.6.5:compile
[INFO] |  +- org.scala-lang:scala-reflect:jar:2.11.8:compile
[INFO] |  \- com.fasterxml.jackson.module:jackson-module-paranamer:jar:2.6.5:compile
[INFO] +- org.apache.derby:derby:jar:10.11.1.1:test
[INFO] +- org.apache.ivy:ivy:jar:2.4.0:compile
[INFO] +- oro:oro:jar:2.0.8:compile
[INFO] +- org.seleniumhq.selenium:selenium-java:jar:2.52.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-chrome-driver:jar:2.52.0:test
[INFO] |  |  \- org.seleniumhq.selenium:selenium-remote-driver:jar:2.52.0:test
[INFO] |  |     +- cglib:cglib-nodep:jar:2.1_3:test
[INFO] |  |     +- com.google.code.gson:gson:jar:2.3.1:test
[INFO] |  |     \- org.seleniumhq.selenium:selenium-api:jar:2.52.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-edge-driver:jar:2.52.0:test
[INFO] |  |  \- org.apache.commons:commons-exec:jar:1.3:test
[INFO] |  +- org.seleniumhq.selenium:selenium-firefox-driver:jar:2.52.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-ie-driver:jar:2.52.0:test
[INFO] |  |  +- net.java.dev.jna:jna:jar:4.1.0:test
[INFO] |  |  \- net.java.dev.jna:jna-platform:jar:4.1.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-safari-driver:jar:2.52.0:test
[INFO] |  +- org.seleniumhq.selenium:selenium-support:jar:2.52.0:test
[INFO] |  +- org.webbitserver:webbit:jar:0.4.14:test
[INFO] |  \- org.seleniumhq.selenium:selenium-leg-rc:jar:2.52.0:test
[INFO] +- org.seleniumhq.selenium:selenium-htmlunit-driver:jar:2.52.0:test
[INFO] |  +- net.sourceforge.htmlunit:htmlunit:jar:2.18:test
[INFO] |  |  +- xalan:xalan:jar:2.7.2:test
[INFO] |  |  |  \- xalan:serializer:jar:2.7.2:test
[INFO] |  |  +- org.apache.httpcomponents:httpmime:jar:4.5.2:test
[INFO] |  |  +- net.sourceforge.htmlunit:htmlunit-core-js:jar:2.17:test
[INFO] |  |  +- xerces:xercesImpl:jar:2.11.0:test
[INFO] |  |  +- net.sourceforge.nekohtml:nekohtml:jar:1.9.22:test
[INFO] |  |  +- net.sourceforge.cssparser:cssparser:jar:0.9.16:test
[INFO] |  |  |  \- org.w3c.css:sac:jar:1.3:test
[INFO] |  |  +- commons-logging:commons-logging:jar:1.2:test
[INFO] |  |  \- org.eclipse.jetty.websocket:websocket-client:jar:9.2.12.v20150709:test
[INFO] |  |     \- org.eclipse.jetty.websocket:websocket-common:jar:9.2.12.v20150709:test
[INFO] |  |        \- org.eclipse.jetty.websocket:websocket-api:jar:9.2.12.v20150709:test
[INFO] |  +- commons-collections:commons-collections:jar:3.2.2:compile
[INFO] |  \- org.apache.httpcomponents:httpclient:jar:4.5.2:test
[INFO] |     \- org.apache.httpcomponents:httpcore:jar:4.4.4:test
[INFO] +- xml-apis:xml-apis:jar:1.4.01:test
[INFO] +- org.hamcrest:hamcrest-core:jar:1.3:test
[INFO] +- org.hamcrest:hamcrest-library:jar:1.3:test
[INFO] +- org.mockito:mockito-core:jar:1.10.19:test
[INFO] |  \- org.objenesis:objenesis:jar:2.1:compile
[INFO] +- org.scalacheck:scalacheck_2.11:jar:1.12.5:test
[INFO] |  \- org.scala-sbt:test-interface:jar:1.0:test
[INFO] +- org.apache.curator:curator-test:jar:2.4.0:test
[INFO] |  +- org.javassist:javassist:jar:3.15.0-GA:compile
[INFO] |  \- org.apache.commons:commons-math:jar:2.2:compile
[INFO] +- net.razorvine:pyrolite:jar:4.9:compile
[INFO] +- net.sf.py4j:py4j:jar:0.10.1:compile
[INFO] +- org.apache.spark:spark-tags_2.11:jar:2.0.0:compile
[INFO] +- org.apache.commons:commons-crypto:jar:1.0.0:compile
[INFO] +- org.spark-project.spark:unused:jar:1.0.0:compile
[INFO] +- org.scalatest:scalatest_2.11:jar:2.2.6:test
[INFO] |  \- org.scala-lang.modules:scala-xml_2.11:jar:1.0.2:compile
[INFO] +- junit:junit:jar:4.12:test
[INFO] \- com.novocode:junit-interface:jar:0.11:test
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2.475 s
[INFO] Finished at: 2016-09-22T06:34:12+08:00
[INFO] Final Memory: 23M/963M
[INFO] ------------------------------------------------------------------------
{code}, GitHub user winningsix opened a pull request:

    https://github.com/apache/hive/pull/103

    HIVE-14029: Update Spark version to 2.0.0

    Changes include:
    * Spark API updates:
    
    1. SparkShuffler#call return Iterator instead of Iterable
    2. SparkListener -> JavaSparkListener
    3. InputMetrics constructor doesn’t accept readMethod
    4. Method remoteBlocksFetched and localBlocksFetched in ShuffleReadMetrics return long type instead of integer
    
    * Dependency upgrade:
    
    1. Jackson: 2.4.2 -> 2.6.5
    2. Netty version: 4.0.23.Final -> 4.0.29.Final
    3. Scala binary version: 2.10 -> 2.11
    4. Scala version: 2.10.4 -> 2.11.8
    
    Test done by smoke tests in a cluster and integration test in Jenkins

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/winningsix/hive HIVE-14029

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hive/pull/103.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #103
    
----
commit 965e57295a83b06db61b22f3fda0bb19e47c248a
Author: Ferdinand Xu <cheng.a.xu@intel.com>
Date:   2016-09-17T19:10:04Z

    HIVE-14029: Update Spark version to 2.0.0

----
, Update patch addressing [~spena]'s comments., Hi [~Ferd], My thought was we should be able to tell the user what are actually needed (i.e. the minimum set of required jars) for HoS to work. Users can decide whether they want to add just the required jars, or all the jars under spark's dir for convenience. This is just something good to have and doesn't block this ticket - we used to add the whole assembly anyway.
Besides, I think not all the dependencies of spark-core are needed because some of them should be already in Hive's classpath, e.g. hadoop, commons, etc., When I tried the patch locally, I got a compile error:
{noformat}
org.apache.hive.hcatalog.templeton.mock.MockUriInfo is not abstract and does not override abstract method relativize(java.net.URI) in javax.ws.rs.core.UriInfo
{noformat}
Does anybody have the same issue?, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829777/HIVE-14029.4.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10555 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1269/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1269/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1269/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829777 - PreCommit-HIVE-Build, I have the same issue. You need to update it with following changes:
{code}
--- a/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java
+++ b/hcatalog/webhcat/svr/src/test/java/org/apache/hive/hcatalog/templeton/mock/MockUriInfo.java
@@ -64,6 +64,14 @@ public UriBuilder getBaseUriBuilder() {
     return null;
   }
 
+  public URI resolve(URI uri) {
+    return null;
+  }
+
+  public URI relativize(URI uri) {
+    return null;
+  }
+
   @Override
   public List<String> getMatchedURIs() {
     // TODO Auto-generated method stub

{code}

I am not quite sure why this happens which can be reproduced in Jenkins. Possibly related to JDK version., Sorry Fer, I meant 2.2 :P. I got confused with numbers., Seems we have two {{javax.ws.rs.core.UriInfo}} interfaces from two jars: javax.ws.rs-api and jersey-core. Before the patch, we only have one from jersey-core. Maybe there's some conflicts in the dependency upgrade. We need to fix it because it breaks build., +1 on identifying the minimum set., Which JDK you're using? Jenkins is using JDK8, I'm using:
{noformat}
java version "1.8.0_91"
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)
{noformat}, Hi [~spena], it's weird why Jenkins can build it successfully. Hi [~lirui] I exclude the {code}javax.ws.rs{code} imported by spark-core in 5th patch., Hi [~lirui], [~xuefuz], HIVE-14825 was created addressing this., The offending jar comes as a dependency of jersey-client:
{noformat}
[INFO] |  +- org.glassfish.jersey.core:jersey-client:jar:2.22.2:compile
[INFO] |  |  +- javax.ws.rs:javax.ws.rs-api:jar:2.0.1:compile
[INFO] |  |  +- org.glassfish.hk2:hk2-api:jar:2.4.0-b34:compile
[INFO] |  |  |  +- org.glassfish.hk2:hk2-utils:jar:2.4.0-b34:compile
[INFO] |  |  |  \- org.glassfish.hk2.external:aopalliance-repackaged:jar:2.4.0-b34:compile
[INFO] |  |  +- org.glassfish.hk2.external:javax.inject:jar:2.4.0-b34:compile
[INFO] |  |  \- org.glassfish.hk2:hk2-locator:jar:2.4.0-b34:compile
[INFO] |  |     \- org.javassist:javassist:jar:3.18.1-GA:compile
{noformat}
I think it's related to SPARK-12154. Spark updated to Jersey 2 and replaced com.sun.jersey with org.glassfish.jersey. Good news is seems we don't pack the jersey stuff in hive-exec. But not sure if this only affects the compile.
[~xuefuz] what do you think about this?, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12829975/HIVE-14029.5.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10559 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testDelegationTokenSharedStore
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1287/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1287/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1287/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12829975 - PreCommit-HIVE-Build, Hi [~lirui], Thanks for the investigation. I'm wondering if that dependency can be excluded in Hive's build? While the latest patch builds, it changes Hive's existing dependency, which might cause some problem.

Also, we are upgrading the following libraries. I'm not sure If it's absolutely necessary. From my build alone, it seems not. [~Ferd], any thoughts?
{quote}
** Jackson: 2.4.2 -> 2.6.5
** Netty version: 4.0.23.Final -> 4.0.29.Final
{quote}, Hi [~xuefuz] These two dependencies (Jackson and Netty) are not required in build. It's required for the runtime. If you try to run some HoS job, it will fail to create Spark client since API changes in these two library. You can see failed queries above for the reference.
, Let's see whether it breaks qtest after removing org.glassfish.jersey related dependencies from Spark_core, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12830171/HIVE-14029.6.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 16 failed/errored test(s), 10629 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testAddJarsAndFiles
org.apache.hive.spark.client.TestSparkClient.testCounters
org.apache.hive.spark.client.TestSparkClient.testErrorJob
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
org.apache.hive.spark.client.TestSparkClient.testMetricsCollection
org.apache.hive.spark.client.TestSparkClient.testRemoteClient
org.apache.hive.spark.client.TestSparkClient.testSimpleSparkJob
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1299/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1299/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1299/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 16 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12830171 - PreCommit-HIVE-Build,  [~xuefuz] [~lirui], Removing the dependency about org.glassfish.jersey will fail QTest with following error. I think we should use and commit the 5th patch instead.  Any thoughts about it? 

{code}
2016-09-23T23:48:02,527  INFO [Driver] util.log: Logging initialized @3685ms
Exception in thread "Driver" java.lang.NoClassDefFoundError: org/glassfish/jersey/servlet/ServletContainer
	at org.apache.spark.status.api.v1.ApiRootResource$.getServletHandler(ApiRootResource.scala:193)
	at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:75)
	at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:81)
	at org.apache.spark.ui.SparkUI$.create(SparkUI.scala:215)
	at org.apache.spark.ui.SparkUI$.createLiveUI(SparkUI.scala:157)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:443)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at org.apache.hive.spark.client.RemoteDriver.<init>(RemoteDriver.java:157)
	at org.apache.hive.spark.client.RemoteDriver.main(RemoteDriver.java:516)
	at org.apache.hive.spark.client.SparkClientImpl$2.run(SparkClientImpl.java:228)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.glassfish.jersey.servlet.ServletContainer
	at java.net.URLClassLoader$1.run(URLClassLoader.java:372)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 11 more
{code}

, [~Ferd], you mean unit test will fail right? I don't see failed spark qtest in last QA report. For qtest/runtime, we use spark-submit to submit the app, so spark should add all its dependencies to classpath. One problem I can think of is if we don't exclude the glassfish jersey, will hive pull two versions of jersey into its classpath, i.e. the lib dir? If so, that can cause problem for hive's functionalities that depend on jersey., Hi [~lirui]
 bq.  you mean unit test will fail right?
It's my fault, it's failing some unit tests. HMM, the failed cases are caused by lack of those jars. To fix them, include Glassfish related jars in *test only*. Attached is the new version addressing above., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12830239/HIVE-14029.7.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10629 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1300/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1300/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1300/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12830239 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12830248/HIVE-14029.7.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10629 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.rpc.TestRpc.testClientTimeout
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1301/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1301/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1301/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12830248 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12830260/HIVE-14029.7.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 10629 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUse
org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUse
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1303/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1303/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1303/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12830260 - PreCommit-HIVE-Build, 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12830442/HIVE-14029.8.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10640 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
org.apache.hive.spark.client.TestSparkClient.testJobSubmission
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1309/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1309/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1309/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12830442 - PreCommit-HIVE-Build, I tried the 5th patch locally. The jersey 2 stuff won't be pulled to lib, or the hive-exec jar. I also tried to identify the minimum set. I managed to run some simple queries (spark on yarn) with only {{scala-library, spark-core, spark-network-common, spark-network-shuffle}}. However, if we want to support local mode, we need more jars added to hive's lib, including the jersey 2. Then we may have conflict problem. Other than that, I think the 5th patch is enough for us (although I think we should exclude jersey 2 instead of just javax.ws.rs).

If we still want to go the way as the 6th, 7th patches, maybe we can look at how we handle the guava conflict in the pom of spark-client and do something similar., [~lirui], thank you for your investigation. Can you please update HIVE-14825 about the minimum jar set?
bq. If we still want to go the way as the 6th, 7th patches, maybe we can look at how we handle the guava conflict in the pom of spark-client and do something similar.

We can do it in a separate ticket about Guava conflict.  For the 7th patch, there's still one failed test case "org.apache.hive.spark.client.TestSparkClient.testJobSubmission" which I can't reproduce locally. Let's wait for another HIVE QA report to see whether it's reproducible., Yeah I'll update HIVE-14825 once I have identified the minimum set for different modes.
bq. We can do it in a separate ticket about Guava conflict.
I mean we used to have guava conflict (HIVE-7387), which is similar to this one: spark uses a newer version while hive/hadoop stick to the old one. At the end, spark shaded guava in the assembly to solve the issue (SPARK-2848). You can refer to the pom of spark-client about how to explicitly add the guava jars to run the unit tests., 

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12830453/HIVE-14029.8.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10640 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_mapjoin]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ctas]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_join_part_col_char]
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3]
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMetaDataCounts
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testAddJarConstructorUnCaching
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1311/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1311/console
Test logs: http://ec2-204-236-174-241.us-west-1.compute.amazonaws.com/logs/PreCommit-HIVE-Build-1311/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12830453 - PreCommit-HIVE-Build, The latest patch passed all tests. [~xuefuz] [~lirui], do you have any further comments? I'd like to commit it if you have no further comments about the latest patch., Thank you for providing this information. I will try to investigate it in Spark side. Considering Spark 2.0.0 is already released, if there is some work to do in Spark side, we may have to wait for next release., Committed to the master. [~aihuaxu] [~szehon] [~xuefuz] [~lirui] [~spena] [~stakiar] Thank you for the reviews., Github user winningsix closed the pull request at:

    https://github.com/apache/hive/pull/103
, Should this be documented in the wiki?

* [Hive on Spark: Getting Started | https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started], Thanks [~leftylev] for the remind. It should be updated in WIKI. Shall we add a new section for Hive on Spark 2.0?, Since different versions of Spark are not binary compatible, it'd be good if we can document the min/max supported Spark version for each release of Hive, e.g. the minimum supported Spark version is 2.0.0 for Hive 2.2.0. Should have done this in previous upgrades :(, Okay, I added a TODOC2.2 label., [~lirui] Oh, Sounds like a big compatibility change for Hive 2.x series. 
[~xuefuz] Do you know how we handle these breaking changes on Hive versions? , [~Ferd] can you also add an "Incompatible Change" Flag to this JIRA.

I'm guessing this should go into the Hive 2.2.0 release since its an incompatible change, and I agree we should document this all on the wiki. I don't know much about Spark 2, but will Hive-on-Spark2 be able to run against a Spark1 cluster, or vice versa?, bq. I'm guessing this should go into the Hive 2.2.0 release since its an incompatible change

Agree.

bq. will Hive-on-Spark2 be able to run against a Spark1 cluster, or vice versa?

AFAIK, it will not able to run against Spark1 cluster for the dependency conflicts. If we want to support different Spark cluster, we may need a shim loader for Spark in Spark client. Do we have a strong requirement for that?, We probably don't need a shim loader right now. I don't know of any requirements to have one, so we should be good for now. If users starting hitting upgrade issues then it may be something to consider in the future., [~Ferd] and [~lirui], yes we should add a section on Spark versions that are compatible with different Hive releases, and include as much information as possible.

* [Hive on Spark:  Getting Started | https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started]

I was about to add such a section at the beginning of the doc (before Spark Installation) but hesitated because I don't know what version(s) can be used with the installation instructions., [~xuefuz] [~Ferd] [~lirui] Back to compatibility discussion, I think we should continue keeping Spark 1.x compatibility on Hive 2.x series (as we did on Hive 1.x with Hadoop 1.x/2.x). If there are users using Spark 1.x, then they won't be able to upgrade to Hive 2.2, and they do not necessary need to upgrade to Spark 2.0 as it is still a new release, and not many people upgrade to a 2.0 version immediately.

What do you thing about this guys? is it important to keep compatibility on Hive 2.x until we release Hive 3.0 in the future?, [~spena], Keeping b/c is a good thing in general. Before we take the effort (which seems a lot) to do it, I think we should clearly understand and define what b/c is in this case. Spark is rapidly releasing w/o much b/c in mind. So far, Hive on Spark has once depended on Spark 1.2, 1.3, 1.4, 1.5, and 1.6. I'm not sure what versions of Spark Hive has been released with, but one thing is clear, Spark isn't b/c between these releases. Before Spark community has a good sense of keeping b/c in their APIs, it's going to be very hard and burdensome for Hive to maintain support for different Spark releases, not to mention the library dependency issues we have had.

I'm okay to start thinking of a shim layer to support multiple versions of Spark, but it sounds daunting to me due to the dynamics of Spark project., Interesting, so even between Spark 1.x versions, Hive wasn't compatible at all with them? This is going to be a lot of work as you said. If Spark 2.1 isn't compatible with Spark 2.0 for instance, then we will have a shim layer with minor changes per Spark version to keep compatibility.

[~xuefuz] Were there users in the community complaining about Spark 1.x incompatibilities with Hive in the past? , Spark claims API compatibility within a major release, but it doesn't seem so based on our experience. 
https://issues.apache.org/jira/browse/HIVE-9726
https://issues.apache.org/jira/browse/HIVE-10999
https://issues.apache.org/jira/browse/HIVE-11473
https://issues.apache.org/jira/browse/HIVE-12828
In two of the four upgrades, there are incompatibility API changes.

Spark is still a young project, so people may have lower expectation on this., Hmm even with a shim layer, it's difficult to support different Spark versions if b/c is not maintained between minor releases of Spark.
I'm wondering if the Spark used by Hive can be considered as some kind of embedded binaries that exclusively used for HoS. On Hive side, we just need to set spark.home pointing to this Spark. User's other Spark applications, e.g. SparkSQL, streaming, can still run against the current Spark they have in the cluster. Will this make it easier for the upgrade?
I think we also need to be more careful to upgrade Spark in the future, if the upgrade is breaking compatibility. For such upgrade, we need to firstly make sure there's no obvious regression in functionality and performance.]