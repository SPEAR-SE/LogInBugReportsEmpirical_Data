[Attaching query explain plan, This bug looks familiar - ref: HIVE-12463
, Hmm, the branch I'm on (derivative of master) has the fix for HIVE-12463, I ran this with the same logging in BytesColumnVector

{code}
+    if (sourceBuf.length < start + length || length < 0) {
+       throw new RuntimeException(String.format("[%d] %d > %d + %d", elementNum, sourceBuf.length, start, length));
+    }
{code}

{code}
Caused by: java.lang.RuntimeException: [205] 8388608 > 3935989 + -2
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:153)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow$StringReaderByValue.apply(VectorDeserializeRow.java:350)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeByValue(VectorDeserializeRow.java:690)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultSingleValue(VectorMapJoinGenerateResultOperator.java:183)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.finishInner(VectorMapJoinInnerGenerateResultOperator.java:180)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:373)
        ... 26 more
{code}, Assigned to me, while I investigate deeper., The issue is most likely an off-by-1 error somewhere in the hashtable reader.

I confirmed that we're not getting the null byte for the data correctly & is reading part of the 1st value as the isNull byte., Digging deeper, the bug disappears if this is turned into a primary key only join.

{code}
create temporary table cr stored as orc as select distinct * from (select *, row_number() over (partition by c_current_cdemo_sk) as r from tpcds_bin_partitioned_orc_10.customer) x where r = 1;

-- works because the hashtable has no >1 values for each key
select c_customer_sk, c_customer_id from cr, tpcds_bin_partitioned_orc_10.customer_demographics where c_current_cdemo_sk = cd_demo_sk limit 20; 

select c_customer_sk, c_customer_id from tpcds_bin_partitioned_orc_10.customer, tpcds_bin_partitioned_orc_10.customer_demographics where c_current_cdemo_sk = cd_demo_sk limit 20;
{code}, Minimal test-case for the bug.

When run with the native vectorized hashtable, it hands out two rows which are incorrect.

{code}
154500  AAAAAAAAEILFCAAA
6       NULL
NULL    �AAAAAAAAMNMFAAAA�=AAAAAAAANDHAGAAA��AAAAAAAANBDMEAAA[��AAAAAAAALLPOEAAA�[�
{code}]