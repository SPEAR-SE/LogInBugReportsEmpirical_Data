[Tentative fix. I'll have to rebase this once HIVE-9609 goes in., 

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12723802/HIVE-10250.1.patch

{color:red}ERROR:{color} -1 due to 15 failed/errored test(s), 8663 tests executed
*Failed tests:*
{noformat}
TestMinimrCliDriver-bucketmapjoin6.q-constprog_partitioner.q-infer_bucket_sort_dyn_part.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-external_table_with_space_in_location_path.q-infer_bucket_sort_merge.q-auto_sortmerge_join_16.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-groupby2.q-import_exported_table.q-bucketizedhiveinputformat.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-index_bitmap3.q-stats_counter_partitioned.q-temp_table_external.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-infer_bucket_sort_map_operators.q-join1.q-bucketmapjoin7.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-infer_bucket_sort_num_buckets.q-disable_merge_for_bucketing.q-uber_reduce.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-infer_bucket_sort_reducers_power_two.q-scriptfile1.q-scriptfile1_win.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-leftsemijoin_mr.q-load_hdfs_file_with_space_in_the_name.q-root_dir_external_table.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-list_bucket_dml_10.q-bucket_num_reducers.q-bucket6.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-load_fs2.q-file_with_header_footer.q-ql_rewrite_gbtoidx_cbo_1.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-parallel_orderby.q-reduce_deduplicate.q-ql_rewrite_gbtoidx_cbo_2.q-and-1-more - did not produce a TEST-*.xml file
TestMinimrCliDriver-ql_rewrite_gbtoidx.q-smb_mapjoin_8.q - did not produce a TEST-*.xml file
TestMinimrCliDriver-schemeAuthority2.q-bucket4.q-input16_cc.q-and-1-more - did not produce a TEST-*.xml file
org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.testListener
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3335/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/3335/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-3335/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 15 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12723802 - PreCommit-HIVE-TRUNK-Build, Funny, I can't reproduce the {{TestAuthorizationPreEventListener}} test faillure.
, I looked into the test failure, and here's what I see:

Using PartitionWrapper(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.api.Partition) expects the Table object to be initialized, and the Partition.getSd() to not be null.
As of the time the preEventListener is fired, the org.apache.hadoop.hive.metastore.api.Partition has not yet had its sd initialized. So, we might want to add a bit to the PartitionWrapper(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.api.Partition) ctor so that it's similar to the PartitionWrapper(org.apache.hadoop.hive.metastore.api.Partition,  PreEventContext) ctor, and then we can use a TableWrapper instead. Note that if the TableWrapper itself has a null sd, we have a potential failure condition - fetching using  context.getHandler().get_table_core is the right call instead of using a TableWrapper then. It might even be worth fetching a Table once and caching it, instead of creating a TableWrapper and caching it.

, Per discussion with Mithun, this patch needs additional work, and has been backed out of being required for branch-1.2. It'll be fixed in trunk when it's fixed, and be part of the next release.]