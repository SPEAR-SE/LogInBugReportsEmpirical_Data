[+1, looks good. will commit after tests pass., I just committed. Thanks  Ning!, Uploading a patch for branch-0.6., "the largest file is the correct file" 
Is that generally true or an absolute fact?, The assumption of Map-reduce is 
if we give same input and same m/r function, the output should be always the same.

Otherwise the map-reduce fault tolerance mechanism is wrong., running test on branch-0.6, @Edward, this is a heuristics that should be generally true. The good news is that we are not aware of any exceptions that violate the rule (assuming multiple attempts of the same task give deterministic results). 

The reason that we are relying on heuristics here is that the old Hadoop API doesn't not support exception handling outside Mapper's map() function. The bug presents if an exception was thrown by Hadoop's RecordReader layer and it does not pass the message to the Mapper. When the mapper.close() is called there is not way the mapper know whether there is an exception happened in the Hadoop code path. A better way to handle this is to use the new Hadoop API that gives more control to the application layer. This heuristics is a workaround based on the old Hadoop API. 
, committed to branch-0.6 as well. Thanks John!, A better fix would be to catch next() in HiveRecordReader/CombineHiveRecordReader etc. and set the abort flag in ExecMapper in case of an exception.
There will be exactly 1 successful mapper in that case., Agree that we should catch the exception in (Combine)HiveRecordReader, but they are only used in map side. In the reducer, RecordReader was not called and there could also be exceptions outside of reducer(). This fix catches that case as well.

I've filed another JIRA HIVE-1543 for catching exceptions in RecrodReaders. 

, Let us fix it in the follow-up ]