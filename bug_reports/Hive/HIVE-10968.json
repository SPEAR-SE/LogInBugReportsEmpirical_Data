{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hsubramaniyan&avatarId=23453",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hsubramaniyan&avatarId=23453",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hsubramaniyan&avatarId=23453",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=hsubramaniyan&avatarId=23453"
            },
            "displayName": "Hari Sankar Sivarama Subramaniyan",
            "key": "hsubramaniyan",
            "name": "hsubramaniyan",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=hsubramaniyan",
            "timeZone": "America/Los_Angeles"
        },
        "components": [{
            "description": "Tracks issues related to HiveServer2",
            "id": "12320408",
            "name": "HiveServer2",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12320408"
        }],
        "created": "2015-06-08T22:57:04.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10438",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10438",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10438",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10438"
            },
            "displayName": "Takahiko Saito",
            "key": "taksaito",
            "name": "taksaito",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=taksaito",
            "timeZone": "America/Los_Angeles"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2015-06-08T23:02:33.803+0000",
        "customfield_12310222": "10002_*:*_2_*:*_268812120_*|*_1_*:*_1_*:*_335439_*|*_5_*:*_2_*:*_174048901_*|*_4_*:*_1_*:*_657392",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "2.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i2fshb:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Sun Jun 14 02:14:38 UTC 2015",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "NO PRECOMMIT TESTS\n\nRun the following via beeline:\n{noformat}0: jdbc:hive2://localhost:10001> analyze table all100kjson compute statistics;\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO parse.ParseDriver: Parsing command: analyze table all100kjson compute statistics\n15/06/05 20:44:11 INFO parse.ParseDriver: Parse Completed\n15/06/05 20:44:11 INFO log.PerfLogger: <\/PERFLOG method=parse start=1433537051075 end=1433537051077 duration=2 from=org.\napache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Invoking analyze on original query\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Starting Semantic Analysis\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Completed phase 1 of Semantic Analysis\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Get metadata for source tables\n15/06/05 20:44:11 INFO metastore.HiveMetaStore: 5: get_table : db=default tbl=all100kjson\n15/06/05 20:44:11 INFO HiveMetaStore.audit: ugi=hadoopqa        ip=unknown-ip-addr      cmd=get_table : db=default tbl=a\nll100kjson\n15/06/05 20:44:11 INFO metastore.HiveMetaStore: 5: get_table : db=default tbl=all100kjson\n15/06/05 20:44:11 INFO HiveMetaStore.audit: ugi=hadoopqa        ip=unknown-ip-addr      cmd=get_table : db=default tbl=a\nll100kjson\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Get metadata for subqueries\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Get metadata for destination tables\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Completed getting MetaData in Semantic Analysis\n15/06/05 20:44:11 INFO common.FileUtils: Creating directory if it doesn't exist: hdfs://dal-hs211:8020/user/hcat/tests/d\nata/all100kjson/.hive-staging_hive_2015-06-05_20-44-11_075_4520028480897676073-5\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Set stats collection dir : hdfs://dal-hs211:8020/user/hcat/tes\nts/data/all100kjson/.hive-staging_hive_2015-06-05_20-44-11_075_4520028480897676073-5/-ext-10000\n15/06/05 20:44:11 INFO ppd.OpProcFactory: Processing for TS(5)\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr\n.PartitionPruner>\n15/06/05 20:44:11 INFO log.PerfLogger: <\/PERFLOG method=partition-retrieving start=1433537051345 end=1433537051345 durat\nion=0 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>\n15/06/05 20:44:11 INFO metastore.HiveMetaStore: 5: get_indexes : db=default tbl=all100kjson\n15/06/05 20:44:11 INFO HiveMetaStore.audit: ugi=hadoopqa        ip=unknown-ip-addr      cmd=get_indexes : db=default tbl\n=all100kjson\n15/06/05 20:44:11 INFO metastore.HiveMetaStore: 5: get_indexes : db=default tbl=all100kjson\n15/06/05 20:44:11 INFO HiveMetaStore.audit: ugi=hadoopqa        ip=unknown-ip-addr      cmd=get_indexes : db=default tbl\n=all100kjson\n15/06/05 20:44:11 INFO physical.NullScanTaskDispatcher: Looking for table scans where optimization is applicable\n15/06/05 20:44:11 INFO physical.NullScanTaskDispatcher: Found 0 null table scans\n15/06/05 20:44:11 INFO physical.NullScanTaskDispatcher: Looking for table scans where optimization is applicable\n15/06/05 20:44:11 INFO physical.NullScanTaskDispatcher: Found 0 null table scans\n15/06/05 20:44:11 INFO physical.NullScanTaskDispatcher: Looking for table scans where optimization is applicable\n15/06/05 20:44:11 INFO physical.NullScanTaskDispatcher: Found 0 null table scans\n15/06/05 20:44:11 INFO physical.Vectorizer: Validating MapWork...\n15/06/05 20:44:11 INFO physical.Vectorizer: Input format: org.apache.hadoop.mapred.TextInputFormat, doesn't provide vect\norized input\n15/06/05 20:44:11 INFO parse.ColumnStatsSemanticAnalyzer: Completed plan generation\n15/06/05 20:44:11 INFO ql.Driver: Semantic Analysis Completed\n15/06/05 20:44:11 INFO log.PerfLogger: <\/PERFLOG method=semanticAnalyze start=1433537051077 end=1433537051367 duration=2\n90 from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:all100kjson.s, type:strin\ng, comment:null), FieldSchema(name:all100kjson.i, type:int, comment:null), FieldSchema(name:all100kjson.d, type:double,\ncomment:null), FieldSchema(name:all100kjson.m, type:map<string,string>, comment:null), FieldSchema(name:all100kjson.bb,\ntype:array<struct<a:int,b:string>>, comment:null), FieldSchema(name:all100kjson.t, type:timestamp, comment:null)], prope\nrties:null)\n15/06/05 20:44:11 INFO log.PerfLogger: <\/PERFLOG method=compile start=1433537051074 end=1433537051367 duration=293 from=\norg.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO ql.Driver: Starting command(queryId=hadoop_20150605204411_2c72579d-2080-410e-981c-eaa00a0fb8f2):\nanalyze table all100kjson compute statistics\n15/06/05 20:44:11 INFO hooks.ATSHook: Created ATS Hook\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.h\nadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO log.PerfLogger: <\/PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1433537051\n371 end=1433537051372 duration=1 from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO ql.Driver: Query ID = hadoop_20150605204411_2c72579d-2080-410e-981c-eaa00a0fb8f2\n15/06/05 20:44:11 INFO ql.Driver: Total jobs = 1\n15/06/05 20:44:11 INFO log.PerfLogger: <\/PERFLOG method=TimeToSubmit start=1433537051370 end=1433537051372 duration=2 fr\nom=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=task.MAPRED.Stage-0 from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:11 INFO ql.Driver: Launching Job 1 out of 1\n15/06/05 20:44:11 INFO ql.Driver: Starting task [Stage-0:MAPRED] in serial mode\n15/06/05 20:44:11 INFO exec.Task: Number of reduce tasks is set to 0 since there's no reduce operator\n15/06/05 20:44:11 INFO ql.Context: New scratch dir is hdfs://dal-hs211:8020/tmp/hive/hadoopqa/d87d0c8a-3d02-42f5-b856-b3\ne82fddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7\n15/06/05 20:44:11 INFO mr.ExecDriver: Using org.apache.hadoop.hive.ql.io.CombineHiveInputFormat\n15/06/05 20:44:11 INFO exec.Utilities: Processing alias all100kjson\n15/06/05 20:44:11 INFO exec.Utilities: Adding input file hdfs://dal-hs211:8020/user/hcat/tests/data/all100kjson\n15/06/05 20:44:11 INFO exec.Utilities: Content Summary not cached for hdfs://dal-hs211:8020/user/hcat/tests/data/all100k\njson\n15/06/05 20:44:11 INFO ql.Context: New scratch dir is hdfs://dal-hs211:8020/tmp/hive/hadoopqa/d87d0c8a-3d02-42f5-b856-b3\ne82fddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7\n15/06/05 20:44:11 INFO log.PerfLogger: <PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>\n15/06/05 20:44:11 INFO exec.Utilities: Serializing MapWork via kryo\n15/06/05 20:44:11 INFO log.PerfLogger: <\/PERFLOG method=serializePlan start=1433537051497 end=1433537051573 duration=76\nfrom=org.apache.hadoop.hive.ql.exec.Utilities>\n15/06/05 20:44:11 ERROR mr.ExecDriver: yarn\n15/06/05 20:44:11 INFO impl.TimelineClientImpl: Timeline service address: http://dal-hs211:8188/ws/v1/timeline/\n15/06/05 20:44:11 INFO client.RMProxy: Connecting to ResourceManager at dal-hs211/100.76.114.34:8032\n15/06/05 20:44:11 INFO fs.FSStatsPublisher: created : hdfs://dal-hs211:8020/user/hcat/tests/data/all100kjson/.hive-stagi\nng_hive_2015-06-05_20-44-11_075_4520028480897676073-5/-ext-10000\n15/06/05 20:44:11 INFO impl.TimelineClientImpl: Timeline service address: http://dal-hs211:8188/ws/v1/timeline/\n15/06/05 20:44:11 INFO client.RMProxy: Connecting to ResourceManager at dal-hs211/100.76.114.34:8032\n15/06/05 20:44:11 INFO exec.Utilities: PLAN PATH = hdfs://dal-hs211:8020/tmp/hive/hadoopqa/d87d0c8a-3d02-42f5-b856-b3e82\nfddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7/-mr-10002/9ae2fc99-11a6-4257-8631-326f1bf007e2/map.xml\n15/06/05 20:44:11 INFO exec.Utilities: PLAN PATH = hdfs://dal-hs211:8020/tmp/hive/hadoopqa/d87d0c8a-3d02-42f5-b856-b3e82\nfddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7/-mr-10002/9ae2fc99-11a6-4257-8631-326f1bf007e2/reduce.xml\n15/06/05 20:44:11 INFO exec.Utilities: ***************non-local mode***************\n15/06/05 20:44:11 INFO exec.Utilities: local path = hdfs://dal-hs211:8020/tmp/hive/hadoopqa/d87d0c8a-3d02-42f5-b856-b3e8\n2fddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7/-mr-10002/9ae2fc99-11a6-4257-8631-326f1bf007e2/reduce.xml\n15/06/05 20:44:11 INFO exec.Utilities: Open file to read in plan: hdfs://dal-hs211:8020/tmp/hive/hadoopqa/d87d0c8a-3d02-\n42f5-b856-b3e82fddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7/-mr-10002/9ae2fc99-11a6-4257-8631-326f1bf007e2\n/reduce.xml\n15/06/05 20:44:11 INFO exec.Utilities: File not found: File does not exist: /tmp/hive/hadoopqa/d87d0c8a-3d02-42f5-b856-b\n3e82fddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7/-mr-10002/9ae2fc99-11a6-4257-8631-326f1bf007e2/reduce.xml\n\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1820)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1791)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1704)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:586)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNameno\ndeProtocolServerSideTranslatorPB.java:365)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMetho\nd(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)\n        at java.security.AccessController.doPrivileged(Native Method)\nGetting log thread is interrupted, since query is done!\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)\n\n15/06/05 20:44:11 INFO exec.Utilities: No plan file found: hdfs://dal-hs211:8020/tmp/hive/hadoopqa/d87d0c8a-3d02-42f5-b8\n56-b3e82fddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7/-mr-10002/9ae2fc99-11a6-4257-8631-326f1bf007e2/reduce\n.xml\n15/06/05 20:44:11 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the To\nol interface and execute your application with ToolRunner to remedy this.\n15/06/05 20:44:12 INFO log.PerfLogger: <PERFLOG method=getSplits from=org.apache.hadoop.hive.ql.io.CombineHiveInputForma\nt>\n15/06/05 20:44:12 INFO exec.Utilities: PLAN PATH = hdfs://dal-hs211:8020/tmp/hive/hadoopqa/d87d0c8a-3d02-42f5-b856-b3e82\nfddd099/hive_2015-06-05_20-44-11_075_4520028480897676073-7/-mr-10002/9ae2fc99-11a6-4257-8631-326f1bf007e2/map.xml\n15/06/05 20:44:12 INFO io.CombineHiveInputFormat: Total number of paths: 1, launching 1 threads to check non-combinable\nones.\n15/06/05 20:44:12 INFO io.CombineHiveInputFormat: CombineHiveInputSplit creating pool for hdfs://dal-hs211:8020/user/hca\nt/tests/data/all100kjson; using filter path hdfs://dal-hs211:8020/user/hcat/tests/data/all100kjson\n15/06/05 20:44:12 INFO input.FileInputFormat: Total input paths to process : 1\n15/06/05 20:44:12 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 3, size le\nft: 0\n15/06/05 20:44:12 INFO io.CombineHiveInputFormat: number of splits 1\n15/06/05 20:44:12 INFO io.CombineHiveInputFormat: Number of all splits 1\n15/06/05 20:44:12 INFO log.PerfLogger: <\/PERFLOG method=getSplits start=1433537052313 end=1433537052326 duration=13 from\n=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>\n15/06/05 20:44:12 INFO mapreduce.JobSubmitter: number of splits:1\n15/06/05 20:44:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1433486452546_0004\n15/06/05 20:44:13 INFO impl.YarnClientImpl: Submitted application application_1433486452546_0004\n15/06/05 20:44:13 INFO mapreduce.Job: The url to track the job: http://dal-hs211:8088/proxy/application_1433486452546_00\n04/\n15/06/05 20:44:13 INFO exec.Task: Starting Job = job_1433486452546_0004, Tracking URL = http://dal-hs211:8088/proxy/appl\nication_1433486452546_0004/\n15/06/05 20:44:13 INFO exec.Task: Kill Command = D:\\hdp\\\\hadoop-2.7.1.2.3.0.0-2245\\bin\\hadoop.cmd job  -kill job_1433486\n452546_0004\n15/06/05 20:44:20 INFO exec.Task: Hadoop job information for Stage-0: number of mappers: 1; number of reducers: 0\n15/06/05 20:44:20 WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.had\noop.mapreduce.TaskCounter instead\n15/06/05 20:44:20 INFO exec.Task: 2015-06-05 20:44:20,542 Stage-0 map = 0%,  reduce = 0%\n15/06/05 20:44:45 WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.had\noop.mapreduce.TaskCounter instead\n15/06/05 20:44:45 INFO exec.Task: 2015-06-05 20:44:45,669 Stage-0 map = 100%,  reduce = 0%\n15/06/05 20:44:47 WARN mapreduce.Counters: Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.had\noop.mapreduce.TaskCounter instead\n15/06/05 20:44:47 ERROR exec.Task: Ended Job = job_1433486452546_0004 with errors\n15/06/05 20:44:47 INFO impl.YarnClientImpl: Killed application application_1433486452546_0004\n15/06/05 20:44:47 INFO hooks.ATSHook: Created ATS Hook\n15/06/05 20:44:47 INFO log.PerfLogger: <PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apac\nhe.hadoop.hive.ql.Driver>\n15/06/05 20:44:47 INFO log.PerfLogger: <\/PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=143353\n7087863 end=1433537087864 duration=1 from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:47 ERROR ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedT\nask\n15/06/05 20:44:47 INFO log.PerfLogger: <\/PERFLOG method=Driver.execute start=1433537051370 end=1433537087865 duration=36\n495 from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:47 INFO ql.Driver: MapReduce Jobs Launched:\n15/06/05 20:44:47 WARN mapreduce.Counters: Group FileSystemCounters is deprecated. Use org.apache.hadoop.mapreduce.FileS\nystemCounter instead\n15/06/05 20:44:47 INFO ql.Driver: Stage-Stage-0: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL\n15/06/05 20:44:47 INFO ql.Driver: Total MapReduce CPU Time Spent: 0 msec\n15/06/05 20:44:47 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:47 INFO log.PerfLogger: <\/PERFLOG method=releaseLocks start=1433537087866 end=1433537087866 duration=0 fr\nom=org.apache.hadoop.hive.ql.Driver>\n15/06/05 20:44:47 ERROR operation.Operation: Error running hive query:\norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 f\nrom org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.M\napRedTask (state=08S01,code=2)\njava.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.h\nive.ql.exec.mr.MapRedTask\n        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)\n        at org.apache.hive.beeline.Commands.execute(Commands.java:848)\n        at org.apache.hive.beeline.Commands.sql(Commands.java:713)\n        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)\n        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)\n        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)\n        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)\n        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)\n{noformat}",
        "duedate": null,
        "environment": "Windows",
        "fixVersions": [
            {
                "archived": false,
                "description": "",
                "id": "12332154",
                "name": "1.3.0",
                "released": false,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12332154"
            },
            {
                "archived": false,
                "id": "12332384",
                "name": "1.2.1",
                "releaseDate": "2015-06-26",
                "released": true,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12332384"
            },
            {
                "archived": false,
                "description": "Hive 2.0.0",
                "id": "12332641",
                "name": "2.0.0",
                "releaseDate": "2016-02-15",
                "released": true,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12332641"
            }
        ],
        "issuelinks": [{
            "id": "12527905",
            "inwardIssue": {
                "fields": {
                    "issuetype": {
                        "avatarId": 21141,
                        "description": "A new feature of the product, which has yet to be developed.",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                        "id": "2",
                        "name": "New Feature",
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                        "subtask": false
                    },
                    "priority": {
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                        "id": "3",
                        "name": "Major",
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
                    },
                    "status": {
                        "description": "The issue is open and ready for the assignee to start work on it.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                        "id": "1",
                        "name": "Open",
                        "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                        "statusCategory": {
                            "colorName": "blue-gray",
                            "id": 2,
                            "key": "new",
                            "name": "To Do",
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2"
                        }
                    },
                    "summary": "Make JSON SerDe First-Class SerDe"
                },
                "id": "13140541",
                "key": "HIVE-18785",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/13140541"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12527905",
            "type": {
                "id": "12310051",
                "inward": "is superceded by",
                "name": "Supercedes",
                "outward": "supercedes",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310051"
            }
        }],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935"
            },
            "id": "12310843",
            "key": "HIVE",
            "name": "Hive",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310843"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10438",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10438",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10438",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10438"
            },
            "displayName": "Takahiko Saito",
            "key": "taksaito",
            "name": "taksaito",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=taksaito",
            "timeZone": "America/Los_Angeles"
        },
        "resolution": {
            "description": "A fix for this issue is checked into the tree and tested.",
            "id": "1",
            "name": "Fixed",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1"
        },
        "resolutiondate": "2015-06-14T02:14:38.000+0000",
        "status": {
            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
            "id": "6",
            "name": "Closed",
            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "Windows: analyze json table via beeline failed throwing Class org.apache.hive.hcatalog.data.JsonSerDe not found",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2018-02-23T16:27:46.000+0000",
        "versions": [],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HIVE-10968/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HIVE-10968/watchers",
            "watchCount": 4
        },
        "workratio": -1
    },
    "id": "12836307",
    "key": "HIVE-10968",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12836307"
}