{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "description": "Security Subcomponent",
            "id": "12317300",
            "name": "Authorization",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12317300"
        }],
        "created": "2016-05-23T13:06:52.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Alexandre Linte",
            "key": "bigdataorange",
            "name": "BigDataOrange",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=BigDataOrange",
            "timeZone": "Europe/Paris"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": null,
        "customfield_12310222": null,
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i2yd87:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Jun 02 16:18:43 UTC 2016",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "Hi,\n\nAs the owner of an Hive database I can modify the Hive database metadata whereas I only has the read and execute permission on the Hive database repository.\nI was expected to not be able to modify these metadata.\n\nContext:\n- Hive database configured with the Storage Based Authorization strategy.\n- Hive client authorization is disabled.\n- Metastore side security is activated.\n\nPermission configuration:\n{noformat}\ndr-x--x---   - hive9990    hive9990             0 2016-05-20 17:10 /path/to/hive/warehouse/p09990.db\n{noformat}\n\nALTER command as hive9990 user:\n{noformat}\nhive (p09990)>  ALTER DATABASE p09990 SET DBPROPERTIES ('comment'='database altered');\nOK\nTime taken: 0.277 seconds\nhive (p09990)> DESCRIBE DATABASE EXTENDED p09990;\nOK\np09990          hdfs://path/to/hive/warehouse/p09990.db        hdfs    USER    {comment=database altered}\n{noformat}\n\nConfiguration of hive-site.xml on the metastore:\n{noformat}\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n \n  <property>\n      <name>hive.security.authorization.enabled<\/name>\n      <value>false<\/value>\n      <description>enable or disable the Hive client authorization<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.security.metastore.authorization.manager<\/name>\n      <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider<\/value>\n      <description>authorization manager class name to be used in the metastore for authorization.\n      The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.metastore.pre.event.listeners<\/name>\n      <value>org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener<\/value>\n      <description>This turns on metastore-side security.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.security.metastore.authorization.auth.reads<\/name>\n      <value>true<\/value>\n      <description>If this is true, the metastore authorizer authorizes read actions on database and table.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.security.authorization.manager<\/name>\n      <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider<\/value>\n      <description>The Hive client authorization manager class name.\n  The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.security.authorization.createtable.owner.grants<\/name>\n      <value>ALL<\/value>\n      <description>the privileges automatically granted to the owner whenever a table gets created. \n       An example like \"select,drop\" will grant select and drop privilege to the owner of the table<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.users.in.admin.role<\/name>\n      <value>hdfs<\/value>\n      <description>Comma separated list of users who are in admin role for bootstrapping.\n    More users can be added in ADMIN role later.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.metastore.warehouse.dir<\/name>\n      <value>/path/to/hive/warehouse/<\/value>\n      <description>location of default database for the warehouse<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.cli.print.current.db<\/name>\n      <value>true<\/value>\n      <description>Whether to include the current database in the Hive prompt.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.metastore.uris<\/name>\n      <value>thrift://hiveserver2http01:9083<\/value>\n      <description>Thrift uri for the remote metastore. Used by metastore client to connect to remote metastore.<\/description>\n  <\/property>\n\n  <property>\n      <name>javax.jdo.option.ConnectionDriverName<\/name>\n      <value>com.mysql.jdbc.Driver<\/value>\n      <description>JDBC Driver<\/description>\n  <\/property>\n\n  <property>\n      <name>javax.jdo.option.ConnectionURL<\/name>\n      <value>jdbc:mysql://hivedb01/metastore<\/value>\n      <description>JDBC connect string for a JDBC metastore<\/description>\n  <\/property>\n\n  <property>\n      <name>javax.jdo.option.ConnectionUserName<\/name>\n      <value>metastore<\/value>\n      <description>username to use against metastore database<\/description>\n  <\/property>\n\n  <property>\n      <name>javax.jdo.option.ConnectionPassword<\/name>\n      <value>********<\/value>\n      <description>password to use against metastore database<\/description>\n  <\/property>\n\n  <property>\n      <name>datanucleus.autoCreateSchema<\/name>\n      <value>false<\/value>\n      <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.metastore.authorization.storage.checks<\/name>\n      <value>true<\/value>\n      <description>Should the metastore do authorization checks against the underlying storage\n  for operations like drop-partition (disallow the drop-partition if the user in\n  question doesn't have permissions to delete the corresponding directory\n  on the storage).<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.metastore.sasl.enabled<\/name>\n      <value>true<\/value>\n      <description>If true, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.metastore.kerberos.keytab.file<\/name>\n      <value>/path/to/metastore.keytab<\/value>\n      <description>The path to the Kerberos Keytab file containing the metastore thrift server's service principal.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.metastore.kerberos.principal<\/name>\n      <value>primary/instance@realm<\/value>\n      <description>The service principal for the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.max.start.attempts<\/name>\n      <value>30<\/value>\n      <description>This number of times HiveServer2 will attempt to start before exiting, sleeping 60 seconds between retries. The default of 30 will keep trying for 30 minutes.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.transport.mode<\/name>\n      <value>binary<\/value>\n      <description>Server transport mode. \"binary\" or \"http\".<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.http.port<\/name>\n      <value>10001<\/value>\n      <description>Port number when in HTTP mode.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.http.path<\/name>\n      <value>bdcorp<\/value>\n      <description>Path component of URL endpoint when in HTTP mode.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.use.SSL<\/name>\n      <value>false<\/value>\n      <description>Set this to true for using SSL encryption in HiveServer2<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.keystore.path<\/name>\n      <value><\/value>\n      <description>SSL certificate keystore location<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.keystore.password<\/name>\n      <value><\/value>\n      <description>SSL certificate keystore password.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.authentication.pam.services<\/name>\n      <value><\/value>\n      <description>List of the underlying pam services that should be used when auth type is PAM.\n  A file with the same name must exist in /etc/pam.d<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.min.worker.threads<\/name>\n      <value>5<\/value>\n      <description>Minimum number of Thrift worker threads<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.max.worker.threads<\/name>\n      <value>500<\/value>\n      <description>Maximum number of Thrift worker threads<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.worker.keepalive.time<\/name>\n      <value>60<\/value>\n      <description>Keepalive time (in seconds) for an idle worker thread. \n    When number of workers > min workers, excess threads are killed after this time interval.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.http.cookie.auth.enabled<\/name>\n      <value>true<\/value>\n      <description>When true, HiveServer2 in HTTP transport mode will use cookie based authentication mechanism.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.http.cookie.max.age<\/name>\n      <value>86400s<\/value>\n      <description>Maximum age in seconds for server side cookie used by HiveServer2 in HTTP mode.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.http.cookie.path<\/name>\n      <value><\/value>\n      <description>Path for the HiveServer2 generated cookies.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.http.cookie.domain<\/name>\n      <value><\/value>\n      <description>Domain for the HiveServer2 generated cookies.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.http.cookie.is.secure<\/name>\n      <value>true<\/value>\n      <description>Secure attribute of the HiveServer2 generated cookie.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.http.cookie.is.httponly<\/name>\n      <value>true<\/value>\n      <description>HttpOnly attribute of the HiveServer2 generated cookie.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.async.exec.threads<\/name>\n      <value>100<\/value>\n      <description>Number of threads in the async thread pool for HiveServer2<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.async.exec.shutdown.timeout<\/name>\n      <value>10<\/value>\n      <description>Time (in seconds) for which HiveServer2 shutdown will wait for async\n  threads to terminate<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.async.exec.keepalive.time<\/name>\n      <value>10<\/value>\n      <description>Time (in seconds) that an idle HiveServer2 async thread (from the thread pool) will wait\n  for a new task to arrive before terminating<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.long.polling.timeout<\/name>\n      <value>5000<\/value>\n      <description>Time in milliseconds that HiveServer2 will wait, before responding to asynchronous calls that use long polling<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.async.exec.wait.queue.size<\/name>\n      <value>100<\/value>\n      <description>Size of the wait queue for async thread pool in HiveServer2.\n  After hitting this limit, the async thread pool will reject new requests.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.port<\/name>\n      <value>10000<\/value>\n      <description>Port number of HiveServer2 Thrift interface.\n  Can be overridden by setting $HIVE_SERVER2_THRIFT_PORT<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.bind.host<\/name>\n      <value>hiveserver2http01<\/value>\n      <description>Bind host on which to run the HiveServer2 Thrift interface.\n  Can be overridden by setting $HIVE_SERVER2_THRIFT_BIND_HOST<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.authentication<\/name>\n      <value>KERBEROS<\/value>\n      <description>\n    Client authentication types.\n       NONE: no authentication check\n       LDAP: LDAP/AD based authentication\n       KERBEROS: Kerberos/GSSAPI authentication\n       CUSTOM: Custom authentication provider\n               (Use with property hive.server2.custom.authentication.class)\n       PAM: Pluggable authentication module.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.custom.authentication.class<\/name>\n      <value><\/value>\n      <description>\n    Custom authentication class. Used when property\n    'hive.server2.authentication' is set to 'CUSTOM'. Provided class\n    must be a proper implementation of the interface\n    org.apache.hive.service.auth.PasswdAuthenticationProvider. HiveServer2\n    will call its Authenticate(user, passed) method to authenticate requests.\n    The implementation may optionally extend Hadoop's\n    org.apache.hadoop.conf.Configured class to grab Hive's Configuration object.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.authentication.kerberos.principal<\/name>\n      <value>primary/instance@realm<\/value>\n      <description>\n    Kerberos server principal\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.authentication.kerberos.keytab<\/name>\n      <value>/path/to/hiveserver2.keytab<\/value>\n      <description>\n    Kerberos keytab file for server principal\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.authentication.spnego.principal<\/name>\n      <value>primary/instance@realm<\/value>\n      <description>\n    SPNego service principal, optional,\n    typical value would look like HTTP/_HOST@EXAMPLE.COM\n    SPNego service principal would be used by hiveserver2 when kerberos security is enabled\n    and HTTP transport mode is used.\n    This needs to be set only if SPNEGO is to be used in authentication.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.authentication.spnego.keytab<\/name>\n      <value>/path/to/spnego.keytab<\/value>\n      <description>\n    keytab file for SPNego principal, optional,\n    typical value would look like /etc/security/keytabs/spnego.service.keytab,\n    This keytab would be used by hiveserver2 when kerberos security is enabled\n    and HTTP transport mode is used.\n    This needs to be set only if SPNEGO is to be used in authentication.\n    SPNego authentication would be honored only if valid\n    hive.server2.authentication.spnego.principal\n    and\n    hive.server2.authentication.spnego.keytab\n    are specified\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.authentication.ldap.url<\/name>\n      <value>setindatabag<\/value>\n      <description>\n    LDAP connection URL\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.authentication.ldap.baseDN<\/name>\n      <value>setindatabag<\/value>\n      <description>\n    LDAP base DN\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.enable.doAs<\/name>\n      <value>true<\/value>\n      <description>\n   Setting this property to true will have HiveServer2 execute\n    Hive operations as the user making the calls to it.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.execution.engine<\/name>\n      <value>mr<\/value>\n      <description>\n    Chooses execution engine. Options are: mr (Map reduce, default) or tez (hadoop 2 only)\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.mapjoin.optimized.hashtable<\/name>\n      <value>true<\/value>\n      <description>Whether Hive should use a memory-optimized hash table for MapJoin. \n    Only works on Tez, because memory-optimized hash table cannot be serialized.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.mapjoin.optimized.hashtable.wbsize<\/name>\n      <value>10485760<\/value>\n      <description>Optimized hashtable (see hive.mapjoin.optimized.hashtable) uses a chain of buffers to store data. \n    This is one buffer size. Hashtable may be slightly faster if this is larger, \n    but for small joins unnecessary memory will be allocated and then trimmed.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.prewarm.enabled<\/name>\n      <value>false<\/value>\n      <description>\n    Enables container prewarm for tez (hadoop 2 only)\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.prewarm.numcontainers<\/name>\n      <value>10<\/value>\n      <description>\n    Controls the number of containers to prewarm for tez (hadoop 2 only)\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.table.type.mapping<\/name>\n      <value>CLASSIC<\/value>\n      <description>\n   This setting reflects how HiveServer2 will report the table types for JDBC and other\n   client implementations that retrieve the available tables and supported table types\n     HIVE : Exposes Hive's native table types like MANAGED_TABLE, EXTERNAL_TABLE, VIRTUAL_VIEW\n     CLASSIC : More generic types like TABLE and VIEW\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.thrift.sasl.qop<\/name>\n      <value>auth<\/value>\n      <description>Sasl QOP value; Set it to one of following values to enable higher levels of\n     protection for HiveServer2 communication with clients.\n      \"auth\" - authentication only (default)\n      \"auth-int\" - authentication plus integrity protection\n      \"auth-conf\" - authentication plus integrity and confidentiality protection\n     This is applicable only if HiveServer2 is configured to use Kerberos authentication.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.tez.container.size<\/name>\n      <value>-1<\/value>\n      <description>By default tez will spawn containers of the size of a mapper. This can be used to overwrite.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.tez.java.opts<\/name>\n      <value><\/value>\n      <description>By default tez will use the java opts from map tasks. This can be used to overwrite.<\/description>\n  <\/property>\n\n  <property>\n      <name>hive.tez.log.level<\/name>\n      <value>INFO<\/value>\n      <description>\n    The log level to use for tasks executing as part of the DAG.\n    Used only if hive.tez.java.opts is used to configure java opts.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.tez.smb.number.waves<\/name>\n      <value>1<\/value>\n      <description>The number of waves in which to run the SMB (sort-merge-bucket) join. \n    Account for cluster being occupied. Ideally should be 1 wave.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.tez.cpu.vcores<\/name>\n      <value>-1<\/value>\n      <description>By default Tez will ask for however many CPUs MapReduce is configured to use per container. \n    This can be used to overwrite the default.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.tez.auto.reducer.parallelism<\/name>\n      <value>false<\/value>\n      <description>Turn on Tez' auto reducer parallelism feature. When enabled, Hive will still estimate data sizes and set parallelism estimates. \n    Tez will sample source vertices' output sizes and adjust the estimates at runtime as necessary.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.auto.convert.join<\/name>\n      <value>true<\/value>\n      <description>\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.auto.convert.join.noconditionaltask<\/name>\n      <value>true<\/value>\n      <description>\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.auto.convert.join.noconditionaltask.size<\/name>\n      <value>1<\/value>\n      <description>\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.vectorized.execution.enabled<\/name>\n      <value>true<\/value>\n      <description>This flag should be set to true to enable vectorized mode of query execution. The default value is false.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.vectorized.execution.reduce.enabled<\/name>\n      <value>false<\/value>\n      <description>This flag should be set to true to enable vectorized mode of the reduce-side of query execution. The default value is true.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.cbo.enable<\/name>\n      <value>true<\/value>\n      <description>When true, the cost based optimizer, which uses the Calcite framework, will be enabled.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.fetch.task.conversion<\/name>\n      <value>more<\/value>\n      <description>Some select queries can be converted to a single FETCH task, minimizing latency. \n    Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts \n    (which incur RS \u2013 ReduceSinkOperator, requiring a MapReduce task), lateral views and joins.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.fetch.task.conversion.threshold<\/name>\n      <value>1073741824<\/value>\n      <description>Input threshold (in bytes) for applying hive.fetch.task.conversion. \n    If target table is native, input length is calculated by summation of file lengths. \n    If it's not native, the storage handler for the table can optionally implement the org.apache.hadoop.hive.ql.metadata.InputEstimator interface. \n    A negative threshold means hive.fetch.task.conversion is applied without any input length threshold.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.fetch.task.aggr<\/name>\n      <value>false<\/value>\n      <description>Aggregation queries with no group-by clause (for example, select count(*) from src) execute final aggregations in a single reduce task.\n    If this parameter is set to true, Hive delegates the final aggregation stage to a fetch task, possibly decreasing the query time.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.spark.job.monitor.timeout<\/name>\n      <value>60<\/value>\n      <description>Timeout for job monitor to get Spark job state.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.spark.client.future.timeout<\/name>\n      <value>60<\/value>\n      <description>Timeout for requests from Hive client to remote Spark driver.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.spark.client.connect.timeout<\/name>\n      <value>1000<\/value>\n      <description>Timeout for remote Spark driver in connecting back to Hive client.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.spark.client.channel.log.level<\/name>\n      <value><\/value>\n      <description>Channel logging level for remote Spark driver. One of DEBUG, ERROR, INFO, TRACE, WARN. If unset, TRACE is chosen.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.tez.default.queues<\/name>\n      <value><\/value>\n      <description>\n    A list of comma separated values corresponding to yarn queues of the same name.\n    When hive server 2 is launched in tez mode, this configuration needs to be set\n    for multiple tez sessions to run in parallel on the cluster.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.tez.sessions.per.default.queue<\/name>\n      <value>1<\/value>\n      <description>\n    A positive integer that determines the number of tez sessions that should be\n    launched on each of the queues specified by \"hive.server2.tez.default.queues\".\n    Determines the parallelism on each queue.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.server2.tez.initialize.default.sessions<\/name>\n      <value>false<\/value>\n      <description>\n    This flag is used in hive server 2 to enable a user to use hive server 2 without\n    turning on tez for hive server 2. The user could potentially want to run queries\n    over tez without the pool of sessions.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.support.sql11.reserved.keywords<\/name>\n      <value>true<\/value>\n      <description>Whether to enable support for SQL2011 reserved keywords. When enabled, will support (part of) SQL2011 reserved keywords.\n      <\/description>\n  <\/property>\n\n  <property>\n      <name>hive.aux.jars.path<\/name>\n      <value><\/value>\n      <description>A comma separated list (with no spaces) of the jar files<\/description>\n  <\/property>\n\n<\/configuration>\n{noformat}\n\nBest regards.",
        "duedate": null,
        "environment": "Hadoop 2.7.2, Hive 1.2.1, Kerberos.",
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935"
            },
            "id": "12310843",
            "key": "HIVE",
            "name": "Hive",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310843"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Alexandre Linte",
            "key": "bigdataorange",
            "name": "BigDataOrange",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=BigDataOrange",
            "timeZone": "Europe/Paris"
        },
        "resolution": null,
        "resolutiondate": null,
        "status": {
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "id": "1",
            "name": "Open",
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "statusCategory": {
                "colorName": "blue-gray",
                "id": 2,
                "key": "new",
                "name": "To Do",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2"
            }
        },
        "subtasks": [],
        "summary": "Read & eXecute permissions on Database allows to ALTER it.",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2016-06-02T16:18:43.000+0000",
        "versions": [{
            "archived": false,
            "id": "12332384",
            "name": "1.2.1",
            "releaseDate": "2015-06-26",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12332384"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HIVE-13819/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HIVE-13819/watchers",
            "watchCount": 1
        },
        "workratio": -1
    },
    "id": "12972052",
    "key": "HIVE-13819",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12972052"
}