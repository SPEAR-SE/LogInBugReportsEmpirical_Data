[Regarding to semantics of directory's modification time, adding [~steve_l] / [~arpitagarwal] for suggestions.

Regarding to this scan behavior, I propose to entirely remove the if check: 
{code}
      if (modTime != newModTime
          || (scanTime/1000) == (modTime/1000)
          || (scanTime/1000 + 1) == (modTime/1000)) {
          // ...
      }
{code} 

Not sure how bad it could impact performance. Wanna to hear thoughts from [~jlowe] / [~vinodkv], There are two options
 # Have FS specific scan algorithms.
 # Have both algos for all FSes - i.e. instead of completely removing the modification-time check, have it (helps HDFS), but augment it with a fall-back more relaxed monitoring which just scans all users' directories every so often (helps those cloud FSes)

bq. Not sure how bad it could impact performance.
In long running large mutli-tenant cluster scenarios, this is bad.  This is true for all HDFS deployments, and may be some deployments in the cloud. For cloud deployments that are per user/tenant, it's okay there., * as noted by Wangda, S3 doesn' t have a modtime, so probing for changes that way is out
* and as noted by Vinod, big HDFS clusters need this

I don't want FS-specific algorithms (yet...), because it complicates testing & integration. I'd prefer some control here. Making the check something you can optionally disable is good for testing, as you can test it locally.

w.r.t FS-specific algorithms, theres not much they can do differently there other than
* poll for some explicitly set file (e.g .UPDATE), which kicks it off, adding a new problem: how to create?
* integrate with cloud event sources which are set up to add a new event when a file is added to a container. Lots of integration, testing & scale fun there: not something I'd be in a rush to do.

One thing to consider though: if the scanning includes subdirectories then listFiles(path, recursive=true) is orders of magnitude more efficient on S3A (and any other connector which can do bulk listings): we want to use that for any recursive polling.

Proposed: initially, just let us turn off the directory timestamp check, maybe switch to listFiles() for the probe, with a recurse option to do the recursive scan

, +[~Thomas Demoor], [~fabbri], [~ehiggs],  
{quote}integrate with cloud event sources which are set up to add a new event when a file is added to a container.
{quote}
While this might not be fun it might be the correct solution. Have the various cloud event sources (AWS Lambda, Google Cloud Functions, etc) been wrapped like S3/WASB/GCS, etc have been implemented as HCFSs? Is it required? Or do we just slurp up a kafka stream for the events and assume someone's made a bridge?
{quote}One thing to consider though: if the scanning includes subdirectories then listFiles(path, recursive=true) is orders of magnitude more efficient on S3A (and any other connector which can do bulk listings): we want to use that for any recursive polling.\{quote}
{quote}
 

HDFS-13616 (Batch listing of multiple directories) may also be relevant here for Hadoop., bq.  just let us turn off the directory timestamp check
How about adding new configuration that retain existing flow and  use else condition for other FS scanning? , [~stevel@apache.org] / [~ehiggs] / [~rohithsharma]

Thanks for your suggestions.  To me there're two major options.

1. As mentioned by [~rohithsharma], Make the behavior (skip dir timestamp check) to be configurable to avoid surprising users of HDFS-backed clusters. This config can be marked as private/unstable so we change this in the future. 

2. Considering cloud storages are not identical as mentioned, another approach is to add a option to make the whole {{scanIntermediateDirectory}} becomes pluggable folder-scan policy, and {{UserLogDir}} is private to the policy. We can implement list by recursive=true for some FS and =false for others, and can poll special files, etc.

I would not prefer to {{turn off the directory timestamp check}}, and I prefer #1 over #2. 

Thoughts? Can we can a conclusion for this so we can start fixing the problem sooner if possible?

 , Reported offline by [~deepesh]: 

After remove the timestamp check completely, we saw a NPE when trying to get the job: 
{code:java}
Exception in thread "main" java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.TypeConverter.fromYarn(TypeConverter.java:324)
	at org.apache.hadoop.mapreduce.TypeConverter.fromYarn(TypeConverter.java:302)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:440)
	at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:637)
	at org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:184)
	at org.apache.hadoop.mapreduce.tools.CLI.getJob(CLI.java:530)
	at org.apache.hadoop.mapreduce.tools.CLI.run(CLI.java:268)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.mapred.JobClient.main(JobClient.java:1274){code}
I think there're some corner cases to take care when get job status.

Given both Rohith and me are quite busy in other stuffs, as discussed offline, [~asuresh] could you help to take a look at this issue? I can help with reviews., I've attached MAPREDUCE-7101.001.patch as a possible fix., Thanks [~tmarquardt]. [~leftnoteasy], can you give a quick rev while Jenkins goes thru it ?, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 27s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 12s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 24m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 29s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 10s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  9s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m  6s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  6s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m 50s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 55s{color} | {color:green} hadoop-mapreduce-client-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m 28s{color} | {color:green} hadoop-mapreduce-client-hs in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 23s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 70m 32s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:abb62dd |
| JIRA Issue | MAPREDUCE-7101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12927520/MAPREDUCE-7101.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  xml  |
| uname | Linux 56576681daa3 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / a16623d |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_171 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7439/testReport/ |
| Max. process+thread count | 460 (vs. ulimit of 10000) |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs U: hadoop-mapreduce-project/hadoop-mapreduce-client |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7439/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks [~tmarquardt].
The patch looks good to me. +1  The comment describing the new field in JHAdminConfig is wrong - minor thing I can fix before committing.

Will wait till EOD before committing if anyone has issues with the patch.

Given that this patch retains the default behavior, and specific cloud deployments can choose to always scan.
Maybe a pluggable FS specific scan is probably a better long term solution, but I agree with [~leftnoteasy] and [~rohithsharma] that we should go ahead with the approach in this patch to unblock.


, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 11m 56s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  6s{color} | {color:red} MAPREDUCE-7101 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:abb62dd |
| JIRA Issue | MAPREDUCE-7101 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12927536/MAPREDUCE-7101.001.patch |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/7440/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks again for the patch [~tmarquardt].
Fixed the comment and checked into trunk and branch-2. (The prev jenkins failure was because it picked the updated patch after I committed it), SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14415 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14415/])
MAPREDUCE-7101. Add config parameter to allow JHS to alway scan user dir (arun suresh: rev 5670e89b2ec69ab71e32dcd5acbd3a57ca6abea5)
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/resources/mapred-default.xml
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java
, Pushed to branch-3.1 as well.]