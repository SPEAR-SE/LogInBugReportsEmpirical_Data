[I believe this is dup with HDFS-788., The slight difference I can see based on both descriptions is that this Jira states that the disk that gets filled up is deterministic (either the first disk of the list of disks, or the disk that is also configured to store logs)., Right, but the underlying issue is the same, that it's still trying to allocate to this volume.

The workaround I know of is to set the reserve amount equal to block size * number of expected concurrent writers., It is deterministic due to Yahoo! specific configs though.  For example, in the case of LinkedIn, we specifically put the mapreduce spill space into a separate, fixed-size file system so it would not be deterministic for us.  There really is no other work around, because it is fairly impossible to control how large and how many distributed caches, tmp space, etc, a job may use.  [.. and thus one of my big complaints about using the negative math to determine the size of HDFS.  It doesn't really work in practice.  You need to control -both- sizes.]

FWIW, this is really the same condition as "datanode fails on one bad file system" (whichever JIRA that is)., bq. FWIW, this is really the same condition as "datanode fails on one bad file system" (whichever JIRA that is).

I think it's a bit different than that one - in the case of "out of space", you don't want to blacklist the volume, since as you said, the space usage is fluctuating and if a disk has been out of space once, it isn't the case that it will always be so.


As for the usefulness of "reserved", would it be more useful if it were relative to the _remaining_ space instead of the _capacity_? That is to say, a DN will check the remaining disk on the volume, and not ever write a block to that volume if there is less than _reserved_ available. In some ways, it might make sense to allow for both - one is a "max amount of usage for DFS on the volume" and the other is "don't write to this volume if there's less then X free" (for the case when you've under-reserved)

Should we move this discussion to an HDFS ticket?, bq. Should we move this discussion to an HDFS ticket?

Honestly, there is no point in discussing the issues around reservation.  It is just one of those topics I've thrown my hands up on and have opted for a workaround (dedicated mapred and hdfs filesystems).  There is an ancient JIRA where I think this already took place anyway.

bq. I think it's a bit different than that one - in the case of "out of space", you don't want to blacklist the volume, since as you said, the space usage is fluctuating and if a disk has been out of space once, it isn't the case that it will always be so.

I was thinking in a more general sense (bad disk=woops, we do the wrong thing). :)  But, yes, of course you are correct here.  

We should probably add some edge case testing: what happens to process X when disk is full to help spot these things., bq. We should probably add some edge case testing: what happens to process X when disk is full to help spot these things.

Yep, I have an EC2 cluster where each node has a 10G partition and a 100G partition for this purpose. Very handy :) I hope to get some automated testing for this into HDFS-788, Since "fixed".  (New problem is that execution capacity should decrease due to less IO capability, but that's a different JIRA.)]