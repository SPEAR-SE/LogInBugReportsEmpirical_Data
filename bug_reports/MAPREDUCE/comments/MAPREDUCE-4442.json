[Attached AM logs for the full job; the timestamps should correlate to the application timestamps., Here is the snippet from resource mgr logs, relevant to the the time this happened., I think I'm also running into this on both 2.0.0-alpha and 0.23.3.  I am executing the job remotely and polling the job status.  As soon as the job completes (successfully), I get the IOException at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:315).  After job submission (before completion) checking the job status works fine.
, The client is supposed to try the operation 3 times by default, with 3 IPC level retries for each attempt. 
yarn.app.mapreduce.client.max-retries and yarn.app.mapreduce.client-am.ipc.max-retries.
Are these set to something other than the default ?

The defaults, in most cases, should be sufficient for the application state to be updated in the RM, which would trigger a JobHistory redirect on one of the attempts. (Exceptions may be logged during the first 2 attempts, but should not be thrown by the client API)
The log posted above does actually mention a redirect "Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server". In this case, it looks like there may have been an issue communicating with the JobHistory server (after the AM connection failed and the client was redirected). Are additional client logs available, or is it possible to get logs at DEBUG level ?

IAC, we could modify the AM to wait for the RM state for the app to move to FINISHED, before exiting - to avoid failures when the RM is slow to move an app to the FINISHED state., Here is the workaround we've put; the time where job counters, state etc are unavailable appears to depend upon the number of mappers/reducers for the job: the more tasks there are, the more time it will take to get access to counters restored.

We didn't have to worry about this issue any more after the workaround described below was put in place.

{code}
   RunningJob job = jobThatCompleted;
  // Allow 20 seconds wait, and 0.005 seconds additional per mapper/reducer
        long maxWaitInSecs = 20 + Math.round((numMaps + numReduces) * 0.005);
        long currWait = 0;
        long waitIncr = 1;

        do {
            try {
                return job.getCounters();
                
            } catch (IOException e) {
                LOG.error("Could not access job counters due to exception;" + e);
                if (currWait < maxWaitInSecs) {
                    LOG.info("Waiting for " + waitIncr + " seconds before another attempt; max remaining wait="
                                    + (maxWaitInSecs - currWait));
                    try {
                        Thread.sleep(waitIncr * 1000);
                    } catch (InterruptedException ie) {
                        LOG.warn("interrupted waitForJobCountersAccess() ", ie);
                    }
                    currWait += waitIncr;
                    waitIncr += 1;
                    continue;
                } else {
                    LOG.error("Exceeded maximum wait for job counters; aborting with received exception", e);
                    throw new RuntimeException(e);
                }
            }
        } while (currWait < maxWaitInSecs);
{code}, Thanks. That explains some of it. The history server needs to read the entire history file before it can serve out job level information. That becomes very inefficient for larger jobs. MAPREDUCE-3755 is meant to fix this. Clients should not need to change code.]