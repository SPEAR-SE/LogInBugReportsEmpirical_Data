[Attached a full log, when you are running it do you see anything here?
"/tmp/Jetty_0_0_0_0_43554_mapreduce____ljbmlg/webapp/webapps/mapreduce/" 
can you see what it is extracting there and why there is no ".keep" file? is that kind of a lock ? , [~jianhe] - I'm getting the same error. Have you made any progress debugging?, [~jclum], I made a clean build and didn't reproduce this any more, [~jianhe], I am using Hadoop-2.3 released version. I am facing same problem again and again at least once in day. All application attempts failed to start HTTP server in tern NPE while getting webApp.getHttpPort() at registeration. With same cluster and client configurations I submitted new job and got succeeded. I couldn't get real cause for the issue, but observation is 
1. After job is failed, I checked for directory */tmp/Jetty_0_0_0_0_59894_mapreduce____n26zu/webapp* in container run. I could not see this directory, is it bug in Jetty!!?
2. All attempt for this job failed, but with same client configuration new job got succeeded., My impression on this issue was I submitted a job,  the first few attempts(2 or 3) of the job all failed because of the above reason.  Eventually the last attempt got passed.  But after I made a clean build and re-deploy the cluster, I couldn't reproduce anymore. Feel free to reopen this if necessary, and also share some logs. tx, I reopen this issue since I am facing this at least once in a day randomly., I am keeping it in Unassined state. Please feel free to pick up., Attached MRAppMaster log that failed to start WebApps., Thoughts about for AM crash
1.WebApp start can fail even because of address already in use.But in code, even web app start failed, ignoring and continuing for other service to start.I feel this need to be handled which avoids NPE. , I got the same error. Have anyone got the solution?, I've seen this exact error when users consumed all the inodes in the /tmp file system.  Is there any way to have jetty extract to a particular directory other than /tmp?, I got the same error. Is there any workaround or fix for it?, Couple of leads on a work-around.

MAPREDUCE-6472 (fixed in 2.7.2) which sets the java.io.tmpdir which is the directory to the container's working directory

Reference Documentation
http://www.eclipse.org/jetty/documentation/current/ref-temporary-directories.html

In versions before 2.7.2, you can always suppliment the job specific yarn.app.mapreduce.am.command-opts by adding -Djava.io.tmpdir=./tmp or add it in the cluster wide admin settings yarn.app.mapreduce.am.admin-command-opts.

This has the added benefit of getting cleaned up when a job is finished. If a job dies with data written to /tmp, it will be not be cleaned up and can pollute the tmp file system and suffer the effects (jobs fail, slow jobs, etc).

, One thing else regarding this failure is that in some cases, node manager boxes can be running a tmp directory cleaner process which can inadvertently delete old entries in the /tmp directory. This could affect long running jobs depending on the settings., The instance that I am running is 2.7.1, which is prior to the above fix.  I won't be able to test on a 2.7.2+ system for a while, but will apply the -Djava.io.tmpdir option to yarn.app.mapreduce.am.admin-command-opts .  Thanks, Jonathan., Thanks for your quick response, Jonathan. ]