[NOTE: Node2 had issues so the system took it offline (0 containers). 
Perhaps this is what confused the MapReduce application?, This sounds like a bug in either headroom calculation or in RMContainerAllocator where the AM decides whether to preempt reducers.  Could you look in the AM log and see what it saw for the headroom and whether it made any attempt at all to ramp down reducers?, I'm not the only one who ran into this: http://hortonworks.com/community/forums/topic/mapreduce-race-condition-big-job/, I think this is a case of task preemption not working since the headroom calculation is not correct. Can you verify you are using the capacity scheduler.

YARN-1198, I'm wondering if the fact that the nodemanger memory has a fractional remainder when it's "full" triggers the issue.  With tasks all being 512MB that means each node will have 152MB remaining.  I'm guessing that with enough nodes those remainders will add up to appear to be enough space to run another task but in reality that task cannot be scheduled since the memory being reported is fragmented., I downloaded the Application Master log and attached it to this issue. (I changed the domainname of the nodes) , I have not actively configured any scheduling.
So I guess it is running the 'default' setting ?
, I'm pretty sure you're using the CapacityScheduler since that's been the default in Apache Hadoop for some time now.  I'm not positive about the HDP release, but I suspect it, too, is configured to use the CapacityScheduler by default.

After a quick examination of the AM log, it looks like a couple of things are going on.  The AM is blacklisting one of the nodes, and we can see that node not being used in the cluster picture.  There's a known issue with headroom calculation not taking into account blacklisted nodes.  See YARN-1680. 

The node ends up being blacklisted because the NM shot a number of the tasks for being over container limits.  It looks like the containers are being allocated as using 500MB but the JVM heap sizes are set to 512MB.  Note that the container size includes the size of the entire process tree for the task.  That's not just the heap, so it needs to also include thread stacks, JVM data, JVM code, any subprocesses launched (e.g.: hadoop streaming) etc.  If you really need a 512MB heap then I'd allocate 768MB or maybe even 1024MB containers, depending on what the tasks are doing.

It does look like some fractional memory shenanigans could be involved here, as the picture shows most of the nodes having only 200MB free.  It'd be interesting to know if you still hit the deadlock after fixing the cause of the blacklisting., I took the 'dead' node (node2) offline (completely stopped all hadoop/yarn related deamons) and ran the same job again after it had disappeared in all overviews.
Now it does complete all mappers., Where/how can I determine for sure if the capacity scheduler is used?, You can click on the Tools->Configuration link on the UI and verify yarn.resourcemanager.scheduler.class is org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler or look for capacityscheduler in the RM logs., Confirmed It is using the CapacityScheduler:
{code}
<property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    <source>yarn-default.xml</source>
</property>
{code}

I'm going to fiddle with the memory setting tomorrow., I have trouble finding the spot where this 500MB per container is defined.
Can you give me some input where this is specified?
, The container size for maps, reduces, and the MR ApplicationMaster are specified in mapreduce.map.memory.mb, mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.mb, respectively., I changed some of the memory settings and now the job completes successfully.
This was with the mapreduce.job.reduce.slowstart.completedmaps at it's default value of 0.05 (5%)
Apparently without the blacklisted node it works fine and the fractional memory shenanigans doesn't impact the job., Shall we mark this a duplicate of YARN-1680 then?  Sounds like if the memory of the blacklisted node had been removed from the reported headroom the AM would have acted appropriately., Reading through the description of YARN-1680 sure seems like the root cause of my problem.
So yes, go ahead and mark this one as a duplicate.]