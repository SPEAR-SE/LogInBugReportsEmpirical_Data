[I've started to analyze the crash and to compare the run on PPC64LE with the run on x86è64.
However, I do not understand what the test is aimed to do. That would help.

How to reproduce :
cd hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask
 mvn -l mvn.Test.OpenJDK.KVTest7.res test -Pnative,src -Drequire.snappy -Dmaven.test.failure.ignore=true -Dsurefire.heap=-Xmx2048m -fn -X -Dtest=org.apache.hadoop.mapred.nativetask.kvtest.KVTest

Maven/Java environment:
$ mvn -version
Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T11:29:23-06:00)
Maven home: /opt/apache-maven-3.2.5
Java version: 1.7.0_79, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.79-2.5.5.1.ael7b_1.ppc64le/jre
Default locale: en_US, platform encoding: ISO-8859-1
OS name: "linux", version: "3.10.0-229.ael7b.ppc64le", arch: "ppc64le", family: "unix"


Stack is:
Core was generated by `java -Xmx4096m -XX:MaxPermSize=768m -XX:+HeapDumpOnOutOfMemoryError -jar /home/'.
Program terminated with signal 6, Aborted.
#0  0x00003fffa55ce6b8 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install glib2-2.40.0-4.ael7b.ppc64le glibc-2.17-78.ael7b.ppc64le libffi-3.0.13-16.ael7b.ppc64le libgcc-4.8.3-9.ael7b.ppc64le libselinux-2.2.2-6.ael7b.ppc64le libstdc++-4.8.3-9.ael7b.ppc64le pcre-8.32-14.ael7b.ppc64le snappy-1.1.0-3.ael7b.ppc64le xz-libs-5.1.2-9alpha.ael7b.ppc64le zlib-1.2.7-14.ael7b.ppc64le
(gdb) bt
#0  0x00003fffa55ce6b8 in raise () from /lib64/libc.so.6
#1  0x00003fffa55d098c in abort () from /lib64/libc.so.6
#2  0x00003fffa524edcc in os::abort (dump_core=<optimized out>) at /usr/src/debug/java-1.7.0-openjdk-1.7.0.79-2.5.5.1.ael7b_1.ppc64le/openjdk/hotspot/src/os/linux/vm/os_linux.cpp:1630
#3  0x00003fffa542977c in VMError::report_and_die (this=0x3fff84f9c108) at /usr/src/debug/java-1.7.0-openjdk-1.7.0.79-2.5.5.1.ael7b_1.ppc64le/openjdk/hotspot/src/share/vm/utilities/vmError.cpp:1073
#4  0x00003fffa525c264 in JVM_handle_linux_signal (sig=<optimized out>, info=0x3fff84f9d068, ucVoid=0x3fff84f9c2f0, abort_if_unrecognized=<optimized out>)
    at /usr/src/debug/java-1.7.0-openjdk-1.7.0.79-2.5.5.1.ael7b_1.ppc64le/openjdk/hotspot/src/os_cpu/linux_ppc/vm/os_linux_ppc.cpp:437
#5  0x00003fffa524b7dc in signalHandler (sig=<optimized out>, info=<optimized out>, uc=<optimized out>)
    at /usr/src/debug/java-1.7.0-openjdk-1.7.0.79-2.5.5.1.ael7b_1.ppc64le/openjdk/hotspot/src/os/linux/vm/os_linux.cpp:4361
#6  <signal handler called>

#7  NativeTask::WritableUtils::ReadVLongInner (pos=0x3fff89dd0000 <Address 0x3fff89dd0000 out of bounds>, len=@0x3fff84f9d310: 4294967184)
    at /home/reixt/HADOOP-2.7.0/hadoop-FromApache-Trunk-201504241115/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/native/src/util/WritableUtils.cc:68
                value = (value << 8) | *(uint8_t*)pos;

#8  0x00003fff853d7ebc in ReadVLong (len=<optimized out>, pos=<optimized out>)
    at /home/reixt/HADOOP-2.7.0/hadoop-FromApache-Trunk-201504241115/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/native/src/util/WritableUtils.h:64
                 return ReadVLongInner(pos, len);

#9  ReadVInt (len=<optimized out>, pos=<optimized out>)
    at /home/reixt/HADOOP-2.7.0/hadoop-FromApache-Trunk-201504241115/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/native/src/util/WritableUtils.h:69
          inline static int32_t ReadVInt(const char * pos, uint32_t & len) {
                return (int32_t)ReadVLong(pos, len);

#10 nextKey (keyLen=@0x3fff89b75518: 0, this=0x3fff89b8dbe0)
    at /home/reixt/HADOOP-2.7.0/hadoop-FromApache-Trunk-201504241115/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/native/src/lib/IFile.h:74

 const char * nextKey(uint32_t & keyLen) {
    int64_t t1 = _reader.readVLong();
    int64_t t2 = _reader.readVLong();
    if (t1 == -1) {
      return NULL;
    }
    const char * kvbuff = _reader.get((uint32_t)(t1 + t2));
    uint32_t len;
    switch (_kType) {
    case TextType:
        keyLen = WritableUtils::ReadVInt(kvbuff, len);  <---------------

#11 NativeTask::IFileMergeEntry::next (this=0x3fff89b75500)
    at /home/reixt/HADOOP-2.7.0/hadoop-FromApache-Trunk-201504241115/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/native/src/lib/Merge.h:211
          virtual bool next() {
    _key = _reader->nextKey(_keyLength);        <---------------

#12 0x00003fff853d7774 in NativeTask::Merger::initHeap (this=0x3fff89b8db40)
    at /home/reixt/HADOOP-2.7.0/hadoop-FromApache-Trunk-201504241115/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/native/src/lib/Merge.cc:83

....


Traces have been added in:
- src/main/native/src/lib/IFile.h
  const char * nextKey(uint32_t & keyLen) {
    int64_t t1 = _reader.readVLong();
    int64_t t2 = _reader.readVLong();
    if (t1 == -1) {
      return NULL;
    }
    const char * kvbuff = _reader.get((uint32_t)(t1 + t2));
    uint32_t len;
    switch (_kType) {
    case TextType:
std::cout << "nextKey 0 t1: " << t1 << std::endl ;
std::cout << "nextKey 0 t2: " << t2 << std::endl ;
std::cout << "nextKey 1 t12: " << t1+t2 << std::endl ;
std::cout << "nextKey 2 t12: " << (uint32_t)(t1+t2) << std::endl ;
      keyLen = WritableUtils::ReadVInt(kvbuff, len);
      break;
- src/main/native/src/util/WritableUtils.c
int64_t WritableUtils::ReadVLongInner(const char * pos, uint32_t & len) {
std::cout << "0 &pos: " << &pos << std::endl ;
std::cout << "1  pos: " <<  pos << std::endl ;
std::cout << "2 *pos: " << *pos << std::endl ;
std::cout << "2 (hex)*pos: " << std::hex << (int) *pos << std::dec << std::endl ;
std::cout << "2 (int)*pos: " << (int)*pos << std::endl ;
std::cout << "2 (int64_t)*pos: " << (int64_t)*pos << std::endl ;
  bool neg = *pos < -120;
std::cout << "3  neg: " << neg << std::endl ;
  len = neg ? (-119 - *pos) : (-111 - *pos);
std::cout << "4  len: " << len << std::endl ;
  const char * end = pos + len;
std::cout << "5  end: " << end << std::endl ;
  int64_t value = 0;
  while (++pos < end) {
    value = (value << 8) | *(uint8_t*)pos;
  }
  return neg ? (value ^ -1LL) : value;
}


With the above added traces, output is:
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
0 &pos: 0x3fff80e2d2b0
1  pos: ^A ��X~\~Egv^B^H^...........8d\x19\xb2P~\
2 *pos: ^A
2 (int)*pos: 1
2 (int64_t)*pos: 1
3  neg: 0
4  len: 4294967184


On x86_64, I have (with same traces):
......
15/04/29 14:28:49 INFO Mid-spill: { id: 4, collect: 183 ms, in-memory sort: 13 ms, in-memory records: 48288, merge&spill: 49 ms, uncompressed size: 5030953, real size: 3742238 path: /tmp/hadoop-reixt/mapred/local/localRunner/reixt/jobcache/job_local2085218591_0015/attempt_local2085218591_0015_m_000000_0/output/spill4.out }
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 173
nextKey 0 t2: 8
nextKey 1 t12: 181
nextKey 2 t12: 181
0 &pos: 0x7fd5186f0098
1  pos: ?? "`\xcc\xe0\xa4\x85........
2 *pos: ?
2 (hex)*pos: ffffff8f
2 (int)*pos: -113
2 (int64_t)*pos: -113
3  neg: 0
4  len: 2
5  end:  "`\xcc\xe0\xa4\x85\xc4r\xb1........
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 2
nextKey 0 t2: 8
nextKey 1 t12: 10
nextKey 2 t12: 10
nextKey 0 t1: 166
nextKey 0 t2: 8
nextKey 1 t12: 174
nextKey 2 t12: 174
0 &pos: 0x7fd5186f0098
1  pos: ??  \x1a\x16c\x17\xca\x83\xdeN\xd0w9;C0........
2 *pos: ?
2 (hex)*pos: ffffff8f
2 (int)*pos: -113
2 (int64_t)*pos: -113
3  neg: 0
4  len: 2
5  end:   \x1a\x16c\x17\xca\x83\xdeN\xd0w9;C0........

So, on PPC64LE, pos is 1 instead of -113, generating a "negative" very big length (2^32-1)-111 = 4294967184 instead of  (-111 - *pos) = (-111 - (-113)) = 2 .

So, why *pos = 1 on PPC64LE instead of *pos = -113 on x86_64 ?

I need help for adding more tracing.

Interestingly, launching the Java command shown by "Forking ..." does not generate the crash.
The crash only appears when using "mvn".
So adding traces seems to me the easier way for getting more date., In short: I'd like to receive some help by experts of this code in order to quickly understand what is the root cause of this crash.
It looks that the crash appears long time after a bug has corrupted data, when reading data. And I have no idea about what this test is aimed to test and how it does it and what is the code to look at., All these tests seem to crash:
org.apache.hadoop.mapred.nativetask.kvtest.KVTest
org.apache.hadoop.mapred.nativetask.kvtest.LargeKVTest
org.apache.hadoop.mapred.nativetask.compresstest.CompressTest
org.apache.hadoop.mapred.nativetask.nonsorttest.NonSortTest
org.apache.hadoop.mapred.nativetask.combinertest.OldAPICombinerTest
org.apache.hadoop.mapred.nativetask.combinertest.CombinerTest
org.apache.hadoop.mapred.nativetask.combinertest.LargeKVCombinerTest
, All these tests seem to crash:
org.apache.hadoop.mapred.nativetask.kvtest.KVTest
org.apache.hadoop.mapred.nativetask.kvtest.LargeKVTest
org.apache.hadoop.mapred.nativetask.compresstest.CompressTest
org.apache.hadoop.mapred.nativetask.nonsorttest.NonSortTest
org.apache.hadoop.mapred.nativetask.combinertest.OldAPICombinerTest
org.apache.hadoop.mapred.nativetask.combinertest.CombinerTest
org.apache.hadoop.mapred.nativetask.combinertest.LargeKVCombinerTest
, In short: I'd like to receive some help by experts of this code in order to quickly understand what is the root cause of this crash., Mandotory patches for Hadoop on PPC64LE :
HADOOP-11505.001.patch
MAPREDUCE-6241.002.patch

Issue is still there with yesterday's Hadoop trunk version. Still investigating., Here is a more complete trace file, with details before the crash., The crash appears when code is reading data that was built BEFORE.
Thus, it seems very complex to understand where data were corrupted before., It is trunk. Not 2.7.1 . Changed., Sorry for the late reply, from the bug descriptions, I suspect this is bigendian/littleendian or unaligned memory access issue. 
Not sure how PPC64LE handles unaligned memory access. I don't have a PPC64LE env, 
looks like the crash occours when doing final merge & spill, basically mapper write mid-spill files, and read them to do merge at last, so could you diff the generatered mid-spill files to see if they are the same? If not, trace the IFile writing code?
, Hi Binglin, Thanks for answering !
About BigEndian/LittleEndian, I'm now using RHEL7.1 on PPC64LE : Little Endian. So, there should not be an issue about this.
About unaligned memory access, BLOCK_SIZE on PPC64LE is not 4096. So it may have an impact. Or it is something else.
Yes, it looks like something is corrupted at creation and generates the crash late, when reading and merging, long time after the issue appeared.
Thanks for the suggestions for finding the root cause: it is what I need, since the code is over-complicated for me.
Since I have very few understanding of this code, I'm interested with detailed/precise suggestions and instructions about what to trace and where in the code. Would you mind indicating me where it would be useful to add trace instructions ? so that you or other experts of this could more easily locate the issue., See the TR file you uploaded:
{noformat}
15/05/22 08:05:33 INFO Native Total MemoryBlockPool: num_partitions 1, min_block_size 1280K, max_block_size 4096K, capacity 5M
15/05/22 08:05:34 INFO Mid-spill: { id: 0, collect: 1013 ms, in-memory sort: 36 ms, in-memory records: 262144, merge&spill: 80 ms, uncompressed size: 3670018, real size: 3523082 path: /tmp/hadoop-reixt/mapred/local/localRunner/reixt/jobcache/job_local739893329_0006/attempt_local739893329_0006_m_000000_0/output/spill0.out }
15/05/22 08:05:35 INFO Mid-spill: { id: 1, collect: 975 ms, in-memory sort: 36 ms, in-memory records: 262144, merge&spill: 80 ms, uncompressed size: 3670018, real size: 3523317 path: /tmp/hadoop-reixt/mapred/local/localRunner/reixt/jobcache/job_local739893329_0006/attempt_local739893329_0006_m_000000_0/output/spill1.out }
15/05/22 08:05:36 INFO Mid-spill: { id: 2, collect: 978 ms, in-memory sort: 36 ms, in-memory records: 262144, merge&spill: 80 ms, uncompressed size: 3670018, real size: 3523469 path: /tmp/hadoop-reixt/mapred/local/localRunner/reixt/jobcache/job_local739893329_0006/attempt_local739893329_0006_m_000000_0/output/spill2.out }
MapOutputCollector::finalSpill 0 
MapOutputCollector::finalSpill 1 IFileWriter::create:filepath        :/tmp/hadoop-reixt/mapred/local/localRunner/reixt/jobcache/job_local739893329_0006/attempt_local739893329_0006_m_000000_0/output/file.out
{noformat}
the files spill0.out, spill1.out, spill2.out are input files for merge(previously written by , so could you stop the process when merge starts, and compare those files with x86_64 env, to see if they are the same? 
, Hi Binglin,
I can see those spill*.out files on my PPC64LE machine. I guess that the clean step has not been executed since the process crashed before. All files are different.
On x86_64, these files do not appear. Probably that they have been removed by test.

Do you confirm that these files are supposed to be the same whatever the architecture of the test machine ?

Where do you suggest me to stop the merge on x86_64 ?
In: src/main/native/src/lib/Merge.cc ?
At beg of void Merger::merge() ?

Thx, Yes, that should work , [~trex58] , have you resolved this issue yet? I am facing the same error in the tests mentioned by you. 
Here are my environment details:
Ubuntu 14.04 ppc64le 
$ java -version
openjdk version "1.8.0_111"
OpenJDK Runtime Environment (build 1.8.0_111-8u111-b14-3~14.04.1-b14)

Below is the error i got :
{code:borderStyle=solid}
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00003fff6f5b2e60, pid=10468, tid=0x00003fff6e30f1a0
#
# JRE version: OpenJDK Runtime Environment (8.0_111-b14) (build 1.8.0_111-8u111-b14-3~14.04.1-b14)
# Java VM: OpenJDK 64-Bit Server VM (25.111-b14 mixed mode linux-ppc64 compressed oops)
# Problematic frame:
# C  [libnativetask.so.1.0.0+0x52e60]  NativeTask::WritableUtils::ReadVLongInner(char const*, unsigned int&)+0x40
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /var/lib/jenkins/workspace/hadoop-master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/hs_err_pid10468.log
#
{code}

In the hs_err_pid10468.log file , I found the following trace :
{code:borderStyle=solid}
Event: 2.745 Thread 0x00003fff9000a000 Exception <a 'java/lang/ClassNotFoundException': org/apache/commons/configuration2/PropertiesConfigurationCustomizer> (0x00000000d7f19f40) thrown at [/build/openjdk-8-fVIxxI/openjdk-8-8u111-b14/src/hotspot/src/share/vm/classfile/systemDictionary.cpp, line
{code}
However, I am not sure whether this is related to the test failures. 

, Hi Sonia,
I no more work in this area. I'm trying to assign this defect to someone.
Regards, Please assign this to Ayappan ( jira ID).
Thx, I have added Ayappan as a contributor and reassigned this jira per your request., This issue is same as MAPREDUCE-6459. The latest patch attached with that jira fixes the issue in platforms where the default is unsigned char.
Please do the needful. Thanks
, Ok. I am going to close this a duplicate then, since [~Ayappan]/[~trex58] confirmed.]