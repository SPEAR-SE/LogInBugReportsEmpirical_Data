[Fixes attached for 2.0/ and 0.23/., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12573645/MAPREDUCE-5065.branch23.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3412//console

This message is automatically generated., This seems like it could give false comfort.  Rather it would be safer to advise people to, when they attempt to copy files with different block sizes, to either specify -pb or -skipCrc.  So better documentation, warnings and error messages might suffice.  Then the results of a distcp could still be trusted unless you've explicitly specified -skipCrc., Hello, Doug. Thank you for looking at this.

(For the moment, let's ignore that while DistCp code in 2.0/ does honour -skipCrc, 0.23/ code does not. I'll update the 0.23 patch to bring both of these to parity.)

IMO, it will not suffice to only document this in docs/code/warning-messages:

1. The user isn't likely to realize that the default block-sizes differ between source and target. She is even less likely to perceive the difference if the block-sizes on the source-files were explicitly set to a non-default value. (And that's entirely possible with FileSystem.create().)
The most likely manner in which she'd notice is when DistCp fails on checksum-diff, at which point the warning would instruct her to -skipCrc on the rerun.

2. Using -skipCrc will disable checksum-checks on all files copied. It's preferable to apply checks where we can, and skip only where block-sizes differ (because that's a guaranteed failure.)

One alternative is to make -pb/-skipCrc default, but that's undesirable as well., I think we should instead probably instruct her to run with -pb, not -skipCrc.

Another option might be to implement a checksum that's blocksize-independent, for when block sizes are different.  Currently the file checksum works by taking the CRC32 for every 512 byte chunk of the block, combining these with MD5 into a single checksum for the block, then combining these with MD5 into a single checksum for the file.  The first combination is done at the Datanode (in DataXceiver#blockChecksum) and the second at the client (in DFSClient#getFileChecksum).  If instead the client could directly retrieve the list of CRC32s from the datanode then it could combine them into a blocksize-independent checksum (so long as blockSize is a multiple of bytesPerChecksum and bytesPerChecksum is the same between the filesystems, which is ordinarily the case).  Op.java already includes a READ_METADATA operation, presumably intended to return the CRC32s to the client, but it is not implemented.  We'd probably want to extend the getFileChecksum API to permit specifying the type of checksum requested, whether MD5MD5CRC32 or MD5CRC32.  This would be a significant effort and it touches core bits of HDFS so should not be approached lightly., I'm with you on the need for a blocksize-independent checksum. I wasn't convinced that combining CRC32-checksums together to form a higher-level checksum could be correct. (Thanks for the explanation.)

{quote}
instruct her to run with -pb, not -skipCrc.
{quote}

Yep, that should take care of #2 (above), but not #1. The user will still need to fail first and rerun, because she's unlikely to know that some of her source-files might have non-default block-sizes. Unless the checksum calculation is fixed (or -pb is default), I don't think DistCp should enforce a check that's a guaranteed failure, under unforeseeable circumstances., Updated patches so that post-copy checksum comparisons are dropped with -skipCrc, on Hadoop-0.23. This brings 0.23 implementation to parity with 2.0/., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12573882/MAPREDUCE-5065.branch-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-distcp.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3419//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3419//console

This message is automatically generated., Sorry it took so long, but I think I see your argument now, Doug.

We'd rather have the false positive (and re-run), rather than silently skip CRC-checks and risk a bad data-copy.

Making -pb default is probably still a bad thing (because there'd be no option *not* to preserve block-size). And the cost of the re-run can be mitigated with -update.

I'll change the patches., bq. Another option might be to implement a checksum that's blocksize-independent...

Reading whole metadata may be too much, especially for huge files. It will be better if we make computation happen where the data is. :)
 
Most hashing is incremental, so if DFSClient feeds the last state of hash into the next datanode and let it continue updating it, the result will be independent of block size. The current way of doing file checksum allows calculating individual block checksums in parallel, but we are not taking advantage of it in DFSClient anyway. So I don't think there won't be any significant changes in performance or overhead.

We should probably continue this discussion in a separate jira., bq. So I don't think there won't be any significant changes in performance or overhead.
Sorry, unintended double negation., Filed HDFS-4605 for block-size independent FileChecksum in HDFS., Modified patch not to skip CRC-checks, and instead suggest using either -pb or -skipCrc, in case of checksum difference.

Updated tests for the same., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12574096/MAPREDUCE-5065.branch-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

  {color:red}-1 one of tests included doesn't have a timeout.{color}

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-distcp.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3424//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3424//console

This message is automatically generated., Reviewed latest patch.  Looks good.  +1, Review comments:
* Add a reasonable timeout to the test case. This is a relatively new rule. It applies even when you are modifying existing test cases. Please take account that tests may run on a slower hardware.
* If we suggest -skipCrc along with -pb, we should probably inform users of the risk of skipping validation., Updated patches with Kihwal's review comments. (Thanks for the review, Kihwal.), {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12577712/MAPREDUCE-5065.branch-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-distcp.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3511//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3511//console

This message is automatically generated., The patch looks good to me. [~cutting], are you okay with the change?, +1 This looks great to me.  Thanks!, Integrated in Hadoop-trunk-Commit #3618 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3618/])
    MAPREDUCE-5065. DistCp should skip checksum comparisons if block-sizes are different on source/target. Contributed by Mithun Radhakrishnan. (Revision 1468629)

     Result = SUCCESS
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1468629
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
, I've committed this to trunk, branch-2 and branch-0.23. , Integrated in Hadoop-Yarn-trunk #186 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/186/])
    MAPREDUCE-5065. DistCp should skip checksum comparisons if block-sizes are different on source/target. Contributed by Mithun Radhakrishnan. (Revision 1468629)

     Result = SUCCESS
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1468629
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
, Integrated in Hadoop-Hdfs-0.23-Build #584 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/584/])
    MAPREDUCE-5065. DistCp should skip checksum comparisons if block-sizes are different on source/target. Contributed by Mithun Radhakrishnan. (Revision 1468636)

     Result = UNSTABLE
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1468636
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/branches/branch-0.23/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
, Integrated in Hadoop-Hdfs-trunk #1375 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1375/])
    MAPREDUCE-5065. DistCp should skip checksum comparisons if block-sizes are different on source/target. Contributed by Mithun Radhakrishnan. (Revision 1468629)

     Result = FAILURE
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1468629
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
, Integrated in Hadoop-Mapreduce-trunk #1402 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1402/])
    MAPREDUCE-5065. DistCp should skip checksum comparisons if block-sizes are different on source/target. Contributed by Mithun Radhakrishnan. (Revision 1468629)

     Result = SUCCESS
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1468629
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapper.java
]