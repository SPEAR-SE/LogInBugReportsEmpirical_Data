[Is there anyone looking at this issue?, Is this still a issue for Hadoop 2.x? 
If not, I will close it on April 14th, 2014., Yeah, I think so. I think the tight loop is mainly because the Fetcher in MapReduce keeps retrying when it runs into such an exception. Need to write a test-case to show this., Thank you for the reply, [~vinodkv].
I will retarget this issue towards 2.x. , We managed to hit this issue (multiple times) while doing rolling restart of nodemanagers. , Is this issue resolved ? If not, i will look into this issue., Moving bugs out of previously closed releases into the next minor release 2.8.0., This issue replicated at my end 
Reason :- 
previously hadoop was running in unsecured (kerberos) environment
And i ran Maprjobs
appcache Directory was created inside the yarn//nm/usercache/username/ 
Automatically access rights on this directory is drwx--x---

currently i switch my hadoop from unsure to secure environment
i am not able to run any mapreduce jobs 

Workaround :
Go To yarn//nm/usercache/username/
Delete appcache Directory, Restart yarn service 
and re-run the failure jobs, Automatically appcache will created with  drwx--S--- Access rights 

Possible Solution:
while switching from unsecured environment to  secured Environment then Access rights should be check on all the directories  


]