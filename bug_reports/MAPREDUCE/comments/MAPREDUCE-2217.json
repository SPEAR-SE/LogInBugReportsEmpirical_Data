[in the ExpireLaunchingTasks,run() may calls the JobTracker.killTask(). that can implement it., Hi Lianhui,
Yes, that's the idea. I have attached a patch for this. Currently we will do ExpireLaunchingTasks.removeTask() for any task that updates.
But we should avoid remove the task that is UNASSIGNED so they can be killed by ExpireLaunchingTasks.run() as you described.
, I will try to add a unit test for this., Uploading the same patch generated off of branch-1.

We noticed this issue when one of the task trackers was faulty, and the unassigned tasks on that TaskTracker were not expired leading to job incompletion.

+1 for the patch., Karthik: Thank you for working on this., Sorry for the delay, just got around to this.

Uploading a patch that exposes the bug on clusters with some hosts with a 1 in their hostname. Running a sample pi job with 4 nodes with common prefix followed by 01-04, results in the job hanging at 75% map progress.
, The patch posted on 16/Nov fixes the issue.

To verify this I ran a hadoop cluster of 4 nodes with both MR-2217.patch and expose-bug-mr-2217.patch. The tasks assigned to machine01 timeout, and are subsequently scheduled on other nodes, and the job completes. Without MR-2217.patch, the job doesn't progress even after an hour. I used pi job with 8 mappers and 1000 input splits for this., Re-uploading the patch for Jenkins sanity., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562996/MR-2217.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3187//console

This message is automatically generated., +1. Nice job forcing the problem to verify the fix., Thanks Scott & Karthik. Committed to branch-1., Closed upon release of Hadoop 1.2.0., Have you folks take Distributed Cache into consideration? During a large cache download, tasktrackers are being put in the "UNASSIGNED" stated as well, does that means jobs with "large" distributed cache won't even get a chance to launch at all?, IIRC, the expiration timeout is 10 minutes by default. Agree it could be a problem if the jobs take longer than 10 minutes to localize. Is this a real concern? If so, would bumping up the expiration timeout be an acceptable workaround? , Karthik,

Yes, this is currently a concern for us (many users tasks unable to launch after upgraded from 1.0.4 to 1.2.1). Which exact timeout param do you suggest we should modify?, Bob,
Are you sure that your tasks are not launching? In our case, task status changes to "Error launching task", but it continues to work on TT - and can calculate some results successfully (or in case of same output name for attempts partially overwrite output for next attempt - and both are usually end up in failed status).
In the same time, JT thinks that first attempt not exists anymore - and when TT tries to send heartbeat to JT, it writes to log "Serious problem.  While updating status, cannot find taskid", but do nothing., We ended up having to up the "mapred.tasktracker.expiry.interval" to give enough time to allow the distributed cache to get localized. , [~Grigory Turunov] - we recently ran into this issue as well, and working on a fix. Will update this thread once we have it ready. ]