[Potential problem I see here is that reducer preemption logic is mainly dependent on headroom (available resources) returned by RM. After discussing with [~vinodkv] and [~sseth] offline.. There are certain important points we need to take care of
* If we ever hit the situation where I have assignedMaps=0,assignedReducers>0,scheduledMaps>0,scheduledRed>=0...then 
** I should wait for some time..
*** we are proposing time to be min[ (some percentage of average map reduce task completion time) , (some configurable number * AM-RM heartbeat interval) ]
** if we don't get any new container for map task during above interval then we will follow 
*** first remove all the scheduled reducer requests as done today in RMContainerAllocator#preemptReducesIfNeeded()
*** remove as many reducers as required to allocate a single map task.
** We should keep doing above steps repeatedly after above interval of time if we don't get any new map task. Also we should avoid ramping up later and cap the reducer count to the current running reducers as there is no point in requesting and canceling later the reducer requests/ killing running reducers in future (As we already using up to the capacity of the running user).

, also there looks to be a problem with below code. You can either preempt reducer or schedule new but not both at the same time....  any thoughts? Planning to fix this as a part of this

{code}
    if (recalculateReduceSchedule) {
      preemptReducesIfNeeded();
      scheduleReduces(
          getJob().getTotalMaps(), completedMaps,
          scheduledRequests.maps.size(), scheduledRequests.reduces.size(), 
          assignedRequests.maps.size(), assignedRequests.reduces.size(),
          mapResourceReqt, reduceResourceReqt,
          pendingReduces.size(), 
          maxReduceRampupLimit, reduceSlowStart);
      recalculateReduceSchedule = false;
    }
{code}, attaching a very basic patch.. tested it locally on my machine.
* When the cluster gets saturated it will start preempting reducers after waiting for map task. 
* Right now I am using fix interval of 2 min but this will be updated with a min of multiple of hearbeat intervals or avg map task finish time.

Please let me know if the approach taken is correct., We are hitting this issue frequently causing job to hang forever, any update on this issue?, [~varun_saxena] - this appears to be very similar to issues fixed in MAPREDUCE-6513 and MAPREDUCE-6514. Can this be closed as a duplicate? , I was initially thinking of having a configuration to ramp up reducers if maps are hanging for a while but as per discussion on MAPREDUCE-6689, this may lead to suboptimal job performance at it will be very hard to decide a right configuration value for this.

We haven't encountered any job hang issues in our deployments since MAPREDUCE-6513, MAPREDUCE-6514 has gone in our branch.
So I am fine with closing it. Maybe we can check with defect reporter too. cc [~ojoshi].

]