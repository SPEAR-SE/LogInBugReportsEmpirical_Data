[This is similar to the FairScheduler problem described in YARN-3054.  Since it always picks on the lowest priority containers, for MapReduce that is always the maps.  Some jobs can get preempted often enough to ever allow it to finish., This is related to a [discussion|http://mail-archives.apache.org/mod_mbox/hadoop-mapreduce-dev/201702.mbox/%3CCACO5Y4wVm-9_3uES+qVvi2ypzsGTvu9jbEgVfTb79unPH-E=tw@mail.gmail.com%3E] on mapreduce-dev@ on the incomplete, work-conserving preemption logic. The MR AM should react by killing reducers when it gets a preemption message (checkpointing their state, if possible)., Thanks, Chris!  Having the AM react to the preemption message in the heartbeat will definitely help a lot for common cases, even if it doesn't do any work-conserving logic and just kills the reducers.

However there's still an issue because the preemption message is too general.  For example, if the message says "going to preempt 60GB of resources" and the AM kills 10 reducers that are 6GB each on 6 different nodes, the RM can still kill the maps because the RM needed 60GB of contiguous resources.  Fixing that requires the preemption message to be more expressive/specific so the AM knows that its actions will indeed prevent the preemption of other containers.

I still wonder about the logic of preferring lower container priorities regardless of how long they've been running.  I'm not sure container priority always translates well to how important a container is to the application, and we might be better served by preferring to minimize total lost work regardless of container priority., Hi [~jlowe]

During inter-queue preemption improvement time, there were a bunch of thoughts regarding a plugin-policy to select containers from an app for preemption. 
Now we do this based on container priority. Few more good params were
- % of work completed
- time remaining to finish a container
- locality of preempted container (Whether this will help the demanding queue's app for better placement)
- +type of container+ as discussed here (map/reduce is better to preempt)

However all or some of these may not be available always or it may not well suit for a given usecase. An idea of having a *pre-computed preemption cost* per container may be a good idea. And priority of container could attribute to that cost, and other params as well (if configured)., bq. However there's still an issue because the preemption message is too general. For example, if the message says "going to preempt 60GB of resources" and the AM kills 10 reducers that are 6GB each on 6 different nodes, the RM can still kill the maps because the RM needed 60GB of contiguous resources.

I haven't followed the modifications to the preemption policy, so I don't know if the AM will be selected as a victim again even after satisfying the contract (it should not). The preemption message should be expressive enough to encode this, if that's the current behavior. If the RM will only accept 60GB of resources from a single node, then that can be encoded in a ResourceRequest in the preemption message.

Even if everything behaves badly, killing the reducers is still correct, right? If the job is still entitled to resources, then it should reschedule the map tasks before the reducers. There are still interleavings of requests that could result in the same behavior described in this JIRA, but they'd be stunningly unlucky.

bq. I still wonder about the logic of preferring lower container priorities regardless of how long they've been running. I'm not sure container priority always translates well to how important a container is to the application, and we might be better served by preferring to minimize total lost work regardless of container priority.

All of the options [~sunilg] suggests are fine heuristics, but the application has the best view of the tradeoffs. For example, a long-running container might be amortizing the cost of scheduling short-lived tasks, and might actually be cheap to kill. If the preemption message is not accurately reporting the contract the RM is enforcing, then we should absolutely fix that. But I think this is a MapReduce problem, ultimately., I agree the first step to fixing this is getting the MR AM to react to the preemption messages.  We cannot in a general sense solve these kinds of problems strictly from the YARN side, as YARN does not know the true costs and dependency graph of application containers.  I still think there are issues with fragmentation that will be aggravated as container sizes approach node sizes, but those are secondary issues that can be cleaned up in separate JIRAs.]