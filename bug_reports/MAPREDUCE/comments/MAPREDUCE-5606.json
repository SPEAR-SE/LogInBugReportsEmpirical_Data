[Does this still happen when you upgrade to the most recent version of Hadoop 1.x (or better yet, Hadoop 2.2?), thanks for your reply,  f we hava a plan upgrade the hadoop to 2.0.X next month,but there  now we need to do some test first.because the version on the production is modify base on 1.0.3.

i think  1.0.3  version will last at least for  a month.

before this time ，the server work very well more then 1 years.

now i can do is monitor the jobTracker log，when found some datanode was crash，then add this datanode to hostexclude.

can u give me some idea, or why it happened?

need  i turn down  the socket timeout  ,fail time?

thanks, I've seen this happen in more recent 1.x versions too.  In my case, it happened while writing job history files to HDFS.  The problem is that this occurs while holding a global lock (inside a synchronized method of the {{JobTracker}} object).  This prevents the JT from getting other useful work done, like accepting new job submissions or displaying the web UI.  You might be able to confirm this by inspecting a thread dump of your JT process while this is happening.

If your investigation shows the same root cause (blocked writing history files to HDFS), then you can disable this and instead only write history to the local file system.  If the configuration parameter hadoop.job.history.location is set to a location on HDFS, then remove this.  (It will default to the standard Hadoop log directory on the local file system.)

There is also hadoop.job.history.user.location.  If unspecified, this will default to writing per-job history files in each job's output directory in HDFS.  You can disable these files by setting the value to none, like this:

{code}
<property>
  <name>hadoop.job.history.user.location</name>
  <final>true</final>
  <value>none</value>
</property>
{code}

To fix this issue completely, we'd need to move the logic for writing history outside of the {{JobTracker}} monitor.  Really any kind of I/O performed while holding a global lock is problematic due to the risk of failure., I just noticed an old issue, MAPREDUCE-1144, which appears to be the same thing.  Is this a duplicate?, Hi Chris,

By adding the below property hope it will resolve temporarily, how to resolve this permanently.

<property>
  <name>hadoop.job.history.user.location</name>
  <final>true</final>
  <value>none</value>
</property>

Earlier we had used Hadoop 1.0.3 and 1.0.4 versions, but we never face this type of issue. Only we are facing in hadoop version 1.2.1, That config setting can be considered a permanent fix if you're not interested in saving user history to HDFS.  OTOH, if you are interested in saving user history to HDFS, then there is no permanent fix yet.  That's what this issue tracks.  :-), Hi Chris,
        Thanks for your help.we have set the property hadoop.job.history.user.location  to none  for a week  .   Just as you said，It now work  well.i closed   this jira., <property>
<name>hadoop.job.history.user.location</name>
<final>true</final>
<value>none</value>
</property>, Hi,

Do I need to set this property only in jobtracker node mapred Conf or in
whole hdfs cluster.

, just set on jobTracker ,and need restart the jobTracker!, I'm reopening this.  There is an actual bug here (holding a global lock in the JT while doing I/O).  Despite the config workaround I described, I don't think we can really call it resolved.

What I'm not sure about is if this is a duplicate of MAPREDUCE-1144.  If anyone on that issue can tell, then we can close this as duplicate., I originally debugged the user.location bug with [~cnauroth].

It indeed is the same issue as MAPREDUCE-1144.

We haven't done a 1.x release in ~2 years. Unlikely this issue is going to be fixed/released. This shouldn't happen in 2.x, which was a complete rewrite. 

Closing this as Won't Fix.]