[This is almost certainly fixed by now., We encountered the stack trace in this issue's description a few days ago. The SequenceFile "corruption" (unreadability) happens because of an integer math overflow [1], if the BytesWritable size is > Integer.MAX_VALUE / 3 (about 682MB). Here is [2] a stackoverflow discussion about this.

[1] https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BytesWritable.java#L123
[2] http://stackoverflow.com/questions/24127304/negativearraysizeexception-when-creating-a-sequencefile-with-large-1gb-bytesw, This is fixed in trunk as HADOOP-11901 and should be closed as a dupe of that one.
I am no longer on the contributor list for the MAPREDUCE project so I can't do it.]