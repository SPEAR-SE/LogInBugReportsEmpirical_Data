[Saw something similar
{noformat}
08-08-20 15:00:04,983 INFO org.apache.hadoop.mapred.Merger: Merging 2 sorted segments
2008-08-20 15:00:04,984 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 2391740 bytes
2008-08-20 15:00:05,025 INFO org.apache.hadoop.mapred.MapTask: Index: (32617022, 2391738, 2391738)
2008-08-20 15:00:21,204 WARN org.apache.hadoop.mapred.TaskRunner: Parent died.  Exiting attempt_20080820144820_0001_m_000264_0
2008-08-20 15:00:22,466 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.io.FileNotFoundException: 
   File xxx/taskTracker/jobcache/job_20080820144820_0001/attempt_20080820144820_0001_m_000264_0/output/spill0.out does not exist.
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:420)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:244)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:116)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:274)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:357)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1101)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:769)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:255)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2318)
{noformat}, Do not see this issue in the trunk any longer. Resolving this issue., Most of my mappers are dying with this error. I am using Hadoop 20.2. Any suggestions for a work around?

010-05-17 14:03:42,738 INFO org.apache.hadoop.mapred.Merger: Merging 22 sorted segments
2010-05-17 14:03:43,099 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.io.FileNotFoundException: /hive4/mapred/local/taskTracker/jobcache/job_201005141621_0137/attempt_201005141621_0137_m_000000_0/output/spill15.out
	at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:167)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:356)
	at org.apache.hadoop.mapred.Merger$Segment.init(Merger.java:205)
	at org.apache.hadoop.mapred.Merger$Segment.access$100(Merger.java:165)
	at org.apache.hadoop.mapred.Merger$MergeQueue.merge(Merger.java:418)
	at org.apache.hadoop.mapred.Merger$MergeQueue.merge(Merger.java:381)
	at org.apache.hadoop.mapred.Merger.merge(Merger.java:77)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1522)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1154)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:549)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:623)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)

, re-opening. we are seeing this a lot on hadoop-20 (yahoo distribution):

1. reducers not able to fetch map outputs because map side tasktracker cannot locate map output
2. mappers not able to locate previously spilled data

Scott has added logging that is telling us that:
- for #1. that the map output file was actually present/created at the time the map was first reported to be done
- that we have not removed the mapoutput file (from the TT code path deleting the files) before the reducer fetch request came in

so something very fishy - seems like either the files disappear in the interim - or that the localdirallocator is not being able to find things that are actually present. , my bad. i didn't realize that we have some new changes that are not reflected in trunk. this seems to be the most likely the result of those changes. closing again., Seen the same thing on Amazon's Elastic Map Reduce (aka hadoop 0.2.x)

Trying to narrow this down further., We were able reproduce it with every job run. Now we switched from lzo to gz compression for the input and turned off intermediate compression and that worked.

So I am not sure this was (also - like another issue) related the lzo compression or because of a bug in the intermediate compression.]