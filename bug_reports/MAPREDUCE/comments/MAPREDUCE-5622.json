[RM Log:
{code:xml}
yarn-hdpcrm-resourcemanager-ocean24.log.1:2013-11-11 13:58:36,738 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: allocate: applicationAttemptId=appattempt_1383828930654_0030_000001 container=container_1383828930654_0030_01_000073 host=ocean21 type=NODE_LOCAL

yarn-hdpcrm-resourcemanager-ocean24.log.1:2013-11-11 13:58:36,743 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: allocate: applicationAttemptId=appattempt_1383828930654_0030_000001 container=container_1383828930654_0030_01_000074 host=ocean21 type=NODE_LOCAL
{code}
\\
NM Log:
{code:xml}
2013-11-11 13:57:38,904 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container (Container: [ContainerId: container_1383828930654_0030_01_000073, NodeId: ocean21:23240, NodeHttpAddress: ocean21:23239, Resource: <memory:6144, vCores:1>, Priority: 20, State: NEW, Token: null, Status: container_id {, app_attempt_id {, application_id {, id: 30, cluster_timestamp: 1383828930654, }, attemptId: 1, }, id: 73, }, state: C_NEW, ])  to task attempt_1383828930654_0030_m_000061_0 on node ocean21:23240
2013-11-11 13:57:38,904 DEBUG [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned based on rack match /default-rack


2013-11-11 13:57:38,905 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container (Container: [ContainerId: container_1383828930654_0030_01_000074, NodeId: ocean21:23240, NodeHttpAddress: ocean21:23239, Resource: <memory:6144, vCores:1>, Priority: 20, State: NEW, Token: null, Status: container_id {, app_attempt_id {, application_id {, id: 30, cluster_timestamp: 1383828930654, }, attemptId: 1, }, id: 74, }, state: C_NEW, ])  to task attempt_1383828930654_0030_m_000063_0 on node ocean21:23240
2013-11-11 13:57:38,905 DEBUG [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned based on rack match /default-rack
{code}
\\
Here RM says allocating the container as NODE_LOCAL but AM is assigning based on rack match. , This stems from the fact that in general AMs can't always correlate container assignments to container requests, and in this case the AM is mistakenly mis-matching the assignments to the tasks that made the requests.

Ideally YARN should allow AMs to associate a piece of app-specific data to container requests and have that returned in the corresponding container assignment.  That would make it trivial for AMs to associate requests with assignments and therefore not screw up the scheduling on their end.  However that's a significant impact on the protocol and RM scheduler, as it currently collates all requests for an application at a particular priority as a simple count and therefore cannot distinguish which request corresponds to a granted container.]