[bq. For file systems such as S3, which do not have permission concept
Interesting. Seems like there is some kind of password based protection i.e. only auth and no authz for S3, but S3FileSystem isn't supporting any of that. We can definitely abstract this check out, and as much as I hate it, put in specific checks for S3FileSystem. Ideally, FileSystem should have an API to check ownership., We could have two approaches
  1) introduce a new variable , fs.authorization.supported  . We can do the owner checks only if the the variable is true
pros. Easy to implement , less changes 
cons. Has to set by the user 

2) Introduce a new Api in the filesystem boolean isAuthorizationSupoorted() and any FileSystem , eg S3 should return the false etc. This is the responsibility of the FileSytem implementor to set the variable accordingly . We use this in JobSubmissionFiles to determine if we need to do the owner check. 
pros. User need not have to worry as the filesytem implementor know best
cons. More/incompatible  changes
, bq. FileSystem should have an API to check ownership.
I like the idea of exposing a FileSystem API for checking the ownership, something like {{FileSystem#isOwnedByUser(String usernameâ€¦)}}. We had a problem with this check on Windows with many tests that use the local file system. Check out HADOOP-8457 to see what we did in branch-1-win.

Just for completeness :), another, 3rd option is to have S3 implement setPermissions/setOwner FileSystem APIs. We ended up doing this with our Azure FileSystem implementation to be able to run MR on top of it.
, Let's not add in new config items. Adding a filesytem API is fine like I first mentioned. Can you create a sister JIRA in Hadoop Common and propose that there? If that doesn't go well, we can always do specific checks in MR as the last resort., Changing the FileSystem interface would amount to an incompatible change. Must we really introduce an API for an assumed functionality?

Has someone also checked if the original description (which is a check that came in with MR security I think, but has no condition wrap) is the only point of issue or if there are other points where things have to be conditionally wrapped to support a non-true-DFS like S3 to _run_ MR (should we support that)?, S3 is not a filesystem; it, Swift and others are blobstore, with no real notion of directories, renames and deletes are non-atomic, time to retrieve data is potentially both slow and nondeterministic.

Rather than try and make changes throughout future versions of Hadoop to accomodate the use of S3 as a staging area, can I ask a question: Why do you need to do this? ]