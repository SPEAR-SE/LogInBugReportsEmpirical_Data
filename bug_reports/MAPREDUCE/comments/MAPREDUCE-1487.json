[Hi, 

I have a similar problem in Hadoop 0.20.2. 

I am trying to write a BSONWritable class that extends WritableComparable. It can be considered as an equivalent of IntWritable, FloatWritable,etc. However the length of the Key/Values produced by the Map step may/may not constant. 

I stepped through the Code and Specifically the ReduceContext.nextKeyValue() method line 115 does a buffer.reset().

    DataInputBuffer next = input.getKey();
    currentRawKey.set(next.getData(), next.getPosition(), 
                      next.getLength() - next.getPosition());
    buffer.reset(currentRawKey.getBytes(), 0, currentRawKey.getLength());
    key = keyDeserializer.deserialize(key);
    next = input.getValue();
    buffer.reset(next.getData(), next.getPosition(), next.getLength());
    value = valueDeserializer.deserialize(value);

In the debugger, I see that the value length field in the  next object is 103. However, the next.getLength() returns the length of the data itself. In the reset method, this causes the 'count' field to increase on every call. 

In the next step, the valueDeserializer fails as the data does not get reset properly due to the length field and hence the deserialization fails. The only way for me to get an accurate length of the Value in bytes is with the next.length field above. If the next.getLength() returns the correct value, then the deserialization step wrill work fine. 

I stepped through the same with another Map Reduce step that has an IntWritable output. So in the valueDeserializer.deserialize step, it calls the IntWritable.readFields() (which is pretty simple and reads only the next four bytes). 

, Will this issue be prioritized? What is the procedure for these kinds of requests? I do not want to modify source code and then have the same issue when i upgrade to a newer version. 

For now, I do not see a workaround for me in this case. I will still keep looking around for the same. , i am using 0.20.2 , I can confirm this issue in 0.20.2 and by code inspection in 0.21.0.  The short-term fix I would propose is to add a method reset(DataInputBuffer other) to DataInputBuffer which preserves the slightly schizophrenic length~~endPosition semantics., It's very embarrassing this issue isn't fixed. Do the developers realise Hadoop cannot even copy data from mapper to reducer without corruption?]