[Would be some options...

* cause an error
* add option to ignore these errors (log missing blocks in Warning level), Breaks infinite loop in CombineFileInputFormat.getMoreSplits by ignoring corrupted blocks., I attached a patch for the git HEAD. It shows error messages and ignores corrupted blocks., Hey Sadayuki. Good to see you here on JIRA :) I think the patch you've attached is against the 0.20 branch. Can you please provide a patch against trunk as well? Thanks., Sadayuki, thank you for submitting a patch on this.  I've been bitten by this one too.

This patch would log warnings about "corrupted files".  Is it really true that this indicates corruption?  My experience has been that I've seen this happen when CombineFileInputFormat tries to read newly written files that have not yet had their first block flushed.  This isn't really corruption, so I'm wondering if logging warnings about corrupt files would give a user the wrong impression that the cluster is suffering from corruption.

To workaround this, I've been running my jobs with a private patch of CombineFileInputFormat that adds this to the constructor for OneFileInfo:

    // Bail out if the block has no locations.  This guards against an
    // infinite loop in getMoreSplits.  This change is not present in open
    // source Hadoop.
    if (oneblock.length <= 0)
      continue;

That prevents these blocks from ever entering the getMoreSplits logic in the first place.  If you're interested in that approach instead, let me know, and I'll put the patch together.  I'd still need to add a unit test for it too.

Thanks again,
--Chris
, @Kazuki
Are you hitting to same problem as MAPREDUCE-2185?]