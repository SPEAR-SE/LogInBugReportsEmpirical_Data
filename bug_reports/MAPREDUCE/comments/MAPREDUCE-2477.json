[Attached is an error log that shows what the exception looks like when the servers {{fs.default.name}} can't be resolved by the client when running from the pig shell.

The script that produced this was this:

{code}
raw = LOAD '/user/bob/some_file.txt' AS (a, b, c, d);
A = LIMIT raw 10;
dump A;
{code}, It's really wired that we will use the master setting to submit a job. Is that done by mapreduce as part of job submission protocol? Did you see that in mapreduce job?, I was wondering the same thing after I filed this. I can't tell if this is from the MR job submission doing this on the client or Pig. Can you tell from the stack trace I provided?, The stack is all from mapreduce. I wonder if you will see the same thing in a pure mapreduce job., Yup. I just tried the same with WordCount and got the same failure, so this doesn't seem to be a Pig bug. Do you think this is a valid MapReduce bug?

{code}
Exception in thread "main" java.net.UnknownHostException: unknown host: colo1-hadoop-nn-r0-n0
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:195)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:850)
        at org.apache.hadoop.ipc.Client.call(Client.java:720)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
        at $Proxy0.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:359)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:207)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:170)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:82)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1419)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:1444)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1432)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
        at org.apache.hadoop.mapred.JobClient.getFs(JobClient.java:504)
        at org.apache.hadoop.mapred.JobClient.configureCommandLineOptions(JobClient.java:608)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:802)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:771)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1290)
        at cnwk.hadoop.mapreduce.WordCount.main(WordCount.java:54)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{code}, I would say yes. Client side use server side configuration, that seems counter intuitive., Moved from Pig JIRA to MapReduce.]