[Are you running with local mode or psuedo-distributed mode or fully-distributed mode when you are facing this?
http://hadoop.apache.org/core/docs/r0.18.2/quickstart.html
, I'm running on fully-distributed mode on a two node cluster and I start the application using the bin/hadoop jar command.
I tested the same with a one node "cluster" but it is the same.
Only running it on Windows within Eclipse with a jobtracker "local" works., Michael - Thanks for the clarification.

The part-00000 file should be on HDFS, what is the output of the command:
$ bin/hadoop fs -ls <mr-job-output-directory>
, I just tested it on hadoop .19.0 and the problem exist with that version as well.

The outut is:
{code}
hadoop18@bock:~$ bin/hadoop fs -ls  /ana/sesdata/2009/01/19/fcbac6fa-b163-4101-ba63-a3a022def010/
Found 1 items
drwxr-xr-x   - hadoop18 supergroup          0 2009-01-20 12:35 /ana/sesdata/2009/01/19/fcbac6fa-b163-4101-ba63-a3a022def010/_logs
hadoop18@bock:~$
{code}, Are you specifying the output path with port 8020?  (like hdfs://somehost.com:8020/ana/sesdata ) 
If so, you might be hitting HADOOP-4717.
, Hi,

I managed to track down the problem. It seems to be a combination of two things which cause this data loss:

    * I use a string like "hdfs://192.168.1.4:9000/ana" as "hadoop.hdfs.defaultfs" and I do a fs.setWorkingDirectory("hdfs://192.168.1.4:9000/ana");
    * The Path object I'm using for the output (TextOutputFormat.setOutputPath) is not created by myself, it is created by using a Path instance retured by the fs.listStatus[0].getPath().
      In detail, I'm doing a
{code}
      Path somePath = fs.listStatus(someOtherPath)[n].getPath()
      Path output = new Path(somePath, "subfolder");
      TextOutputFormat.setOutputPath(jobConf, output);
{code}
I created a small application which shows issue.
The application takes three arguments:
1) pass "1" tu use an implementation which fails or "0" to use an implementation which works
2) the "hadoop.hdfs.defaultfs" value
3) the setWorkingDirectory value.

I execute the class like " bin/hadoop jar /tmp/example.jar org.test.TestMR 1 hdfs://BOCK:9000/test/ hdfs://BOCK:9000/test/" and it does not write the output but doees not fail!

{code}
package org.test;
import java.io.IOException;
import java.util.Iterator;

import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.RunningJob;
import org.apache.hadoop.mapred.TextOutputFormat;
import org.apache.log4j.Logger;


public class TestMR implements Mapper<LongWritable, Text, Text, Text>, Reducer<Text, Text, Text, Text> {
	static Logger logger = Logger.getLogger(TestMR.class);
	
	public void map(LongWritable key, Text value, OutputCollector<Text, Text> out, Reporter reporter) throws IOException {
		logger.info("map: key=" + key + ", value=" + value);
		out.collect(new Text(key.toString()), value);
	}

	public void configure(JobConf jobConf) {
	}

	public void close() throws IOException {
	}	
	
	public void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> out, Reporter reporter) throws IOException {
		while(values.hasNext()) {
			Text value = values.next();
			logger.info("reduce: key=" + key + ", value=" + value);
			out.collect(new Text(key), new Text(value));
		}
	}

	public static void main(String[] args) throws IOException {
		JobConf jobConf = new JobConf();
		if(args.length < 3) {
			logger.error("usage: 0|1 hadoop.hdfs.defaultfs hadoop.hdfs.workdir");
			return;
		}
		logger.info("setting defaultfs and workdir");
		jobConf.set("hadoop.hdfs.defaultfs", args[1]);

		FileSystem fs = FileSystem.get(jobConf);
		fs.setWorkingDirectory(new Path(args[2]));
		fs.mkdirs(fs.getWorkingDirectory());

		jobConf.setJobName(TestMR.class.getName());
		jobConf.setOutputKeyClass(Text.class);
		jobConf.setOutputValueClass(Text.class);

		logger.info("in: we use the 'in' directory. We create it and put some dummy data in it.");
		fs.delete(new Path(fs.getWorkingDirectory(), "in"), true);
		fs.mkdirs(new Path(fs.getWorkingDirectory(), "in"));
		FSDataOutputStream dataOut = fs.create(new Path(fs.getWorkingDirectory(), "in/data"));
		dataOut.write("some data".getBytes());
		dataOut.sync();
		dataOut.flush();
		dataOut.close();
		
		FileInputFormat.setInputPaths(jobConf, new Path("in"));

		logger.info("out: we use a subdirectory of the 'out' directory as the output of your job. Therefor we create the out folder first.");
		fs.delete(new Path(fs.getWorkingDirectory(), "out"), true);
		fs.mkdirs(new Path(fs.getWorkingDirectory(), "out"));
		Path out = null;
		
		if(args.length > 0 && "1".equals(args[0])) {
			logger.info("using broken folder");
			out = getBrokenOutFolder();
		} else {
			logger.info("using working folder");
			out = getWorkingOutFolder(fs);
		}
		logger.info("setting output using path: " + out.toString());
		TextOutputFormat.setOutputPath(jobConf, new Path(out, "subfolder"));

		jobConf.setMapperClass(TestMR.class);
		jobConf.setReducerClass(TestMR.class);
		jobConf.setJarByClass(TestMR.class);
		
		JobClient jobClient = new JobClient(jobConf);
		RunningJob runningJob = jobClient.submitJob(jobConf);
		
		while(!runningJob.isComplete()) {
			logger.info("still running...");
			try {
				Thread.sleep(1000);
			} catch (InterruptedException e) {
				e.printStackTrace();
			}
		}
	}

	private static Path getWorkingOutFolder(FileSystem fs) throws IOException {
		
		Path root = new Path("/test");
		for(FileStatus fileStatus : fs.listStatus(root)) {
			if("out".equals(fileStatus.getPath().getName())) { return fileStatus.getPath(); }
		}
		return null;
	}

	private static Path getBrokenOutFolder() throws IOException {
		return new Path("out");
	}
	
}
{code}
If I start the application like "bin/hadoop jar /tmp/example.jar org.test.TestMR 1 hdfs://BOCK:9000/test/ /test/" it works.
, Michael, could you try HADOOP-4717 patch? 
(or wait for 0.18.3), I'm going to close this issue as stale.]