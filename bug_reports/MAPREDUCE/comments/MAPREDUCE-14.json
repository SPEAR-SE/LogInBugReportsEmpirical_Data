[Christian, please try a build with hadoop-2070, which was committed last night. I suspect you are seeing the same problem., Owen, I am not convinced that this is related to Hadoop-2070.

As a matter of fact, as far as I can see the original patch for HADOOP-1788 flushed the buffer when the application is done, but missed two cases: when the application does not use a java inputformat and when it gets aborted. That's why the first 3 out of 4 unit tests were successful; the 4th unit test does not use a java input format, therefore, it failed. I would have thought that without any flushing all unit tests would have failed.

Our application uses a java inputformat and has the input flushed (it actually completes). Also the symptom of missing flushing is that the task it self times out.

The current issue seems to be related to bad communication between task and taskTracker. I checked the tasktracker log around the same time, and found that it was running out of threads 

2007-10-18 19:27:43,719 INFO org.mortbay.http.SocketListener: LOW ON THREADS ((40-40+0)<1) on SocketListener0@0.0.0.0:50060

2007-10-18 19:27:44,432 INFO org.apache.hadoop.mapred.TaskTracker: task_200710172336_0016_m_000045_1 <status msg>
2007-10-18 19:27:44,432 INFO org.apache.hadoop.mapred.TaskTracker: task_200710172336_0016_r_000889_1 <statusmsg> > reduce
2007-10-18 19:27:44,437 INFO org.apache.hadoop.mapred.TaskTracker: task_200710172336_0016_r_003314_0 0.70760524% reduce > reduce
2007-10-18 19:27:44,997 INFO org.apache.hadoop.mapred.TaskTracker: task_200710172336_0016_r_003312_0 0.6736239% reduce > reduce
2007-10-18 19:27:45,430 INFO org.apache.hadoop.mapred.TaskTracker: task_200710172336_0016_m_001001_1 0<statusmsg>

2007-10-18 19:27:45,498 WARN org.mortbay.http.SocketListener: OUT OF THREADS: SocketListener0@0.0.0.0:50060

2007-10-18 19:27:47,436 INFO org.apache.hadoop.mapred.TaskTracker: task_200710172336_0016_m_000045_1 <statusmsg>
, With input from Owen and Devaraj, I will try to use less jetty threads to reduce contention and see whether this helps, Even with 30 jetty threads (instead of default 40) this still happens frequently, generating a very long tail for the job. I noticed that during that time the nodes running map tasks are disk bound.
I will now try with 20 jetty threads, but the protocol between Task and TaskTracker should be fixed to use retries.
I make this a blocker because it slows us down tremendously., I looked at the system resources during the time a node exhibits the timeout problem. At the end of the map phase, nodes which are just  finishing the map tasks (probably merging the output taking of the order of  an hour in our case such that there is a good chance that all of the map tasks do the merging at the same time), with reduce tasks already fetching remote map output, can be completely overwhelmed (load factor > 10, several processes running at relatively high CPU, and some disks at high utilization with at least one at 100%). No wonder that we get socket timeouts.

Changing tasks to do retries when there are socket connection timeouts would  only be a bandaid. Rather, the taskTracker should monitor system resources and throttle the reduce tasks (maybe even the map tasks) during contention., Christian, for this job, could you please set the tasks per node to 2 and see whether it improves the situation?, A couple of days ago, I ran the job successfully with 20 (instead of 30) jetty threads (although with some long tail because of the socket connection timeouts).
To save time I would like to not repeat the job as this time, because we made some progress in a batch of jobs and are further down the line.
Once we hit the problem again (requires a job with some substantial intermediate output), I will try your suggestion., Some thoughts : if we have the thing about monitoring/throttling tasks, we can have the monitoring in place for the running tasks and based on the load on system resources the tasktracker can ask for a new task or not (as part of the heartbeat). However, for your case (and for tasktracker running long running tasks in general), this may or may not work since the system goes into that state towards the middle of the tasks' lives. So, the option is to kill some currently running task(s) or to reduce the (nice) priority of some. This runtime tuning can be made an optional feature since it might adversely affect the performance of some jobs (since in this approach we speculate/analyze, etc.).

The other option, as I commented on earlier, is to control the number of running tasks for a job on a tasktracker. That can be configured by the user (and this affects whether a new task for that job is assigned to a tasktracker or not, when the latter asks for a new task). The runtime tuning is probably the best approach, but this is a simpler one and should generally help (and maybe we can implement both).

Thoughts?, We changed the max number of tasks per node to 2, but currently scaling issues with the namenode are obscuring this issue here (also, we still run with 20 jetty threads).

It would be nice if the user was not forced to reduce the number of tasks when the node resources could handle as many simultaneous applications as CPU's, but not the post-processing inside the infrastructure (simultaneous merging of map tasks and remote fetching by reduce tasks). Would it be possible to optionally let the reduce tasks hold off fetching for a while when the map tasks do the final merge and/or serialize the final merge of map tasks, but do it only when resource contention is detected?, Christian, since this issue doesn't break anything and, as you pointed out, things work with a different configuration, I think that we should downgrade the Priority to 'Major'. The fix for this issue seems to involve quite a lot of benchmarking work and IMO should not block a release., Determined to not be a blocker.  Assigning to 0.16, My apologies for late response.
Will try to get by with using more mappers producing less output., Haven't seen this occur with any of recent versions, having also seen jobs with more tasks than was reported here. Works just fine.

Looked to be a user issue, here.]