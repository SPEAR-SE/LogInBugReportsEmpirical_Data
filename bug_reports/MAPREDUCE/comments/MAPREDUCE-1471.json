[Jim, all file-based output-formats check to ensure that their output-directory is *not* present when they start i.e. 'working_path' is owned by one and only one job, hence this behaviour is correct., It is a pretty common use case for me, and I'm guessing others, to write a number of files to the same output directory from concurrent jobs. 

For instance counting the number of occurrences of a daily event for a entire month

Setting up the directory structure:
.../2008/07/

With all of the count files for that month ending up in that directory.

When running concurrent jobs 1 and 2, hadoop creates the following temporary directories/files with a modified version of SequenceFileOutputFormat, that allows me name the file what I desire and run with the same working path for multiple jobs.
.../2008/07/_temporary/_attempt_local_0001_r_000000_0
.../2008/07/_temporary/_attempt_local_0002_r_000000_0

When the job 1 completes, in order to clean up it's temporary files, it removes
.../2008/07/_temporary/

This then blows away the temporary files for job 2.

I would say that normally this is not a hadoop problem because of the way the I extended SequenceFileOutputFormat to allow for mutliple jobs to have the same working path so long as the output file name is unique (which it is). However, it is the cleanupJob in FileOutputCommitter that causes the problem, and since the committer in FileOutputFormat is private, I cannot extend and replace the FileOutputCommitter with my own. Currently I have overidden the getOutputCommiter(context) method in my out FileOutputFormat to work around this, but if the class ever starts accessing the committer without going through that method, I'm in trouble again.

So....I'd really appreciate it if the commiter in FIleOutputCommitter is protected rather than private in FileOutputFormat so that it can be overridden for client applications or the jobs only cleanup the temporary files that they create rather than recursively deleting the highest level directory (_temporary).

Thanks,
Jim

, bq. If the file name output is guaranteed by the client application to be unique, the temporary files/directories should also be guaranteed to be unique to avoid this problem
I like this idea. But, this should be done in your own custom OutputFormat/OutputCommitter. 
We cannot change FileOutputFormat/Committer for the following reason:
Currently the output file names are part-m-00000 or part-r-00000 for tasks m_00000 and r_00000 irrespective of the jobid. Unless user changes these file names to be different for different jobs, user can not run concurrent jobs with same output path. If he runs without considering final output file names, output is unpredictable; One job will replace the output files from the other job. i.e. part-m-00000 is the output of either job1 or job2 and the other output is lost! 
So, FileOutputFormat makes sure that output directory does not exist already. If user has made sure that the output file names are unique for each job by overriding FileOutputFormat, then i would say he can write his own OutputCommitter for running concurrent jobs.

bq. Currently I have overidden the getOutputCommiter(context) method in my out FileOutputFormat to work around this.
This is the correct way to plug your custom OutputCommitter.

, The resolution here was that the dev ought to implement a custom OC for such needs., Jim, it should not be too difficult for you to take the existing FileOutputCommitter and modify it to do what you want.  Be aware though that going to the 2.0 line FileOutputCommitter has changed so that if an application master crashes it can recover and start over without needing to rerun anything that finished successfully before.  Just be aware that the directory structure is different and when upmerging to trunk/2.0 you will probably need to modify your code.]