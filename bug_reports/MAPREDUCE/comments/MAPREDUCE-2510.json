[The following comments are copied from MAPREDUCE-143:
We dump the heap of TaskTracker and analyze it with MAT. We found one instance of "org.mortbay.thread.QueuedThreadPool" occupies 853,258,184 (72.51%) bytes. This object contain a "java.lang.Runnable[]" which has 7200 elements.

The QueuedThreadPool of jetty6 own an array of jobs. If an idle thread is available a job is directly dispatched, otherwise the job is queued to the array. At first the size of the array is _maxThreads(tasktracker.http.threads). When its full, the size grow to array.length() + _maxThreads. Because the grow has no limit, this array can occupy too many memory when there are lots of fetch request from reduce task. So is this jetty6's bug?, What is the exact version of Jetty you're using? A similar bug has been seen in Hadoop clusters before; it was fixed by upgrading Jetty. The current stable release 6.1.26 does not have this behavior (again, assuming it's the same circular buffer issue), Hi Chris, 
our Jetty version is 6.1.14, the same with trunk. Is there an issue about upgrading Jetty to 6.1.26? Why Jetty 6.1.26 does not have this behavior? I saw that Cloudera's cdh3u0 use Jetty 6.1.26.

Thanks, Hi Chris,
HADOOP-6882 upgrade the version of Jetty to 6.1.26. That jira has checked in to 0.20 branch. But I still don't know why 6.1.26 does not have this behavior., bq. Is there an issue about upgrading Jetty to 6.1.26?

None that I'm aware of. Upgrading from Jetty5 to Jetty6 was painful, but upgrading within Jetty6 has been very easy.

bq. Why Jetty 6.1.26 does not have this behavior?

This showed up in a cluster while I was on call. The heap dump of the TaskTracker showed exactly what you found, i.e. a large number of Runnable instances in an array. Looking at the 6.1.14 source, it was obvious that the circular buffer was failing to free references to requests queued in this buffer, which expanded whenever its throughput lagged behind the request rate. Looking at the 6.1.25 source, references to requests in this buffer were correctly set to {{null}} after dispatching them. I don't know when this was fixed, only that we've been running 6.1.26 and it's the smoothest version of Jetty we've deployed., Hi Chris,
I really appreciate your comments! Jetty 6.1.26 did free the references to Runnable instances. We'll upgrade our cluster's jetty version asap.
Thanks again., > > Is there an issue about upgrading Jetty to 6.1.26?
> None that I'm aware of. Upgrading from Jetty5 to Jetty6 was painful, but upgrading within Jetty6 has been very easy.
>
After 6.1.26 upgrade, I think we started seeing various fetch failure issues that persists on TaskTracker/jetty delaying the jobs. (MAPREDUCE-2529, MAPREDUCE-2530, etc)  So far we haven't found any fixes and instead working on a workaround.
, Hi Koji,
We just trigger this bug in our test cluster with jetty6.1.26. Can you please share your workaround?, After upgrading our product cluster's Jetty version to 6.1.26. The checkpoint become very slow. 

                   fsimage size          download time
Before upgrading   10G                    2 mins
After upgrading    9.95G                 15 mins 

What's more, there are many "JVM BUG(s)" logs in NN's log file:
2011-05-26 22:46:48,807 INFO org.mortbay.log: org.mortbay.io.nio.SelectorManager$SelectSet@173ab5e JVM BUG(s) - injecting delay59 times

2011-05-26 22:46:48,807 INFO org.mortbay.log: org.mortbay.io.nio.SelectorManager$SelectSet@173ab5e JVM BUG(s) - recreating selector 59 times, canceled keys 944 times

According to Jetty 6.1.26's code, Jetty's Selector sleep some time when print above logs.
, We have planned to build our own jetty version based on 6.1.14, with following patches to fix OOM bugs.
JETTY-1157, Don't hold array passed in write(byte[]).
JETTY-861,switched buffer pools to ThreadLocal implementation.
JETTY-1188,Null old jobs in QueuedThreadPool.

It works well in test cluster.]