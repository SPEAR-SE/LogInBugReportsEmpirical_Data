[BTW, the {{ShuffleHandler}} is not aware of the cleanup. The clean up is done in the {{ResourceLocalizationService.java}} {{serviceInit()}} method., I've largely implemented this as part of the prototype for YARN-1336.  I actually have two versions, one that uses FileSystem to store the shuffle tokens and job-to-user mappings and another that uses leveldb.  (The prototype currently has a  leveldb back-end store to simplify some of the race conditions during store and recovery.)  It shouldn't be too much effort to extricate just the ShuffleHandler changes, although there aren't any unit tests for it yet.

As Alejandro pointed out it also needs some help from the NodeManager to keep it from cleaning up the local directories and removing the shuffle output after restarting.  That's also been done as part of the prototype and is relatively straightforward, but we're still missing a mechanism for distinguishing the restart case vs. shutdown/decommission case and some other cleanup., Thanks for the update, [~jlowe]. Glad to know this is being handled as part of YARN-1336. Looks like it is better to just wait for YARN-1336 to be done, and address any remaining items here. Please feel free to take this over too. 

Curious - is there a branch for the YARN-1336 changes I can take an early peek at? , No branch yet, just an ugly internal prototype that needs a bit of cleanup before publishing.  I meant to post either a branch or sets of draft patches to the JIRAs last month but got swamped with other stuff.  I'll try to get some things posted i the next week or so., bq. I've largely implemented this as part of the prototype for YARN-1336.
Can this be closed as duplicate then? Or make it a sub-task if there isn't a corresponding child under YARN-1336., There's no corresponding sub-task under YARN-1336 since these are changes specific to the ShuffleHandler in MapReduce.  Since JIRA doesn't let us mark this as a subtask of a different JIRA project, I think it's best to mark it as related and leave it for tracking the MapReduce-specific changes.  Alternate suggestions for tracking the MR-specific changes related to YARN-1336?, Patch to recover shuffle handler state using leveldb as a state store.  Note that this patch depends upon the changes in YARN-1757., Oops, I missed the new proto file in the previous patch.  I also fixed a bug in the unit test where it wasn't properly emulating a restart since the ShuffleHandler uses static members to track jobs., Updating patch to include the same license blurb added in YARN-1704 since the leveldb dependency is being added to the mapreduce project., This is now unblocked since YARN-1757 has been committed.  Refreshing patch based on latest trunk., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12639250/MAPREDUCE-5652-v4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4492//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4492//console

This message is automatically generated., Approach looks good. Comments:
# How do we handle applications that finish while the NM is down? 
# Code related to initStateStore should ideally go into serviceInit(), primarily to future-proof against us supporting (re)starting stopped services.
# Use the constant {{JOB}} here?
{code}
      iter.seek(bytes("job"));
{code}
# ShuffleHandler#recordJobShuffleInfo: addJobToken() should come after attempt to include in the store? Fail early if we can't write to the store for any reason. The place where we call this method, we catch-ignore all exceptions.
# ShuffleHandler#close() should probably take care of clearing the static maps. Alternately, we could just make those maps non-static.
# ShuffleHander#forgetJob() - should we make those two maps
# Do we need to change hadoop-mapreduce-project/pom.xml, given we already add the dependencies in the shuffle module?
# Nice test., Point 6 above - should we make those two maps non-static?, Thanks for the review, Karthik!

bq. How do we handle applications that finish while the NM is down?

The RM is already telling the NM about finished applications, and since it's not creating a new RMNodeImpl when it rejoins it should continue to tell it about subsequent finished applications since the last time the node heartbeated.

There may be some races in there where the RM thinks the NM processed the heartbeat response but the NM crashed before it completed processing it.  If that's an issue we could fix it by having the NM send the list of apps it thinks are still active when it re-registers, giving the RM an opportunity to correct it on the next heartbeat.  However that's outside of the scope (and project) of this JIRA.  I'll try to address that in YARN-1354 or possibly a separate YARN JIRA.

bq. Code related to initStateStore should ideally go into serviceInit(), primarily to future-proof against us supporting (re)starting stopped services.

serviceStart() indirectly calls initStateStore to open the database, and serviceStop() closes the database.  If we want later to support restarting stopped services then we need to continue to open the database in serviceStart().  Moving the initStateStore call to init rather than start means we will try to use a closed database after the service is restarted, or am I missing something?

bq. Use the constant JOB here?

Fixed.  I had to change its visibility to public in order to access it.

bq. ShuffleHandler#recordJobShuffleInfo: addJobToken() should come after attempt to include in the store?

Fixed.

bq. ShuffleHandler#close() should probably take care of clearing the static maps. Alternately, we could just make those maps non-static.

I made userRsrc and secretManager regular members rather than static.  Originally I wasn't sure why they were static and didn't want to mess with that too much as part of this JIRA, but it's problematic for testing this.  Apparently they always were static, but I couldn't find any reason for them to be so.  Even in light of multiple ShuffleHandler instances, I don't see why it's something we need (or necessarily even want) to share.  Manually ran the unit tests under hadoop-mapreduce-client-jobclient to verify there wasn't something fishy going on with the multi-instance ShuffleHandler in mini clusters or something like that.

bq. ShuffleHander#forgetJob() - should we make those two maps non-static?

Removed forgetJob() now that the members are not static.

bq. Do we need to change hadoop-mapreduce-project/pom.xml, given we already add the dependencies in the shuffle module?

Yes, if I remove the dependency from that pom.xml then the leveldb jar doesn't show up in the resulting dist tree under mapreduce/lib/.  Maybe may need the equivalent of YARN-888 for hadoop-mapreduce-project poms to only declare them in the leaf modules and still have them be picked up properly., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12639878/MAPREDUCE-5652-v5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4505//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4505//console

This message is automatically generated., bq. However that's outside of the scope (and project) of this JIRA. I'll try to address that in YARN-1354 or possibly a separate YARN JIRA.
Thanks for the clarification. Let us track this in YARN-1354. 

bq. serviceStart() indirectly calls initStateStore to open the database, and serviceStop() closes the database.
I see. Makes sense to leave it in serviceStart(). Do you think renaming initStateStore to initAndOpenStore or startStore is reasonable? 

bq. Maybe may need the equivalent of YARN-888 for hadoop-mapreduce-project poms to only declare them in the leaf modules and still have them be picked up properly.
MAPREDUCE-5362 . Let us try to get that in first. Mind taking a look? I ll also look at it shortly., bq. Do you think renaming initStateStore to initAndOpenStore or startStore is reasonable? 

Changed it to startStore.

bq. MAPREDUCE-5362 . Let us try to get that in first. Mind taking a look?

Posted some comments to that JIRA but haven't seen any activity for a bit.  In the interim this patch works without the changes from MAPREDUCE-5362.  If MAPREDUCE-5362 happens to go in before this is committed then I'll update it accordingly., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12641270/MAPREDUCE-5652-v6.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4543//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4543//console

This message is automatically generated., Nice work. Jason, I would like to clarify how the following scenarios are handled. Perhaps they are covered at the YARN layer as part of https://issues.apache.org/jira/browse/YARN-1336.

1. NM crash scenario. There is a corner case, after RM notifies NM regarding the completion of a specific application, right before AuxServices get the chance to process the event, NM crashes. The app entry won't be removed after the recovery store after NM is restarted, as APPLICATION_STOP won't be delivered to NM for that application after NM restart.

2. NM graceful shutdown. It seems ContainerManagerImpl's serviceStop will generate ContainerManagerEventType.FINISH_APPS event. That means AuxServices could clean up and remove it from the recovery store as part of NM shutdown., Thanks for the feedback, Ming!

For the NM crash scenario above we need YARN-1336 so applications are persisted outside of an individual aux service.  Once that's present, it remembers when applications are finishing and persists that before responding to the NM heartbeat telling it of the apps that have just finished.  Upon recovery it will recover the application and re-send the finish event which will send the app stop event to the aux services.

For the graceful shutdown scenario we need YARN-1336 and/or YARN-1362.  Either YARN-1336 will never send app stop events upon NM shutdown if recovery is enabled or we need to be able to distinguish between a graceful NM shutdown and an NM kill/crash to know whether to send the app stop event to aux services., Thanks, Jason. It is good to know it will be taken care of at YARN layer. I will post some more comments at YARN-1336.

1. Does leveDB's delete method throw exception? JNI has some exception handling and the caller needs to retrieve the exceptions, etc.
2. It seems like recover/restore are common in NM/RM restart. Any abstract interface defined for that?, bq. 1. Does leveDB's delete method throw exception? JNI has some exception handling and the caller needs to retrieve the exceptions, etc.

Nice catch!  I didn't notice there were _two_ DBExceptions flying around in leveldb code.  org.fusesource.leveldbjni.internal.NativeDB.DBException comes from the JNI layer and derives from IOException, and it was the one I was familiar with.  However the wrapper code around the JNI layer catches that exception and rethrows it as org.iq80.leveldb.DBException which is a RuntimeException.  That means we need to wrap all calls that can throw the runtime form and either handle them directly or rethrow as an IOException if it's not appropriate to let the RuntimeException leak out of the method.

Updated the patch to deal with the runtime DBException when necessary.  I'll also have to make similar changes in the NMLevelDBStateStore for the other NM restart patches.

bq. 2. It seems like recover/restore are common in NM/RM restart. Any abstract interface defined for that?

They both support recovery but the forms in which they do it are very different (e.g.: types of state persisted are significantly different, backing store types have no overlap, etc.)  There could be a generic Recoverable interface that supports a recover() method, but I'm not sure what value that adds.  Did you have a particular interface in mind or ideas on how it would be used?, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12641736/MAPREDUCE-5652-v7.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4554//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4554//console

This message is automatically generated., Sigh.  I discovered that leveldb's DBIterator isn't consistent with the DB interface and throws raw RuntimeException rather than the derived DBException.  That means whenever we're interacting with the database via the iterator we risk leaking what should be caught as a DBException since it's a raw RuntimeException instead.

There's a few approaches I considered to work around this:

# Catch RuntimeException rather than DBException for the code blocks that interact with the iterator.
# Catch RuntimeException and if the cause is NativeDB.DBException then throw an IOException otherwise rethrow the original exception.
# Wrap DBIterator in a private wrapper class which catches RuntimeException for each method invoked and rethrows it as DBException.

I dismissed the first approach since it's too ham-fisted.  We're likely to catch NPEs and other unrelated RuntimeException and handle them as if they were leveldb errors.  The second approach has the drawback that it knows a bit too much about the DBIterator implementation in that it's digging into the RuntimeException looking for a specific cause.  If the cause were to switch to the iq80 DBException or some other type then we'd leak it instead of converting it.  Therefore I went with the third approach. It's still catching raw RuntimeException like the first approach, but it has the advantage that the try..catch block is localized to just the iterator method being invoked.  Also if leveldb's iterator is ever fixed in the future to throw DBException then we can simply remove the wrapper rather than change all the try..catch code blocks that work with the iterator.

, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12641927/MAPREDUCE-5652-v8.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4558//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4558//console

This message is automatically generated., +1 to the wrapper approach. We should probably go ahead and make it a util class, and ping yarn-dev@ so other features using LevelDB are aware of this., As for the utility class, where would it go?  I'm assuming yarn-server-common, otherwise we end up with clients picking it up.  We'd have to add the leveldb dependency there which is unfortunate for those servers that don't really need it (i.e.: resourcemanager, webproxy)., yarn-server-common seems the best place., 1. Regarding generic interface for restore/recover,  I agree there is no much benefit to generalize things for the sake of it. One scenario could be something like ShuffleHandler, some ShuffleHandlers support recovery, some don't. NM can ask if a specific ShuffleHandler if it supports recovery, NM will manage the underlying store and pass the store object to ShuffleHandler and ShuffleHandler manages the serialization and deserialization, etc. If NM decides to change the underlying store and ShuffleHandler doesn't need to change. But at this point, it seems unnecessary.
2. If ShuffleHandler gets DBException during recoverState as part of serviceStart, should ShuffleHandler ignore the exception and continue like the store doesn't exist? The argument for ignoring it is it is soft state and ShuffleHandler can still run without it. Or maybe this can be configurable., Filed YARN-1987 to cover the DBIterator wrapper and updating the patch to use that new wrapper class.  Note that the patch includes YARN-1987 so Jenkins can comment.

bq. If ShuffleHandler gets DBException during recoverState as part of serviceStart, should ShuffleHandler ignore the exception and continue like the store doesn't exist?

Failure to recover should be a rare situation where the DB is corrupted/inaccessible or there's some schema incompatibility between versions if an upgrade occurs during the NM downtime.  It should be investigated and corrected, otherwise the errors will likely be glossed over and we will continue to fail to shuffle across NM restarts from that point forward despite the user specifying otherwise.

We could add a config to request a "best effort" mode where it will continue despite the inability to recover, but is that an NM-wide config, a config just for the shuffle handler, or something else?  If we want a config to control this I propose we address it in a followup JIRA., Sounds good, we can use a new jira to cover "best effort" work.

The patch looks good. Just to confirm, protobuf should be backward compatible, e.g., the store state serialized with version 2.4 should be readable by NM/MR compiled with version 2.5.

On an unrelated note, based on how NM's AuxServices' serviceStart handles error for each AuxService' serviceStart, if one AuxService throws some exception, the rest of AuxServices' serviceStart will be skipped. That isn't important given we only have one AuxService. Perhaps there is some policy around that as well, should NM skip failed AuxService? It seems in general we might need to improve AuxService handling if there are other AuxServices., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12642848/MAPREDUCE-5652-v9-and-YARN-1987.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4573//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4573//console

This message is automatically generated., bq. Just to confirm, protobuf should be backward compatible, e.g., the store state serialized with version 2.4 should be readable by NM/MR compiled with version 2.5.

Yes, the protobuf incompatibility between 2.4 and 2.5 is an issue with the interfaces to the protobuf code and not an incompatibility with the data layout of protobuf messages.

bq. On an unrelated note, based on how NM's AuxServices' serviceStart handles error for each AuxService' serviceStart, if one AuxService throws some exception, the rest of AuxServices' serviceStart will be skipped.

I might be reading the code incorrectly, but it looks like AuxServices#serviceStart doesn't try to handle exceptions coming from individual aux services at all.  If one of their startups throws then it will be converted into a RuntimeException (by AbstractService#start) which will bubble up out of AuxServices and likely all the way up such that the NM startup will fail.

As you pointed out before, a better way to handle aux services would be to run them outside of the NM (maybe even within containers).  It'd also be nice to make them more dynamic, such that application submissions can provide an aux service they require.  They could be started on demand, ref-counted, and stopped accordingly based on usage, which would be a smoother answer to rolling upgrades and sharing multiple versions of the aux services within the cluster.  This is of course a non-trivial amount of work for another JIRA. ;-), [~jlowe] - now that YARN-1987 is committed, mind updating the patch against the latest trunk. We should be able to get this in soon. , Updated patch to trunk., +1, pending Jenkins., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644621/MAPREDUCE-5652-v10.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4601//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4601//console

This message is automatically generated., Thanks for the contribution and patience with multiple reviews, Jason.

Just committed this to trunk and branch-2., FAILURE: Integrated in Hadoop-Yarn-trunk #561 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/561/])
MAPREDUCE-5652. NM Recovery. ShuffleHandler should handle NM restarts. (Jason Lowe via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594329)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/LICENSE.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobID.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/proto/ShuffleHandlerRecovery.proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/pom.xml
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1779 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1779/])
MAPREDUCE-5652. NM Recovery. ShuffleHandler should handle NM restarts. (Jason Lowe via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594329)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/LICENSE.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobID.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/proto/ShuffleHandlerRecovery.proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/pom.xml
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1753 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1753/])
MAPREDUCE-5652. NM Recovery. ShuffleHandler should handle NM restarts. (Jason Lowe via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594329)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/LICENSE.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobID.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/proto/ShuffleHandlerRecovery.proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/pom.xml
, SUCCESS: Integrated in Hadoop-trunk-Commit #5605 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5605/])
MAPREDUCE-5652. NM Recovery. ShuffleHandler should handle NM restarts. (Jason Lowe via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594329)
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/LICENSE.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/dev-support/findbugs-exclude.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobID.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/pom.xml
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/proto/ShuffleHandlerRecovery.proto
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestShuffleHandler.java
* /hadoop/common/trunk/hadoop-mapreduce-project/pom.xml
]