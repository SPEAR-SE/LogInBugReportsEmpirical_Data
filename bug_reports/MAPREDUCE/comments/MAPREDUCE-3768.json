[Eli - Do we know why Hive behaves this way? We've seen significant gains, with literally thousands of jobs, with MAPREDUCE-2450. Maybe something odd with Hive?, I don't. This is a pretty straightforward query and it runs 7x faster on MR1 and the same MR2 build w/o MR-2450 so I suspect MR-2450 had an unintended side-effect, making a thousand jobs faster doesn't imply it didn't introduce a new bug., Eli, the question was, did you look into the root cause. Yes, it could have caused a regression or just exposed a bug in Hive - I've seen several examples of both., Here's a jstack during test execution. Possible that it could be a bug in Hive but more likely an issue of 2450 interacting with the LocalJobRunner.

{noformat}
"LocalJobRunner Map Task Executor #0" prio=5 tid=102804800 nid=0x10ef1e000 in Object.wait() [10ef1d000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7bd6c6890> (a java.lang.Object)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.mapred.Task$TaskReporter.stopCommunicationThread(Task.java:726)
	- locked <7bd6c6890> (a java.lang.Object)
	at org.apache.hadoop.mapred.Task.done(Task.java:935)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}, Thanks, I'll take a look., Raw patch. Eli, could you please try this out with Hive.
The patch interrupts the TaskReporter sleep., I don't have cycles right now but you can reproduce with the above hive command w/ and w/o your patch.   , The patch essentially gets rid of upto 3 seconds per task. Looking at the trace, it seems related. OTOH, noticed that the runtime difference was 2 minutes versus 15 minutes. Unless this hive test runs a large number of jobs / task waves - this patch probably won't fix the issue. Have opened a separate jira for the 3 second wait - MAPREDUCE-3809.
Haven't had a chance to run the hive test. Don't have the cycles rightnow to understand how the hive build system deals with 23 and local jars. , Eli,
Please test and confirm as we do not have cycles now., Also, given MAPREDUCE-3809 is in, we believe this is fixed. Hence reducing the priority, please reprioritize if it isn't fixed yet. Thanks.]