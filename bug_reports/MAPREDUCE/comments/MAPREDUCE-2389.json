[Attached is the partial output of a systemtap script I wrote which tracks all access to files named *.out.index. You can see the file is opened and successfully read a couple of times, but the third time, it gets a spurious EOF reading the last 8 bytes of the file.

I manually verified this file on disk and it is not actually truncated or modified in any way., Easiest way to reproduce this is to run a large sleep job on a small cluster. I've been using sleep -mt 1 -rt 1 -m 10000 -r 10000 on 5 node clusters. In such a job I usually see 100-200 of these failures.

Exception trace:

Map output lost, rescheduling: getMapOutput(attempt_201103152313_0001_m_000591_0,437) failed :
java.io.IOException: Error Reading IndexFile
	at org.apache.hadoop.mapred.IndexCache.readIndexFileToCache(IndexCache.java:113)
	at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:66)
	at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:3488)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:816)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at java.io.DataInputStream.readLong(DataInputStream.java:399)
	at org.apache.hadoop.mapred.SpillRecord.<init>(SpillRecord.java:74)
	at org.apache.hadoop.mapred.SpillRecord.<init>(SpillRecord.java:54)
	at org.apache.hadoop.mapred.IndexCache.readIndexFileToCache(IndexCache.java:109)
	... 23 more
, Smells sort of similar to this JVM bug: http://bugs.sun.com/view_bug.do?bug_id=6693490
Also reproduced this using a recent build of JDK7, though..., Improved my systemtap script to include calls to dup2

[12909] Wed Mar 16 20:59:27 2011: open "/data/4/scratch/todd/haus-mrlocal/taskTracker/todd/jobcache/job_201103161358_0001/attempt_201103161358_0001_m_000339_0/output/file.out.index", O_RDONLY 163
[12909] Wed Mar 16 20:59:27 2011 - dup2  66 -> /data/4/scratch/todd/haus-mrlocal/taskTracker/todd/jobcache/job_201103161358_0001/attempt_201103161358_0001_m_000339_0/output/file.out.index 163

corresponds almost exactly to:
2011-03-16 13:59:27,994 WARN org.apache.hadoop.mapred.TaskTracker: getMapOutput(attempt_201103161358_0001_m_000339_0,22) failed :
java.io.IOException: Invalid request  Map Id = attempt_201103161358_0001_m_000339_0 Reducer = 22 Index Info Length = 0

So almost certainly due to a JVM bug with NIO using dup2 in a racy way that overwrites an open fd!, Well, this is interesting. I rolled back my Jetty to 6.1.14 (from 6.1.26) and the problem seems to have gone away.

So, something that changed between Jetty versions seems to trigger the JVM bug more often., We are also seeing this exception with CDH3 (0.20.2+923.21-1~maverick-cdh3), John: as a workaround you can roll back Jetty to 6.1.14 by replacing the jar in the lib directory. Is this causing jobs to fail for you or just the occasional task? What OS and JVM version?, Todd: Thanks we will try that.  Most of the time it seems to just be causing occasional task failure, but we have also seen the exception happen on a handful of slaves simultaneously and eventually push the jobtracker to run out of heap.  Right now our heap is 1gb for the jobtracker with 90 slaves so we'll try increasing it and see if the problem persists with that and the older version of Jetty.

OS: Ubuntu 10.10
JVM:
java version "1.6.0_24"
Java(TM) SE Runtime Environment (build 1.6.0_24-b07), I also have the same problem on CDH3u0 and was able to workaround it by reverting Jetty 6.1.26 to Jetty 6.1.14.

Environment:
Ubuntu 10.04 x64
Java(TM) SE Runtime Environment (build 1.6.0_15-b03), I've seen the same exact "Map output lost" exception as Todd using Amazon EMR
It wouldn't matter that much except that it occurs during the reduce phase after all the map tasks have already completed.  This causes the entire job to stall while the one map task is re-run and it's frequent enough to basically grind the entire job to a halt.
Does anyone know if there's a way to use an older version of Jetty with Amazon EMR?
, Is there any update to this? We are hitting this on EMR, with 20-30 exceptions per job., Still unclear why this happens on Jetty 6.1.26, but in CDH we've been shipping a custom build of Jetty for a while now - see https://issues.apache.org/jira/browse/MAPREDUCE-2980 -- and it seems to fix the issue.]