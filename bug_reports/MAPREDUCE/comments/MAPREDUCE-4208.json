[ccw,

Without a heap dump, a stack trace, or even process logs for the hung process I am not really sure what more we can do but tell you that it looks like it was an intermittent issue, because it recovered after rerunning the task.  The task should have timed out after a default of 10 mins with no progress being made, but I don’t know if you disabled that, or if there is something the mr code is doing to prevent it.  If you can post some of that information to this JIRA we might be able to help, otherwise I think we will have to close this as Incomplete., OK，I will try to reproduce this problem and tell you more message about it tomorrow., I checkd all logs,but there is no error .I want to know why each job encounter this problem until I kill one or more child process. I set the property 'mapred.task.timeout' ,but it is unhelpful for this.  I know the child task is not dead , but also it never work. It is terrible that every time I need to kill the hanging task.I think  it need to be controlled by JT or TT., So have you been able to reproduce it?  I also think that it is a really bad bug that I want to fix, but I don't know where to start debugging it without more information about how it happened., Thank you! I found the cause of the problem . The fundamental reason is that data access of Hbase . Because the data(region server) of map task is not on local region server ,so each data access is very slow .
But I think there must be a timeout mechanism to prevent the occurrence of such events. If a task is running too slow ,Hadoop should automatically kill the task rather than manually kill.
Thank you!, Hi ccw:
 whether the speculative is on? and the speculative is useful for your scenario?

I think that the timeout mechanism is not good. because that the service will spend 

long time indeed.

so, if kill the task by timeout mechanism, and the job is failed in fact.
and is the same with killing the job.


so, I suggest that it is suitable for user's application to kill the job when the job does not complete for long time.
, There are two types of timeouts that happen for a task.  The first is a user configurable timeout with a default of 10 min.  If a task does not make any progress for 10 min then the task will timeout.  Progress is defined as reading a record of input, writing a record of output, or explicitly in your map/reduce code calling a method in the Context to indicate that you are still making progress. 

The other timeout is that if your task does not heartbeat into the AM for 5 min the task is assumed to be dead.

In your case you are still making progress, it is just very slow, so neither of the timeouts would take place.  You could possible set the progress interval to be something very small and that would probably detect the problem.  We do not want to change the default value for the progress interval because there are lots of jobs that may be doing significant amounts of computation for each record, and we don't want to kill them off.

mapreduce.task.timeout is the timeout you would want to configure.]