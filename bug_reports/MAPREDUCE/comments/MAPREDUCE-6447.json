[https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java#L254  
   return  (requestedSize < maxSingleShuffleLimit) ; =>  return (requestedSize < maxSingleShuffleLimit) &&((usedMemory +requestedSize)< memoryLimit) ;, the default  mapreduce.reduce.shuffle.input.buffer.percent is 0.9 ,and the default mapreduce.reduce.shuffle.memory.limit.percent is 0.25。
if we want to avoid the issue，need to decrease the value。
 
eg: mapreduce.reduce.shuffle.input.buffer.percent =0.6, Hi,shuzhangyao.I have shared the same experiences. I ever thought about  your method .This method is mandatory to limit within the memoryLimt.  It maybe is effective. But I think that the original code  is  reasonable .  We take the default parameter to calculate it.  maxSingleShuffleLinit = memorylimit  * 0.25. The number of Fetcher is 5. memorylimit =  Runtime.getRuntime().maxMemory() * 0.7 . While all fetcher is working, 5*0.25*0.7 <1,in theory,it does not occur the OutOfMemory of Java heap. Even though we do not add the code "usedMemory +requestedSize)< memoryLimit" ,in theory ,it should not occur this phenomenon of Outof Memory. We can talk about this problem. whether It be casued by Jvm and  unreasonable allocation of memory in  special data input ？

, the default mapreduce.reduce.shuffle.input.buffer.percent is 0.9,so memorylimit = Runtime.getRuntime().maxMemory() * 0.9 .
0.9*0.25*5=1.125>1., the default mapreduce.reduce.shuffle.input.buffer.percent is 0.9,so memorylimit = Runtime.getRuntime().maxMemory() * 0.9 .
0.9*0.25*5=1.125>1., the default mapreduce.reduce.shuffle.input.buffer.percent is 0.9,so memorylimit = Runtime.getRuntime().maxMemory() * 0.9 .
0.9*0.25*5=1.125>1., Hi guys, thank you for reporting this issue. Do you mean that should we fix the default value to avoid the exception on this jira?, yes, for posterity, Jason Zhu does a pretty good job of root-causing this on his blog - [1]

tl;dr - 
{quote}
Consequently, the product of 'mapreduce.reduce.shuffle.input.buffer.percent' multiplies by 'mapreduce.reduce.shuffle.parallelcopies' should not be greater than 1, or, in the extreme case, OOM will be thrown provided all mappers complete at the same time and all the outputs is in the manner of in-memory shuffling.

But here's a compromise between OOM and throughput. The product can be greater than 1 if some OOMs is tolerable.
{quote}

[1]: http://jason4zhu.blogspot.com/2014/11/shuffle-error-by-java-lang-out-of-memory-error-java-heap-space.html, Hi guys,
I'm experiencing exactly the same issue, even I decreased the values following way:
mapreduce.reduce.shuffle.input.buffer.percent=0.70
mapreduce.reduce.shuffle.memory.limit.percent=0.2
mapreduce.reduce.shuffle.parallelcopies=4 

According to the above calculations 0.7 * 0.2 * 4 = 0.56 < 1

Could someone explain it?


I'm running very long MapReduce process, where some of the reducers almost deterministically fail on OOME, repeated attempts still cause OOME and the whole restarted application with reduced shuffle parameters still fails (after several days of computation).

Is there any reason why InMemoryMapOutput blindly allocates memory despite the actual situation?
Could Fetcher.copyMapOutput recover from that situation by falling into an alternative MapOutput?

Thanks , With relation to this issue I have manually performed a kind of configuration "bracketing", where I manually altered the job configuration of a still running application before another attempt.
To do it manually is a bit complicated as you have to modify the job configuration in HDFS stagging folder of the application as well as on each data node in the local appcache/<app>/filecache.

However what if there would be a mechanism to prepare such alternate configuration(s), which will be used for second and following attempts after a task fail?

For example in job configuration I would set:
mapreduce.reduce.shuffle.input.buffer.percent=0.70
mapreduce.reduce.shuffle.memory.limit.percent=0.4
mapreduce.reduce.shuffle.parallelcopies=30
attempt-2.mapreduce.reduce.shuffle.input.buffer.percent=0.70
attempt-2.mapreduce.reduce.shuffle.memory.limit.percent=0.2
attempt-2.mapreduce.reduce.shuffle.parallelcopies=4
attempt-3.mapreduce.reduce.shuffle.input.buffer.percent=0.20
attempt-3.mapreduce.reduce.shuffle.memory.limit.percent=0.2
attempt-3.mapreduce.reduce.shuffle.parallelcopies=2

Such mechanism should not be hard to implement and it would allow to have alternate fail-over configurations prepared in advance instead of long-term repeated bug hunting.

Thanks, [~prateekrungta] and [~shuzhangyao]: I have seen this same issue a number of times and people keep referring to this open MR issue.
I have dug into this and have found that there is nothing wrong with the calculation and there is no need for a change in the way we handle this in the code. There is no guarantee that any method devised for the internal calculation will guarantee that you do not get an OOM error. In all cases I have run into I have been able to fix it with a configuration change in MR and the JVM.

Let me explain what I have found and why the issue will not be solved by changing the internal calculations.

When the JVM throws an OOM for a reducer I collected a heap dumps and looked at what was allocated at the point in time that the OOM was thrown. It most cases the OOM was not thrown due to the total heap being used. As an example: the JVM heap for the reducer was set to 9216MB or 9GB the heap dump showed only a 5896M heap usage. Looking at the usage of the heap it showed that the shuffle input buffer usage was well within its limits.
We then tried to lower the {{mapreduce.reduce.shuffle.input.buffer.percent}} from the default 0.9 to 0.6 and found that it did not solve the issue. There was still an OOM around the same point with approximately the same usage of the heap. Lowering it further to 0.4 allowed the job to finish but we saw that the JVM never peaked above about 60% of the assigned heap. This causes a lot of waste on the cluster and is thus not a solution we could accept.
Further checks of the GC logging showed that all heap usage was in the old generation for each of the OOM cases. That explains why the reducer was throwing an OOM and the heap dump: it had run out of space in the old generation, not the total heap. Within the heap the old generation can take about 2/3 of the total heap. This is based on the default settings for the generation sizing in the heap [1].

The question then became what caused the JVM to run out of old generation and not using its young generation? This could be explained by the logging from the reducer we had. The reducer logs showed that it was trying to allocate a large shuffle response. In my case about 1.9GB. Eventhough this is a large shuffle response it was within all the limit. The JVM will allocate large objects often directly in the old generation instead of allocating it in the young generation. This behaviour could cause an OOM error in the reducer while not using the full heap and just running out of old generation.

Back to the calculations. In the buffer we load all the shuffle data but we set a maximum of 25% of the total buffer in one shuffle response. This is the in memory merge limit. If the shuffle response is larger than 25% of the buffer size we do not store it in the buffer but directly merge to disk. A shuffle response is only accepted and downloaded if we can fit it in memory or if it goes straight to disk. The check and increase of the buffer usage happens before we start the download. Locking makes sure only one thread does this at a time, the number of paralel copies is thus not important. Limiting could lead to a deadlock as explained in the comments in the code. Since we need to prevent deadlocks we allow one shuffle (one thread) to go over that limit. If we would not allow this we could deadlock the reducer. The reducer would be in a state that it can not download new data to reduce. There would also never be a trigger that would cause the data in memory to be merged/spilled and thus the buffer stays as full as it is.

Based on all that the maximum size of all the data we could store in the shuffle buffer would be:
{code}
mapreduce.reduce.shuffle.input.buffer.percent = buffer% = 70%
mapreduce.reduce.shuffle.memory.limit.percent = limit% = 25%
heap size = 9GB
maximum used memory = ((buffer% * (1 + limit%)) * heap size) - 1 byte
{code}
If that buffer does not fit in the old generation we could throw an OOM error without really running out of memory. This is especially true when the individual shuffle sizes are large but not hit the in memory limit. Everything is still properly calculated and limited. We also do not unknowningly use more than the configured buffer size. If we go over we know exactly how much.

The way we worked around the problem without increasing the size of the heap. We did this by changing the generations. The old generation inside the heap was changed by increasing the "NewRatio" setting from 2 (default) to 4. We also changed the "input.buffer.percent" setting to 65%. That worked in our case with the 9GB as the maximum heap for the reducer. Different heap sizes combined with a different "memory.limit.percent" might require a different "Newratio" setting.

[1] https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/sizing.html]