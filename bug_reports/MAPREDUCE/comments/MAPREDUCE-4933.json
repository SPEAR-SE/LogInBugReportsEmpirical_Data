[Can we really lose map data? If so this is a Blocker not a Major., If I understand how things work, yes.  The length is used to calculate onDiskBytes.  If onDiskBytes is 0, (which can happen falsely without the patch), the following code won't get called:
{code}
        final int numInMemSegments = memDiskSegments.size();
        diskSegments.addAll(0, memDiskSegments);
        memDiskSegments.clear();
        RawKeyValueIterator diskMerge = Merger.merge(
            job, fs, keyClass, valueClass, codec, diskSegments,
            ioSortFactor, numInMemSegments, tmpDir, comparator,
            reporter, false, spilledRecordsCounter, null);
        diskSegments.clear();
        if (0 == finalSegments.size()) {
          return diskMerge;
        }
        finalSegments.add(new Segment<K,V>(
              new RawKVIteratorReader(diskMerge, onDiskBytes), true));
{code}

which if I understand correctly means that some on-disk data won't be incorporated in the final merge., The background to coming across this was trying to apply MAPREDUCE-2264, which caused a bunch test failures for me due to missing map data.  MAPREDUCE-2264 annotates the FileStatus with extra data, but doesn't change the timing of writer.close()/getFileStatus().  The code has been around since 2008, and must be working, so I would not be surprised if the map data loss is a false alarm, but at least for MAPREDUCE-2264's sake, it should be fixed nonetheless. , The patch looks right to me even if it is not losing data in practice. Bobby, would you agree? , I agree that the patch looks correct, and should go in either way.  I just wanted to know how critical this is, and it looks critical., Oh by the way +1.

I am happy to check this in, but I just added [~mattf] so he can say better exactly where he wants us to check this into.  For the mean time I am just going to put it into branch-1 (1.2.0-SNAPSHOT)., great, thx. +1, I'll commit it in a few., Thx Bobby, please go ahead and commit it then., OK I have dug into this a bit more and I am not as scared as I was previously.  I could definitely be wrong so if someone could confirm my analysis on this I would appreciate it.

So there are two places where we could get issues.  The first is because we get the file size after writing and without closing the file.  This is not a huge issue because that size is only used to approximate how much space will be needed for the final merge file, which we just wrote out so who cares.

The second case is a race between writing the merge data out to a file and reading it back in.  This will only happen on the final merge when the reducer has too much buffered in memory and we need to spill to disk (we exceeded mapred.job.reduce.input.buffer.percent).  But even then it becomes a race between java's buffered IO getting pushed out to disk and another part of the code reading it back in.  I have a hard time believing that we are going to lose that race ever.  i think we can move the back to major, sorry about scaring everyone., Sandy - have you seen this in branch-2?, It's not a problem in branch-2. The corresponding code, in MergeManager#finalMerge, does the correct thing, i.e. doesn't ask for the file's length before closing it., When using a blobstore as the destination -such as the s3:// filesystem, none of the data is written until the stream is closed -the result of the {{getFileStatus()}} call could even be a file not found.

, added 1.2.0 to fixversion, per CHANGES.txt and above comments., Closed upon release of Hadoop 1.2.0.]