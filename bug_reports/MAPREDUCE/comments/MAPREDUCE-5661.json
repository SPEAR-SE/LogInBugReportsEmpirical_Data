[Note that YarnChild.configureLocalDirs sets this property based on an environment variable, which is itself derived from yarn.nodemanager.local-dirs, and therefore most of the other references are really coming from what was specified in yarn.nodemanager.local-dirs and not what was configured by an admin.  The notable exception would be jobs run in local mode.

Also note that the shuffle handler is a bit special in that it is the one piece of MapReduce code that runs as part of the YARN nodemanager process and not as part of a job or a client.  It is more likely yarn.nodemanager.local-dirs is configured on a particular YARN node than mapreduce.cluster.local.dir, so I think it's appropriate that variable is used in the shuffle handler case.  I don't think mapreduce.cluster.local.dir is even set on some of our clusters, as the MapReduce framework configures this variable for tasks when running under YARN.  I wouldn't expect it to have to be configured by admins at all unless supporting jobs in local mode and for some reason the default isn't sufficient., The description for {{mapreduce.cluster.local.dir}} implies that that directory will receive significant load:

{code:xml}
<property>
  <name>mapreduce.cluster.local.dir</name>
  <value>${hadoop.tmp.dir}/mapred/local</value>
  <description>
      The local directory where MapReduce stores intermediate
      data files.  May be a comma-separated list of
      directories on different devices in order to spread disk i/o.
      Directories that do not exist are ignored.
  </description>
</property>
{code}

Since you are suggesting that the default (typically in /tmp) is sufficient, perhaps that description should be altered?  I'm observing that the shuffle is creating the majority of the disk I/O in my MapReduce jobs, which is using the {{yarn.nodemanager.local-dirs}}., Yes, in a YARN cluster the majority of the I/O on a node for MapReduce should be in the yarn.nodemanager.local-dirs directories.  Those settings are replicated to mapreduce.cluster.local.dir during task startup under YARN, so admins don't normally have to configure mapreduce.cluster.local.dir.  I would expect an explicit setting of mapreduce.cluster.local.dir to only take effect when running a job in local mode, which usually isn't a very big job and therefore the default of somewhere under /tmp is probably fine for most of those cases.

So to sum up, tasks are using mapreduce.cluster.local.dir but the directories listed there are derived from yarn.nodemanager.local-dirs in a YARN cluster.  Setting mapreduce.cluster.local.dir in mapred-site.xml would have no effect for most MapReduce jobs in a YARN cluster., {quote}
Note that YarnChild.configureLocalDirs sets this property based on an environment variable
{quote}

Ah.  I see now.  I didn't realize that "this property" meant {{mapreduce.cluster.local.dir}}.  Got it, thanks., [~jlowe], given the explanation above, should this JIRA be closed?, Closing as shufflehandler should continue to use the yarn property since it's part of the nodemanager process., The description for mapreduce.cluster.local.dir (below) is misleading and could be improved by saying it is NOT a replacement for deprecated property mapred.local.dir. 

Unfortunatley, Link http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/DeprecatedProperties.html 
lists mapreduce.cluster.local.dir as the replacement for mapred.local.dir.  Instead it should explain intermediate files will be written
to yarn.nodemanager.local-dirs.

<property>
  <name>mapreduce.cluster.local.dir</name>
  <value>${hadoop.tmp.dir}/mapred/local</value>
  <description>
      The local directory where MapReduce stores intermediate
      data files.  May be a comma-separated list of
      directories on different devices in order to spread disk i/o.
      Directories that do not exist are ignored.
  </description>
</property>]