[MAPREDUCE-2219 wasn't committed to 0.20.205, so that's not the cause of this... or did you mean it might be a solution?, Hi, sorry, I thought MAPREDUCE-2219 had introduced the permissions check on mapred.system.dir (resetting it to 700 on jobtracker startup) but I didn't read it closely enough. You are correct, it is not the cause of this.

I think a potential solution is to have the jobtracker not reset the permissions on mapred.system.dir on startup although there may be other good reasons to have the permissions set the way they are., We are able to submit jobs as non super user without having to change the perm of the mapred.system.dir.

Could you let us know what the value for "mapreduce.jobtracker.staging.root.dir" and what paths were the mapreduce jobs reading/writing. Mapreduce needs execute permissions on the paths its processing as the appropriate user., <property>
  <name>mapreduce.jobtracker.staging.root.dir</name>
  <value>/user</value>
</property>

Otherwise, I need to change the permissions on the tmp directory also

The paths M/R jobs are reading/writing are specific to the user, just using hadoop examples like the pi job and terasort, Added 1.0.0 to list of releases after reinstalling test cluster and hitting same problem, Actually in 1.0.0, this problem doesn't occur as long as you add the following to mapred-site.xml

<property>
<name>mapreduce.jobtracker.staging.root.dir</name>
<value>/user</value>
</property>

Maybe this should be documented in https://hadoop.apache.org/common/docs/r1.0.0/cluster_setup.html - that would probably be sufficient (or else maybe change that to the default behaviour?), Sorry for the chatter here, but after formatting my 1.0.0 test cluster and restarting, I ran into the problem again.

It turns out the problem goes away if you run a m/r job as hadoop superuser locally, before allowing remote users to run jobs - I guess some directory or file is getting created during this process. Happy to provide more info if needed., I see same problem as noted above in both v1.0 and v1.0.2

Running the first map reduce job as user "mapred" fixed the problem., You must be using fairscheduler? If so the problem is resolved by MAPREDUCE-4451 (yes it affects non-kerberos config as well)., Closed upon release of Hadoop 1.2.0.]