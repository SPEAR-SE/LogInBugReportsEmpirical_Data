[Attached a revert patch., Looks like FrameworkCounterGroup needs to be updated to protect itself from out-of-bounds counter indices coming from the task report.  Until that has been rolled out as a baseline we can't ever safely add new counters without breaking this code (or using a distributed cache deploy for MapReduce, see below).  Ultimately moving to protocol buffers would be even better, but that's not something we could accomplish until a major release like Hadoop 3.0.

I should point out that jobs would not be susceptible to this failure if they deployed MapReduce via HDFS rather than picking it up from the nodes.  See http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html.  That deployment method has the benefit that a job always runs with a consistent version of MapReduce, and therefore changes like this _would_ be OK during rolling upgrade.  Jobs would either run entirely with the old MapReduce version or the new one instead of a hodgepodge of both which could lead to errors like this.

If distributed cache deploy is not sufficient and this still needs to be reverted then normally we would simply revert MAPREDUCE-6829 from branch-2, remove 2.9.0 from the fix version of that JIRA, and mark this as a duplicate of that JIRA.  2.9.0 never shipped, so we don't need a tracking JIRA to note something was removed since we never shipped a 2.x release saying it was added., Thank you, Jason. I agree the distributed cache deployment solves this the issue, however I am not sure how many users use the other option. I think we should revert from branch-2. My understanding is that there is no rolling upgrade requirement from 2.x to 3.0., OK I'll revert MAPREDUCE-6829 from branch-2 and update that JIRA accordingly., Thank you, [~jlowe]!]