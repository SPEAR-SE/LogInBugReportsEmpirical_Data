[We are seeing the exact same thing. After a lot of experimenting, it seems to happen consistently when we set the number of reducers very large for a given job/data set. 

After increasing the number of slots with these parameters:
yarn.nodemanager.resource.memory-mb
mapreduce.tasktracker.map.tasks.maximum
mapreduce.tasktracker.reduce.tasks.maximum

We were able to run over 100 reducers for wordcount. It now occurs for wordcount when reducers are over 500. However it also occurs with other larger jobs and only 30 reducers.

Any help, is much appreciated. , Any update on this issue..? had seen similar issue,,can we introduce  some config like {{mapreduce.job.end.wait-time.millis}} and make it configurable..? , [~brahmareddy], If you would like to provide a patch for this, Please take it. Thanks., Uploaded the draft patch.Kindly review once.., I'm confused as to why increasing the sleep period is an appropriate fix for this.  Even if the AM doesn't stick around the job client should be redirected to the history server if the AM has already exited.  Is the job history not correct on this state as well?

Normally for a job to fail at least one task fails (ignoring the cases where we fail during job init or job commit).  Can someone explain the sequence of events that allows the job to be marked failed due to task failure but no tasks are in the FAILED state?  Normally a job will fail because a task reported failure, and at that point that task should be in the FAILED state.  Is there an AM log or some other evidence that shows the sequence of state transitions that leads to this problem?, I faced this issue long time back during the version 2.0.4-alpha. I don't think it is still an issue with the changes happened over the time.

bq. Is the job history not correct on this state as well?
Yes, the job history was also showing the job state as FAILED and the task state as KILLED but we could see the attempt for the task as succeeded.

I don't have logs for this issue since it happened some time ago. [~brahmareddy], Can you give details or share the log for the issue you are facing if you have anything?
]