[Snippet from log of an AM that hit this in practice.  Note that the task container launch context error bubbles all the way up to the AsyncDispatcher which ends up shutting down the JVM.

{noformat}
2013-02-08 08:01:53,714 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
org.apache.hadoop.yarn.YarnException: java.io.FileNotFoundException: File does not exist: hdfs://xx:xx/user/someuser/somepath/somejar.jar
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createCommonContainerLaunchContext(TaskAttemptImpl.java:672)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext(TaskAttemptImpl.java:720)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1246)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1218)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:957)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:138)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:996)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:988)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.FileNotFoundException: File does not exist: hdfs://xx:xx/user/someuser/somepath/somejar.jar
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:738)
	at org.apache.hadoop.fs.FileSystem.resolvePath(FileSystem.java:682)
	at org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(MRApps.java:371)
	at org.apache.hadoop.mapreduce.v2.util.MRApps.setupDistributedCache(MRApps.java:319)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createCommonContainerLaunchContext(TaskAttemptImpl.java:642)
	... 14 more
2013-02-08 08:01:53,715 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
2013-02-08 08:01:53,716 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler.
2013-02-08 08:01:53,716 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that iSignalled is: true
2013-02-08 08:01:53,716 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true
{noformat}

The JVM shutdown makes the AM think it was killed when really it was an IOException while setting up a task container launch context., Please explain the use case.
All I can understand is you wrote MR job which is trying to access some jar in hdfs and if that the respective jar does not exist the MR-AM is dying with above stacktrace?
, Though I think the issue is in below code.
Class: TaskAttemptImpl
Function: createCommonContainerLaunchContext
code:

    } catch (IOException e) {

      throw new YarnException(e);
    }


But I need to replicate the issue in-order to fix it.
Could you please suggest the step you are doing, so I can replicate the bug

Thanks
Abhi
, I'm not exactly sure what happened in this case, as I'm just documenting the poor error handling by the AM on a job I was asked to analyze.  From the stacktrace it looks like the AM was trying to setup the common portion of the task launch contexts and encountered an IOException while processing distributed cache files because they were deleted.  Maybe someone submitted a job whose distributed cache files in HDFS were deleted while the job was still in-flight?

Anyway the problem is, as you point out, that the AM is not properly handling exceptions while setting up the common container launch context for tasks.  If an error occurs while setting that up, it should fail the job with the job diagnostics indicating the exception message and stacktrace rather than simply exiting with no diagnostics., Though I am not sure who is handling the cleanseing of exception handling in AM, but as for now, we can set diagnostics in catch statement of the below code.
If some one is dealing with excpetion handling I can take up the task for the same :) 

Let me know., Setting the diagnostics isn't sufficient if we then continue to throw the exception.  As mentioned above, the exception ends up bubbling all the way up to the AsyncDispatcher handler thread which forcibly exits the process.  That's not good and leads to a misleading status like KILLED.  There needs to be better exception handling as well as diagnostics., If the jar required for the job gets deleted in the middle while job is running, then job should be failed and AM for the specific job should die, isn't it  a expected behaviour? 
If it is a expected then we can gracefully kill the AM with an appropriate diagnostics.

Please correct me if am wrong.
, Yes, the AM cannot succeed if it cannot create the common task container launch context.  However the whole point of this JIRA is that it should mark the job as FAILED or ERROR with an appropriate diagnostic message for the application rather than marking the job as KILLED.  The latter status leads users to believe someone or something killed the job which is not the case.]