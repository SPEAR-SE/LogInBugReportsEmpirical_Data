[Verified distributed shell its working fine .. 
{code}
./hadoop org.apache.hadoop.yarn.applications.distributedshell.Client -jar ../share/hadoop/yarn/hadoop-yarn-applications-distributedshell-*.jar -shell_command ls -container_memory 1024 -master_memory 1024
{code}
Only for mapreduce application seems too have problem., Hello [~bibinchundatt].  The exception looks like an incompatibility between client and server protobuf RPC message definitions.  Can you please share what version of Hadoop is showing this problem?  Are you running different versions for different components?, [~cnauroth]
Using trunk build with secure. Both client and server are of same version. Can HADOOP-12579 cause this?
, {{WritableRpcEngine}} is removed as part of HADOOP-12579 the same is causing this to fail.The below check is not available as part of {{WritableRpcEngine}} .
{code}
      if (args.length != 2) { // RpcController + Message
        throw new ServiceException("Too many parameters for request. Method: ["
            + method.getName() + "]" + ", Expected: 2, Actual: "
            + args.length);
      }
      if (args[1] == null) {
        throw new ServiceException("null param while calling Method: ["
            + method.getName() + "]");
      }

{code}
, YarnChild uses TaskUmbilicalProtocol which still relies on WritableRpcEngine. This must be handled before removing the engine. It involves major development work rather than just a bug fix, so I guess we have to revert HADOOP-12579. Any comment?, HADOOP-12579 was reverted to avoid this and MAPREDUCE-6706 was opened to migrate TaskUmbilicalProtocol. I guess this will be resolved as a duplicate? [~asuresh], any comment? Thanks!, [~drankye]
Since MAPREDUCE-6706 is available to track the {{ProtocolRPCEngine}} change, we can close this JIRA. Thank you for handling the revert.
, Sorry for the late response, [~bibinchundatt]. Yes I think this should be closed., Resolved this as a duplicate.]