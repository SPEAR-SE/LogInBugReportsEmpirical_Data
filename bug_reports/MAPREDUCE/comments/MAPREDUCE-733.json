[Just looked at the code causing this. This happens whenever there is an attempt to unreserve a job's tasks from a TaskTracker even though the reservation is for a job other than this job. This supposedly must have been done during MAPREDUCE-516 itself, but unfortunately missed (https://issues.apache.org/jira/browse/MAPREDUCE-516?focusedCommentId=12721792&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12721792).

The resultant behavior is that when a task fails, one heartbeat of the TT is missed, but the next heartBeat passes through. This is because the first heartBeat marks the task as FAILED on the JobTracker and so the faulty code isn't invoked for the same TT again in further heartBeats. This leaves inconsistent state on the JT, for e.g, immediately following this is the code for creation of task completion event which would never be created for this task. This issue HAS to be fixed immediately because of the side effects.

One more thing I've observed while going through this is that reservations are not removed on a TaskTracker that is globally blacklisted either via large task-failure count or via unhealthy status., This patch seems to fix the expection, though I'm not sure why the test-case doesn't fail even with this exception.

The crux of the patch is:

{code}
-          taskTracker.unreserveSlots(TaskType.MAP, this);
-          taskTracker.unreserveSlots(TaskType.REDUCE, this);
+          if (trackersReservedForMaps.containsKey(trackerName)) {
+            taskTracker.unreserveSlots(TaskType.MAP, this);
+          }
+          if (trackersReservedForReduces.containsKey(trackerName)) {
+            taskTracker.unreserveSlots(TaskType.REDUCE, this);
+          }
{code}

----

I've also noticed that JobInProgress.addTrackerTaskFailure wasn't synchronized which breaks our invariant of locking JobTracker and JobInProgress (in-order), in particular from the call originating at JobTracker.lostTaskTracker. Maybe it wasn't necessary before - anyway it's been fixed now., bq. One more thing I've observed while going through this is that reservations are not removed on a TaskTracker that is globally blacklisted either via large task-failure count or via unhealthy status.

I swear I saw globally blacklisted trackers being declared 'lost' (via JobTracker.lostTaskTracker)! Maybe I'm just getting old... *smile*

Anyway, I'm very surprised that the information about a tracker being globally blacklisted (and declared 'unhealthy' ?) isn't propagated to the job (via JobInProgress.addTrackerTaskFailure). This is a serious drawback of the current implementations of the feature(s). This isn't 'critical' but I think we should address them asap via a separate jira. Thoughts?, bq. tracker being globally blacklisted (and declared 'unhealthy' ?) isn't propagated to the job (via JobInProgress.addTrackerTaskFailure).
Do you mean to say that for any job that is currently running *and* for future jobs, this should be done? Is there a good use case for this? The reason i am asking this is because globally blacklisted trackers are not considered for assigning new tasks at all. We may run into race conditions (especially for future jobs), where a globally blacklisted tracker may not be considered by jobs for assigning new tasks to, even when it is marked healthy (globally). For example, a job starts an hour before the tracker is supposed to be marked healthy, but since the job blacklists it prematurely, even after it is marked healthy, the job cannot make use of this tracker. This can probably be handled but it might complicate the logic of global blacklisting & per-job blacklisting., bq. Do you mean to say that for any job that is currently running and for future jobs, this should be done?

I meant for jobs which currently have tasks scheduled on the faulty tasktracker... , Patch for the yahoop hadoop-20 branch., bq. One more thing I've observed while going through this is that reservations are not removed on a TaskTracker that is globally blacklisted either via large task-failure count or via unhealthy status.

I had filed MAPREDUCE-682 for tracking this. Arun, if you remember, we had discussed this a couple of days back. We decided it was not a major problem. For a short while these reservations might remain for the blacklisted nodes and count against the job. But the nodes which are healthy can pick up and run the tasks of the job (in the next wave), which might have run on the blacklisted trackers., These changes look good. Again, I think a test case would be helpful. For that, we'll do something similar as in MAPREDUCE-734 where we are adding a test via the FakeObjectUtilities APIs., I ran TestTrackerBlacklistAcrossJobs and it passed and logs do not have "java.io.IOException: java.lang.RuntimeException".  I also brought up a  cluster, submitted jobs randomly and killed some task attempts and did not come across this string in jobtracker log. , Attaching patch adding new test case:

The patch will apply after the patch for MAPREDUCE-734 has been applied., Added some comments to the test case., Sigh. I just realized there was a bug in the original fix as well. trackersReservedForMaps is a map of tasktrackers to FallowSlotInfo. The fix checks for presence of a tracker by name instead of the tasktracker object. So, while this will work to fix the original bug in this JIRA, it will cause a new bug that it will not remove reservations even in a valid case. Unfortunately, this indicates that we need one more test case that tests the positive condition., Fixing a bug in previous patch.

* Added new test case to test issue., Removing log statements which were not needed in previous patch., Latest patch with removing log statements., Internal Y! distribution patch, attaching output from ant test-patch
{noformat}
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 9 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}, This looks good to me. +1., Core and contrib tests passed locally., I committed this to trunk. Thanks, Arun and Sreekanth !, Integrated in Hadoop-Mapreduce-trunk #20 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/20/])
    ]