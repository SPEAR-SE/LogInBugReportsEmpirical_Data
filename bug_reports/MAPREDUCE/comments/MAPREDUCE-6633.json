[In Fetcher#copyMapOutput method, I added Exception to catch block so that it will retry on any compression related Exception.
{noformat}
  try {
        // Go!
        LOG.info("fetcher#" + id + " about to shuffle output of map "
            + mapOutput.getMapId() + " decomp: " + decompressedLength
            + " len: " + compressedLength + " to " + mapOutput.getDescription());
        mapOutput.shuffle(host, is, compressedLength, decompressedLength,
            metrics, reporter);
      } catch (java.lang.InternalError e) {
        LOG.warn("Failed to shuffle for fetcher#"+id, e);
        throw new IOException(e);
      }
{noformat}
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 17s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 36s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 25s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 27s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 18s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 39s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 29s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 31s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 27s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 25s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 16s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 33s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 27s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 27s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 20s {color} | {color:green} hadoop-mapreduce-client-core in the patch passed with JDK v1.8.0_74. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 2m 37s {color} | {color:red} hadoop-mapreduce-client-core in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 20s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 23m 20s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.7.0_95 Failed junit tests | hadoop.mapreduce.tools.TestCLI |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:fbe3e86 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12795454/MAPREDUCE-6633.patch |
| JIRA Issue | MAPREDUCE-6633 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 8e3b6fa2658d 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / e8fc81f |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_74 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6398/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6398/artifact/patchprocess/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-core-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6398/testReport/ |
| modules | C: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core U: hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/6398/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

, Thanks [~shahrs87] for reporting this issue and providing a patch.

Overall, the patch looks good. I am a little nervous about re-fetching for _any_ exception. If there is a runtime exception on the reducer (memory error, NPE, etc.), maps would be re-run unnecessarily. Although I do understand that the risk of that is low, and in any case, no data would be lost, just a little time and wasted resources. What are your thoughts?, bq.  If there is a runtime exception on the reducer (memory error, NPE, etc.), maps would be re-run unnecessarily. 
In this case the decompressor threw RuntimeException (ArrayIndexOutOfBondsException is a subclass).
If we had re run the map on another node, the job would have succeeded.

bq. I am a little nervous about re-fetching for any exception.
I understand your concern but I think its a good change according to me., Ran the failed junit failure on bith jdk7 and jdk8.
Both of them passed fine on my machine.
{noformat}
Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.54 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.tools.TestCLI
testGetJob(org.apache.hadoop.mapreduce.tools.TestCLI)  Time elapsed: 0.084 sec  <<< FAILURE!
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.mapreduce.tools.TestCLI.testGetJob(TestCLI.java:181)
{noformat}, {quote}
In this case the decompressor threw RuntimeException (ArrayIndexOutOfBondsException is a subclass).
If we had re run the map on another node, the job would have succeeded.
...
I understand your concern but I think its a good change according to me.
{quote}
Thanks [~shahrs87]]. It would be ideal to come up with a subset that would cover only the exceptions that could be thrown, but I agree that the change is fine as it is.
+1, FAILURE: Integrated in Hadoop-trunk-Commit #9586 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/9586/])
MAPREDUCE-6633. AM should retry map attempts if the reduce task (epayne: rev 1fec06e037d2b22dafc64f33d4f1231bef4ceba8)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestFetcher.java
, [~eepayne]: Thanks for the reviews and committing.
Does it make sense to fix it in 2.7 branch also ?, Thanks [~shahrs87]. I cherry picked this back to 2.7., Closing the JIRA as part of 2.7.3 release., Hi, I have recently encountered this issue. Is there a work around that doesn't involve upgrading? We don't seem to have a bad drive. Thanks, If upgrading to get the fix isn't an option (i.e.: move to Hadoop >= 2.7.3 or pulling this patch into your current release) then you could try changing intermediate codecs, e.g.: lz4, snappy, etc. to see if the problem still occurs., Thanks for replying. The table I have is just a delimited input and output table. Do you know what the reason is for the failure ? Maybe if I can understand the reason I can figure out how to get around it for my situation. Thanks, If you're getting the same stacktrace during shuffle then this typically has been a case of corrupted shuffle input due to bad hardware.  If you are getting this reliably across different nodes (both receiver and sender) then that is unlikely to be a hardware corruption issue but rather a bug in the software somewhere.  This change doesn't fix the source of the corrupted shuffle input, rather it just allows the reducer to survive the exception so it can report the fetch failure to the AM.  After enough fetch failure reports, the AM should re-run the map task which usually fixes the corrupted data and allows the job to recover.
, Thanks for all the information Jason. I'll rerun it in the morning and see if I can get it to fallback to another data node. When you say it's a bug in the software do you think the Hadoop software rather than the query? I presume a hardware corruption would be visible in ambari  and also by logging on to the data node and running a disk check? Thanks., If this is very repeatable and seemingly tied only to a specific job while others run fine then it sounds like it's not the same type of problem this JIRA was addressing, i.e.: better recovery handling for corrupted compressed files.  If it is a software bug, the stacktrace should be very indicative in this case where the bug probably lies -- if the stacktrace shows the reducer is in the shuffle phase then it is likely an issue in the Hadoop framework since it handles almost everything in the shuffle phase.  If the stacktrace indicates the reducer is in the reduce phase then the balance tips more towards user code in the reducer.  Anyway at this point this discussion is rapidly diverging from the purpose of this JIRA and should be tracked in a separate JIRA if this indeed looks like a bug in the Hadoop framework.]