[I've been doing more digging. With some understanding of what the container monitor is doing now, I went back and checked the process tree dump when my application master was killed. What I see is that there actually appear to be two processes running:

 |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
 |- 21669 21666 21666 21666 (java) 105 4 2152300544 18593 java -Xmx512M -cp ./package/* kafka.yarn.ApplicationMaster /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 2 1 1316626117280 com.linkedin.TODO 1
 |- 21666 20570 21666 21666 (bash) 0 0 108638208 303 /bin/bash -c java -Xmx512M -cp './package/*' kafka.yarn.ApplicationMaster /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 2 1 1316626117280 com.linkedin.TODO 1 1>/tmp/logs/application_1316626117280_0002/container_1316626117280_0002_01_000001/stdout 2>/tmp/logs/application_1316626117280_0002/container_1316626117280_0002_01_000001/stderr

So it appears that the bash command (which I assume is coming from task.sh) is taking almost no memory, but the java command that it triggers is taking 2 gigs in virtual memory. No idea why. When I run the bash -c .. command outside of YARN, it's taking about 10 megs of physical memory, and almost no virtual memory., cat'd out task.sh for my AM:


#!/bin/bash

export YARN_HOME="/tmp/hadoop-mapreduce/hadoop-mapreduce-0.24.0-SNAPSHOT-LATEST/bin/.."
export YARN_LOCAL_DIRS="/tmp/nm-local-dir/usercache/criccomi/appcache/application_1316638915485_0002"
export JVM_PID="$$"
export HADOOP_TOKEN_FILE_LOCATION="/tmp/nm-local-dir/usercache/criccomi/appcache/application_1316638915485_0002/container_1316638915485_0002_01_000001/container_tokens"
ln -sf /tmp/nm-local-dir/usercache/criccomi/appcache/application_1316638915485_0002/filecache/-3545403967124200139/kafka-streamer.tgz package
exec setsid /bin/bash -c "cat ./task.sh 1>/tmp/logs/application_1316638915485_0002/container_1316638915485_0002_01_000001/stdout 2>/tmp/logs/application_1316638915485_0002/container_1316638915485_0002_01_000001/stderr "

Obviously, my java ... command goes where the `cat ./task.sh` is., One reason for VMEM spikes are forks, are u seeing any?, Regarding what I said in the email above, when I comment out the registerWithResourceManager, I don't get killed. I re-checked the process usage when I do this:


2011-09-21 14:29:24,914 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(402)) - Memory usage of ProcessTree 28080 for container-id container_1316640481414_0001_01_000001 : Virtual 2123530240 bytes, limit : 2147483648 bytes; Physical 61272064 bytes, limit -1 bytes

It appears that without the registerWithResourceManager call, things are staying just BARELY under the kill limit, so it's not the register call that's leaking memory. Wondering if it's something to do with Java's fork behavior?, Hey Arun,

Am I seeing any spikes, or forks? Doesn't it always fork when it execs task.sh? As far as spikes go, it would appear that it's what I'm seeing. The odd thing is that it doesn't seem to be going down. I tried suggesting a System.gc(), but it didn't help. The process is sitting just under the 2 gig kill limit with vmem usage.

Cheers,
Chris, Hmm, I dropped my NM's -Xmx setting to 512m from 1024m. The VMEM setting STILL sits at just under the 2 gig kill limit.

This seems pretty interesting: http://stackoverflow.com/questions/561245/virtual-memory-usage-from-java-under-linux-too-much-memory-used

Reading., OK, I took a pmap of my application master, and, unsurprisingly, it's taking nearly 2 gigs. When I looked at the individual entries, I see a lot of normal stuff, but then a few really massive anon's:

00007fedc8021000  65404K -----    [ anon ]
00007fedcc000000    132K rwx--    [ anon ]
00007fedcc021000  65404K -----    [ anon ]
00007fedd0000000   3544K rwx--    [ anon ]
00007fedd0376000  61992K -----    [ anon ]
00007fedd4000000   3736K rwx--    [ anon ]
00007fedd43a6000  61800K -----    [ anon ]
00007fedd8000000    132K rwx--    [ anon ]
00007fedd8021000  65404K -----    [ anon ]
00007fedde16f000  96836K r-x--  /usr/lib/locale/locale-archive
00007fede4000000    132K rwx--    [ anon ]
00007fede4021000  65404K -----    [ anon ]
00007fede8000000    132K rwx--    [ anon ]
00007fede8021000  65404K -----    [ anon ]
00007fedec000000    132K rwx--    [ anon ]
00007fedec021000  65404K -----    [ anon ]
00007fedf0000000    132K rwx--    [ anon ]
00007fedf0021000  65404K -----    [ anon ]
00007fedf4000000    132K rwx--    [ anon ]
00007fedf4021000  65404K -----    [ anon ]
00007fedf8000000    132K rwx--    [ anon ]
00007fedf8021000  65404K -----    [ anon ]
00007fedfc000000    132K rwx--    [ anon ]
00007fedfc021000  65404K -----    [ anon ]
00007fee00000000    132K rwx--    [ anon ]
00007fee00021000  65404K -----    [ anon ]
00007fee04000000    132K rwx--    [ anon ]
00007fee04021000  65404K -----    [ anon ]
00007fee08000000    132K rwx--    [ anon ]
00007fee08021000  65404K -----    [ anon ]
00007fee0c000000    132K rwx--    [ anon ]
00007fee0c021000  65404K -----    [ anon ]

...

00007fee12f2e000  34376K rwx--    [ anon ]
00007fee150c0000  64768K rwx--    [ anon ]
00007fee19000000 526784K rwx--    [ anon ]
00007fee39270000  51904K rwx--    [ anon ]
00007fee3c520000  60288K -----    [ anon ]

Collectively, those guys are taking up well over a gig. Continuing to experiment.

Any ideas?, Chris, as Milind mentioned on list, can u try tuning your MALLOC_ARENA_MAX ala HADOOP-7154?

Do an 'svn up' to get MAPREDUCE-2880 and set an env for your AM: 

MALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-4}, It worked! THANKS!

That sucked.

For the record, I just set export MALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-4}, and made sure echo $MALLOC_ARENA_MAX printed 4 in my RM shell, NM shell, and AM.

pmap is now showing 1.3megs instead of 2 gigs. WOOT.

Thanks again!, Cool.

Chris, do you want to provide a patch to add MALLOC_ARENA_MAX automatically to containers? We can do this in the NM., Sure thing. Will get on it tomorrow AM. Headed out the door at the moment., Per vinod, new ticket to track this: https://issues.apache.org/jira/browse/MAPREDUCE-3068, @Chris, glad to know that it worked! Ironically, I had discovered this issue with RHEL6 when working on another project earlier this year at LinkedIn :-), @Milind, lol yea. It's these linux boxes that they threw us on. :P, Resolving correctly as a dup of MAPREDUCE-3068.]