[Confirmed, happens all the time at least a few hundred times a day.

We use Hadoop-0.18.3 , Hi,
I also have faced the issue.
To identify the problem , we have checked the userlogs present in
hadoop-0.18.3/logs/userlogs/{$task_id}/stdout or stderr
 where {$task_id} can be found from jobtracker's url (default url is jobtrackermachine:50030). For example, {$task_id} will be like attempt_200907061121_0002_m_000010_0.

 For me it was an out of memory error.
, I got the following error when using hadoop 0.20.2 on Red Hat Enterprise Linux Server release 5.2. SUN JDK 1.6 update 18  is used.

java.io.IOException: Task process exit with nonzero status of 134. at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:418)

After upgrading JDK from update 18 to update 22, the error has gone. 

, Here is another way to reproduce the problem.
I create a cluster on Amazon EC2 m1.large instances and I try to run a simple distcp from hdfs to hdfs using a relatively small file. I end up in a jvm crash.
See the attached hs_err_pid21757.log

If I create a cluster of type m1.small instances, then everything it works perfectly.

It seems that there is a problem related to 64bit JVM only.

I tried to set up 
export HADOOP_TASKTRACKER_OPTS="-XX:+UseCompressedOops"
in  /etc/hadoop/conf.dist/hadoop-env.sh (then restarted the nodes) but no luck!, I upgraded the jdk to the latest one. The same result (hs_err_pid23896.log)., All of my 64bit jvm issue has been solved with https://issues.apache.org/jira/browse/WHIRR-146, Closing as stale.]