[Ah! I see that the map output bytes and map output records shown are the counters that represent the values before the combiner. Combine input records and Combine output records are shown in the counters.
May be it would be better to have counter for combine output bytes also, because that is the size of the intermediate file generated by map task ?, FILE_BYTES_WRITTEN is correct then. Combiner output bytes would be nice if we have code paths that make it different from FILE_BYTES_WRITTEN. Before that the counter would be redundant., In map task, if multiple spills happen and multiple merges happen on those spills, then the FILE_BYTES_WRITTEN will include all those records again and again. No ?

So the actual map task's output file(i.e. the intermediate file) content could be of size (FILE_BYTES_WRITTEN - SpilledRecords) bytes ?, Ah! Thats wrong. Since SpilledRecords counter gives number of records(and not bytes), it is not just difference of FILE_BYTES_WRITTEN and SpilledRecords.
Since we don't have the number of bytes written during spills(i.e. the bytes corresponding to SpilledRecords), it seems difficult to get the actual mapoutputFileSize from the existing counters. Right ?, Updated Title/Summary to reflect the actual issue., Ravi,
Shouldn't _'Combine output records'_ counter value correspond to _HDFS_BYTES_WRITTEN_ counter value (num-reducers == 0) or _FILE_BYTES_WRITTEN_  counter value (num-reducers > 0). So from your example, 210 bytes written via 8 records. Can you kindly check if the spill logic directly uses _File_ objects instead of using _LocalFileSystem_? If the spill logic uses _File_ objects, then multiple spills shouldn't affect the final bytes written, right?, Unfortunately, spill file is created using FileSystemObject.create() method. So spills are contributing to File_BYTES_WRITTEN counter.

Here are the counters of map task of wordcount example for the same input data(1 map task, 1 reduce task in both jobs) with 2 different values of io.sort.mb:

(1) io.sort.mb=50

FileSystemCounters
	FILE_BYTES_READ 	39,268,167
	FILE_BYTES_WRITTEN 	78,536,360
	HDFS_BYTES_READ 	20,971,627

Map-Reduce Framework
	Combine input records 	402,994
	Combine output records 	383,997
	CPU_MILLISECONDS 	7,190
	Failed Shuffles 	0
	GC time elapsed (ms) 	62
	Map input records 	163,676
	Map output bytes 	38,559,489
	Map output records 	402,994
	Merged Map outputs 	0
	PHYSICAL_MEMORY_BYTES 	123,650,048
	Spilled Records 	767,994
	SPLIT_RAW_BYTES 	107
	VIRTUAL_MEMORY_BYTES 	582,451,200

(2) io.sort.mb=1

FileSystemCounters
	FILE_BYTES_READ 	75,090,212
	FILE_BYTES_WRITTEN 	114,350,720
	HDFS_BYTES_READ 	20,971,627

Map-Reduce Framework
	Combine input records 	796,792
	Combine output records 	777,203
	CPU_MILLISECONDS 	9,990
	Failed Shuffles 	0
	GC time elapsed (ms) 	72
	Map input records 	163,676
	Map output bytes 	38,559,489
	Map output records 	402,994
	Merged Map outputs 	0
	PHYSICAL_MEMORY_BYTES 	76,664,832
	Spilled Records 	1,134,831
	SPLIT_RAW_BYTES 	107
	VIRTUAL_MEMORY_BYTES 	595,406,848

It seems combiner also gets called many times when multiple merges happen. So combine output records is also not the correct value of map-task-output-records.]