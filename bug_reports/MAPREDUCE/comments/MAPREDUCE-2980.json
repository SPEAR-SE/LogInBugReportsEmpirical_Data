[I've been working with the Jetty folks on this, and they pointed me at an experimental branch (6.1.22-z6) which has some hacks in the NIO parts that seem to prevent the issue. I have verified that the 10,000 map by 10,000 reduce job completes with no fetch failures on the test cluster. In fact, no fetch failures after 20+ runs of this job.

They're thinking about merging this branch and calling it 6.1.27. At that point we could upgrade Hadoop to use 6.1.27. I'd also like to consider an alternate release (6.1.26.hadoop.1) which is a build I've prepared by simply patching 6.1.26 with only the NIO changes, since the planned 6.1.27 contains a number of other unrelated changes. It may make sense to include this custom patch build in the maintenance release series (20x) if we are concerned by any of the other Jetty changes not having had enough time to bake., FWIW, I ran 150 sleep jobs over the weekend, each with 10,000 map by 10,000 reduce. Didn't see any fetch failures, and none of the TTs got stuck in the "spinning" state.

The branch I tested is here: https://github.com/toddlipcon/jetty-hadoop-fix, Todd, thanks for digging into this!

I'm happy to upgrade to jetty-6.1.27, but don't see much of a point with running our custom builds. Also, with MAPREDUCE-2524 this becomes less critical - thus, I think we should just wait for jetty-1.6.27. , I agree it's less critical for the shuffle, but we're also seeing an issue where the NN drops an HTTP connection in the middle of long fsck response, in particular when it's under other load (eg big checkpoints). It's really spurious and hard to reproduce, but we have some inkling that it's related to this issue.

I've been bugging the jetty folks about timeline for a 6.1.27 release, but it may be a couple months off. I figured that 6.1.26".1" would be an interim solution for 205 and/or 206 until a 6.1.27 release is ready and can be QAed. Are you -1 or just not wild about it? FWIW the patch is not a custom change, bur rather just the NIO-related changes that will be integrated for 6.1.27., Arun - we have customers asking for this. Is it OK to apply the NIO changes to get around the connection dropping until jetty 1.6.27 is out?
, This is an long standing issue that is affecting production installation. Without significant operations monitoring, I wouldn't call 0.20.2xx a stable release for production., Shall I ping the dev list to see what the opinions are? I agree that (a) it's unfortunate to ship a patched Jetty, and (b) that the current fetch failure rate is unacceptable in 20x. Unfortunately to fix (b) we need to do (a)... as of yet, no word on Jetty 6.1.27 release timeline either. It seems to be low on their priority list (sort of like if someone asked us to release Hadoop 0.18.4 or something!), Sent a note to mapreduce-dev@ to solicit more opinions on this issue. We have rolled out this patch to some customers and into QA for our next CDH release., I am definitely interested in a fix for this.  Todd, any update on when the Jetty 6.1.27 release might take place?  

My personal opinion would be to use the custom patch build if 6.1.27 isn't going to be released for some time.  , Should we instead be looking at upgrading to Jetty 7.x?  It doesn't look like the Jetty dev community is supporting 6.x in any significant way any more, see for example
[JETTY-1374 Hadoop's TaskTracker OOM after upgrade to jetty6|http://jira.codehaus.org/browse/JETTY-1374?focusedCommentId=271808&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-271808], Upgrading to jetty 7 in the 20x series seems like a bit too much risk for a maintenance series. Doing it trunk is probably a good idea, though it will present compatibility issues with other projects like HBase that want to work against Hadoop 0.20 and Hadoop 0.23 both.

I haven't heard any more from the Jetty folks about 6.1.27 - I'll try to ping Greg this afternoon., I'm afraid I don't see a qualitative difference between the "risk" of transitioning from 6.1.14 -> 6.1.26 vs the risk of transitioning from 6.1.26 -> 7.x.  In fact, if we transition to a version of 7.x that is currently viewed as being *stable* (what a concept!) then it might be less risky than transitioning to 6.1.27, which does not have any claim of being stable., Jetty 7.x is API-incompatible as I understand it. So the effect would ripple down to all dependent projects that also use Jetty in any places. There are many of those (HBase is the first that comes to mind)., So this is not fixed in 1.0.0 either, as far as I can see..., Is it possible that this causes SocketTimeOut exceptions in HDFS as well? I'm getting excluded datanodes when copying large single files ( > 1TB). As far as I can see it's not due to xceivers or any physical bound (RAM / core / IO loads are fine), and I don't see anything in the NN / DN logs. I've worked around it by increasing dfs.socket.timeout to 10 minutes, but the network patterns I see in Ganglia are worrying - every 10 to 30 minutes a complete drop of activity for some minutes. It might of course be a problem with our switches or our bonded interfaces as well..., Todd, any update on the use of jetty 6.1.27?
It seems far less riskier than moving to jetty 7.

We also have users complaining about this. 
The new health_check helps a bit but is not ideal., Hey Kihwal. 6.1.27 still hasn't been released. We've been shipping a patched version of 6.1.26 with some fixes provided by Greg Wilkins - the tag is here: https://github.com/toddlipcon/jetty-hadoop-fix/tree/6.1.26.cloudera.1

The problems aren't 100% gone with this build but they seem to be improved -- at least nothing's been escalated to me in a few months, so I'm assuming it's a good sign! The other patch we've recently added is MAPREDUCE-3184, which is similar to the health check script approach - it just suicides the TT if it detects the problem., hi Todd, is jetty 6.1.27 released now? Or which version you are using at present? Whe downgrade to jetty 6.1.14 but it seems that it cause tasktracker memory problem. org.mortbay.jetty.nio.SelectChannelConnector$ConnectorEndPoint use to much memory in tasktracker., Still no 6.1.27. We've been shipping the version I linked to from github above: https://github.com/toddlipcon/jetty-hadoop-fix/tree/6.1.26.cloudera.1

That, combined with MAPREDUCE-3184 has made the problem quite livable.

We also found that the upgrade from 6.1.26 to the github branch improved performance noticeably for shuffle-intensive jobs., 6.1.27 hasn't been released since the last comment on the issue back in
June'12. So, I think it is time to move on to Jetty 7 or even 8 - as has been
proposed elsewhere - along with upgrade to servlet 3.0 API.
, This wouldn't specifically affect branch-2, since MR no longer relies on Jetty servlets for shuffling there (given the new Netty based, more performant Shuffle handler that replaced it a while ago). Please correct me if I'm wrong.

That said, the Jetty fixes may still be required for the other parts that still use it for various other data transfer purposes, or for folks who need to use it via their own plugins., why don't upgrade to the latest version of Jetty? it's incompatible? , I have been trying to find information about a 500 error that sporadically occurs when we run webhdfs.
Error seems to come from the Jetty 6.1.26 cloudera branch.

Could this be related to something else?
What are the plans to move to latest Jetty?

Our system is running Cloudera CDH 5.1.0-1.cdh5.1.0.p0.53 on Debian 7 Wheezy with JDK 1.7.0_55-b13.

{noformat}
INTERNAL_SERVER_ERROR
java.nio.channels.CancelledKeyException
	at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
	at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77)
	at org.mortbay.io.nio.SelectChannelEndPoint.updateKey(SelectChannelEndPoint.java:325)
	at org.mortbay.io.nio.SelectChannelEndPoint.blockReadable(SelectChannelEndPoint.java:242)
	at org.mortbay.jetty.HttpParser$Input.blockForContent(HttpParser.java:1169)
	at org.mortbay.jetty.HttpParser$Input.read(HttpParser.java:1122)
	at java.io.InputStream.read(InputStream.java:101)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:84)
	at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.put(DatanodeWebHdfsMethods.java:239)
	at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.access$000(DatanodeWebHdfsMethods.java:87)
	at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods$1.run(DatanodeWebHdfsMethods.java:205)
	at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods$1.run(DatanodeWebHdfsMethods.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.hdfs.server.datanode.web.resources.DatanodeWebHdfsMethods.put(DatanodeWebHdfsMethods.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1183)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:843)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{noformat}, Hi,
I am OOO from 8/16~9/2, returning on Wednesday Sept 3rd. Please expect delays in my response.

Thanks,
Alinda
]