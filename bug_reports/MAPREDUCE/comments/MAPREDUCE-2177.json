[It is forced to block because the buffer is full. Returning from collect without serializing the emitted record would be an error, as would serializing the record over data allocated to the spill. Changing the call as you suggest would affect correctness, unless you're arguing that the task should fail if the spill takes more than some set amount of time. If the task timeout is killing the task, then it's working as designed, and equivalently to the proposed mechanism.

There are many reasons the spill could take a long time. Running with a combiner, using a non-{{RawComparator}}, spilling to a failing/slow disk, etc. It's possible you're seeing a race condition that causes the collection thread to miss the signal, but the fix would not be to add a timeout to the wait, but to fix the locking. Can you get a stack trace from a map task stuck in this state? If the job is rerun over the same data, do the same tasks hang? Do the timeouts occur on particular machines? Does the task succeed on later attempts on different machines?, Chris, I think Ted's point is not that it should return after a timeout, but that it should call reporter.progress and then go back to waiting. This seems valid - if the mapper thread is blocked because the buffer is full, either the buffer spill thread should be calling progress() as it spills the buffer to disk, or the blocked thread should periodically unblock to call progress(), don't you think?

I think so long as the spiller is actually making some progress getting bytes to disk, it shouldn't cause a task failure - this kind of "alive but very slow" scenario is supposed to be handled by speculation rather than suicide :), SpillThread doesn't currently have reference to TaskReporter.
It is easier to use short timeout for spillDone.awaitNanos() so that Buffer.write() can call progress()., {quote}SpillThread doesn't currently have reference to TaskReporter.
It is easier to use short timeout for spillDone.awaitNanos() so that Buffer.write() can call progress().{quote}

That prevents the task from being killed, but its semantics are incorrect. Todd's suggestion- calling progress() during the merge- at least ensures that the task is doing work; reporting progress from a thread that isn't actually proceeding is broken. Isn't progress already reported during the merge? Can you provide more detail on the environment where you're observing this?, bq.  calling progress() during the merge- at least ensures that the task is doing work; reporting progress from a thread that isn't actually proceeding is broken. Isn't progress already reported during the merge? Can you provide more detail on the environment where you're observing this?

Chris, we've seen this at Y! too, this might just be a bug in progress reporting during the merge., I didn't capture stack trace of MapTask when this happened - will do next time.

We can add call to reporter.progress() in sortAndSpill()
But since we don't know how long each call to writer.append() / combinerRunner.combine() would take, there is no guarantee that we can prevent this issue from happening., The occurrence in our cluster may have something to do with the fact that we run HBase region server alongside task tracker.

Reporting progress from a thread that isn't blocked by long write to disk or combiner call is one option. We can put some limit on the total amount of time spillDone.awaitNanos() calls take in the following loop:
                while (kvstart != kvend) {
                  reporter.progress();
                  spillDone.awaitNanos();
                }
, The progress reporting during the merge is not on every record emitted. For jobs with combiners that emit far fewer records than they consume, it's possible that the framework fails to report progress, though (1) IIRC it reports at least once for every partition and (2) that wouldn't explain why the job is taking so much longer for a particular spill.

Adding some reporting in the reader could make sense, but we could use more information. Adding progress reporting only to prevent the job from being killed may be the wrong fix.

bq. But since we don't know how long each call to writer.append() / combinerRunner.combine() would take, there is no guarantee that we can prevent this issue from happening.

If the task is stuck, then it should be killed. I agree that the timeout mechanism's granularity is too coarse to measure all progress, but the overhead of measuring every event is too high to be the default.

bq. Reporting progress from a thread that isn't blocked by long write to disk or combiner call is one option. We can put some limit on the total amount of time spillDone.awaitNanos() calls take in the following loop:

Again, _that_ thread isn't making progress. It shouldn't prevent the task from getting killed if the merge is truly stuck.

Ted, please provide some details on the job you're running (w/ combiner? do reexecutions succeed? does this happen on particular machines? do other tasks complete normally while another is in this state?)., When this issue appears again, I will collect more details., The combiner code path doesn't report progress at some configured interval, so it can timeout when map output is large. The right fix is probably in CombineOutputCollector#collect to report progress a la Merger#writeFile., Found MAPREDUCE-2187 to be more closely related to the fix I was intending (though this seems closely related per the previous comment), so putting back original description and revoking ownership.]