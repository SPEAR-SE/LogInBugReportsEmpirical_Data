[Gunther, which version of Hadoop does this appear in?, [~sandyr] This is branch-1., Since Hive is badly broken on branch-1, I think the prudent course of action is to revert MAPREDUCE-2931 from branch-1 for now. Thoughts?, Looking into this.  The relevant code is also in trunk (from MAPREDUCE-1367), so it's likely that the problem exists there as well.

Is this occurring in every run?  Any steps for how to reproduce?, [~sandyr] Thanks for looking into it. We are seeing tens of Hive tests fail with this. 

[~hagleitn] - Can you please let [~sandyr] know a few? Thanks!, Yes, it would be prudent to revert MAPREDUCE-2931 from branch-1 while this is diagnosed and fixed.

Looking at the code, I wonder if the problem is with using the static EMPTY_COUNTERS (e.g. in getCurrentCounters()) rather than creating a new empty Counters object each time?

BTW this shouldn't affect the released version 1.1.2 since MAPREDUCE-2931 went in 1.2.0.
, Actually, getCurrentCounters is badly broken.

Because it's modifying EMPTY_COUNTERS by summing into it... the same problem exists in trunk.

Similarly, LJR.initCounters sets up all mapCounters & reduceCounters to be equal to EMPTY_COUNTERS.

, We should try and void the "a = b" paradigm, when a & b are of type Counters., Furthermore, iterating over Counters isn't thread-safe (copy-safe) since the map-thread might be modifying the Counter, the RPC layer saves us since it makes a safe (sync) copy.

I wonder what else exhibits this problem in LJR, I mean other than Counters..., Thought - how about inventing a deep-copy (Writable.write & Writable.readFields) RPC layer which doesn't use sockets? This seems the safest fix to prevent all such issues for LJR in future..., I had a brief IRC chat with [~tomwhite].

Rather than revert, I propose we do the following ASAP:
# Fix re-use of EMPTY_COUNTERS by using 'new Counters()' everywhere; and implement a RO version of EMPTY_COUNTERS which throws exceptions on modifications (to be really safe).
# Fix LJR.statusUpdate to do a deep-copy of Counters using Writable.write & Writable.readFields to simulate the RPC semantics.
# Pray that we don't get bitten by this same issue (lack of copy in RPC layer)... *smile*. Of course, we need to review all implementations of TaskUmbilical/JobSubmissionProtocol methods in LJR to verify that Counters is the only problem.

Thoughts?, [~sandyr] Is there a chance you can take a crack at this ASAP (assuming you are *ok* with the proposal). The urgency stems from the fact that we need this fixed for 1.2 release (and 2.x of course). Thanks!, [~acmurthy], looking at this right now and will have a patch by today.  The re-use of EMPTY_COUNTERS was my first hunch when I took a look at this yesterday, but, while I agree that using new Counters() everywhere is better practice, I don't think that's what's causing the problem.  EMPTY_COUNTERS is used as an operand in sums, but never summed into.  2. seems more likely to fix it.  I'll post a patch that does both of these just to be safe.

I ran a multi-map local job and repeatedly called getCounters on it to try to reproduce the problem, but couldn't get the exception. [~hagleitn], if I post a patch, would it be easy for you to verify it?

, Uploaded a patch for branch-1, [~sandyr] Thanks for taking this up!

The deep-copy of TaskStatus might be an overkill since deep-copy of Counters would suffice, but I think it's good to be paranoid.

Only other point - can we delete EMPTY_COUNTERS since it's not used anymore?

If you can update the patch, I'll commit right-away to unblock Hive.

Also, can you please post a similar patch for trunk? Thanks!, bq. Only other point - can we delete EMPTY_COUNTERS since it's not used anymore?

Uh, never mind I missed this hunk., I'm going to commit the branch-1 patch to unblock Hive testing, while [~sandyr] updates a patch for trunk., I just committed to branch-1 & branch-1.2 to unblock Hive testing. I ran TestLocalRunner and worked fine, as expected.

I'll close this jira once we commit similar fix to trunk. Thanks Sandy!, Thanks Arun!  I'll put a trunk patch up today., Uploaded trunk patch, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12580418/MAPREDUCE-5166.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3545//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3545//console

This message is automatically generated., I just committed this. Thanks Sandy!, Integrated in Hadoop-trunk-Commit #3659 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3659/])
    MAPREDUCE-5166. Fix ConcurrentModificationException due to insufficient synchronization on updates to task Counters. Contributed by Sandy Ryza. (Revision 1471796)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1471796
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java
, Integrated in Hadoop-Yarn-trunk #194 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/194/])
    MAPREDUCE-5166. Fix ConcurrentModificationException due to insufficient synchronization on updates to task Counters. Contributed by Sandy Ryza. (Revision 1471796)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1471796
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java
, Integrated in Hadoop-Hdfs-trunk #1383 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1383/])
    MAPREDUCE-5166. Fix ConcurrentModificationException due to insufficient synchronization on updates to task Counters. Contributed by Sandy Ryza. (Revision 1471796)

     Result = FAILURE
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1471796
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java
, Integrated in Hadoop-Mapreduce-trunk #1410 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1410/])
    MAPREDUCE-5166. Fix ConcurrentModificationException due to insufficient synchronization on updates to task Counters. Contributed by Sandy Ryza. (Revision 1471796)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1471796
Files : 
* /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java
, Closed upon release of Hadoop 1.2.0., Hi All, I am getting this error on the Hive version hive-1.2.1. Could you kindly advice me with the steps  to apply the above patch?

Regards,
Uday]