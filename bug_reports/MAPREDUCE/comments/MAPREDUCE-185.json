[Sort is actually the place where most checksum errors have been reported.  I believe this is because sorting keeps data in memory longer than other operations, increasing the chance that it will be corrupted there.  Does this node have ECC memory?  If so, memory errors are unlikely.  Sorting also accounts for a large portion of the number of times data is written to disk, so the corruption could have happened there.  It would be worth examining the syslog on that node to see if any disk or memory errors are reported.

I assume the reduce was rescheduled and completed?  If so, then I will resolve this issue.
, 
Yes, the task was rescheduled and finished successfully. That is the good news. 
The syslog does not have any error reports.

I think it is possible that the problem is due to a software bug. Thus, it is better to leave it open for tracking purpose.



, 


Hi,

I'm getting the same problem but don't know enough about Hadoop to
reschedule the reduce task.  Could somebody point me to some
documentation or let me know how to do this?

Many thanks,
K.



, Sorry, I should have added that I get the error whilst in the fetch stage using Nutch:

2007-01-03 02:29:15,062 WARN  dfs.DistributedFileSystem - Moving bad file C:\tmp\hadoop\mapred\local\reduce_dyohnt\map_0.out to C:\tmp\hadoop\mapred\local\reduce_dyohnt\bad_files\map_0.out.61591645
2007-01-03 02:29:15,093 WARN  mapred.LocalJobRunner - job_kh2gl7
org.apache.hadoop.fs.ChecksumException: Checksum error: /tmp/hadoop/mapred/local/reduce_dyohnt/map_0.out at 217448448
	at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:123)
	at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:99)


K., I've been seeing the same problem, and other checksum problems. I am somewhat sceptical of the suggestion that is a memory hardware issue, but to be thorough I tried replacing my memory.  The errors continued. If there is any additional information I can provide to help track the probem down, please let me know.  
Running on a single Windows Server 2003 (with cygwin) as both namenode and datanode.
Strangely, some large map/reduce jobs never get checksum errors in the maps or reduces, but one particular job always does. 
 
In addition I have been getting many lost map outputs due to checksum errors.  The error usually disappears when the task is retried:

Map output lost, rescheduling: getMapOutput(task_0008_m_000007_0,0) failed :
org.apache.hadoop.fs.ChecksumException: Checksum error: /tmp/hadoop-sshd_server/mapred/local/task_0008_m_000007_0/file.out at 60215808
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.verifySum(ChecksumFileSystem.java:258)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:211)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:167)
	at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at java.io.DataInputStream.read(DataInputStream.java:132)
	at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:1674)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
	at org.mortbay.http.HttpServer.service(HttpServer.java:954)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)

I'm alse getting errors in final output of the previous map/reduce job which is fed in as input to the next job.  These errors do not disappear when the map task retries:

org.apache.hadoop.fs.ChecksumException: Checksum error: hdfs://xxx.xxx.xxx:9900/aa/datamining/deviations_part-00002_step-00001/part-00000 at 13781504
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.verifySum(ChecksumFileSystem.java:258)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:211)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:167)
	at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:93)
	at java.io.DataInputStream.readInt(DataInputStream.java:372)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1523)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1436)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1482)
	at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:73)
	at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)
, > I tried replacing my memory. The errors continued.

To be clear, did you replace it with ECC memory?
, 
No. Sorry, should have been clearer. Replaced it with non-ECC memory. You still think this may be the cause?  Can you explain why you think this would fix the problem.  

Reading the other similar issues on checksum errors, it appears that files (or their checksum files) at various stages of the map/reduce processing are becoming corrupted when written to disk.  There are reports of errors in map output, during sorting, and in reduce output.

It smacks of a tricky threading issue. Both because hadoop is fairly complex in its use of threads, and because the bug is intermittent.

, > Replaced it with non-ECC memory.

Did you see Dennis Kubes' message?

http://mail-archives.apache.org/mod_mbox/lucene-hadoop-dev/200705.mbox/%3c465C3065.9050501@dragonflymc.com%3e

This has come up before.  Folks who have ECC memory don't see checksum errors.  Perhaps we should add this to an FAQ., Apologies again.. I didn't see Dennis' reply..  Will try to convince management to replace with ECC memory.  I am running tests to see if we should invest in a cluster and move some of our recommendation system algorithms over to hadoop.

, > I am running tests to see if we should invest in a cluster [ ... ]

FYI, Amazon EC2 provides an easy way to experiment with Hadoop.

http://wiki.apache.org/lucene-hadoop/AmazonEC2


, This looked to have been a user-end issue, but was not closed earlier.]