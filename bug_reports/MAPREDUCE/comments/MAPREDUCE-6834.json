[I tried to save NMTokens and containers form previous attempt in RMCommunicator and checked the steps to reproduce again. Application finished successfully with no exceptions. Could somebody review my patch (001) ?, Thanks for the report and the patch!  I moved this to the MapReduce project since that covers the MRAppMaster and where the initial patch is targeted.

Is this a scenario where somehow the MRAppMaster is asking to preserve containers across app attempts?  I ask because ApplicationMasterService normally does not call setNMTokensFromPreviousAttempts on RegisterApplicationMasterResponse unless getKeepContainersAcrossApplicationAttempts on the application submission context is true.  Last I checked the MapReduce client (YARNRunner) wasn't specifying that when the application is submitted to YARN.

I agree this would be an issue if MapReduce supported preserving containers across application attempts.  We have MRAppMasters that are being restarted/recovered all the time in our clusters and have never seen this issue, but none of them are telling YARN to preserve containers across app attempts either.

General comments on the patch itself, assuming eventually it would be necessary once MRAppMaster supports container preservation:
- setNMTokens is of questionable benefit since it's a trivial iteration over other public methods.  If kept it should just take a Collection as the parameter type.  It doesn't need to be a List.
- why are we trying to schedule the previously assigned containers during serviceInit rather than serviceStart?  I would not expect the service to allocate/assign containers until started.
- containersFromPreviousAttempt should be set to null after being passed to scheduling since the reference is no longer necessary beyond that point.  Otherwise that memory will persist for the duration of the MRAppMaster.
- it would be great to have a regression unit test that exercises this scenario
, We have also seen issue very much similar to this. The MR AM was preempted, right after the second attempt was started, it got some container allocations from RM and then MR AM assigned them to tasks attempts. Eventually, the job failed due to too many "No NMToken sent" exceptions in launching containers., Tracing the code in MR, it looks like MR just uses whatever NM tokens are contained in the allocation response from RM to launch containers with NMs., bq. The MR AM was preempted, right after the second attempt was started, it got some container allocations from RM and then MR AM assigned them to tasks attempts. Eventually, the job failed due to too many "No NMToken sent" exceptions in launching containers.

Assuming there weren't changes to have MapReduce start trying to preserve containers across app attempts, I don't see how this can be a MapReduce issue.  It is the RM's responsibility to ensure corresponding NM tokens are conveyed for each allocation returned, so the RM seems to be the one messing up here.  I noticed it is the scheduler's responsibility to return the NM tokens in the Allocation, maybe this is something specific to fairscheduler?  That would explain why we've never seen something like this, since we're not running that scheduler.  However I thought the handling of the NM tokens for allocations was essentially the same between the schedulers.

This patch relies on the registration response specifying containers that were preserved from the old attempt.  Unless I'm missing something, the RM only fills out this registration response field if the app specifies it is preserving containers, and I do not believe the MapReduce app framework ever does that.  Therefore even if you guys are seeing a similar issue, unless MapReduce asks to preserve containers across attempts I don't see how this patch will address that problem., bq.  the RM only fills out this registration response field if the app specifies it is preserving containers, and I do not believe the MapReduce app framework ever does that.Therefore even if you guys are seeing a similar issue, unless MapReduce asks to preserve containers across attempts I don't see how this patch will address that problem.
Agreed. My intension was more to report another instance. We should probably move this jira to YARN for scheduler folks to look at? 

bq. However I thought the handling of the NM tokens for allocations was essentially the same between the schedulers.
Yeah, it looks like both schedulers use the common code to handle this. [~abalitsky1], which scheduler were you running?, Looks like very much like the same problem at https://issues.apache.org/jira/browse/YARN-3112, Yes, which is why I've been asking all along if this is a case where MapReduce has been modified to preserve containers across AM attempts.  If that is indeed the case then this is essentially a bug report against an internal patch that is not part of Apache Hadoop., Thanks for the clarification, [~jlowe]. We have not made changes to preserve containers in MR. Chasing the code in more details, I came to a similar conclusion as https://issues.apache.org/jira/browse/YARN-3112?focusedCommentId=14299003&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14299003   MR relies on YARN RM to get the NMtokens needed to launch containers with NMs. Given the code today, it is possible that a null NMToken is sent to MR, which contracts with the javadoc in SchedulerApplicationAttempt.java here
{code:java}
  // Create container token and NMToken altogether, if either of them fails for
  // some reason like DNS unavailable, do not return this container and keep it
  // in the newlyAllocatedContainers waiting to be refetched.
  public synchronized ContainersAndNMTokensAllocation {...}
{code}
This could be a duplicate of YARN-3112., Hi [~haibochen], [~jlowe]
Sorry for late reply. 

{quote}
Is this a scenario where somehow the MRAppMaster is asking to preserve containers across app attempts? I ask because ApplicationMasterService normally does not call setNMTokensFromPreviousAttempts on RegisterApplicationMasterResponse unless getKeepContainersAcrossApplicationAttempts on the application submission context is true. Last I checked the MapReduce client (YARNRunner) wasn't specifying that when the application is submitted to YARN.
{quote}

Actually you are right. I did not consider that MR doesn't support AM work-preserving restart and currently I see that my first patch isn't good solution for this problem. Thanks for the review!

{quote}
Aleksandr Balitsky, which scheduler were you running?
{quote}

I'm running Fair Scheduler. I don't think that this issue depends on a scheduler, but I will check it with another schedulers. 

{quote}
We have not made changes to preserve containers in MR. Chasing the code in more details, I came to a similar conclusion as https://issues.apache.org/jira/browse/YARN-3112?focusedCommentId=14299003&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14299003 MR relies on YARN RM to get the NMtokens needed to launch containers with NMs. Given the code today, it is possible that a null NMToken is sent to MR, which contracts with the javadoc in SchedulerApplicationAttempt.java here
{quote}

I totally agree with you that we have not made changes to preserve containers in MR. But the solution that you mentioned contradicts YARN design:
{quote}
As for network optimization, NMTokens are not sent to the ApplicationMasters for each and every allocated container, but only for the first time or if NMTokens have to be invalidated due to the rollover of the underlying master key
{quote}

That's so true, it is possible that a null NMToken is sent to MR. NMTokens sends only after first creation, it's designed feature. Then it saves to NMTokenCache from AM side. It's not necessary to pass NM tokens during each allocation interaction. So, it's not the best decision to clear NMTokenSecretManager cache during each allocation, because it disables "cache" feature and new NM Tokens will be generated (instead of using instance from cache) during each allocation response. IMHO, we shouldn't do this, because it's not the fix for root cause. It looks like workaround. , Based the patch which claims to fix the problem, I would argue it is not a duplicate.  The patch is all about transferring containers from the previous attempt in the registration response, but that is not filled in unless the application was submitted with preservation of containers across application attempts.  MapReduce does not do this, therefore I don't see how this patch helps the problem unless MapReduce was patched to do so.  I agree the symptom is similar to YARN-3112, but I doubt a fix for YARN-3112 will address [~abalitsky1]'s issue if the original patch in this JIRA also corrected it., Ah, comment race!  ;-)

[~abalitsky1] so if I understand correctly, you're saying that the patch in this JIRA does _not_ fix the issue?  I'm trying to resolve that with [this comment|https://issues.apache.org/jira/browse/MAPREDUCE-6834?focusedCommentId=15770392&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15770392].  If you agree that the patch here isn't appropriate, then I agree we should just duplicate this to YARN-3112., [~jlowe], yep, 001 patch isn't good from design point of view. 
I'm going to investigate the code again to find the root cause. , [~jlowe], I'm not sure that YARN-3112 the same issue that I reported. 

From YARN-3112 description:
{code}
New AM has inherited the old tokens from previous AM according to my configuration (keepContainers=true), so the token for new containers are replaced by the old one in the NMTokenCache.
{code}

I have not used "keep-containers-across-application-attempts" feature, and I definitely didn't faced with problem when the token for new containers are replaced by the old one in the NMTokenCache (my debug can confirm it), because AM was restarted and all old tokens was removed. 

, bq. IMHO, we shouldn't do this, because it's not the fix for root cause. It looks like workaround
Sorry for the confusion. I should have been more specific, I meant to say that the root cause analysis looks correct to me. I agree with you that we should not follow the proposed fix there.
, After deeper investigation I have found the root cause:
https://github.com/apache/hadoop/blob/branch-2.7.0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java#L299-L327

Mentioned code should be executed only when "work-preserving AM restart" is true. Resource Manager saves new NMTokens into NMTokenSecretManagerInRM and sends this tokens only once with AM register response. But MR's AM doesn't handle those tokens, because it doesn't support work-preserving AM restart. Obviously, RM will no longer send those tokens again during next allocation requests.

When I use MR job (or another kind of frameworks, that doesn't support work-preserving AM restart), RM shouldn't retrieve previous attempt's containers and corresponding NM tokens. 

But as far as I see, this problem has bean already fixed in scope of YARN-3136. So 2.8.0 and next versions don't have this issue. 

https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java#L272-L273

[~haibochen], [~jlowe], thank you guys for the help and review. , Thanks [~abalitsky1] for your detailed analysis! Agree with the root cause your identified. We have not seen the issue either in branches that contain YARN-3136. Do you think this is the same issue as in YARN-3112? If so, we should probably link our discussion here and close it., [~haibochen], I think that this issue not the same as in YARN-3112.

From YARN-3112 description:
{noformat}
New AM has inherited the old tokens from previous AM according to my configuration (keepContainers=true), so the token for new containers are replaced by the old one in the NMTokenCache.
{noformat}

I have not used "keep-containers-across-application-attempts" feature, and I definitely didn't faced with problem when the token for new containers are replaced by the old one in the NMTokenCache (my debug can confirm it), because AM was restarted and all old tokens was removed., The posted stacktrace and affect version (2.6.0) in YARN-3113 makes me think the version was an apache release that does not include MR's support of keep-container-across-application-attempts (thus, the analysis there may be incorrect, I think). Anyway, we cannot say for sure. Let's keep it open and do not link it as a duplicate then.]