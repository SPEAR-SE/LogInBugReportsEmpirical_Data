[Ramya found this while running tests to verify the node-health-check feature. Thanks Ramya!, The problem happens only when the health check script itself launches sub-processes.

When the health check script launches sub-processes itself like the 'sleep' in the test, even though the original bash shell which launches the health script gets terminated after timeout period, the sub-processes themselves are not. Due of this, the i/o streams being read by TT for getting the output of the health-check script are not closed. The process object in TT address space is blocked on reading output (from the sub-processes now) and so TT never marks the node as unhealthy.

This error scenario also prevents all sub-sequent runs of the health script by just queuing them., The problem in gist: 

* In bash scripts all the commands are spawned using fork + exec. Similar to system() syscall. The process hierarchy is as follows:
{noformat}
15772 pts/0    S      0:00  \_ /bin/bash ./myscript.sh
15773 pts/0    S      0:00  |   \_ sleep 100000
{noformat}
* So when kill 15772 is sent, the signal is not delivered to child.
* So parent exits and sleep does not check if the parent is alive or not continues doing its work as it is a long running process.

The problem is similar to what is mentioned and addressed in HADOOP-2721

For this problem all the node health script which is spawned should do: {{setsid; exec(node_health_path)}} and then instead of Process.destory() we do {{kill -pid}}, now the problem is that pid of the process is now passed on to java, so it will change to {{setsid; echo $$ ;exec(node_health_path)}} and we should read the input stream to get process id.

Or alternate solution to the problem is the node health script configured manages its children  :-), Closing this as stale.  ]