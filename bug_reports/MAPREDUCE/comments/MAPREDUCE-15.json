[This is a duplicate of HADOOP-153., hadoop-153 is addressing application errors only at this point. i thought there was agreement that recordreaders should implement any logic for skipping corrupt records internally (and outside the scope of 153). please see:

http://issues.apache.org/jira/browse/HADOOP-153?focusedCommentId=12593425#action_12593425, So what is the approach here?

1. Internal errors (negative lengths, any others?)
  throw exception and skip to next sync block
2. reader errors (ie. errors in the deserializer)
  skip to next record

Is that your intent?, The recordReader should somehow inform the framework about skipping any of the records. I think it wont be acceptable for the user job to skip a record and not notify about it.
Doing it may require RecordReader interface to change. One way of doing that without changing the interface would be throw a specific exception from next(). The framework can catch it and based on the exception type can decide to again call next() or just fail the job. Also it would allow framework to keep something like skipped records counter., sorry - i didn't see Owen's comment earlier: - yes #2 was the intent.

I agree that this causes the absence of any policy hooks (max number of errors tolerable, reporting errors out to JobTracker.) - and i think that's leading to Sharad's comment. One problem with his proposal is how will the RecordReader differentiate between the first and second next() call?

One simple method to integrate this with the policy framework would be for the recordreader to export an error counter (as an additional interface). The TT/JT can make go/no-go decision based on the number of errors they observe. By default a recordreader would attempt to skip bad records, while raising the error count - but based on job spec - this may lead to job being aborted by Hadoop.

One wrinkle is that the number of errors is not the same as the amount of data skipped. That may be a design point (report bytes skipped versus # of bad records).
, {quote}One problem with his proposal is how will the RecordReader differentiate between the first and second next() call? {quote}
Does RecordReader really need to differentiate? When next() is called the first time, the RecordReader would skip to the sane record boundary, BEFORE throwing an Exception (a subclass of IOException, something like SkippedRecordException), so that framework knows that the record has been skipped. Calling next() again would read the next() record.
This way we are also not forcing all RecordReaders to implement this feature. 

{quote} One simple method to integrate this with the policy framework would be for the Recordreader to export an error counter (as an additional interface). {quote}
 That would be the better way. But given that lot of user code may get impacted, we can try to avoid interface change as far as possible.

makes sense?, cool - that sounds perfect., Should the framework carry a {{mapreduce.max.corrupt.input.record.fraction}} and {{mapreduce.max.corrupt.input.record.excess}} ?

The framework catches and skips the corrupt records, and the bad and good record counts flow back to the job tracker as counters, and if the number of bad records ever exceeds, {{...fraction * number-of-good-records + ...excess}} the job is killed?, Curious what the current state is; can the RecordReader skip bad records? This seems like the best default behavior in a complex distributed environment where bad records are non-trivial..., We encountered the stack trace in this issue's description a few days ago. The SequenceFile "corruption" (unreadability) happens because of an integer math overflow [1], if the BytesWritable size is > Integer.MAX_VALUE / 3 (about 682MB). Here is [2] a stackoverflow discussion about this.

[1] https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BytesWritable.java#L123
[2] http://stackoverflow.com/questions/24127304/negativearraysizeexception-when-creating-a-sequencefile-with-large-1gb-bytesw]