[The pic of command failed, More details:

{code}
2014-04-28 08:40:29,574 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 50 bytes
2014-04-28 08:40:29,705 INFO org.apache.hadoop.mapred.Task: Task:attempt_201404220359_0033_r_000000_0 is done. And is in the process of commiting
2014-04-28 08:40:30,774 INFO org.apache.hadoop.mapred.Task: Task attempt_201404220359_0033_r_000000_0 is allowed to commit now
2014-04-28 08:40:30,785 INFO org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Saved output of task 'attempt_201404220359_0033_r_000000_0' to /dex/en??/output
2014-04-28 08:40:30,790 INFO org.apache.hadoop.mapred.Task: Task 'attempt_201404220359_0033_r_000000_0' done.

[serengeti@wdc-vhadp-pub2-dhcp-72-245 ~]$ hadoop fs -ls /dex/en??/output
-rw-r--r--   3 serengeti hadoop          0 2014-04-28 08:40 /dex/en??/output/_SUCCESS
-rw-r--r--   3 serengeti hadoop         36 2014-04-28 08:40 /dex/en??/output/part-r-00000
drwxrwxr-x   - serengeti hadoop          0 2014-04-28 08:40 /dex/en中文/output/_logs
{code}

Looks like the some of the Chinese character is replaced to "??",  _SUCCESS and part files are affected, but _SUCCESS file are just fine.

, Checked hadoop2 does not has this problem, setting -Dfile.encoding=UTF-8 for java explicitly solve the problem, hi, once change the input and output path directories and try to re execute the job....]