[> I propose we bump up the child-vm default heapsize to 512M; too many people are getting burnt by 200M.

Why the rush?  This is for trunk, not an urgent fix to a blocker, where a short-term fix might be appropriate.  If io.sort.mb is using more than the 100MB that's allocated to it by default, then we should fix that there, not just by bumping the heap size.

Amazon EC2 is a recommended environment for Hadoop.  It's standard instance has 1.7GB.  Hadoop, by default, will run 2 map tasks and 2 reduce tasks at a time per node, in addition to probably running a datanode and a tasktracker.  That's six JVMs.  At 200MB each, these would take 1.2GB.  But a 200MB heap requires more than 200MB, so that sounds about right for a 1.7GB machine.  Upping this to 512MB would mean that the default configuration would require as much as 3GB.

The defaults should aim for the low side of average, so that things work out-of-the-box for most folks without alteration.  Only folks with really outdated, underpowered hardware should need to change defaults to get things to run out of the box, no?

, Point taken... 

However it's prolly 4 * 200M + 2 * 1000M = 2.8G, since HADOOP_HEAPSIZE defaults to 1000M... is it lowered for Hadoop daemons running on Amazon?

Anyway, HADOOP-1867 should be the right approach (auto-magically configuring buffers based on the given heapsize), so I'd rather fix that... and close this as "Won't Fix"., > HADOOP-1867 should be the right approach [ ... ]

+1.  It'd be great if that worked.
, This is sorta being handled via MAPREDUCE-4316 today. However, the -Xmx200m has served well with io.sort.mb as 100 all these years. Whatever the issue was here, it had gone away (was some other leak).

Resolving as dupe., With HDFS-4053 we should revisit this., Very old JIRA.

With MRv2, we have largely also moved _away_ from default Xmx values.

Closing this as invalid.]