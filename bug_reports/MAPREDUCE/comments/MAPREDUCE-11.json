[Also, we should remove the <date> field from the job history filename format. Anyway the date is already there in the jobID and that is a part of the filename., Attaching a patch that simplifies the job history filename and recovery. Changes are as follows :
# job history filename is of the format _hostname_jobid_username_jobname_
# conf filenames are of the format _hostname_jobid_conf.xml_
# upon every restart all the new updates will be directed to _history-file.recover_
# once the job finishes the _history-file.recover_ file will be renamed to _history-file_
# note that the master file ( _hostname_jobid_username_jobname_) will exist throughout the lifecycle of the job
# if the jobtracker restart again, new updates will be lost
# there is no searching involved in any case
# for now the old jobhistory files are supported via web-ui

Tested the patch locally and so far no issues. Result of test-patch 
[exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 9 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.


Running ant tests now and testing in progress.

Things I tested
# submitted a job allowed it to completed. New job files move to done folder. 
# submitted a job and killed the jobtracker  while job files was empty, restarted the jobtracker and the files upon completion move to done folder
# submitted a job and killed the jobtracker  while job files was written, restarted the jobtracker and the files upon completion move to done folder. job was also recovered
# checked webui
 ## history shows old and new files (there is no difference between the layout)
 ## history pages for old and new jobs have functional links (check random links and conf links)
 ## search facility in history works across files , All mapred tests passed and all contrib tests passed except TestStreamingExitStatus and TestStreamingStderr., 
 These are the test scenaros that are tried:

1)	Some normal jobs ( sleep and random writer) gets submitted and it gets completed successfully and goes into done folder. All web-links related to it goes smoothly including "filter". The contents of the job file shows success.
2)	Some jobs are submitted. JT is restarted to see if there is a recover file as well as the original job file. The jobs complete successfully and goes into the done folder. The contents of the job file shows success.
3)	Some jobs are submitted. JT goes for a restart. JT comes up back again and jobs gets resubmitted. Jobs are 20% completed. JT restarts again. Jobs gets resubmitted from the beginning instead of from the 20%. JT restarts again. Jobs are again resubmitted. The jobs complete successfully and goes into the done folder. The contents of the job file shows success.
4)	Some jobs are submitted. JT is stopped. After 5 minutes, it is restarted. One of the TTs gets job level blacklisted. Still, the jobs are resubmitted and passes to the "done" folder.
5)	Kill job tracker as soon as some jobs starts, when they had a "0" file size. Restart JT. These jobs should get resubmitted and pass successfully.
6)	Submit jobs, do JT restart and when jobs gets resubmitted, kill those jobs using "job -kill". They shd be killed properly and move to done folder with correct file contents.


Two issue found are :
1) 
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /user/hadoopqa/input3/_logs/history/servername.inktomisearch.com_job_200907151220_0003_conf.xml for DFSClient_-123 on client IPAddress because current leaseholder is trying to recreate file.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:888)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:821)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:565)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:964)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:960)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:958)

        at org.apache.hadoop.ipc.Client.call(Client.java:743)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
        at $Proxy0.create(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy0.create(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:2932)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:436)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:206)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:529)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:503)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:484)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:391)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:383)
        at org.apache.hadoop.mapred.JobHistory$JobInfo.logSubmitted(JobHistory.java:1082)
        at org.apache.hadoop.mapred.JobInProgress.logToJobHistory(JobInProgress.java:589)

2) 2009-07-15 11:36:31,576 ERROR org.mortbay.log: /jobdetailshistory.jsp
java.lang.IllegalArgumentException: No enum const class org.apache.hadoop.mapred.JobHistory$RecordTypes.b0VIM
        at java.lang.Enum.valueOf(Enum.java:192)
        at org.apache.hadoop.mapred.JobHistory$RecordTypes.valueOf(JobHistory.java:233)
        at org.apache.hadoop.mapred.JobHistory.parseLine(JobHistory.java:465)
        at org.apache.hadoop.mapred.JobHistory.access$200(JobHistory.java:75)
        at org.apache.hadoop.mapred.JobHistory$MetaInfoManager.<init>(JobHistory.java:337)
        at org.apache.hadoop.mapred.JobHistory.parseHistoryFromFS(JobHistory.java:415)
        at org.apache.hadoop.mapred.DefaultJobHistoryParser.parseJobTasks(DefaultJobHistoryParser.java:50)
        at org.apache.hadoop.mapr

Both are intermittent issues., Job history is no longer used for recovery and hence this issue is no longer valid.]