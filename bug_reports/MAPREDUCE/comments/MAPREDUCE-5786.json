[Couldn't this lead to file descriptor exhaustion on the NM side?  Thinking of cases where we're running many large jobs on the cluster with thousands of reducers each and:

- reducer decides it wants to shuffle to memory but there isn't enough memory yet so it waits for the memory merge to complete (which could take a while)
- reducer is waiting for a subsequent map to complete (which could take many minutes or hours)

Seems like we could have a situation where reducers start piling up on the shuffle handler and camping out., Thanks for comments Jason.  We need to have "mapreduce.shuffle.enable.keep.alive" to enable keep-alive in the ShuffleHandler and "mapreduce.shuffle.enable.keep.alive.timeout" to determine the time-out value for the persistent connection.  E.g, "Keep-Alive: timeout=60" header specifies that the connection will be kept alive for 60 seconds after which the connection will be closed.  This will allow us to tune persistent connection duration on large clusters with different job patterns., All sub-tasks  (only one actually) are done.]