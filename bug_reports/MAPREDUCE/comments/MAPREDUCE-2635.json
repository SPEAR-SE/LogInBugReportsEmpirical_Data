[Please add in your node's basic JT/TT configurations and your input format used.

I tried to replicate this in my single node instance with my own inputformat added along (emptysplit like thing, with no location preference, and with a single rounded dummy record reader), but wasn't able to. Both jobs launch normally and everything fails as expected (2 tt slots)., Harsh,

Sorry I forgot to mention. Yes. This does not occur in a single node
cluster. we have the max map and reduce tasks to be 20 and reuse jvm set to
true.

The input format is in the code sepcified as MyINputFormat.java

-Sudhan S


, Harsh, Sudharshan,

    This occurs irrespective of single/multiple nodes when the cluster capacity is less than the number of the maps of the parent job. 
In the mapreduce application given, the parent Job is spanning child jobs for each map input and waiting for those child jobs to complete. When the parent job map waiting for the child job to complete, it is updating the status also. 

When the cluster capacity is less than the number of the maps of the parent job, all the slots in the cluster occupies by the parent job mappers and these mappers internally spans the child jobs, these child jobs will be waiting for the map slots to get free and parent jobs will be waiting for the child jobs to complete. This will never happen and waiting forever.  
Here the problem is with the application logic, it needs to be corrected in the application itself. There is no problem with mapreduce respective to this.

If the cluster capacity is greater than the number of the maps of the parent job, tasks are failing and finally Job is failing.

This issue can be invalidated., Hi Devaraj,

You are right in saying the jobs will hang indefinitely due to slot
unavailability with cyclic jobs.

But the issue here is different. The issue occurs even on clusters with
higher slot capacity. The set up I have spans 2 slaves with 20 map slots in
each.

Another update is that while the jobs on the initial submit are hanging, if
I trigger this set another time all the four jobs complete
immediately(actually fail as I throw an explicit exception)

Hence, my opinion is that we should keep this open till we get to know the
real reason.

Thanks
Sudharsan S


, I'm still unable to reproduce this. And Oozie does similar things, but never have we run into such a situation. At best, this was probably a local issue. If we run into this again someday and have logs, we can reopen it.

Thanks all!]