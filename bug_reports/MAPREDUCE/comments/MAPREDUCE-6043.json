[The value of this variables in the first case were this:
{code}
headroom = 0; 
pr = 0; //reducer preemption was never called at this app
{code}
so the triggering condition for reducer preemption is summarized to:
{code}
am * |m| < |m|
{code}
or 
{code}
am < 1
{code}
In this erroneous case we had two assigned mappers that were not successfully removed from the list and hence prevent preemption from kicking in. Those mappers were finished but the MRAppMaster did not hear anything about them afterwards so called them successful after one minute timeout:
{code}
2014-08-20 04:25:21,665 INFO [Ping Checker] org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: Expired:attempt_xxx_yyy_m_000288_0 Timed out after 60 secs
2014-08-20 04:25:21,665 WARN [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Task attempt attempt_xxx_yyy_m_000288_0 is done fromTaskUmbilicalProtocol's point of view. However, it stays in finishing state for too long
2014-08-20 04:25:21,665 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_xxx_yyy_m_000288_0 TaskAttempt Transitioned from FINISHING_CONTAINER to SUCCESS_CONTAINER_CLEANUP
{code}, After a preemption, the task id is added to
{code}
assignedRequests.preemptionWaitingReduces
{code}
And it is later removed once the preemption is successfully finished. In the second scenario we observed, although the preemption was successfully finished its report was not received through RM and hence the variable was not decreased and future preemptions did not kick in.
, I was planning to work on a patch that maintains those variables also in exceptional corner cases. 

Comments are highly appreciated., For the first case you described, the AM currently kills tasks immediately after they said they have completed (either successfully or unsuccessfully).  This behavior may change after MAPREDUCE-5465 but that's not in yet.  Did the kill not take place or otherwise failed in some fashion?  Killing a lingering-but-finished map task is better than preempting a reducer.

For the second case, could you elaborate on how the RM failed to report the completed container to the AM?  That sounds like a bug in YARN rather than MapReduce, but it would be good to know the circumstances in which it occurred.

Both of these sound like they could be cases of YARN failing to convey completed container status to the AM.  If so those seem like bugs in YARN and not MapReduce.  Also could you elaborate on how you're proposing to fix these exceptional corner cases in MapReduce?, Thanks [~jlowe] for the comments. Further investigations suggest that the cause for the lost messages, as you correctly mentioned, was a bug introduced by one of our recent internal patches (the container is successfully finished but its status is not successfully transmitted to MRAppMaster through RM). Nevertheless I was under impression that there is double standards in MRAppMaster, sometimes relying on RM to successfully transmit statuses and sometimes not. But after your comment, I double checked and noticed that the logic for the exceptional case described in case 1 is only available in our internal repository and has not made it to the trunk yet. So, I now understand that the containers statuses reported by NM are guaranteed to be received by MRAppMaster through RM, and MRAppMaster's logic need not to be resilient against such cases.
]