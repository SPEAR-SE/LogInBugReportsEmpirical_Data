[Note that this job had many map attempt failures, and a number of nodes had been blacklisted by the AM as a result.  At one point in the log I saw this message which was a bit troubling:

{noformat}
2013-02-03 16:30:32,164 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Could not map allocated container to a valid request. Releasing allocated container Container: [ContainerId: container_1359150825713_856434_01_003359, NodeId: xx, NodeHttpAddress: xx, Resource: memory: 4608, Priority: {Priority: 5}, State: NEW, Token: ContainerToken { kind: ContainerToken, service: xx }, Status: container_id {, app_attempt_id {, application_id {, id: 856434, cluster_timestamp: 1359150825713, }, attemptId: 1, }, id: 3359, }, state: C_NEW, ]
{noformat}

I suspect the AM couldn't associate it with an outstanding map task and lost the container, and that container is effectively the one needed to complete the final map task and therefore the job.

Note that the priority of the missing container is for a failed map.  I'm wondering if a failed map somehow stole a normal priority request, and when the failed map priority request finally came in there were no more failed attempts to associate with it and the container was dropped.
, I see some convincing evidence in the AM log that what I suspected is true.  There was one less "Assigned from earlierFailedMaps" entry in the log than there were failed map attempts that received containers.  I see one of them was allocated a normal priority container, although I'm not sure how from looking at the code.

Originally I thought trunk and 2.0.3-alpha would have the same issue, but I think MAPREDUCE-4893 inadvertently fixes this scenario.  It changed the logic so it tries to assign containers without locality (i.e.: fast fail maps and reducer priority containers) then falls through to assigning them to normal maps if it still hasn't found an assignment.  Before that change it would throw away a fast fail container if no fast fail map was around to take it.  There's an assert in the code indicating only normal priority map containers are expected, but according to what I've seen it does appear that fast fail maps can somehow steal a normal priority container on occasion, leaving a subsequent fast-fail request to be assigned to the normal map attempt that was stolen from., Great debugging!

bq. I see some convincing evidence in the AM log that what I suspected is true. There was one less "Assigned from earlierFailedMaps" entry in the log than there were failed map attempts that received containers. I see one of them was allocated a normal priority container, although I'm not sure how from looking at the code.
Think this could happen if there's no node or rack-local tasks for a container. The assignToMap in branch-0.23 then falls back to pulling an attempt from 'maps' - which could be a previously failed attempt.

In branch-2, it looks like a container meant for a REDUCE could be allocated to a MAP as well. Not sure if such a scenario will arise, and what problems it could create., bq. Think this could happen if there's no node or rack-local tasks for a container. The assignToMap in branch-0.23 then falls back to pulling an attempt from 'maps' - which could be a previously failed attempt.

I initially thought that as well, but given that the {{maps}} field is a {{LinkedHashMap}} I thought traversal would be in the order they were added to the map.  If that's that case, I'm not sure how a failed attempt for one map task got ahead of the initial attempt of another map task., bq. I initially thought that as well, but given that the maps field is a LinkedHashMap 
getContainerReqToReplace seems to be removing and re-adding entries into 'maps'. Alternately, could speculation cause this - was it enabled for this job ?, bq. getContainerReqToReplace seems to be removing and re-adding entries into 'maps'.

Aha, that's the missing piece.  Great catch!  There was some blacklisting going on with lots of containers being allocated on blacklisted nodes.  getContainerReqToReplace probably moved the initial attempt of one map task after the failed map attempt of another.

bq. Alternately, could speculation cause this - was it enabled for this job?

I think speculation could also be an opportunity for fast fail map attempts to get ahead of normal map attempts, but it was not enabled in this particular case., This is fixed by the changes in MAPREDUCE-4893.  That has since been pulled into branch-0.23, so closing this as a duplicate.]