[it's worse than the title would suggest. compute quotas are also not being honored.

i am observing our ETL pipeline not getting the configured number of map slots - whereas there's another jobs with no minimum slot guarantees that keeps hogging resources. this other task has a crazy looking deficit (from the advanced scheduler page):

Deficit: -17796533s

but regardless - i would have thought that honoring quotas would come before any form of fair scheduling (deficit based or not), Ignore the previous comment (which may be due to a  bug we are trying to diagnose separately). i would like to stick to the initial report - deficit based scheduling causes pending jobs to rise up in priority (effectively). especially when such jobs are large - they tend to hog the cluster once they can be scheduled. 

i guess this does beg the question of why a large deficit was accumulated in the first place (and that maybe due to a bug) - but this does seem to call for some solution in any case.

one of the things that i think u had mentioned would make sense - instead of giving all incoming slots to a task with a large deficit - give it a large enough fraction so that it's 'catching up'. one way could be to assign additional weight multiplicator that's proportional to the deficit. this should leave some slots on an ongoing basis to new jobs without a lot of deficit. thoughts?

, I agree, I think the "catching up" idea could help here. Basically the problem is the following - if you have a job with long tasks, when it makes it to the head of the queue (max deficit), it may grab a *lot* of slots and hold onto them for a while. Instead, we should give it a more moderate share - enough that it can "catch up" at a reasonable time. It may also be good to take into account task durations for the job in this equation - i.e. look further ahead for jobs with large tasks., In fact upon further discussion with Joydeep and Dhruba, we may drop deficits altogether once we add preemption and just use a similar concept for guaranteed shares to make sure pools get their min share in order of how long they've been waiting for it. This will simplify the code and make the scheduler behavior easier to understand., So, do you mean that you won't maintain job deficits at all, and instead you would maintain how much time pools (or jobs) have not got their min share, and sort by that ?, Yes exactly. The last time each pool was at its min / fair share is already being maintained by the preemption patch (HADOOP-4665), so it won't be much work. One other benefit of this change will be that jobs will tend to reuse the same slot more often, leading to more JVM reuse. This can be a bad thing if it leads to poor locality, but HADOOP-4667 will ensure that a job keeps using a node till it runs out of local blocks to read on that node, and then waits and switches to hopefully a node where it has more local data to process. This should give us the best of both JVM reuse and data locality. (When I talked to Arun and Owen about the use of deficits in the fair scheduler before, they were concerned that it may lead to less JVM reuse because jobs will jump between slots more often.), A note on progress for this issue: I have a tested patch that removes deficits and also adds support for FIFO pools, but I am waiting for HADOOP-4665 and HADOOP-4667 to be committed before posting it because it depends on those., This was incorporated into MAPREDUCE-706, right? Can we link that and close as dup?, Yes indeed, this issue was fixed as part of MAPREDUCE-706, so I'm closing it as a duplicate.]