[This was fixed in hadoop-1.1.2 stable release.
if we determine completedMapsInputSize is zero, then job's map tasks MUST be zero, so the estimated output size is zero. 
below is the code:
{code}
  long getEstimatedMapOutputSize() {
    long estimate = 0L;
    if (job.desiredMaps() > 0) {
      estimate = getEstimatedTotalMapOutputSize()  / job.desiredMaps();
    }
    return estimate;
  }
{code}, [~qwertymaniac], is it a problem if the estimate is Long.MAX_VALUE? To frame it differently, if no maps completed (may be due to the infinite amount of work each map does), what other value would we set this to.

[~azuryy], I am not sure if the snippet posted addresses the issue Harsh reported., Hi Karthik,

The estimate blocks out scheduling so its kinda critical. We hold back on scheduling if the estimate (which is incorrect) is higher than the node's disk free space report. This is ending up blocking up all map tasks from getting scheduled.

I think what [~azuryy] is pointing out is that do not try to estimate if completed maps isn't > 0. The code he points out, says this shouldn't ever happen but over the mailing list we did see this happen., Uploading a patch that sets the {{thresholdToUse}} value to at least 1. This value is used in {{getEstimatedTotalMapOutputSize()}} to decide whether to estimate or not., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12586371/mr-5288-1.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3737//console

This message is automatically generated., +1, Thanks Karthik. Thanks Harsh for reviewing it. Committed to branch-1.]