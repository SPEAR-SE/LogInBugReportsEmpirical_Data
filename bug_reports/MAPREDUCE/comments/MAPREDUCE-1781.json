[mapred.tasktracker.map.tasks.maximum is a startup configuration parameter and cannot be modified per job. Even in your first scenario (where it seemed to work), I am guessing that the system started running 1 map per node because of scheduling decisions and not because the tasktrackers were configured to run with only 1 task per node.

bq. Why is this happening and how can I make it work properly (i.e. be able to limit exactly how many mappers I can have at 1 time per node)?

Can  you provide some more details of why you want to limit a job to use only one mapper at a time on a node ?, Thank you, I put the option in the config file and from the preliminary test it works as I intended.

The reason I am using only 1 mapper/node at 1 time is because I am testing the parallelization efficiency of a highly-CPU and memory bound application over Hadoop, focusing on the parallelization via distributed computing, not multicore. 
Additionally, I will use more splits scenarious (different approx of a multiple of the no of nodes) on a heterogenous datacenter in order to determine a good input split size (and also how many nodes might be necessary to keep the scalability efficiency over 0.8). The application time is approx linear with the input size but has poor performance if the input is too small (I'm trying to find the exact point).

In real-life scenarious I will use Hadoop with every resourse it has. However I have a question here: what if the cluster is highly heterogenous and I have both single-cores, dual-cores, dual processors with dual cores, quads, .... - is it possible to specify that I want 4 mappers/processors or am I limited to a static value at the startup of Hadoop?

Regarding the initial problem, I think it would help a lot of people (especially new users) to specify in the config page[ http://hadoop.apache.org/common/docs/current/mapred-default.html ] which parameters are set at startup and which at job runtime., bq. - is it possible to specify that I want 4 mappers/processors or am I limited to a static value at the startup of Hadoop?

The configuration per tasktracker can be different for each node, in general. However, that makes managing configurations much harder. Does that work for you now though ?

bq. which parameters are set at startup and which at job runtime.

OK. Possibly you should file a JIRA asking for this to be explained. But the general rule of thumb is that configurations whose names contain the names of daemons like 'tasktracker' will be start-up only parameters. Configurations whose names contain 'job' or 'task' can be overridden per job., bq. Regarding the initial problem, I think it would help a lot of people (especially new users) to specify in the config page[ http://hadoop.apache.org/common/docs/current/mapred-default.html ] which parameters are set at startup and which at job runtime.
In branch 0.21, the configuration names are standardized through MAPREDUCE-849. The configuration names with prefix as mapreduce.cluster/mapreduce.jobtracker/mapreduce.tasktracker are server level configurations and need to be setup before the cluster is brought up. The other configurations with prefix mapreduce.job/mapreduce.task/mapreduce.map/mapreduce.reduce are job level configurations. 
Documenting all of them in mapred-default is being tracked in MAPREDUCE-1021.

Closing this as invalid.]