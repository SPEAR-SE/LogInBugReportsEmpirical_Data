[Please note that this issue happens with lost nodes (i.e, Unreachable hosts). NM crash with a reachable host is exhibiting a totally a different expected retry behavior. There liveness configurations are coming into play (yarn.resourcemanager.container.liveness-monitor.interval-ms, yarn.nm.liveness-monitor.expiry-interval-ms, yarn.am.liveness-monitor.expiry-interval-ms) as expected., We have developed a patch that should help on this issue by managing NodeManager lost events on AM as described below:
* on nodemanager service unavailibility (crash, oom ...):
		When receiving lost NodeManager events, it failed the impacted attempt and do not go through the cleanup stage.

* on nodemanager server unavailibility with default settings AM detect first that the attempt is in timeout and try to cleanup the attempt:
		When receiving lost NodeManager events, it stop the cleanup process on the impacted container and failed the attempt.

This reduce the duration of the timeout to the timeout for detecting a down NodeManager.
Could you please provide me rights to attached the patch in this request?, Do I have to create another request to provide this potential patch?, This has been sitting idle, so I'm assigning this to Nicolas to help move this forward.

I'm wondering if this is an issue in 2.8 or later.  I would expect the RM to send completed container events along with the lost node event.  After MAPREDUCE-5465 the AM does not attempt to kill containers that have been marked by the RM as already completed.  Seems like porting that portion of the state machine change from MAPREDUCE-5465 would be another viable alternative.
, [~jlowe], MAPREDUCE-5465 is already applied on the hadoop release I use (cdh5.5.0).
I've tested on cdh5.5 and trunk the behaviour when a nodemanager is lost and it is the same. 
The RM send a LostNM event to the AM which try to cleanup containers running on it (on cdh5.5 and on trunk). The attempt is failed only after a timeout to connect to the lost NM.
The main difference between cdh5.5 and the trunk is the timeout being really slower in trunk (3 min instead of 30 min at least).
This is thanks to patches YARN-4414 and YARN-3554
Backporting those patches can be consider sufficient, what do you think about this?, Ah, looks like this would have "just worked" if the AM had not tried to process the NM lost event.  The RM probably also sent container completed events for these containers due to the lost NM, but the AM is trying anyway to kill the containers.  Wondering if MAPREDUCE-6119 would help here (assuming AM is configured to ignore node events).

bq. The main difference between cdh5.5 and the trunk is the timeout being really slower in trunk (3 min instead of 30 min at least). This is thanks to patches YARN-4414 and YARN-3554 Backporting those patches can be consider sufficient, what do you think about this?

Did you mean to say "slower than trunk" rather than "slower in trunk"?   Sure, if the AM spends a lot less time trying to kill the containers it will never be able to kill then that also mitigates the issue.  Not as efficient as not trying in the first place, but yes, I can see lowering the NM client retries/timeouts as a viable approach.
, I mean that cleanup timeout after at least 30 min in cdh5.5 and only after 3 min in trunk.
I will double check if there is in fact some container completed events send by RM and then check the behaviour if we apply MAPREDUCE-6119  to ignore the Lost NM events.]