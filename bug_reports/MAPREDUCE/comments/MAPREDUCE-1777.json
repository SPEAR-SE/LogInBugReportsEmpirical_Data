[Arkady, what is the max java heapsize for the child when you notice this OOM?, 
If the stream mapper stalled for some reason and cannot consume the std input, while the 
Java MapRed wrapper continues to pipe to the mapper, then maybe too much data will be 
accumulated in the std input pipe.
That may cause broken pipe or oom exception. What did the mapper do?



, ran a simple streaming job (/bin/cat) using trunk and did not see any problem


 hadoop jar ./hadoop-0.22.0-alpha-1-streaming.jar -input /tmp/ramach -output /tmp/out1 -mapper /bin/cat -reducer NONE

it ran fine


10/08/19 02:36:51 INFO streaming.StreamJob: Tracking URL: http://ucdev11.inktomisearch.com:50030/jobdetails.jsp?jobid=job_201008060139_0059
10/08/19 02:36:51 INFO mapreduce.Job: Running job: job_201008060139_0059
10/08/19 02:36:52 INFO mapreduce.Job:  map 0% reduce 0%
10/08/19 02:37:08 INFO mapreduce.Job:  map 100% reduce 0%
10/08/19 02:37:13 INFO mapreduce.Job: Job complete: job_201008060139_0059
10/08/19 02:37:13 INFO streaming.StreamJob: Output directory: /tmp/out1


, It has been sitting for a while looks like is no longer a problem. I could not reproduce 
Closing for now]