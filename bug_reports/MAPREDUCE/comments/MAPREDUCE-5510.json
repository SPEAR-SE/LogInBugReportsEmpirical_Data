[  private void preemptReducesIfNeeded() {
    if (reduceResourceReqt == 0) {
      return; //no reduces
    }
    //check if reduces have taken over the whole cluster and there are 
    //unassigned maps
    if (scheduledRequests.maps.size() > 0) {
      int memLimit = getMemLimit();
      int availableMemForMap = memLimit - ((assignedRequests.reduces.size() -
          assignedRequests.preemptionWaitingReduces.size()) * reduceResourceReqt);
      //availableMemForMap must be sufficient to run atleast 1 map
      if (availableMemForMap < mapResourceReqt) {


I consider availableMemForMap should get rid of scheduledRequests.maps.size()*mapResourceReqt, I have analyzed seriously again. At that time. these two big mapreduce jobs were runing meanwhile. these where belong to two queue respectively. As a result root queue was full, because of over-capacity. But one queue was not full. because cluster's resource exhaust and numbers  of reduce had occupied resource in advance. Final map task had hung. I know yarn has a rule when there aren't enough resource to assign, map task will trigger to preempt reduce task. kill some reduce task, to guarantee map task success. But why no effect at that time? 
I research the AM and ResourceManager log, discover that to trigger preemption must leaf queue resource is not enough, it isn't care of parent queue or root queue resource usage rate. Therefore there appear that  cluster's resource exhaust,but don't trigger preemption still.
So I think when ResourceManager is calculating  leaf queue's headroom value, it need computes its parent queue usage rate. if its parent queue is full, jobs inside leaf queue need consider whether trigger preemption.
I will provide a patch later. 
]