[Able to simulate the issue. Deletion fails always (due to GSS API Exception) after refreshing log retention settings.
Will look into it., Moved it to YARN as code changes will be in {{yarn-common}}, Instead of adopting the approach followed in the patch, issue can also be fixed by using a {{ScheduledThreadPoolExecutor}} instead of a {{Timer}}, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m 58s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 33s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 36s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 49s | There were no new checkstyle issues. |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 2  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 33s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |   1m 56s | Tests passed in hadoop-yarn-common. |
| | |  39m 58s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12738240/YARN-3779.01.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / b61b489 |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/8211/artifact/patchprocess/whitespace.txt |
| hadoop-yarn-common test log | https://builds.apache.org/job/PreCommit-YARN-Build/8211/artifact/patchprocess/testrun_hadoop-yarn-common.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8211/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8211/console |


This message was automatically generated., Fixed whitespace issue, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  16m  7s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 36s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 35s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 56s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 35s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |   1m 57s | Tests passed in hadoop-yarn-common. |
| | |  40m 19s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12738254/YARN-3779.02.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / b61b489 |
| hadoop-yarn-common test log | https://builds.apache.org/job/PreCommit-YARN-Build/8212/artifact/patchprocess/testrun_hadoop-yarn-common.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8212/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8212/console |


This message was automatically generated., So the problem is after refreshing, the deletion task is scheduled and executed by the ugi of who executes the refreshing command, right?, [~zjshen], thanks for looking at this.
Its the same user which is used for both starting the history server and for executing the refresh command.
Timer will create a new thread on refresh and from then on, problem occurs.

There is no problem if I use a ScheduledThreadPoolExecutor(with 1 thread) instead as that doesn't spawn a new thread.
So it seems the new thread doesn't take the correct UGI.

Are you able to simulate the issue ?
I hope there is no issue in the way Kerberos has been set up in my cluster., No, I didn't simulate the problem. Just have a quick glance at the code. Log retention refresh will reschedule the deletion task, but this is done in the rpc call by the request user. So I'm not wondering if this changes the ug of the following deletion task. Can you try to print the ugi? Then, we can see what is changed., [~zjshen], GSSException was thrown while calling {{evaluateChallenge}} in SaslRpcClient.java
I had printed the DEBUG logs when I tested this(at the history server side). It seems correct UGI is taken but still error comes.

Below are the logs when error occurs after refresh of log retention settings.
{noformat}
2015-06-05 22:49:24,541 INFO IPC Server handler 0 on 10033  org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger: USER=hdfs	IP=10.19.92.82	OPERATION=refreshLogRetentionSettings	TARGET=HSAdminServer	RESULT=SUCCESS
...
2015-06-05 22:50:04,541 INFO Timer-3  org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: aggregated log deletion started.
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.use.legacy.blockreader.local = false
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.read.shortcircuit = false
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.domain.socket.data.traffic = false
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.domain.socket.path = 
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.replication to 0
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.HAUtil: No HA service delegation token found for logical URI hdfs://hacluster
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.use.legacy.blockreader.local = false
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.read.shortcircuit = false
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.domain.socket.data.traffic = false
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.domain.socket.path = 
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.io.retry.RetryUtils: multipleLinearRandomRetry = null
2015-06-05 22:49:24,553 DEBUG Timer-3  org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@28194a50
2015-06-05 22:49:24,554 DEBUG Timer-3  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP dfs.data.transfer.protection = authentication, configured class dfs.data.transfer.saslproperties.resolver.class = class org.apache.hadoop.security.SaslPropertiesResolver
2015-06-05 22:49:24,554 DEBUG Timer-3  org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2015-06-05 22:50:04,542 DEBUG Timer-3  org.apache.hadoop.ipc.Client: Connecting to host-10-19-92-88/10.19.92.88:65110
2015-06-05 22:50:04,543 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:749)
2015-06-05 22:50:04,544 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
2015-06-05 22:50:04,545 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: getting serverKey: dfs.namenode.kerberos.principal conf value: hdfs/huawei@HADOOP.COM principal: hdfs/huawei@HADOOP.COM
2015-06-05 22:50:04,545 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB is hdfs/huawei@HADOOP.COM
2015-06-05 22:50:04,545 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at huawei
2015-06-05 22:50:04,546 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Use KERBEROS authentication for protocol ClientNamenodeProtocolPB
2015-06-05 22:50:04,547 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedActionException as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) 
{noformat}


And these are the logs before error i.e. when everything was working fine.
{noformat}
2015-06-05 22:49:16,989 INFO Timer-2  org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: aggregated log deletion started.

2015-06-05 22:49:17,055 DEBUG Timer-2  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.use.legacy.blockreader.local = false
2015-06-05 22:49:17,055 DEBUG Timer-2  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.read.shortcircuit = false
2015-06-05 22:49:17,055 DEBUG Timer-2  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.domain.socket.data.traffic = false
2015-06-05 22:49:17,055 DEBUG Timer-2  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.domain.socket.path = 
2015-06-05 22:49:17,056 DEBUG Timer-2  org.apache.hadoop.hdfs.DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.replication to 0
2015-06-05 22:49:17,057 DEBUG Timer-2  org.apache.hadoop.hdfs.HAUtil: No HA service delegation token found for logical URI hdfs://hacluster
2015-06-05 22:49:17,057 DEBUG Timer-2  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.use.legacy.blockreader.local = false
2015-06-05 22:49:17,057 DEBUG Timer-2  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.read.shortcircuit = false
2015-06-05 22:49:17,057 DEBUG Timer-2  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.domain.socket.data.traffic = false
2015-06-05 22:49:17,057 DEBUG Timer-2  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.domain.socket.path = 
2015-06-05 22:49:17,057 DEBUG Timer-2  org.apache.hadoop.io.retry.RetryUtils: multipleLinearRandomRetry = null
2015-06-05 22:49:17,057 DEBUG Timer-2  org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@28194a50
2015-06-05 22:49:17,059 DEBUG Timer-2  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP dfs.data.

2015-06-05 22:49:17,061 DEBUG IPC Parameter Sending Thread #0  org.apache.hadoop.ipc.Client: IPC Client (1125964210) connection to /10.19.92.88:65110 from hdfs/huawei@HADOOP.COM sending #3
2015-06-05 22:49:17,062 DEBUG IPC Client (1125964210) connection to /10.19.92.88:65110 from hdfs/huawei@HADOOP.COM  org.apache.hadoop.ipc.Client: IPC Client (1125964210) connection to /10.19.92.88:65110 from hdfs/huawei@HADOOP.COM got value #3
2015-06-05 22:49:17,063 DEBUG Timer-2  org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getListing took 2ms
2015-06-05 22:49:17,065 INFO Timer-2  org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: aggregated log deletion finished.
{noformat}

, Sorry the correct sequence of error logs is as under. After first GSSException, client i.e. historyserver keeps on retrying before giving up.

{noformat}
2015-06-05 22:49:24,541 INFO Timer-3  org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: aggregated log deletion started.
2015-06-05 22:49:24,541 INFO IPC Server handler 0 on 10033  org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger: USER=hdfs	IP=10.19.92.82	OPERATION=refreshLogRetentionSettings	TARGET=HSAdminServer	RESULT=SUCCESS
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.use.legacy.blockreader.local = false
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.read.shortcircuit = false
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.domain.socket.data.traffic = false
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.domain.socket.path = 
2015-06-05 22:49:24,550 DEBUG Timer-3  org.apache.hadoop.hdfs.DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.replication to 0
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.HAUtil: No HA service delegation token found for logical URI hdfs://hacluster
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.use.legacy.blockreader.local = false
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.read.shortcircuit = false
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.client.domain.socket.data.traffic = false
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: dfs.domain.socket.path = 
2015-06-05 22:49:24,552 DEBUG Timer-3  org.apache.hadoop.io.retry.RetryUtils: multipleLinearRandomRetry = null
2015-06-05 22:49:24,553 DEBUG Timer-3  org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@28194a50
2015-06-05 22:49:24,554 DEBUG Timer-3  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP dfs.data.transfer.protection = authentication, configured class dfs.data.transfer.saslproperties.resolver.class = class org.apache.hadoop.security.SaslPropertiesResolver
2015-06-05 22:49:24,554 DEBUG Timer-3  org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2015-06-05 22:49:24,554 DEBUG Timer-3  org.apache.hadoop.ipc.Client: Connecting to /10.19.92.88:65110
2015-06-05 22:49:24,555 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:749)
2015-06-05 22:49:24,557 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
2015-06-05 22:49:24,557 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: getting serverKey: dfs.namenode.kerberos.principal conf value: hdfs/huawei@HADOOP.COM principal: hdfs/huawei@HADOOP.COM
2015-06-05 22:49:24,557 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB is hdfs/huawei@HADOOP.COM
2015-06-05 22:49:24,557 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at huawei
2015-06-05 22:49:24,558 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Use KERBEROS authentication for protocol ClientNamenodeProtocolPB
2015-06-05 22:49:24,559 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedActionException as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-06-05 22:49:24,560 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:668)
2015-06-05 22:49:24,561 WARN Timer-3  org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-06-05 22:49:24,562 DEBUG Timer-3  org.apache.hadoop.ipc.Client: closing ipc connection to /10.19.92.88:65110: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:709)
	at java.security.AccessController.doPrivileged(Native Method)
.......

2015-06-05 22:49:24,562 DEBUG Timer-3  org.apache.hadoop.ipc.Client: IPC Client (1125964210) connection to /10.19.92.88:65110 from hdfs/huawei@HADOOP.COM: closed
2015-06-05 22:49:24,567 INFO Timer-3  org.apache.hadoop.io.retry.RetryInvocationHandler: Exception while invoking getListing of class ClientNamenodeProtocolTranslatorPB over /10.19.92.88:65110. Trying to fail over immediately.
java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "HOST-10-19-92-82/10.19.92.82"; destination host is: "host-10-19-92-88":65110; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:776)
	at org.apache.hadoop.ipc.Client.call(Client.java:1516)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
...........
2015-06-05 22:49:24,568 DEBUG Timer-3  org.apache.hadoop.io.retry.RetryUtils: multipleLinearRandomRetry = null
2015-06-05 22:49:24,569 DEBUG Timer-3  org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@28194a50
2015-06-05 22:49:24,569 DEBUG Timer-3  org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2015-06-05 22:49:24,569 DEBUG Timer-3  org.apache.hadoop.ipc.Client: Connecting to /10.19.92.95:65110
2015-06-05 22:49:24,574 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:749)
2015-06-05 22:49:24,577 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.KerberosInfo(clientPrincipal=, serverPrincipal=dfs.namenode.kerberos.principal)
2015-06-05 22:49:24,577 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: getting serverKey: dfs.namenode.kerberos.principal conf value: hdfs/huawei@HADOOP.COM principal: hdfs/huawei@HADOOP.COM
2015-06-05 22:49:24,578 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: RPC Server's Kerberos principal name for protocol=org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB is hdfs/huawei@HADOOP.COM
2015-06-05 22:49:24,578 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Creating SASL GSSAPI(KERBEROS)  client to authenticate to service at huawei
2015-06-05 22:49:24,579 DEBUG Timer-3  org.apache.hadoop.security.SaslRpcClient: Use KERBEROS authentication for protocol ClientNamenodeProtocolPB
2015-06-05 22:49:24,580 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedActionException as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-06-05 22:49:24,585 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:668)
2015-06-05 22:49:24,585 WARN Timer-3  org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2015-06-05 22:49:24,585 DEBUG Timer-3  org.apache.hadoop.security.UserGroupInformation: PrivilegedActionException as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) cause:java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
.....
....(several similar logs)

2015-06-05 22:49:24,699 ERROR Timer-3  org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: Error reading root log dir this deletion attempt is being aborted
java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "HOST-10-19-92-82/10.19.92.82"; destination host is: "host-10-19-92-95":65110; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:776)
	at org.apache.hadoop.ipc.Client.call(Client.java:1516)
	at org.apache.hadoop.ipc.Client.call(Client.java:1443)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
....

2015-06-05 22:49:24,699 INFO Timer-3  org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: aggregated log deletion finished.
{noformat}, [~varun_saxena] Thanks for the logs. Could you apply the patch and print the ugi ?, Sure. Will share DEBUG logs for that too., [~xgong], after applying the patch, debug log on refreshing log retention setting is something as under. I will update both success and error logs too, a little while later.

{noformat}
2015-06-11 14:49:56,973 DEBUG org.apache.hadoop.ipc.Server: Socket Reader #1 for port 10033: responding to null from 10.19.92.82:30295 Call#-33 Retry#-1 Wrote 22 bytes.
2015-06-11 14:49:56,981 DEBUG org.apache.hadoop.ipc.Server:  got #-3
2015-06-11 14:49:57,014 DEBUG org.apache.hadoop.ipc.Server: Successfully authorized userInfo {
  effectiveUser: "hdfs/huawei@HADOOP.COM"
}
protocol: "org.apache.hadoop.mapreduce.v2.api.HSAdminRefreshProtocol"

2015-06-11 14:49:57,014 DEBUG org.apache.hadoop.ipc.Server:  got #0
2015-06-11 14:49:57,015 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 0 on 10033: org.apache.hadoop.mapreduce.v2.api.HSAdminRefreshProtocol.refreshLogRetentionSettings from 10.19.92.82:30295 Call#0 Retry#0 for RpcKind RPC_PROTOCOL_BUFFER
2015-06-11 14:49:57,016 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2082)
2015-06-11 14:49:57,027 INFO org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer: HS Admin: refreshLogRetentionSettings invoked by user hdfs
2015-06-11 14:49:57,027 DEBUG org.apache.hadoop.ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@2dfaea86
2015-06-11 14:49:57,079 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) from:org.apache.hadoop.yarn.client.RMProxy.getProxy(RMProxy.java:136)
2015-06-11 14:49:57,079 DEBUG org.apache.hadoop.yarn.ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2015-06-11 14:49:57,079 DEBUG org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ApplicationClientProtocol
2015-06-11 14:49:57,080 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@2dfaea86
2015-06-11 14:49:57,081 INFO org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: aggregated log deletion started.
2015-06-11 14:49:57,081 INFO org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger: USER=hdfs IP=10.19.92.82  OPERATION=refreshLogRetentionSettings   TARGET=HSAdminServer    RESULT=SUCCESS
2015-06-11 14:49:57,081 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs/huawei@HADOOP.COM (auth:KERBEROS) from:org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:83)
2015-06-11 14:49:57,081 DEBUG org.apache.hadoop.ipc.Server: Served: refreshLogRetentionSettings queueTime= 11 procesingTime= 55
2015-06-11 14:49:57,082 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 0 on 10033: responding to org.apache.hadoop.mapreduce.v2.api.HSAdminRefreshProtocol.refreshLogRetentionSettings from 10.19.92.82:30295 Call#0 Retry#0
2015-06-11 14:49:57,083 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 0 on 10033: responding to org.apache.hadoop.mapreduce.v2.api.HSAdminRefreshProtocol.refreshLogRetentionSettings from 10.19.92.82:30295 Call#0 Retry#0 Wrote 32 bytes.
2015-06-11 14:49:57,083 DEBUG org.apache.hadoop.ipc.Client: IPC Client (889891977) connection to /10.19.92.82:65110 from hdfs/huawei@HADOOP.COM sending #5
2015-06-11 14:49:57,084 DEBUG org.apache.hadoop.ipc.Client: IPC Client (889891977) connection to /10.19.92.82:65110 from hdfs/huawei@HADOOP.COM got value #5
2015-06-11 14:49:57,084 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getListing took 1ms
2015-06-11 14:49:57,085 INFO org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService: aggregated log deletion finished.
{noformat}, [~xgong], also updated complete logs, one for demonstrating the problem and other demonstrating the fix(after patch above has been applied). Moreover, this issue can be fixed if I use {{ScheduledThreadPoolExecutor}} with one thread(which is anyways recommended for use over Timer) but as that fix wasn't directly related to the issue, hence didnt submit that as a solution., By updated I mean attached., [~varun_saxena], do you know why ugi is still the same, but kerberos authentication gets failed?, Went through the ticket.

Figured out why you needed to do this: The incoming UGI from the RPC layer doesn't have the kerberos credentials. So if the user-name is the same, the remote UGI cannot talk to HDFS over kerberos authentication.

Comments on the patch
 - Let's call the UGI as login-UGI and use the UserGroupInformation.getLoginUser() call. In fact, you should simply copy the usage of ResourceManager.rmLoginUGI - Point this to UserGroupInformation.getCurrentUser() if security is not enabled, otherwise point it to UserGroupInformation.getLoginUser().
 - Also, usually in other services, we do login in serviceStart(). So you may want to move the initialization of the UGI to beginning of serviceStart.

I think the same bug may happen in refreshJobRetentionSettings() of Jobhistory. [~varun_saxena], can you please give it a try and verify that? We may have to file a MR ticket for this.

IAC, I think these issues have been around for a while. If we can get these fixes in and get them verified this week, I'll include them in 2.7.1 otherwise, we can move them to 2.7.2., Thanks for helping the issue, Vinod! It sounds the right cause of this issue. I checked refreshJobRetentionSettings, which should have the same problem because of accessing HDFS too.

I'm thinking it is more clear to fix the problem inside HSAdminServer. We still need to cache the correct loginUGI. Then, inside HSAdminServer, once we verified user's permission on a certain command, we use loginUGI to complete the following process instead of the remote user. Thoughts?, [~vinodkv], [~zjshen],
I had checked {{refreshJobRetentionSettings}} too when this issue came. And issue didn't happen there.
This issue comes in the case of refreshLogRetentionSettings as a new thread is invoked(upon cancellation of {{Timer}}) which creates a new DFS Client to connect to namenode.

In case of refresh Job retention settings, we use a {{ScheduledThreadPoolExecutor}} instead hence a new thread is not spawned on refresh. We simply cancel the {{ScheduledFuture}}. And in this case, issue doesn't happen., Will update the patch as per suggestions tomorrow morning., [~varun_saxena], I agree with Zhijie here. We may be lucky for now in case of refreshJobRention call depending on how we spawn threads. To future proof ourselves, I think the right behaviour is to simply depend on loginUser in both the cases., [~vinodkv], thats correct.
So do you want me to raise another JIRA for that ? Or do it as part of this one only ?, Added a patch and submitted it, fixing both cases. This JIRA should move to MAPREDUCE. But not moving it because not sure if Jenkins will be able to post results of the submitted patch then, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m 56s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 46s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 52s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 28s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   0m 55s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   5m 53s | Tests passed in hadoop-mapreduce-client-hs. |
| | |  43m 25s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12740836/YARN-3779.03.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 055cd5a |
| hadoop-mapreduce-client-hs test log | https://builds.apache.org/job/PreCommit-YARN-Build/8301/artifact/patchprocess/testrun_hadoop-mapreduce-client-hs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8301/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf908.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8301/console |


This message was automatically generated., Moved this to MAPREDUCE as code change is in MAPREDUCE code, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  19m 50s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |  10m 28s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  11m 23s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 27s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 33s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 46s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 41s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m  9s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   6m 26s | Tests passed in hadoop-mapreduce-client-hs. |
| | |  52m 47s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12740836/YARN-3779.03.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / c7d022b |
| hadoop-mapreduce-client-hs test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5820/artifact/patchprocess/testrun_hadoop-mapreduce-client-hs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5820/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5820/console |


This message was automatically generated., [~varun_saxena], can you add a test in TestHSAdminServer? You can do a refresh as a different user and validate that the daemon-user does the real refresh?, [~vinodkv], sure. Will do so., [~vinodkv], added test., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m 28s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 54s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 59s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 30s | There were no new checkstyle issues. |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   0m 53s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   6m  4s | Tests passed in hadoop-mapreduce-client-hs. |
| | |  43m 20s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12741120/MAPREDUCE-6410.04.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 445b132 |
| whitespace | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5829/artifact/patchprocess/whitespace.txt |
| hadoop-mapreduce-client-hs test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5829/artifact/patchprocess/testrun_hadoop-mapreduce-client-hs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5829/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5829/console |


This message was automatically generated., Looks good. Fixing the white-space issue myself.., \\
\\
| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m 36s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 30s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 42s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 27s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   0m 50s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   5m 53s | Tests passed in hadoop-mapreduce-client-hs. |
| | |  42m 34s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12741170/MAPREDUCE-6410.05.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / fac4e04 |
| hadoop-mapreduce-client-hs test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5832/artifact/patchprocess/testrun_hadoop-mapreduce-client-hs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5832/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf902.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5832/console |


This message was automatically generated., Committed this to trunk, branch-2 and branch-2.7. Thanks Varun!, Thanks for the review and commit [~vinodkv], Thanks [~zjshen] and [~xgong] for the review as well, FAILURE: Integrated in Hadoop-Yarn-trunk #968 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/968/])
MAPREDUCE-6410. Fixed MapReduce JobHistory server to use the right (login) UGI to refresh log and cleaner settings. Contributed by Varun Saxena. (vinodkv: rev d481684c7c9293a94f54ef622a92753531c6acc7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server/TestHSAdminServer.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/server/HSAdminServer.java
* hadoop-mapreduce-project/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Yarn-trunk-Java8 #238 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/238/])
MAPREDUCE-6410. Fixed MapReduce JobHistory server to use the right (login) UGI to refresh log and cleaner settings. Contributed by Varun Saxena. (vinodkv: rev d481684c7c9293a94f54ef622a92753531c6acc7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/server/HSAdminServer.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server/TestHSAdminServer.java
* hadoop-mapreduce-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #227 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/227/])
MAPREDUCE-6410. Fixed MapReduce JobHistory server to use the right (login) UGI to refresh log and cleaner settings. Contributed by Varun Saxena. (vinodkv: rev d481684c7c9293a94f54ef622a92753531c6acc7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server/TestHSAdminServer.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/server/HSAdminServer.java
* hadoop-mapreduce-project/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #2166 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2166/])
MAPREDUCE-6410. Fixed MapReduce JobHistory server to use the right (login) UGI to refresh log and cleaner settings. Contributed by Varun Saxena. (vinodkv: rev d481684c7c9293a94f54ef622a92753531c6acc7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/server/HSAdminServer.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server/TestHSAdminServer.java
* hadoop-mapreduce-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #236 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/236/])
MAPREDUCE-6410. Fixed MapReduce JobHistory server to use the right (login) UGI to refresh log and cleaner settings. Contributed by Varun Saxena. (vinodkv: rev d481684c7c9293a94f54ef622a92753531c6acc7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server/TestHSAdminServer.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/server/HSAdminServer.java
* hadoop-mapreduce-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2184 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2184/])
MAPREDUCE-6410. Fixed MapReduce JobHistory server to use the right (login) UGI to refresh log and cleaner settings. Contributed by Varun Saxena. (vinodkv: rev d481684c7c9293a94f54ef622a92753531c6acc7)
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server/TestHSAdminServer.java
* hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/server/HSAdminServer.java
* hadoop-mapreduce-project/CHANGES.txt
]