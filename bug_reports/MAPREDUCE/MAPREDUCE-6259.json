{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "zhihai xu",
            "key": "zxu",
            "name": "zxu",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zxu",
            "timeZone": "America/Los_Angeles"
        },
        "components": [{
            "id": "12315343",
            "name": "jobhistoryserver",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12315343"
        }],
        "created": "2015-02-13T03:20:41.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "zhihai xu",
            "key": "zxu",
            "name": "zxu",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zxu",
            "timeZone": "America/Los_Angeles"
        },
        "customfield_10010": null,
        "customfield_12310191": [{
            "id": "10343",
            "self": "https://issues.apache.org/jira/rest/api/2/customFieldOption/10343",
            "value": "Reviewed"
        }],
        "customfield_12310192": null,
        "customfield_12310220": "2015-03-16T09:00:08.802+0000",
        "customfield_12310222": "10002_*:*_1_*:*_4279113129_*|*_1_*:*_1_*:*_2695512241_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "1.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i25kv3:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Tue May 05 15:31:13 UTC 2015",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "-1 job submit time cause IllegalArgumentException when parse the Job history file name and JOB_INIT_FAILED cause -1 job submit time in JobIndexInfo.\nWe found the following job history file name which cause IllegalArgumentException when parse the job status in the job history file name.\n{code}\njob_1418398645407_115853--1-worun-kafka%2Dto%2Dhdfs%5Btwo%5D%5B15+topic%28s%29%5D-1423572836007-0-0-FAILED-root.journaling-1423572836007.jhist\n{code}\nThe stack trace for the IllegalArgumentException is\n{code}\n2015-02-10 04:54:01,863 WARN org.apache.hadoop.mapreduce.v2.hs.PartialJob: Exception while parsing job state. Defaulting to KILLED\njava.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0\n\tat java.lang.Enum.valueOf(Enum.java:236)\n\tat org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)\n\tat org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)\n\tat org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)\n\tat org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)\n\tat org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)\n\tat org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1223)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n{code}\n\n\nwhen IOException happened in JobImpl#setup, the Job submit time in JobHistoryEventHandler#MetaInfo#JobIndexInfo will not be changed and the Job submit time will be its [initial value -1|https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java#L1185].\n{code}\n      this.jobIndexInfo =\n          new JobIndexInfo(-1, -1, user, jobName, jobId, -1, -1, null,\n                           queueName);\n{code}\n\nThe following is the sequences to get -1 job submit time:\n1. \na job is created at MRAppMaster#serviceStart and  the new job is at state JobStateInternal.NEW after created\n{code}\n    job = createJob(getConfig(), forcedState, shutDownMessage);\n{code}\n\n2.\nJobEventType.JOB_INIT is sent to JobImpl from MRAppMaster#serviceStart\n{code}\n      JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);\n      // Send init to the job (this does NOT trigger job execution)\n      // This is a synchronous call, not an event through dispatcher. We want\n      // job-init to be done completely here.\n      jobEventDispatcher.handle(initJobEvent);\n{code}\n\n3.\nafter JobImpl received JobEventType.JOB_INIT, it call InitTransition#transition\n{code}\n          .addTransition\n              (JobStateInternal.NEW,\n              EnumSet.of(JobStateInternal.INITED, JobStateInternal.NEW),\n              JobEventType.JOB_INIT,\n              new InitTransition())\n{code}\n\n4.\nthen the exception happen from setup(job) in InitTransition#transition before JobSubmittedEvent is handled.\nJobSubmittedEvent will update the job submit time. Due to the exception, the submit time is still the initial value -1.\nThis is the code InitTransition#transition\n{code}\npublic JobStateInternal transition(JobImpl job, JobEvent event) {\n      job.metrics.submittedJob(job);\n      job.metrics.preparingJob(job);\n      if (job.newApiCommitter) {\n        job.jobContext = new JobContextImpl(job.conf, job.oldJobId);\n      } else {\n        job.jobContext = new org.apache.hadoop.mapred.JobContextImpl(job.conf, job.oldJobId);\n      }\n      try {\n        setup(job);\n        job.fs = job.getFileSystem(job.conf);\n        //log to job history\n        JobSubmittedEvent jse = new JobSubmittedEvent(job.oldJobId,\n              job.conf.get(MRJobConfig.JOB_NAME, \"test\"), \n            job.conf.get(MRJobConfig.USER_NAME, \"mapred\"),\n            job.appSubmitTime,\n            job.remoteJobConfFile.toString(),\n            job.jobACLs, job.queueName,\n            job.conf.get(MRJobConfig.WORKFLOW_ID, \"\"),\n            job.conf.get(MRJobConfig.WORKFLOW_NAME, \"\"),\n            job.conf.get(MRJobConfig.WORKFLOW_NODE_NAME, \"\"),\n            getWorkflowAdjacencies(job.conf),\n            job.conf.get(MRJobConfig.WORKFLOW_TAGS, \"\"));\n        job.eventHandler.handle(new JobHistoryEvent(job.jobId, jse));\n        //TODO JH Verify jobACLs, UserName via UGI?\n\n        TaskSplitMetaInfo[] taskSplitMetaInfo = createSplits(job, job.jobId);\n        job.numMapTasks = taskSplitMetaInfo.length;\n        job.numReduceTasks = job.conf.getInt(MRJobConfig.NUM_REDUCES, 0);\n\n        if (job.numMapTasks == 0 && job.numReduceTasks == 0) {\n          job.addDiagnostic(\"No of maps and reduces are 0 \" + job.jobId);\n        } else if (job.numMapTasks == 0) {\n          job.reduceWeight = 0.9f;\n        } else if (job.numReduceTasks == 0) {\n          job.mapWeight = 0.9f;\n        } else {\n          job.mapWeight = job.reduceWeight = 0.45f;\n        }\n\n        checkTaskLimits();\n\n        long inputLength = 0;\n        for (int i = 0; i < job.numMapTasks; ++i) {\n          inputLength += taskSplitMetaInfo[i].getInputDataLength();\n        }\n\n        job.makeUberDecision(inputLength);\n        \n        job.taskAttemptCompletionEvents =\n            new ArrayList<TaskAttemptCompletionEvent>(\n                job.numMapTasks + job.numReduceTasks + 10);\n        job.mapAttemptCompletionEvents =\n            new ArrayList<TaskCompletionEvent>(job.numMapTasks + 10);\n        job.taskCompletionIdxToMapCompletionIdx = new ArrayList<Integer>(\n            job.numMapTasks + job.numReduceTasks + 10);\n\n        job.allowedMapFailuresPercent =\n            job.conf.getInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 0);\n        job.allowedReduceFailuresPercent =\n            job.conf.getInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT, 0);\n\n        // create the Tasks but don't start them yet\n        createMapTasks(job, inputLength, taskSplitMetaInfo);\n        createReduceTasks(job);\n\n        job.metrics.endPreparingJob(job);\n        return JobStateInternal.INITED;\n      } catch (Exception e) {\n        LOG.warn(\"Job init failed\", e);\n        job.metrics.endPreparingJob(job);\n        job.addDiagnostic(\"Job init failed : \"\n            + StringUtils.stringifyException(e));\n        // Leave job in the NEW state. The MR AM will detect that the state is\n        // not INITED and send a JOB_INIT_FAILED event.\n        return JobStateInternal.NEW;\n      }\n    }\n{code}\n\nThis is the code JobImpl#setup\n{code}\n    protected void setup(JobImpl job) throws IOException {\n\n      String oldJobIDString = job.oldJobId.toString();\n      String user = \n        UserGroupInformation.getCurrentUser().getShortUserName();\n      Path path = MRApps.getStagingAreaDir(job.conf, user);\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"startJobs: parent=\" + path + \" child=\" + oldJobIDString);\n      }\n\n      job.remoteJobSubmitDir =\n          FileSystem.get(job.conf).makeQualified(\n              new Path(path, oldJobIDString));\n      job.remoteJobConfFile =\n          new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);\n\n      // Prepare the TaskAttemptListener server for authentication of Containers\n      // TaskAttemptListener gets the information via jobTokenSecretManager.\n      JobTokenIdentifier identifier =\n          new JobTokenIdentifier(new Text(oldJobIDString));\n      job.jobToken =\n          new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);\n      job.jobToken.setService(identifier.getJobId());\n      // Add it to the jobTokenSecretManager so that TaskAttemptListener server\n      // can authenticate containers(tasks)\n      job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);\n      LOG.info(\"Adding job token for \" + oldJobIDString\n          + \" to jobTokenSecretManager\");\n\n      // If the job client did not setup the shuffle secret then reuse\n      // the job token secret for the shuffle.\n      if (TokenCache.getShuffleSecretKey(job.jobCredentials) == null) {\n        LOG.warn(\"Shuffle secret key missing from job credentials.\"\n            + \" Using job token secret as shuffle secret.\");\n        TokenCache.setShuffleSecretKey(job.jobToken.getPassword(),\n            job.jobCredentials);\n      }\n    }\n{code}\n\n5.\nDue to the IOException from  JobImpl#setup, the new job is still at state JobStateInternal.NEW\n{code}\n      } catch (Exception e) {\n        LOG.warn(\"Job init failed\", e);\n        job.metrics.endPreparingJob(job);\n        job.addDiagnostic(\"Job init failed : \"\n            + StringUtils.stringifyException(e));\n        // Leave job in the NEW state. The MR AM will detect that the state is\n        // not INITED and send a JOB_INIT_FAILED event.\n        return JobStateInternal.NEW;\n      }\n{code}\nAt the following code of MRAppMaster#serviceStart, The MR AM detect the state is not INITED and send a JOB_INIT_FAILED event.\n{code}\n      // If job is still not initialized, an error happened during\n      // initialization. Must complete starting all of the services so failure\n      // events can be processed.\n      initFailed = (((JobImpl)job).getInternalState() != JobStateInternal.INITED);\n    if (initFailed) {\n      JobEvent initFailedEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT_FAILED);\n      jobEventDispatcher.handle(initFailedEvent);\n    } else {\n      // All components have started, start the job.\n      startJobs();\n    }\n{code}\n\n6.\nAfter JobImpl receives the JOB_INIT_FAILED, it will call InitFailedTransition#transition and enter state JobStateInternal.FAIL_ABORT\n{code}\n          .addTransition(JobStateInternal.NEW, JobStateInternal.FAIL_ABORT,\n              JobEventType.JOB_INIT_FAILED,\n              new InitFailedTransition())\n{code}\n\n7.\nJobImpl will send CommitterJobAbortEvent in  InitFailedTransition#transition \n{code}\n    public void transition(JobImpl job, JobEvent event) {\n        job.eventHandler.handle(new CommitterJobAbortEvent(job.jobId,\n                job.jobContext,\n                org.apache.hadoop.mapreduce.JobStatus.State.FAILED));\n    }\n{code}\n\n8.\nCommitterJobAbortEvent will be handled by CommitterEventHandler#handleJobAbort which will send JobAbortCompletedEvent(JobEventType.JOB_ABORT_COMPLETED)\n{code}\n    protected void handleJobAbort(CommitterJobAbortEvent event) {\n      cancelJobCommit();\n      try {\n        committer.abortJob(event.getJobContext(), event.getFinalState());\n      } catch (Exception e) {\n        LOG.warn(\"Could not abort job\", e);\n      }\n      context.getEventHandler().handle(new JobAbortCompletedEvent(\n          event.getJobID(), event.getFinalState()));\n    }\n{code}\n\n9.\nAfter JobImpl receives the JOB_ABORT_COMPLETED, it will call JobAbortCompletedTransition#transition and enter state JobStateInternal.FAILED\n{code}\n          .addTransition(JobStateInternal.FAIL_ABORT, JobStateInternal.FAILED,\n              JobEventType.JOB_ABORT_COMPLETED,\n              new JobAbortCompletedTransition())\n{code}\n\n10.\nJobAbortCompletedTransition#transition will call JobImpl#unsuccessfulFinish which will send JobUnsuccessfulCompletionEvent with finish time.\n{code}\n    public void transition(JobImpl job, JobEvent event) {\n      JobStateInternal finalState = JobStateInternal.valueOf(\n          ((JobAbortCompletedEvent) event).getFinalState().name());\n      job.unsuccessfulFinish(finalState);\n    }\n  private void unsuccessfulFinish(JobStateInternal finalState) {\n      if (finishTime == 0) setFinishTime();\n      cleanupProgress = 1.0f;\n      JobUnsuccessfulCompletionEvent unsuccessfulJobEvent =\n          new JobUnsuccessfulCompletionEvent(oldJobId,\n              finishTime,\n              succeededMapTaskCount,\n              succeededReduceTaskCount,\n              finalState.toString(),\n              diagnostics);\n      eventHandler.handle(new JobHistoryEvent(jobId,\n          unsuccessfulJobEvent));\n      finished(finalState);\n  }\n{code}\n\n11.\nJobUnsuccessfulCompletionEvent will be handled by JobHistoryEventHandler#handleEvent with type EventType.JOB_FAILED\nBased on the following code, you can see the JobIndexInfo#finishTime is set correctly but JobIndexInfo#submitTime and  JobIndexInfo#jobStartTime are still -1.\n{code}\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_FAILED\n          || event.getHistoryEvent().getEventType() == EventType.JOB_KILLED) {\n        try {\n          JobUnsuccessfulCompletionEvent jucEvent = \n              (JobUnsuccessfulCompletionEvent) event\n              .getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\n          closeEventWriter(event.getJobID());\n          processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n{code}\n\nThe error job history file name in our log is \"job_1418398645407_115853--1-worun-kafka%2Dto%2Dhdfs%5Btwo%5D%5B15+topic%28s%29%5D-1423572836007-0-0-FAILED-root.journaling-1423572836007.jhist\"\nBased on the filename, you can see submitTime is -1, finishTime is 1423572836007 and jobStartTime is 1423572836007.\nThe jobStartTime is not -1, and  jobStartTime is the same as  finishTime.\nIt is because jobStartTime is handled specially in FileNameIndexUtils#getDoneFileName:\n{code}\n    //JobStartTime\n    if (indexInfo.getJobStartTime() >= 0) {\n      sb.append(indexInfo.getJobStartTime());\n    } else {\n      sb.append(indexInfo.getFinishTime());\n    }\n{code}\n\n",
        "duedate": null,
        "environment": null,
        "fixVersions": [
            {
                "archived": false,
                "description": "2.8.0 release",
                "id": "12329060",
                "name": "2.8.0",
                "releaseDate": "2017-03-22",
                "released": true,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12329060"
            },
            {
                "archived": false,
                "description": "2.7.1 release",
                "id": "12331978",
                "name": "2.7.1",
                "releaseDate": "2015-07-06",
                "released": true,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12331978"
            },
            {
                "archived": false,
                "description": "3.0.0-alpha1 release",
                "id": "12335734",
                "name": "3.0.0-alpha1",
                "releaseDate": "2016-09-03",
                "released": true,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12335734"
            }
        ],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096"
            },
            "id": "12310941",
            "key": "MAPREDUCE",
            "name": "Hadoop Map/Reduce",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310941"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "zhihai xu",
            "key": "zxu",
            "name": "zxu",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zxu",
            "timeZone": "America/Los_Angeles"
        },
        "resolution": {
            "description": "A fix for this issue is checked into the tree and tested.",
            "id": "1",
            "name": "Fixed",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1"
        },
        "resolutiondate": "2015-05-04T20:44:27.000+0000",
        "status": {
            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
            "id": "6",
            "name": "Closed",
            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "IllegalArgumentException due to missing job submit time",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2017-01-06T00:57:45.000+0000",
        "versions": [],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-6259/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-6259/watchers",
            "watchCount": 5
        },
        "workratio": -1
    },
    "id": "12774779",
    "key": "MAPREDUCE-6259",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12774779"
}