{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12818939","self":"https://issues.apache.org/jira/rest/api/2/issue/12818939","key":"MAPREDUCE-6312","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-05-01T15:55:43.687+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon May 04 19:17:19 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-6312/watchers","watchCount":7,"isWatching":false},"created":"2015-04-07T14:01:36.206+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326265","id":"12326265","description":"2.5.0 release","name":"2.5.0","archived":false,"released":true,"releaseDate":"2014-08-11"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-05-04T19:17:19.199+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312982","id":"12312982","name":"client","description":"Code for submitting and monitoring jobs"}],"timeoriginalestimate":null,"description":"ClientServiceDelegate initializes its realProxy field to AMProxy for a new or running job. Later when the job finishes it will not update this proxy to query history server and AM will not return valid data for this job.\n\nWe found this while investigating https://issues.cloudera.org/browse/DISTRO-631 that describes Hive failure because it uses loop like \n{code}\n  progress(JobClient jc, RunningJob rj) { ...\n        while (!rj.isComplete() || (extraRounds > 0)) {\n            try {\n                Thread.sleep(1000);\n            } catch (InterruptedException e) {\n            }\n\n            RunningJob newRj = jc.getJob(rj.getID());\n            if (newRj == null) {\n                // under exceptional load, hadoop may not be able to look up status\n                // of finished jobs (because it has purged them from memory). From\n                // hive's perspective - it's equivalent to the job having failed.\n                // So raise a meaningful exception\n                throw new IOException(\"Could not find status of job:\" + rj.getID());\n            } else {\n                rj = newRj;\n            }\n        }\n{code}\nIn this snippet JobClient.getJob will try to create RunningJob instance referring to job file in /user/$USER/.staging even when job is finished and the file is moved to /user/history/done (or /user/history/done_intermediate). \n\nNote that Hive queries can succeed if there is a timing where HDFS performs actual file delete with a delay.\n\nWe can try to write a patch if there is an agreement that this should be fixed.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hive fails due to stale proxy in ClientServiceDelegate","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=radimk","name":"radimk","key":"radimk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Radim Kubacki","active":true,"timeZone":"Europe/Prague"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=radimk","name":"radimk","key":"radimk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Radim Kubacki","active":true,"timeZone":"Europe/Prague"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12818939/comment/14521286","id":"14521286","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=radimk","name":"radimk","key":"radimk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Radim Kubacki","active":true,"timeZone":"Europe/Prague"},"body":"Another related bug filed against Hive - https://issues.apache.org/jira/browse/HIVE-8339 This time there is a patch that has workaround for this problem applied on Hive's side. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=radimk","name":"radimk","key":"radimk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Radim Kubacki","active":true,"timeZone":"Europe/Prague"},"created":"2015-04-30T10:26:37.051+0000","updated":"2015-04-30T10:26:37.051+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12818939/comment/14523360","id":"14523360","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=khajasmh","name":"khajasmh","key":"khajasmh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Khaja Hussain","active":true,"timeZone":"America/New_York"},"body":"We had very similar issue. And found out the /var directory on namenode was full and namenode came down which caused our hive query to fail with this error. I have the details below.\n\nError Log from Hive:\n**************************\n\n2015-05-01 08:51:35,774 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1818.78 sec\njava.io.IOException: Could not find status of job:job_1422659026472_30162\n        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:294)\n        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:547)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:426)\n        at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:136)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1508)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1275)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1093)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:916)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:906)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:359)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:456)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:466)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:749)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nEnded Job = job_1422659026472_30162 with exception 'java.io.IOException(Could not find status of job:job_1422659026472_30162)'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\nMapReduce Jobs Launched:\nJob 0: Map: 3  Reduce: 5   Cumulative CPU: 5376.95 sec   HDFS Read: 4371724652 HDFS Write: 3100170286 SUCCESS\n\n\nLog from NameNode:\n*****************************\n\nlog4j:ERROR Failed to flush writer,\njava.io.IOException: No space left on device\n        at java.io.FileOutputStream.writeBytes(Native Method)\n        at java.io.FileOutputStream.write(FileOutputStream.java:282)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:202)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:272)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:276)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:122)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:212)\n        at org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:59)\n        at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:324)\n        at org.apache.log4j.RollingFileAppender.subAppend(RollingFileAppender.java:276)\n        at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)\n        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n        at org.apache.log4j.Category.callAppenders(Category.java:206)\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.log(Category.java:856)\n        at org.apache.commons.logging.impl.Log4JLogger.info(Log4JLogger.java:176)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2027)\nlog4j:ERROR Failed to flush writer,\njava.io.IOException: No space left on device\n        at java.io.FileOutputStream.writeBytes(Native Method)\n        at java.io.FileOutputStream.write(FileOutputStream.java:282)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:202)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:272)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:276)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:122)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:212)\n        at org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:59)\n        at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:324)\n        at org.apache.log4j.DailyRollingFileAppender.subAppend(DailyRollingFileAppender.java:369)\n        at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)\n        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n        at org.apache.log4j.Category.callAppenders(Category.java:206)\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.log(Category.java:856)\n        at org.apache.commons.logging.impl.Log4JLogger.info(Log4JLogger.java:176)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:7792)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditEvent(FSNamesystem.java:7787)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent(FSNamesystem.java:340)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent(FSNamesystem.java:320)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent(FSNamesystem.java:314)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:3532)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileLinkInfo(NameNodeRpcServer.java:796)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileLinkInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:780)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n        at java.security.AccessController.doPrivileged(Native Method)\n\nLogs from YARN from one of our datanodes:\n*********************************************************\n\n2015-05-01 08:47:39,234 ERROR logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:uploadLogsForContainer(115)) - Cannot create writer for app application_1422659026472_30160. Disabling log-aggregation for this app.\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create file/app-logs/aiqa/logs/application_1422659026472_30160/lpwhp6.npd.com_45454.tmp. Name node is in safe mode.\nResources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off.\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1207)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2235)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2190)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:526)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1410)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at $Proxy75.create(Unknown Source)\n        at sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at $Proxy75.create(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:258)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=khajasmh","name":"khajasmh","key":"khajasmh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Khaja Hussain","active":true,"timeZone":"America/New_York"},"created":"2015-05-01T15:55:43.687+0000","updated":"2015-05-01T15:55:43.687+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12818939/comment/14526434","id":"14526434","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=radimk","name":"radimk","key":"radimk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Radim Kubacki","active":true,"timeZone":"Europe/Prague"},"body":"To Khaja: clue to your problem is IOException 'no space left on device' and it is apparently a different issue. What I reported can be reproduced on a working cluster with a simple map-reduce client.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=radimk","name":"radimk","key":"radimk","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Radim Kubacki","active":true,"timeZone":"Europe/Prague"},"created":"2015-05-04T08:58:29.757+0000","updated":"2015-05-04T08:58:29.757+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12818939/comment/14527098","id":"14527098","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=khajasmh","name":"khajasmh","key":"khajasmh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Khaja Hussain","active":true,"timeZone":"America/New_York"},"body":"To Radim: My failure had the same exception triggered. It is clear from the log that my issue is different from your. In this case you can ignore the comments. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=khajasmh","name":"khajasmh","key":"khajasmh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Khaja Hussain","active":true,"timeZone":"America/New_York"},"created":"2015-05-04T19:17:19.199+0000","updated":"2015-05-04T19:17:19.199+0000"}],"maxResults":4,"total":4,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-6312/votes","votes":3,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2cwsf:"}}