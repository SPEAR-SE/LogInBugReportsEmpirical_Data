{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12523922","self":"https://issues.apache.org/jira/rest/api/2/issue/12523922","key":"MAPREDUCE-3065","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2011-09-21T21:21:59.828+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Nov 13 18:51:15 UTC 2015","customfield_12310420":"35505","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_69583059_*|*_4_*:*_1_*:*_22428_*|*_5_*:*_2_*:*_130731586606","customfield_12312321":null,"resolutiondate":"2015-11-13T18:51:15.290+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-3065/watchers","watchCount":2,"isWatching":false},"created":"2011-09-21T21:11:23.291+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320354","id":"12320354","description":"hadoop-2.0.0-alpha release","name":"2.0.0-alpha","archived":false,"released":true,"releaseDate":"2012-05-23"}],"issuelinks":[{"id":"12448669","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12448669","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"inwardIssue":{"id":"12524237","key":"MAPREDUCE-3068","self":"https://issues.apache.org/jira/rest/api/2/issue/12524237","fields":{"summary":"Should set MALLOC_ARENA_MAX for all YARN daemons and AMs/Containers","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-11-13T18:51:15.324+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12315341","id":"12315341","name":"nodemanager"}],"timeoriginalestimate":null,"description":"> Hey Vinod,\n> \n> OK, so I have a little more clarity into this.\n> \n> When I bump my resource request for my AM to 4096, it runs. The important line in the NM logs is:\n> \n> 2011-09-21 13:43:44,366 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(402)) - Memory usage of ProcessTree 25656 for container-id container_1316637655278_0001_01_000001 : Virtual 2260938752 bytes, limit : 4294967296 bytes; Physical 120860672 bytes, limit -1 bytes\n> \n> The thing to note is the virtual memory, which is off the charts, even though my physical memory is almost nothing (12 megs). I'm still poking around the code, but I am noticing that there are two checks in the NM, one for virtual mem, and one for physical mem. The virtual memory check appears to be toggle-able, but is presumably defaulted to on.\n> \n> At this point I'm trying to figure out exactly what the VMEM check is for, why YARN thinks my app is taking 2 gigs, and how to fix this.\n> \n> Cheers,\n> Chris\n> ________________________________________\n> From: Chris Riccomini [criccomini@linkedin.com]\n> Sent: Wednesday, September 21, 2011 1:42 PM\n> To: mapreduce-dev@hadoop.apache.org\n> Subject: Re: ApplicationMaster Memory Usage\n> \n> For the record, I bumped to 4096 for memory resource request, and it works.\n> :(\n> \n> \n> On 9/21/11 1:32 PM, \"Chris Riccomini\" <criccomini@linkedin.com> wrote:\n> \n>> Hey Vinod,\n>> \n>> So, I ran my application master directly from the CLI. I commented out the\n>> YARN-specific code. It runs fine without leaking memory.\n>> \n>> I then ran it from YARN, with all YARN-specific code commented it. It again\n>> ran fine.\n>> \n>> I then uncommented JUST my registerWithResourceManager call. It then fails\n>> with OOM after a few seconds. I call registerWithResourceManager, and then go\n>> into a while(true) { println(\"yeh\") sleep(1000) }. Doing this prints:\n>> \n>> yeh\n>> yeh\n>> yeh\n>> yeh\n>> yeh\n>> \n>> At which point, it dies, and, in the NodeManager,I see:\n>> \n>> 2011-09-21 13:24:51,036 WARN  monitor.ContainersMonitorImpl\n>> (ContainersMonitorImpl.java:isProcessTreeOverLimit(289)) - Process tree for\n>> container: container_1316626117280_0005_01_000001 has processes older than 1\n>> iteration running over the configured limit. Limit=2147483648, current usage =\n>> 2192773120\n>> 2011-09-21 13:24:51,037 WARN  monitor.ContainersMonitorImpl\n>> (ContainersMonitorImpl.java:run(453)) - Container\n>> [pid=23852,containerID=container_1316626117280_0005_01_000001] is running\n>> beyond memory-limits. Current usage : 2192773120bytes. Limit :\n>> 2147483648bytes. Killing container.\n>> Dump of the process-tree for container_1316626117280_0005_01_000001 :\n>> |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS)\n>> VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n>> |- 23852 20570 23852 23852 (bash) 0 0 108638208 303 /bin/bash -c java -Xmx512M\n>> -cp './package/*' kafka.yarn.ApplicationMaster\n>> /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 5 1 1316626117280\n>> com.linkedin.TODO 1\n>> 1>/tmp/logs/application_1316626117280_0005/container_1316626117280_0005_01_000\n>> 001/stdout\n>> 2>/tmp/logs/application_1316626117280_0005/container_1316626117280_0005_01_000\n>> 001/stderr\n>> |- 23855 23852 23852 23852 (java) 81 4 2084134912 14772 java -Xmx512M -cp\n>> ./package/* kafka.yarn.ApplicationMaster\n>> /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 5 1 1316626117280\n>> com.linkedin.TODO 1\n>> 2011-09-21 13:24:51,037 INFO  monitor.ContainersMonitorImpl\n>> (ContainersMonitorImpl.java:run(463)) - Removed ProcessTree with root 23852\n>> \n>> Either something is leaking in YARN, or my registerWithResourceManager code\n>> (see below) is doing something funky.\n>> \n>> I'm trying to avoid going through all the pain of attaching a remote debugger.\n>> Presumably things aren't leaking in YARN, which means it's likely that I'm\n>> doing something wrong in my registration code.\n>> \n>> Incidentally, my NodeManager is running with 1000 megs. My application master\n>> memory is set to 2048, and my -Xmx setting is 512M\n>> \n>> Cheers,\n>> Chris\n>> ________________________________________\n>> From: Vinod Kumar Vavilapalli [vinodkv@hortonworks.com]\n>> Sent: Wednesday, September 21, 2011 11:52 AM\n>> To: mapreduce-dev@hadoop.apache.org\n>> Subject: Re: ApplicationMaster Memory Usage\n>> \n>> Actually MAPREDUCE-2998 is only related to MRV2, so that isn't related.\n>> \n>> Somehow, your JVM itself is taking so much of virtual memory. Are you\n>> loading some native libs?\n>> \n>> And how many containers have already been allocated by the time the AM\n>> crashes. May be you are accumulating some per-container data. You can try\n>> dumping heap vai hprof.\n>> \n>> +Vinod\n>> \n>> \n>> On Wed, Sep 21, 2011 at 11:21 PM, Chris Riccomini\n>> <criccomini@linkedin.com>wrote:\n>> \n>>> Hey Vinod,\n>>> \n>>> I svn up'd, and rebuilt. My application's task (container) now runs!\n>>> \n>>> Unfortunately, my application master eventually gets killed by the\n>>> NodeManager anyway, and I'm still not clear as to why. The AM is just\n>>> running a loop, asking for a container, and executing a command in the\n>>> container. It keeps doing this over and over again. After a few iterations,\n>>> it gets killed with something like:\n>>> \n>>> 2011-09-21 10:42:40,869 INFO  monitor.ContainersMonitorImpl\n>>> (ContainersMonitorImpl.java:run(402)) - Memory usage of ProcessTree 21666\n>>> for container-id container_1316626117280_0002_01_000001 : Virtual 2260938752\n>>> bytes, limit : 2147483648 bytes; Physical 77398016 bytes, limit -1 bytes\n>>> 2011-09-21 10:42:40,869 WARN  monitor.ContainersMonitorImpl\n>>> (ContainersMonitorImpl.java:isProcessTreeOverLimit(289)) - Process tree for\n>>> container: container_1316626117280_0002_01_000001 has processes older than 1\n>>> iteration running over the configured limit. Limit=2147483648, current usage\n>>> = 2260938752\n>>> 2011-09-21 10:42:40,870 WARN  monitor.ContainersMonitorImpl\n>>> (ContainersMonitorImpl.java:run(453)) - Container\n>>> [pid=21666,containerID=container_1316626117280_0002_01_000001] is running\n>>> beyond memory-limits. Current usage : 2260938752bytes. Limit :\n>>> 2147483648bytes. Killing container.\n>>> Dump of the process-tree for container_1316626117280_0002_01_000001 :\n>>>        |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS)\n>>> SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n>>>        |- 21669 21666 21666 21666 (java) 105 4 2152300544 18593 java\n>>> -Xmx512M -cp ./package/* kafka.yarn.ApplicationMaster\n>>> /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 2 1 1316626117280\n>>> com.linkedin.TODO 1\n>>>       |- 21666 20570 21666 21666 (bash) 0 0 108638208 303 /bin/bash -c\n>>> java -Xmx512M -cp './package/*' kafka.yarn.ApplicationMaster\n>>> /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 2 1 1316626117280\n>>> com.linkedin.TODO 1\n>>> 1>/tmp/logs/application_1316626117280_0002/container_1316626117280_0002_01_00\n>>> 0001/stdout\n>>> 2>/tmp/logs/application_1316626117280_0002/container_1316626117280_0002_01_00\n>>> 0001/stderr\n>>> \n>>> 2011-09-21 10:42:40,870 INFO  monitor.ContainersMonitorImpl\n>>> (ContainersMonitorImpl.java:run(463)) - Removed ProcessTree with root 21666\n>>> \n>>> I don't think that my AM is leaking memory. Full code paste after the break\n>>> \n>>> 1. Do I need to release a container in my AM even if the AM receives it as\n>>> a finished container in the resource request response?\n>>> 2. Do I need to free any other resources after a resource request (e.g.\n>>> ResourceRequest, AllocateRequest, etc)?\n>>> \n>>> Cheers,\n>>> Chris\n>>> \n>>> \n>>> def main(args: Array[String]) {\n>>>   // YARN will always give our ApplicationMaster\n>>>   // the first four parameters as: <package> <app id> <attempt id>\n>>> <timestamp>\n>>>   val packagePath = args(0)\n>>>   val appId = args(1).toInt\n>>>   val attemptId = args(2).toInt\n>>>   val timestamp = args(3).toLong\n>>> \n>>>   // these are our application master's parameters\n>>>   val streamerClass = args(4)\n>>>   val tasks = args(5).toInt\n>>> \n>>>   // TODO log params here\n>>> \n>>>   // start the application master helper\n>>>   val conf = new Configuration\n>>>   val applicationMasterHelper = new ApplicationMasterHelper(appId,\n>>> attemptId, timestamp, conf)\n>>>     .registerWithResourceManager\n>>> \n>>>   // start and manage the slaves\n>>>   val noReleases = List[ContainerId]()\n>>>   var runningContainers = 0\n>>> \n>>>   // keep going forever\n>>>   while (true) {\n>>>     val nonRunningTasks = tasks - runningContainers\n>>>     val response =\n>>> applicationMasterHelper.sendResourceRequest(nonRunningTasks, noReleases)\n>>> \n>>>     response.getAllocatedContainers.foreach(container => {\n>>>       new ContainerExecutor(packagePath, container)\n>>>         .addCommand(\"java -Xmx256M -cp './package/*'\n>>> kafka.yarn.StreamingTask \" + streamerClass + \" \"\n>>>           + \"1>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/stdout \"\n>>>           + \"2>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR +\n>>> \"/stderr\").execute(conf)\n>>>     })\n>>> \n>>>     runningContainers += response.getAllocatedContainers.length\n>>>     runningContainers -= response.getCompletedContainersStatuses.length\n>>> \n>>>     Thread.sleep(1000)\n>>>   }\n>>> \n>>>   applicationMasterHelper.unregisterWithResourceManager(\"SUCCESS\")\n>>> }\n>>> \n>>> \n>>> class ApplicationMasterHelper(iAppId: Int, iAppAttemptId: Int, lTimestamp:\n>>> Long, conf: Configuration) {\n>>> val rpc = YarnRPC.create(conf)\n>>> val appId = Records.newRecord(classOf[ApplicationId])\n>>> val appAttemptId = Records.newRecord(classOf[ApplicationAttemptId])\n>>> val rmAddress =\n>>> NetUtils.createSocketAddr(conf.get(YarnConfiguration.RM_SCHEDULER_ADDRESS,\n>>> YarnConfiguration.DEFAULT_RM_SCHEDULER_ADDRESS))\n>>> val resourceManager = rpc.getProxy(classOf[AMRMProtocol], rmAddress,\n>>> conf).asInstanceOf[AMRMProtocol]\n>>> var requestId = 0\n>>> \n>>> appId.setClusterTimestamp(lTimestamp)\n>>> appId.setId(iAppId)\n>>> appAttemptId.setApplicationId(appId)\n>>> appAttemptId.setAttemptId(iAppAttemptId)\n>>> \n>>> def registerWithResourceManager(): ApplicationMasterHelper = {\n>>>   val req = Records.newRecord(classOf[RegisterApplicationMasterRequest])\n>>>   req.setApplicationAttemptId(appAttemptId)\n>>>   // TODO not sure why these are blank- This is how spark does it\n>>>   req.setHost(\"\")\n>>>   req.setRpcPort(1)\n>>>   req.setTrackingUrl(\"\")\n>>>   resourceManager.registerApplicationMaster(req)\n>>>   this\n>>> }\n>>> \n>>> def unregisterWithResourceManager(state: String): ApplicationMasterHelper\n>>> = {\n>>>   val finReq = Records.newRecord(classOf[FinishApplicationMasterRequest])\n>>>   finReq.setAppAttemptId(appAttemptId)\n>>>   finReq.setFinalState(state)\n>>>   resourceManager.finishApplicationMaster(finReq)\n>>>   this\n>>> }\n>>> \n>>> def sendResourceRequest(containers: Int, release: List[ContainerId]):\n>>> AMResponse = {\n>>>   // TODO will need to make this more flexible for hostname requests, etc\n>>>   val request = Records.newRecord(classOf[ResourceRequest])\n>>>   val pri = Records.newRecord(classOf[Priority])\n>>>   val capability = Records.newRecord(classOf[Resource])\n>>>   val req = Records.newRecord(classOf[AllocateRequest])\n>>>   request.setHostName(\"*\")\n>>>   request.setNumContainers(containers)\n>>>   pri.setPriority(1)\n>>>   request.setPriority(pri)\n>>>   capability.setMemory(128)\n>>>   request.setCapability(capability)\n>>>   req.setResponseId(requestId)\n>>>   req.setApplicationAttemptId(appAttemptId)\n>>>   req.addAllAsks(Lists.newArrayList(request))\n>>>   req.addAllReleases(release)\n>>>   requestId += 1\n>>>   // TODO we might want to return a list of container executors here\n>>> instead of AMResponses\n>>>   resourceManager.allocate(req).getAMResponse\n>>> }\n>>> }\n>>> \n>>> \n>>> ________________________________________\n>>> From: Vinod Kumar Vavilapalli [vinodkv@hortonworks.com]\n>>> Sent: Wednesday, September 21, 2011 10:08 AM\n>>> To: mapreduce-dev@hadoop.apache.org\n>>> Subject: Re: ApplicationMaster Memory Usage\n>>> \n>>> Yes, the process-dump clearly tells that this is MAPREDUCE-2998.\n>>> \n>>> +Vinod\n>>> (With a smirk to see his container-memory-monitoring code in action)\n>>> \n>>> \n>>> On Wed, Sep 21, 2011 at 10:26 PM, Arun C Murthy <acm@hortonworks.com>\n>>> wrote:\n>>> \n>>>> I'll bet you are hitting MR-2998.\n>>>> \n>>>> From the changelog:\n>>>> \n>>>>   MAPREDUCE-2998. Fixed a bug in TaskAttemptImpl which caused it to fork\n>>>> bin/mapred too many times. Contributed by Vinod K V.\n>>>> \n>>>> Arun\n>>>> \n>>>> On Sep 21, 2011, at 9:52 AM, Chris Riccomini wrote:\n>>>> \n>>>>> Hey Guys,\n>>>>> \n>>>>> My ApplicationMaster is being killed by the NodeManager because of\n>>> memory\n>>>> consumption, and I don't understand why. I'm using -Xmx512M, and setting\n>>> my\n>>>> resource request to 2048.\n>>>>> \n>>>>> \n>>>>>   .addCommand(\"java -Xmx512M -cp './package/*'\n>>>> kafka.yarn.ApplicationMaster \" ...\n>>>>> \n>>>>>   ...\n>>>>> \n>>>>>   private var memory = 2048\n>>>>> \n>>>>>   resource.setMemory(memory)\n>>>>>   containerCtx.setResource(resource)\n>>>>>   containerCtx.setCommands(cmds.toList)\n>>>>>   containerCtx.setLocalResources(Collections.singletonMap(\"package\",\n>>>> packageResource))\n>>>>>   appCtx.setApplicationId(appId)\n>>>>>   appCtx.setUser(user.getShortUserName)\n>>>>>   appCtx.setAMContainerSpec(containerCtx)\n>>>>>   request.setApplicationSubmissionContext(appCtx)\n>>>>>   applicationsManager.submitApplication(request)\n>>>>> \n>>>>> When this runs, I see (in my NodeManager's logs):\n>>>>> \n>>>>> \n>>>>> 2011-09-21 09:35:19,112 INFO  monitor.ContainersMonitorImpl\n>>>> (ContainersMonitorImpl.java:run(402)) - Memory usage of ProcessTree 28134\n>>>> for container-id container_1316559026783_0003_01_000001 : Virtual\n>>> 2260938752\n>>>> bytes, limit : 2147483648 bytes; Physical 71540736 bytes, limit -1 bytes\n>>>>> 2011-09-21 09:35:19,112 WARN  monitor.ContainersMonitorImpl\n>>>> (ContainersMonitorImpl.java:isProcessTreeOverLimit(289)) - Process tree\n>>> for\n>>>> container: container_1316559026783_0003_01_000001 has processes older\n>>> than 1\n>>>> iteration running over the configured limit. Limit=2147483648, current\n>>> usage\n>>>> = 2260938752\n>>>>> 2011-09-21 09:35:19,113 WARN  monitor.ContainersMonitorImpl\n>>>> (ContainersMonitorImpl.java:run(453)) - Container\n>>>> [pid=28134,containerID=container_1316559026783_0003_01_000001] is running\n>>>> beyond memory-limits. Current usage : 2260938752bytes. Limit :\n>>>> 2147483648bytes. Killing container.\n>>>>> Dump of the process-tree for container_1316559026783_0003_01_000001 :\n>>>>>      |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS)\n>>>> SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n>>>>>      |- 28134 25886 28134 28134 (bash) 0 0 108638208 303 /bin/bash -c\n>>>> java -Xmx512M -cp './package/*' kafka.yarn.ApplicationMaster 3 1\n>>>> 1316559026783 com.linkedin.TODO 1\n>>>> \n>>> 1>/tmp/logs/application_1316559026783_0003/container_1316559026783_0003_01_00\n>>> 0001/stdout\n>>>> \n>>> 2>/tmp/logs/application_1316559026783_0003/container_1316559026783_0003_01_00\n>>> 0001/stderr\n>>>>>      |- 28137 28134 28134 28134 (java) 92 3 2152300544 17163 java\n>>>> -Xmx512M -cp ./package/* kafka.yarn.ApplicationMaster 3 1 1316559026783\n>>>> com.linkedin.TODO 1\n>>>>> \n>>>>> 2011-09-21 09:35:19,113 INFO  monitor.ContainersMonitorImpl\n>>>> (ContainersMonitorImpl.java:run(463)) - Removed ProcessTree with root\n>>> 28134\n>>>>> \n>>>>> It appears that YARN is honoring my 2048 command, yet my process is\n>>>> somehow taking 2260938752 bytes. I don't think that I'm using nearly that\n>>>> much in permgen, and my heap is limited to 512. I don't have any JNI\n>>> stuff\n>>>> running (that I know of), so it's unclear to me what's going on here. The\n>>>> only thing that I can think of is that Java's Runtime exec is forking,\n>>> and\n>>>> copying its entire JVM memory footprint for the fork.\n>>>>> \n>>>>> Has anyone seen this? Am I doing something dumb?\n>>>>> \n>>>>> Thanks!\n>>>>> Chris\n>>>> \n>>>> \n>>> \n> \n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"52377","customfield_12312823":null,"summary":"ApplicationMaster killed by NodeManager due to excessive virtual memory consumption","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13109876","id":"13109876","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"I've been doing more digging. With some understanding of what the container monitor is doing now, I went back and checked the process tree dump when my application master was killed. What I see is that there actually appear to be two processes running:\n\n |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n |- 21669 21666 21666 21666 (java) 105 4 2152300544 18593 java -Xmx512M -cp ./package/* kafka.yarn.ApplicationMaster /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 2 1 1316626117280 com.linkedin.TODO 1\n |- 21666 20570 21666 21666 (bash) 0 0 108638208 303 /bin/bash -c java -Xmx512M -cp './package/*' kafka.yarn.ApplicationMaster /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 2 1 1316626117280 com.linkedin.TODO 1 1>/tmp/logs/application_1316626117280_0002/container_1316626117280_0002_01_000001/stdout 2>/tmp/logs/application_1316626117280_0002/container_1316626117280_0002_01_000001/stderr\n\nSo it appears that the bash command (which I assume is coming from task.sh) is taking almost no memory, but the java command that it triggers is taking 2 gigs in virtual memory. No idea why. When I run the bash -c .. command outside of YARN, it's taking about 10 megs of physical memory, and almost no virtual memory.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T21:14:41.066+0000","updated":"2011-09-21T21:14:41.066+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13109879","id":"13109879","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"cat'd out task.sh for my AM:\n\n\n#!/bin/bash\n\nexport YARN_HOME=\"/tmp/hadoop-mapreduce/hadoop-mapreduce-0.24.0-SNAPSHOT-LATEST/bin/..\"\nexport YARN_LOCAL_DIRS=\"/tmp/nm-local-dir/usercache/criccomi/appcache/application_1316638915485_0002\"\nexport JVM_PID=\"$$\"\nexport HADOOP_TOKEN_FILE_LOCATION=\"/tmp/nm-local-dir/usercache/criccomi/appcache/application_1316638915485_0002/container_1316638915485_0002_01_000001/container_tokens\"\nln -sf /tmp/nm-local-dir/usercache/criccomi/appcache/application_1316638915485_0002/filecache/-3545403967124200139/kafka-streamer.tgz package\nexec setsid /bin/bash -c \"cat ./task.sh 1>/tmp/logs/application_1316638915485_0002/container_1316638915485_0002_01_000001/stdout 2>/tmp/logs/application_1316638915485_0002/container_1316638915485_0002_01_000001/stderr \"\n\nObviously, my java ... command goes where the `cat ./task.sh` is.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T21:19:06.902+0000","updated":"2011-09-21T21:19:06.902+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13109884","id":"13109884","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"One reason for VMEM spikes are forks, are u seeing any?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T21:21:59.828+0000","updated":"2011-09-21T21:21:59.828+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13110789","id":"13110789","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Regarding what I said in the email above, when I comment out the registerWithResourceManager, I don't get killed. I re-checked the process usage when I do this:\n\n\n2011-09-21 14:29:24,914 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(402)) - Memory usage of ProcessTree 28080 for container-id container_1316640481414_0001_01_000001 : Virtual 2123530240 bytes, limit : 2147483648 bytes; Physical 61272064 bytes, limit -1 bytes\n\nIt appears that without the registerWithResourceManager call, things are staying just BARELY under the kill limit, so it's not the register call that's leaking memory. Wondering if it's something to do with Java's fork behavior?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T21:30:19.495+0000","updated":"2011-09-21T21:30:19.495+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13111813","id":"13111813","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Arun,\n\nAm I seeing any spikes, or forks? Doesn't it always fork when it execs task.sh? As far as spikes go, it would appear that it's what I'm seeing. The odd thing is that it doesn't seem to be going down. I tried suggesting a System.gc(), but it didn't help. The process is sitting just under the 2 gig kill limit with vmem usage.\n\nCheers,\nChris","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T21:33:42.819+0000","updated":"2011-09-21T21:33:42.819+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112153","id":"13112153","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hmm, I dropped my NM's -Xmx setting to 512m from 1024m. The VMEM setting STILL sits at just under the 2 gig kill limit.\n\nThis seems pretty interesting: http://stackoverflow.com/questions/561245/virtual-memory-usage-from-java-under-linux-too-much-memory-used\n\nReading.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T21:53:17.075+0000","updated":"2011-09-21T21:53:17.075+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112161","id":"13112161","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"OK, I took a pmap of my application master, and, unsurprisingly, it's taking nearly 2 gigs. When I looked at the individual entries, I see a lot of normal stuff, but then a few really massive anon's:\n\n00007fedc8021000  65404K -----    [ anon ]\n00007fedcc000000    132K rwx--    [ anon ]\n00007fedcc021000  65404K -----    [ anon ]\n00007fedd0000000   3544K rwx--    [ anon ]\n00007fedd0376000  61992K -----    [ anon ]\n00007fedd4000000   3736K rwx--    [ anon ]\n00007fedd43a6000  61800K -----    [ anon ]\n00007fedd8000000    132K rwx--    [ anon ]\n00007fedd8021000  65404K -----    [ anon ]\n00007fedde16f000  96836K r-x--  /usr/lib/locale/locale-archive\n00007fede4000000    132K rwx--    [ anon ]\n00007fede4021000  65404K -----    [ anon ]\n00007fede8000000    132K rwx--    [ anon ]\n00007fede8021000  65404K -----    [ anon ]\n00007fedec000000    132K rwx--    [ anon ]\n00007fedec021000  65404K -----    [ anon ]\n00007fedf0000000    132K rwx--    [ anon ]\n00007fedf0021000  65404K -----    [ anon ]\n00007fedf4000000    132K rwx--    [ anon ]\n00007fedf4021000  65404K -----    [ anon ]\n00007fedf8000000    132K rwx--    [ anon ]\n00007fedf8021000  65404K -----    [ anon ]\n00007fedfc000000    132K rwx--    [ anon ]\n00007fedfc021000  65404K -----    [ anon ]\n00007fee00000000    132K rwx--    [ anon ]\n00007fee00021000  65404K -----    [ anon ]\n00007fee04000000    132K rwx--    [ anon ]\n00007fee04021000  65404K -----    [ anon ]\n00007fee08000000    132K rwx--    [ anon ]\n00007fee08021000  65404K -----    [ anon ]\n00007fee0c000000    132K rwx--    [ anon ]\n00007fee0c021000  65404K -----    [ anon ]\n\n...\n\n00007fee12f2e000  34376K rwx--    [ anon ]\n00007fee150c0000  64768K rwx--    [ anon ]\n00007fee19000000 526784K rwx--    [ anon ]\n00007fee39270000  51904K rwx--    [ anon ]\n00007fee3c520000  60288K -----    [ anon ]\n\nCollectively, those guys are taking up well over a gig. Continuing to experiment.\n\nAny ideas?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T22:10:36.418+0000","updated":"2011-09-21T22:10:36.418+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112166","id":"13112166","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"Chris, as Milind mentioned on list, can u try tuning your MALLOC_ARENA_MAX ala HADOOP-7154?\n\nDo an 'svn up' to get MAPREDUCE-2880 and set an env for your AM: \n\nMALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-4}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T22:14:16.396+0000","updated":"2011-09-21T22:14:16.396+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112196","id":"13112196","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"It worked! THANKS!\n\nThat sucked.\n\nFor the record, I just set export MALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-4}, and made sure echo $MALLOC_ARENA_MAX printed 4 in my RM shell, NM shell, and AM.\n\npmap is now showing 1.3megs instead of 2 gigs. WOOT.\n\nThanks again!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T22:53:19.013+0000","updated":"2011-09-21T22:53:19.013+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112229","id":"13112229","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"Cool.\n\nChris, do you want to provide a patch to add MALLOC_ARENA_MAX automatically to containers? We can do this in the NM.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-21T23:38:28.213+0000","updated":"2011-09-21T23:38:28.213+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112247","id":"13112247","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Sure thing. Will get on it tomorrow AM. Headed out the door at the moment.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-22T00:03:57.611+0000","updated":"2011-09-22T00:03:57.611+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112690","id":"13112690","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Per vinod, new ticket to track this: https://issues.apache.org/jira/browse/MAPREDUCE-3068","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-22T16:31:06.250+0000","updated":"2011-09-22T16:31:06.250+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112741","id":"13112741","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=milindb","name":"milindb","key":"milindb","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Milind Bhandarkar","active":true,"timeZone":"America/Los_Angeles"},"body":"@Chris, glad to know that it worked! Ironically, I had discovered this issue with RHEL6 when working on another project earlier this year at LinkedIn :-)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=milindb","name":"milindb","key":"milindb","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Milind Bhandarkar","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-22T17:39:35.780+0000","updated":"2011-09-22T17:39:35.780+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/13112750","id":"13112750","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"@Milind, lol yea. It's these linux boxes that they threw us on. :P","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-22T17:43:06.104+0000","updated":"2011-09-22T17:43:06.104+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12523922/comment/15004495","id":"15004495","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"body":"Resolving correctly as a dup of MAPREDUCE-3068.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-11-13T18:51:15.315+0000","updated":"2015-11-13T18:51:15.315+0000"}],"maxResults":15,"total":15,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-3065/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i09bzz:"}}