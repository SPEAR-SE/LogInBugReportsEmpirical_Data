{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13050380","self":"https://issues.apache.org/jira/rest/api/2/issue/13050380","key":"MAPREDUCE-6863","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-03-13T16:18:56.864+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jul 11 06:37:04 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-6863/watchers","watchCount":4,"isWatching":false},"created":"2017-03-13T04:35:00.435+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327180","id":"12327180","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-11T06:43:08.274+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12315625","id":"12315625","name":"applicationmaster"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12315340","id":"12315340","name":"resourcemanager"}],"timeoriginalestimate":null,"description":"applicationmaster process log is loop on “Waiting for application to be successfully unregistered.”\n\nApplicationMaster log\n{code}\n2017-03-12 01:16:50,854 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1489067586592_112212Job Transitioned from RUNNING to COMMITTING\n2017-03-12 01:16:50,854 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_COMMIT\n2017-03-12 01:16:50,884 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Calling handler for JobFinishedEvent \n2017-03-12 01:16:50,884 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1489067586592_112212Job Transitioned from COMMITTING to SUCCEEDED\n2017-03-12 01:16:50,885 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: We are finishing cleanly so this is the last retry\n2017-03-12 01:16:50,885 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify RMCommunicator isAMLastRetry: true\n2017-03-12 01:16:50,885 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: RMCommunicator notified that shouldUnregistered is: true\n2017-03-12 01:16:50,885 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Notify JHEH isAMLastRetry: true\n2017-03-12 01:16:50,885 INFO [Thread-402] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: JobHistoryEventHandler notified that forceJobCompletion is true\n2017-03-12 01:16:50,885 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Calling stop for all the services\n2017-03-12 01:16:50,886 INFO [Thread-402] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0\n2017-03-12 01:16:50,959 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:260 CompletedReds:0 ContAlloc:261 ContRel:0 HostLocal:115 RackLocal:146\n2017-03-12 01:16:51,212 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://bipcluster:8020/data/yarn/stage/master/.staging/job_1489067586592_112212/job_1489067586592_112212_1.jhist to hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212-1489252563933-master-3934_7823976%3Adw_log_app_page_event_hour_inc.sql%3As4-1489252610863-260-0-SUCCEEDED-root.bigdata.etl.hourlyetl.veryhigh-1489252568534.jhist_tmp\n2017-03-12 01:16:51,255 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212-1489252563933-master-3934_7823976%3Adw_log_app_page_event_hour_inc.sql%3As4-1489252610863-260-0-SUCCEEDED-root.bigdata.etl.hourlyetl.veryhigh-1489252568534.jhist_tmp\n2017-03-12 01:16:51,256 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copying hdfs://bipcluster:8020/data/yarn/stage/master/.staging/job_1489067586592_112212/job_1489067586592_112212_1_conf.xml to hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212_conf.xml_tmp\n2017-03-12 01:16:51,270 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Copied to done location: hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212_conf.xml_tmp\n2017-03-12 01:16:51,274 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212.summary_tmp to hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212.summary\n2017-03-12 01:16:51,276 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212_conf.xml_tmp to hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212_conf.xml\n2017-03-12 01:16:51,277 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Moved tmp to done: hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212-1489252563933-master-3934_7823976%3Adw_log_app_page_event_hour_inc.sql%3As4-1489252610863-260-0-SUCCEEDED-root.bigdata.etl.hourlyetl.veryhigh-1489252568534.jhist_tmp to hdfs://bipcluster:8020/data/yarn/intermediate_done/master/job_1489067586592_112212-1489252563933-master-3934_7823976%3Adw_log_app_page_event_hour_inc.sql%3As4-1489252610863-260-0-SUCCEEDED-root.bigdata.etl.hourlyetl.veryhigh-1489252568534.jhist\n2017-03-12 01:16:51,277 INFO [Thread-402] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopped JobHistoryEventHandler. super.stop()\n2017-03-12 01:16:51,278 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1489067586592_112212_m_000217_0\n2017-03-12 01:16:51,279 INFO [Thread-402] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : sh-hadoop-datanode-128-41.elenet.me:39175\n2017-03-12 01:16:51,284 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1489067586592_112212_m_000217_0 TaskAttempt Transitioned from SUCCESS_FINISHING_CONTAINER to SUCCEEDED\n2017-03-12 01:16:51,294 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Setting job diagnostics to \n2017-03-12 01:16:51,297 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: History url is http://bigdata-rsm.elenet.me:20020/jobhistory/job/job_1489067586592_112212\n2017-03-12 01:16:51,302 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:51,803 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:52,305 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:52,806 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:53,306 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:53,808 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:54,309 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:54,810 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:55,311 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:55,821 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:56,322 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:56,823 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:57,324 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:57,825 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:58,326 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:58,828 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:59,329 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:16:59,830 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:17:00,331 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:17:00,832 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:17:01,333 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n2017-03-12 01:17:01,835 INFO [Thread-402] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Waiting for application to be successfully unregistered.\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12857554","id":"12857554","filename":"jobhistory status.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"created":"2017-03-13T04:46:18.501+0000","size":469684,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12857554/jobhistory+status.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12857555","id":"12857555","filename":"yarn resourcemanager job list status.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"created":"2017-03-13T04:48:11.911+0000","size":549936,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12857555/yarn+resourcemanager+job+list+status.png"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"job finish  but yarn list status is accepted and  applicationmaster is hang on Waiting for application to be successfully unregistered","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13050380/comment/15906851","id":"15906851","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"body":"after kill the job ，on the jobhistory status is SUCCEEDED\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"created":"2017-03-13T04:39:04.316+0000","updated":"2017-03-13T04:41:38.105+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13050380/comment/15907753","id":"15907753","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=haibochen","name":"haibochen","key":"haibochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haibo Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Do you still have the Resource Manager log?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=haibochen","name":"haibochen","key":"haibochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haibo Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-03-13T16:18:56.864+0000","updated":"2017-03-13T16:18:56.864+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13050380/comment/15923427","id":"15923427","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"body":"{code}\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1489067586592_112212 State change from ACCEPTED to KILLING\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1489067586592_112212_000001 with final state: KILLED, and exit status: -1000\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1489067586592_112212_000001 State change from LAUNCHED to FINAL_SAVING\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1489067586592_112212_000001\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1489067586592_112212_000001\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1489067586592_112212_000001 State change from FINAL_SAVING to KILLED\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1489067586592_112212 with final state: KILLED\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1489067586592_112212 State change from KILLING to FINAL_SAVING\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1489067586592_112212\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1489067586592_112212_000001\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1489067586592_112212 State change from FINAL_SAVING to KILLED\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application appattempt_1489067586592_112212_000001 is done. finalState=KILLED\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000001 Container Transitioned from RUNNING to KILLED\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Completed container: container_1489067586592_112212_01_000001 in state: KILLED event:KILL\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Released Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000001\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1489067586592_112212_01_000001 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-216.elenet.me:5134, which currently has 12 containers, <memory:18432, vCores:12> used and <memory:58368, vCores:38> available, release resources=true\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application attempt appattempt_1489067586592_112212_000001 released container container_1489067586592_112212_01_000001 on node: host: sh-hadoop-datanode-250-216.elenet.me:5134 #containers=12 available=58368 used=18432 with event: KILL\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=Application Finished - Killed\tTARGET=RMAppManager\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1489067586592_112212 requests cleared\n2017-03-13 11:34:20,085 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1489067586592_112212,name=3934_7823976:dw_log_app_page_event_hour_inc.sql:s4q1:10.0.146.38:21879:from temp.temp_dw_log_app_page_event_pre_20170311_23 a,user=master,queue=root.bigdata.etl.hourlyetl.veryhigh,state=KILLED,trackingUrl=http://bigdata-rsm.elenet.me:8088/cluster/app/application_1489067586592_112212,appMasterHost=N/A,startTime=1489252563933,finishTime=1489376060085,finalStatus=KILLED\n2017-03-13 11:34:20,287 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tIP=10.0.250.88\tOPERATION=Kill Application Request\tTARGET=ClientRMService\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\n2017-03-13 11:34:20,576 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1489067586592_112212 unregistered successfully. \n2017-03-13 11:34:21,108 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Container container_1489067586592_112212_01_000217 completed with event FINISHED\n2017-03-13 11:34:23,108 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Container container_1489067586592_112212_01_000001 completed with event FINISHED\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tIP=10.0.146.38\tOPERATION=Submit Application Request\tTARGET=ClientRMService\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1489067586592_112212\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1489067586592_112212 State change from NEW to NEW_SAVING\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1489067586592_112212\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1489067586592_112212 State change from NEW_SAVING to SUBMITTED\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Accepted application application_1489067586592_112212 from user: master, in queue: root.bigdata.etl.hourlyetl.veryhigh, currently num of applications: 65\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1489067586592_112212 State change from SUBMITTED to ACCEPTED\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1489067586592_112212_000001\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1489067586592_112212_000001 State change from NEW to SUBMITTED\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Added Application Attempt appattempt_1489067586592_112212_000001 to scheduler from user: master\n2017-03-12 01:16:03,934 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1489067586592_112212_000001 State change from SUBMITTED to SCHEDULED\n2017-03-12 01:16:03,938 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000001 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:03,938 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000001\n2017-03-12 01:16:03,938 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000001 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-216.elenet.me:5134, which has 16 containers, <memory:24576, vCores:16> used and <memory:52224, vCores:34> available after allocation\n2017-03-12 01:16:03,938 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : sh-hadoop-datanode-250-216.elenet.me:5134 for container : container_1489067586592_112212_01_000001\n2017-03-12 01:16:03,938 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\n2017-03-12 01:16:03,938 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1489067586592_112212_000001\n2017-03-12 01:16:03,938 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1489067586592_112212 AttemptId: appattempt_1489067586592_112212_000001 MasterContainer: Container: [ContainerId: container_1489067586592_112212_01_000001, NodeId: sh-hadoop-datanode-250-216.elenet.me:5134, NodeHttpAddress: sh-hadoop-datanode-250-216.elenet.me:8042, Resource: <memory:1536, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.250.216:5134 }, ]\n2017-03-12 01:16:03,939 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1489067586592_112212_000001 State change from SCHEDULED to ALLOCATED_SAVING\n2017-03-12 01:16:03,939 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1489067586592_112212_000001 State change from ALLOCATED_SAVING to ALLOCATED\n2017-03-12 01:16:03,939 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1489067586592_112212_000001\n2017-03-12 01:16:03,939 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1489067586592_112212_01_000001, NodeId: sh-hadoop-datanode-250-216.elenet.me:5134, NodeHttpAddress: sh-hadoop-datanode-250-216.elenet.me:8042, Resource: <memory:1536, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.250.216:5134 }, ] for AM appattempt_1489067586592_112212_000001\n2017-03-12 01:16:03,939 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1489067586592_112212_01_000001 : $JAVA_HOME/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog  -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr \n2017-03-12 01:16:03,939 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1489067586592_112212_000001\n2017-03-12 01:16:03,939 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1489067586592_112212_000001\n2017-03-12 01:16:08,477 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1489067586592_112212_000001 (auth:SIMPLE)\n2017-03-12 01:16:08,480 INFO SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for appattempt_1489067586592_112212_000001 (auth:TOKEN) for protocol=interface org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB\n2017-03-12 01:16:08,480 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration appattempt_1489067586592_112212_000001\n2017-03-12 01:16:08,480 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tIP=10.0.250.216\tOPERATION=Register App Master\tTARGET=ApplicationMasterService\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tAPPATTEMPTID=appattempt_1489067586592_112212_000001\n2017-03-12 01:16:14,034 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1489067586592_112212_01_000001, NodeId: sh-hadoop-datanode-250-216.elenet.me:5134, NodeHttpAddress: sh-hadoop-datanode-250-216.elenet.me:8042, Resource: <memory:1536, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.250.216:5134 }, ] for AM appattempt_1489067586592_112212_000001\n2017-03-12 01:16:14,034 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1489067586592_112212_000001 State change from ALLOCATED to LAUNCHED\n2017-03-12 01:16:14,066 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000002 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,066 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000002\n2017-03-12 01:16:14,066 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000002 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-125.elenet.me:41572, which has 12 containers, <memory:18432, vCores:12> used and <memory:58368, vCores:38> available after allocation\n2017-03-12 01:16:14,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000003 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,067 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000003\n2017-03-12 01:16:14,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000003 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-139-48.elenet.me:20122, which has 12 containers, <memory:18432, vCores:12> used and <memory:58368, vCores:38> available after allocation\n2017-03-12 01:16:14,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000004 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,067 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000004\n2017-03-12 01:16:14,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000004 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-108.elenet.me:40770, which has 13 containers, <memory:19968, vCores:13> used and <memory:56832, vCores:37> available after allocation\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000005 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000005\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000005 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-52.elenet.me:41518, which has 11 containers, <memory:16896, vCores:11> used and <memory:59904, vCores:39> available after allocation\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000006 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000006\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000006 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-133.elenet.me:51670, which has 18 containers, <memory:27648, vCores:18> used and <memory:49152, vCores:32> available after allocation\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000007 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000007\n2017-03-12 01:16:14,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000007 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-179.elenet.me:31301, which has 9 containers, <memory:13824, vCores:9> used and <memory:62976, vCores:41> available after allocation\n2017-03-12 01:16:14,069 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000008 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,069 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000008\n2017-03-12 01:16:14,069 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000008 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-139-98.elenet.me:49018, which has 11 containers, <memory:16896, vCores:11> used and <memory:59904, vCores:39> available after allocation\n2017-03-12 01:16:14,069 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000009 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,069 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000009\n2017-03-12 01:16:14,069 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000009 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-182.elenet.me:31417, which has 12 containers, <memory:18432, vCores:12> used and <memory:58368, vCores:38> available after allocation\n2017-03-12 01:16:14,070 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000010 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,070 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000010\n2017-03-12 01:16:14,070 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000010 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-176.elenet.me:15968, which has 12 containers, <memory:18432, vCores:12> used and <memory:58368, vCores:38> available after allocation\n2017-03-12 01:16:14,070 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000011 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,070 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000011\n2017-03-12 01:16:14,070 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000011 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-181.elenet.me:61532, which has 11 containers, <memory:16896, vCores:11> used and <memory:59904, vCores:39> available after allocation\n2017-03-12 01:16:14,072 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000012 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,072 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000012\n2017-03-12 01:16:14,072 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000012 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-103.elenet.me:34724, which has 10 containers, <memory:15360, vCores:10> used and <memory:61440, vCores:40> available after allocation\n2017-03-12 01:16:14,072 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000013 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,072 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000013\n2017-03-12 01:16:14,072 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000013 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-50.elenet.me:24200, which has 12 containers, <memory:18432, vCores:12> used and <memory:58368, vCores:38> available after allocation\n2017-03-12 01:16:14,073 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000014 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,073 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000014\n2017-03-12 01:16:14,073 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000014 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-173.elenet.me:16759, which has 11 containers, <memory:16896, vCores:11> used and <memory:59904, vCores:39> available after allocation\n2017-03-12 01:16:14,073 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000015 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,073 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000015\n2017-03-12 01:16:14,073 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000015 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-128-34.elenet.me:21900, which has 13 containers, <memory:19968, vCores:13> used and <memory:56832, vCores:37> available after allocation\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000016 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000016\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000016 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-132-96.elenet.me:16210, which has 8 containers, <memory:12288, vCores:8> used and <memory:64512, vCores:42> available after allocation\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000017 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000017\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000017 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-132.elenet.me:12587, which has 12 containers, <memory:18432, vCores:12> used and <memory:58368, vCores:38> available after allocation\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000018 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000018\n2017-03-12 01:16:14,074 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000018 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-56.elenet.me:3088, which has 8 containers, <memory:12288, vCores:8> used and <memory:64512, vCores:42> available after allocation\n2017-03-12 01:16:14,075 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000019 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,075 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000019\n2017-03-12 01:16:14,075 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000019 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-132-70.elenet.me:46701, which has 11 containers, <memory:16896, vCores:11> used and <memory:59904, vCores:39> available after allocation\n2017-03-12 01:16:14,081 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000020 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,081 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000020\n2017-03-12 01:16:14,081 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000020 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-89.elenet.me:22257, which has 9 containers, <memory:13824, vCores:9> used and <memory:62976, vCores:41> available after allocation\n2017-03-12 01:16:14,081 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000021 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,081 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000021\n2017-03-12 01:16:14,081 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000021 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-184.elenet.me:6131, which has 11 containers, <memory:16896, vCores:11> used and <memory:59904, vCores:39> available after allocation\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000022 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000022\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000022 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-250-90.elenet.me:50206, which has 15 containers, <memory:23040, vCores:15> used and <memory:53760, vCores:35> available after allocation\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000023 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000023\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000023 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-139-49.elenet.me:10279, which has 9 containers, <memory:13824, vCores:9> used and <memory:62976, vCores:41> available after allocation\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000024 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000024\n2017-03-12 01:16:14,082 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000024 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-139-56.elenet.me:35007, which has 17 containers, <memory:26112, vCores:17> used and <memory:50688, vCores:33> available after allocation\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000025 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000025\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000025 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-132-49.elenet.me:35587, which has 11 containers, <memory:16896, vCores:11> used and <memory:59904, vCores:39> available after allocation\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000026 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000026\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000026 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-128-81.elenet.me:3438, which has 9 containers, <memory:13824, vCores:9> used and <memory:62976, vCores:41> available after allocation\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000027 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000027\n2017-03-12 01:16:14,083 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000027 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-139-32.elenet.me:56672, which has 14 containers, <memory:21504, vCores:14> used and <memory:55296, vCores:36> available after allocation\n2017-03-12 01:16:14,084 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000028 Container Transitioned from NEW to ALLOCATED\n2017-03-12 01:16:14,084 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=master\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1489067586592_112212\tCONTAINERID=container_1489067586592_112212_01_000028\n2017-03-12 01:16:14,084 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_1489067586592_112212_01_000028 of capacity <memory:1536, vCores:1> on host sh-hadoop-datanode-139-139.elenet.me:9113, which has 10 containers, <memory:15360, vCores:10> used and <memory:61440, vCores:40> available after allocation\n2017-03-12 01:16:14,084 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1489067586592_112212_01_000029 Container Transitioned from NEW to ALLOCATED\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"created":"2017-03-14T02:22:30.507+0000","updated":"2017-07-11T06:39:46.687+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13050380/comment/15924602","id":"15924602","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=haibochen","name":"haibochen","key":"haibochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haibo Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"The RM log you uploaded does not cover the time window (2017-03-12 01:16:51 onwards) during which the ApplicationMaster was stuck. I suspect your RM was busy handling something else, thus, was not able to handle the unregister event before your AM's next unregister attempt","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=haibochen","name":"haibochen","key":"haibochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haibo Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-03-14T17:18:54.499+0000","updated":"2017-03-14T17:18:54.499+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13050380/comment/16081754","id":"16081754","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"body":"{code}\n2017-07-11 04:14:54,964 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle event REGISTERED when appattempt_1499504190019_207162_000001 is ALLOCATED\n2017-07-11 04:14:55,507 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle event STATUS_UPDATE when appattempt_1499504190019_207162_000001 is ALLOCATED\n2017-07-11 04:14:56,050 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle event STATUS_UPDATE when appattempt_1499504190019_207162_000001 is ALLOCATED\n2017-07-11 04:14:56,558 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle event STATUS_UPDATE when appattempt_1499504190019_207162_000001 is ALLOCATED\n2017-07-11 04:14:57,064 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle event STATUS_UPDATE when appattempt_1499504190019_207162_000001 is ALLOCATED\n2017-07-11 04:14:57,569 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle event STATUS_UPDATE when appattempt_1499504190019_207162_000001 is ALLOCATED\n2017-07-11 04:15:00,530 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1499504190019_207162_01_000001, NodeId: sh-hadoop-datanode-139-21.elenet.me:21491, NodeHttpAddress: sh-hadoop-datanode-139-21.elenet.me:8042, Resource: <memory:1536, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.139.21:21491 }, ] for AM appattempt_1499504190019_207162_000001\n2017-07-11 04:15:00,530 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1499504190019_207162_000001 State change from ALLOCATED to LAUNCHED\n2017-07-11 04:15:00,575 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle event STATUS_UPDATE when appattempt_1499504190019_207162_000001 is LAUNCHED\n2017-07-11 04:15:00,755 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1499504190019_207162_01_000002 Container Transitioned from NEW to ALLOCATED\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaiyuyong","name":"zhaiyuyong","key":"zhaiyuyong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"翟玉勇","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-11T06:37:04.129+0000","updated":"2017-07-11T06:38:05.224+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-6863/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3bavr:"}}