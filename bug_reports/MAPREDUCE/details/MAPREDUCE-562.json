{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12427264","self":"https://issues.apache.org/jira/rest/api/2/issue/12427264","key":"MAPREDUCE-562","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/4","id":"4","description":"The problem is not completely described.","name":"Incomplete"},"customfield_12312322":null,"customfield_12310220":"2009-06-06T00:38:41.711+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jul 22 20:50:59 UTC 2014","customfield_12310420":"148894","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_161814303089_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-07-22T20:50:59.436+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-562/watchers","watchCount":16,"isWatching":false},"created":"2009-06-06T00:25:56.380+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12332028","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12332028","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12464924","key":"MAPREDUCE-1800","self":"https://issues.apache.org/jira/rest/api/2/issue/12464924","fields":{"summary":"using map output fetch failures to blacklist nodes is problematic","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-07-22T20:50:59.467+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"We see cases where there may be a large number of mapper nodes running many tasks (e.g., a thousand). The reducers will pull 980 of the map task intermediate files down, but will be unable to retrieve the final intermediate shards from the last node. The TaskTracker on that node returns data to reducers either slowly or not at all, but its heartbeat messages make it back to the JobTracker -- so the JobTracker doesn't mark the tasks as failed. Manually stopping the offending TaskTracker works to migrate the tasks to other nodes, where the shuffling process finishes very quickly. Left on its own, it can take hours to unjam itself otherwise.\n\nWe need a mechanism for reducers to provide feedback to the JobTracker that one of the mapper nodes should be regarded as lost.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"108495","customfield_12312823":null,"summary":"A single slow (but not dead) map TaskTracker impedes MapReduce progress","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12716808","id":"12716808","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"body":"\nHere's a first stab at an algorithm to fix this:\n\n*  Let's define the \"home stretch\" of shuffling, from the perspective of a reducer, as the last X% of the map shards it needs to fetch. Maybe that's 3% or 4%.\n* If during the home stretch, a map task enters the penalty box and stays there for some number of seconds (let's say 30), the reducer can send a message back to the JobTracker voting that the task be marked as \"failed for being too slow.\"\n* The JobTracker requires a quorum of still-shuffling reducers to vote this way. The quorum might be, say, 10%. This way if only two reducers are still waiting, either one of them is a quorum by itself. But if 40 reducers are shuffling, it will take the consensus of a few reducers that a mapper is indeed too slow.\n* If enough votes are given, the JobTracker blacklists a TaskTracker from the job, marks all its map task attempts as failed, and restarts all the map tasks from that node, on other nodes.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-06T00:26:52.384+0000","updated":"2009-06-06T00:26:52.384+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12716810","id":"12716810","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"> ... some number of seconds (let's say 30) ...\nThis probably should depend on the map running time. 30 seconds are nothing if the running time is 2 hours.\n\nI do not understand the algorithm in every detail.  Could give an example?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-06T00:38:41.711+0000","updated":"2009-06-06T00:38:41.711+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12716812","id":"12716812","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"body":"Sure.\n\nTo clarify a bit first: when reducers put mappers in \"the penalty box,\" they're saying that this mapper is being slow, so it should prioritize receiving from other mappers instead. But if there are no other mappers left, then this just makes the reducer spin in circles. If the only mappers left are in the penalty box, then something's probably wrong.\n\nCall the home stretch percentage 4%. There are 1,000 map tasks.  There are 20 reduce tasks. The kill quorum is 10%.\n\nA reducer retrieves 960 map task outputs -- that's 96%, so we're in the last 4%. There are 40 mapper outputs left to receive. The 20 mapper tasks from one node are all in the penalty box. The other twenty mapper outputs get retrieved. The 20 stalled outputs are still in the penalty box. The timeout is triggered, meaning that those 20 tasks just spun in the penalty box for way too long. The reducer votes to tell the JobTracker that the mapper is frozen.\n\nAnother reducer experiences the same problem. It votes the same way.\n\nThe JobTracker has now received 2/20 = 10% of the active shufflers saying that a particular mapper node has not been delivering its outputs. The JobTracker blacklists that TT and marks all its mapper tasks as failed. All twenty of those map tasks go to other nodes. They finish. The reducers grab their outputs, and the reduce process continues.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-06T01:08:54.472+0000","updated":"2009-06-06T01:08:54.472+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12716815","id":"12716815","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"This is a very good observation and the solution seems interesting too (we observed this behavior in our PetaSort experiment too).\n\nHowever, I think the fundamental problem is still not tackled - where the last few mappers will block all reducers. Even if every mapper is running on a different task tracker, you would have all reducers trying to pull from those few mappers and thus would still be very slow - informing JT to spawn mappers to other TTs would not help (and may make the matter even worse 'coz you may end up not making any progress at all).\n\nTo really solve the problem, we probably want to run multiple copies of the same mapper and keep them all, then balance the reducers among those replica instances. This is not an easy fix and may belong to the scope of speculative execution.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2009-06-06T01:55:01.050+0000","updated":"2009-06-06T01:55:01.050+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12717332","id":"12717332","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"body":"Hong,\n\nThis changes the semantics of speculative execution, as I understand it.\n\nSpeculative execution does not strictly guarantee that all mappers will emit the same output for the same input, but it does guarantee that they are all equally \"good.\" So map(A) might return X, but a second speculative execution of map(A) might return Y. Either X or Y will finish first and the JT will use exactly one of these.\n\nYour proposal is that some of the reducers can grab their output shard from the X results, and other reducers can grab their output shard from the Y results.\n\nIf we're willing to tell developers about that new contract and make it an option, then wouldn't be universally applicable. So I still think we'd still need a fallback mechanism like I proposed here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-08T17:22:35.262+0000","updated":"2009-06-08T17:22:35.262+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12717396","id":"12717396","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"I don't think it changes the semantics. You were saying that all reducers should either see output from map(A) or map(B), but not a mixture of both. But this is not the case even without what I am suggesting. Today, a reducer may gets map output from map(A), then the TT that hosts the output of map(A) dies, and all maps on that TT gets re-executed, and other reducers that have not yet fetched output from map(A) will fetch from map(B).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2009-06-08T20:33:50.206+0000","updated":"2009-06-08T20:33:50.206+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12717401","id":"12717401","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"body":"I was under the impression that if a map task died before delivering output to all of the reducers, then after the map task is re-executed elsewhere, all reducers roll back and re-pull from the newer version of the mapper. I could be mistaken though?\n\nBecause if that's the case, then we just need to run another round of speculative execution during the final shufflings, even if the map tasks themselves were marked as \"complete,\" and modify the reducers to try to pull from a list of eligible mappers instead of just a single node.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-08T20:42:47.559+0000","updated":"2009-06-08T20:42:47.559+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12717459","id":"12717459","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"AFAIK, once a reducer finishes pulling map output from a particular mapper, it will no longer pulling the same output again (from a different invocation of the same map task). If not so, then a reducer cannot quit until all other reducers finish fetching map outputs from all maps. This then lead to another implication that if your cluster has fewer reducer slots than total # of reducers, your job will never finish.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2009-06-08T22:10:59.346+0000","updated":"2009-06-08T22:10:59.346+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12720376","id":"12720376","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"body":"Hong,\n\nGood point regarding where reducers pull from. Since multiple waves of reducers are supported, it sounds reasonable to me.\n\nSo maybe the algorithm changes to this; we modify speculative execution in the following way:\n\n* If a map task is launched multiple times via spec. ex., all copies that succeed are eligible to serve reducers concurrently, not just one such copy.\n* The completion of a map task's processing does not cause speculative copies to be killed; they also run to completion.\n* Mapper TaskTrackers report back to the JT (during their heartbeat) the number of reduce shards served / available. If any set of mappers are falling \"too far behind\" the other mappers (e.g., most mappers have served 900/1000 shards, but a couple have only served 50/1000), then we launch additional copies of those mapper tasks on other nodes.\n* Reducers are advised of additional alternate locations to pull a particular map shard. If multiple sources are available, they randomly choose one. If that source is too slow, it tries a different copy.\n\n\nThis gets rid of the need for reducers to \"vote\" and influence the JT's behavior directly.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kimballa","name":"kimballa","key":"kimballa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Aaron Kimball","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-16T22:10:01.979+0000","updated":"2009-06-16T22:10:01.979+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12722282","id":"12722282","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matei","name":"matei","key":"matei","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matei Zaharia","active":true,"timeZone":"America/Los_Angeles"},"body":"I think this approach is the right one, Aaron. However, the tricky part will be determining what metrics to use for deciding that a TaskTracker is slow at serving outputs and for deciding on a reducer when you want to try a second location.\n\nFor identifying slow TT's, just looking at number of shards served might not be enough. First of all, TT's might have run different numbers of tasks. Second, TT's might become slow partway through their lifetime. If all TT's have served 1000/1000 shards and one of them is at 800/1000 and then starts thrashing, you have to realize that at that point. I think there are two possible ways to go:\n1) Instead of looking at shards served, ask reducers periodically about how many shards they are in the process of fetching from each TT. If there is a TT that a lot of reducers are currently fetching from, it's probably being slow (we could even say \"when there are only N tasktrackers left that we are fetching from, start considering them slow\"). This might have to be weighted by number of shards that ran per node. The idea is to match the same kind of detection that a human operator may do (\"hey, everyone is waiting on tasktracker X\").\n2) Look at rate of data being served (in bytes/second) from each TT, as well as the demand (how many shards are being requested). Mark TT's where the rate is below some threshold and the demand is above some other threshold as slow. (These thresholds may be calculated based on how the other TT's did).\n\nFor deciding when to fetch from a new shard in a reducer, the tricky part will be telling loaded TT's apart from slow TT's. The moving around between shards can't be too aggressive.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matei","name":"matei","key":"matei","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matei Zaharia","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-21T00:34:55.711+0000","updated":"2009-06-21T00:34:55.711+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12722454","id":"12722454","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"Sorry for jumping in late, I somehow missed this one.\n\nI thought I'd point out that currently reduces _do_ indeed provide feedback regarding 'failed' shuffles to the JobTracker, which on receipt of sufficient number of complaints from reduces does re-execute the map elsewhere.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-22T02:48:27.948+0000","updated":"2009-06-22T02:48:27.948+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/12723207","id":"12723207","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matei","name":"matei","key":"matei","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matei Zaharia","active":true,"timeZone":"America/Los_Angeles"},"body":"I think the problem here was a slow shuffle, not a failed one. What conditions cause a shuffle to be declared as failed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matei","name":"matei","key":"matei","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matei Zaharia","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-06-23T17:48:03.492+0000","updated":"2009-06-23T17:48:03.492+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12427264/comment/14070879","id":"14070879","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"This is still an interesting issue, but at this point, I feel the need to close this one.  The big reason being that this problem needs to be generalized for YARN and made much less MR specific.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2014-07-22T20:50:59.463+0000","updated":"2014-07-22T20:50:59.463+0000"}],"maxResults":13,"total":13,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-562/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0ixc7:"}}