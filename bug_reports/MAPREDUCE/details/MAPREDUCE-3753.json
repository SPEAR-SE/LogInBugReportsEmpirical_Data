{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12412817","self":"https://issues.apache.org/jira/rest/api/2/issue/12412817","key":"MAPREDUCE-3753","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/4","id":"4","description":"The problem is not completely described.","name":"Incomplete"},"customfield_12312322":null,"customfield_12310220":"2009-01-20T10:20:38.122+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Jul 21 18:15:49 UTC 2014","customfield_12310420":"78105","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_173521882395_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-07-21T18:15:49.283+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-3753/watchers","watchCount":3,"isWatching":false},"created":"2009-01-20T09:44:26.918+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-07-21T18:15:49.310+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"I run into a critical issue with Hadoop 18.2 on my Linux boxes:\n\nThe jobs executes without any complains and they are listed in the\nsucceeded list but there is no output data beside the \"_logs\" directory.\nThe same code works with .17.2.1\n \n\nHere are some sections of the logs:\n\n[logfile]\nhadoop@bock:~/logs$ tail hadoop-hadoop-jobtracker-bock.log\n\n2008-12-23 13:30:56,707 INFO org.apache.hadoop.mapred.JobInProgress:\nChoosing a data-local task task_200812231229_0031_m_000001 for\nspeculation\n\n2008-12-23 13:30:56,707 INFO org.apache.hadoop.mapred.JobTracker: Adding\ntask 'attempt_200812231229_0031_m_000001_1' to tip\ntask_200812231229_0031_m_000001, for tracker\n'tracker_bock:localhost/127.0.0.1:15260'\n\n2008-12-23 13:31:01,065 INFO org.apache.hadoop.mapred.JobInProgress:\nTask 'attempt_200812231229_0031_m_000001_1' has completed\ntask_200812231229_0031_m_000001 successfully.\n\n2008-12-23 13:31:03,177 INFO org.apache.hadoop.mapred.TaskRunner: Saved\noutput of task 'attempt_200812231229_0031_r_000000_0' to\nhdfs://BOCK:9000/ana/oiprocessed/2008/12/23/Sen1/92a74190-2038-4c79-82c4-2de6fdc615db\n\n[/logfile]\n\nBut the folder contains only a \"_logs\" folder which has a history file\nwhich contains:\n\n[logfile]\n\nJob JOBID=\"job_200812231415_0001\" FINISH_TIME=\"1230038377844\"\nJOB_STATUS=\"SUCCESS\" FINISHED_MAPS=\"2\" FINISHED_REDUCES=\"1\"\nFAILED_MAPS=\"0\" FAILED_REDUCES=\"0\" COUNTERS=\"Job Counters .Data-local\nmap tasks:2,Job Counters .Launched reduce tasks:1,Job Counters .Launched\nmap tasks:3,Map-Reduce Framework.Reduce input records:61,Map-Reduce\nFramework.Map output records:61,Map-Reduce Framework.Map output\nbytes:7194,Map-Reduce Framework.Combine output records:0,Map-Reduce\nFramework.Map input records:61,Map-Reduce Framework.Reduce input\ngroups:12,Map-Reduce Framework.Combine input records:0,Map-Reduce\nFramework.Map input bytes:36396,Map-Reduce Framework.Reduce output\nrecords:12,File Systems.HDFS bytes written:1533,File Systems.Local bytes\nwritten:14858,File Systems.HDFS bytes read:38679,File Systems.Local\nbytes\nread:7388,com..ana.scheduling.HadoopTask$Counter.MAPPEED:61\n\"\n[/logfile]\n\nSo what I see is that the system runs successful and it even says it\nwrites data! (\"Map-Reduce Framework.Reduce output records:12,File Systems.HDFS bytes written:1533\")\n\nIf I run the same code with .17.2.1 or in local mode with .18.2 it works\nand I get a part-0000 file with the expected data.\n \n\nPlease tell me if you need additional information.\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"41799","customfield_12312823":null,"summary":"Reduce output data is not written to disk","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fuchsmi","name":"fuchsmi","key":"fuchsmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Fuchs","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fuchsmi","name":"fuchsmi","key":"fuchsmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Fuchs","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Linux version 2.6.22-12-generic (buildd@vernadsky) (gcc version 4.1.3 20070831 (prerelease) (Ubuntu 4.1.2-16ubuntu1)) #1 SMP Sun Sep 23 18:11:30 GMT 2007 running Hadoop 18.2 on two nodes","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12412817/comment/12665391","id":"12665391","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"Are you running with local mode or psuedo-distributed mode or fully-distributed mode when you are facing this?\nhttp://hadoop.apache.org/core/docs/r0.18.2/quickstart.html\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-01-20T10:20:38.122+0000","updated":"2009-01-20T10:20:38.122+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12412817/comment/12665394","id":"12665394","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fuchsmi","name":"fuchsmi","key":"fuchsmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Fuchs","active":true,"timeZone":"Etc/UTC"},"body":"I'm running on fully-distributed mode on a two node cluster and I start the application using the bin/hadoop jar command.\nI tested the same with a one node \"cluster\" but it is the same.\nOnly running it on Windows within Eclipse with a jobtracker \"local\" works.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fuchsmi","name":"fuchsmi","key":"fuchsmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Fuchs","active":true,"timeZone":"Etc/UTC"},"created":"2009-01-20T10:26:35.503+0000","updated":"2009-01-20T10:26:35.503+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12412817/comment/12665397","id":"12665397","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"Michael - Thanks for the clarification.\n\nThe part-00000 file should be on HDFS, what is the output of the command:\n$ bin/hadoop fs -ls <mr-job-output-directory>\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-01-20T10:42:54.076+0000","updated":"2009-01-20T10:42:54.076+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12412817/comment/12665409","id":"12665409","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fuchsmi","name":"fuchsmi","key":"fuchsmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Fuchs","active":true,"timeZone":"Etc/UTC"},"body":"I just tested it on hadoop .19.0 and the problem exist with that version as well.\n\nThe outut is:\n{code}\nhadoop18@bock:~$ bin/hadoop fs -ls  /ana/sesdata/2009/01/19/fcbac6fa-b163-4101-ba63-a3a022def010/\nFound 1 items\ndrwxr-xr-x   - hadoop18 supergroup          0 2009-01-20 12:35 /ana/sesdata/2009/01/19/fcbac6fa-b163-4101-ba63-a3a022def010/_logs\nhadoop18@bock:~$\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fuchsmi","name":"fuchsmi","key":"fuchsmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Fuchs","active":true,"timeZone":"Etc/UTC"},"created":"2009-01-20T11:42:12.791+0000","updated":"2009-01-20T11:42:38.181+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12412817/comment/12665536","id":"12665536","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=knoguchi","name":"knoguchi","key":"knoguchi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Koji Noguchi","active":true,"timeZone":"America/New_York"},"body":"Are you specifying the output path with port 8020?  (like hdfs://somehost.com:8020/ana/sesdata ) \nIf so, you might be hitting HADOOP-4717.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=knoguchi","name":"knoguchi","key":"knoguchi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Koji Noguchi","active":true,"timeZone":"America/New_York"},"created":"2009-01-20T20:05:04.497+0000","updated":"2009-01-20T20:05:04.497+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12412817/comment/12665831","id":"12665831","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fuchsmi","name":"fuchsmi","key":"fuchsmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Fuchs","active":true,"timeZone":"Etc/UTC"},"body":"Hi,\n\nI managed to track down the problem. It seems to be a combination of two things which cause this data loss:\n\n    * I use a string like \"hdfs://192.168.1.4:9000/ana\" as \"hadoop.hdfs.defaultfs\" and I do a fs.setWorkingDirectory(\"hdfs://192.168.1.4:9000/ana\");\n    * The Path object I'm using for the output (TextOutputFormat.setOutputPath) is not created by myself, it is created by using a Path instance retured by the fs.listStatus[0].getPath().\n      In detail, I'm doing a\n{code}\n      Path somePath = fs.listStatus(someOtherPath)[n].getPath()\n      Path output = new Path(somePath, \"subfolder\");\n      TextOutputFormat.setOutputPath(jobConf, output);\n{code}\nI created a small application which shows issue.\nThe application takes three arguments:\n1) pass \"1\" tu use an implementation which fails or \"0\" to use an implementation which works\n2) the \"hadoop.hdfs.defaultfs\" value\n3) the setWorkingDirectory value.\n\nI execute the class like \" bin/hadoop jar /tmp/example.jar org.test.TestMR 1 hdfs://BOCK:9000/test/ hdfs://BOCK:9000/test/\" and it does not write the output but doees not fail!\n\n{code}\npackage org.test;\nimport java.io.IOException;\nimport java.util.Iterator;\n\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileStatus;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.FileInputFormat;\nimport org.apache.hadoop.mapred.JobClient;\nimport org.apache.hadoop.mapred.JobConf;\nimport org.apache.hadoop.mapred.Mapper;\nimport org.apache.hadoop.mapred.OutputCollector;\nimport org.apache.hadoop.mapred.Reducer;\nimport org.apache.hadoop.mapred.Reporter;\nimport org.apache.hadoop.mapred.RunningJob;\nimport org.apache.hadoop.mapred.TextOutputFormat;\nimport org.apache.log4j.Logger;\n\n\npublic class TestMR implements Mapper<LongWritable, Text, Text, Text>, Reducer<Text, Text, Text, Text> {\n\tstatic Logger logger = Logger.getLogger(TestMR.class);\n\t\n\tpublic void map(LongWritable key, Text value, OutputCollector<Text, Text> out, Reporter reporter) throws IOException {\n\t\tlogger.info(\"map: key=\" + key + \", value=\" + value);\n\t\tout.collect(new Text(key.toString()), value);\n\t}\n\n\tpublic void configure(JobConf jobConf) {\n\t}\n\n\tpublic void close() throws IOException {\n\t}\t\n\t\n\tpublic void reduce(Text key, Iterator<Text> values, OutputCollector<Text, Text> out, Reporter reporter) throws IOException {\n\t\twhile(values.hasNext()) {\n\t\t\tText value = values.next();\n\t\t\tlogger.info(\"reduce: key=\" + key + \", value=\" + value);\n\t\t\tout.collect(new Text(key), new Text(value));\n\t\t}\n\t}\n\n\tpublic static void main(String[] args) throws IOException {\n\t\tJobConf jobConf = new JobConf();\n\t\tif(args.length < 3) {\n\t\t\tlogger.error(\"usage: 0|1 hadoop.hdfs.defaultfs hadoop.hdfs.workdir\");\n\t\t\treturn;\n\t\t}\n\t\tlogger.info(\"setting defaultfs and workdir\");\n\t\tjobConf.set(\"hadoop.hdfs.defaultfs\", args[1]);\n\n\t\tFileSystem fs = FileSystem.get(jobConf);\n\t\tfs.setWorkingDirectory(new Path(args[2]));\n\t\tfs.mkdirs(fs.getWorkingDirectory());\n\n\t\tjobConf.setJobName(TestMR.class.getName());\n\t\tjobConf.setOutputKeyClass(Text.class);\n\t\tjobConf.setOutputValueClass(Text.class);\n\n\t\tlogger.info(\"in: we use the 'in' directory. We create it and put some dummy data in it.\");\n\t\tfs.delete(new Path(fs.getWorkingDirectory(), \"in\"), true);\n\t\tfs.mkdirs(new Path(fs.getWorkingDirectory(), \"in\"));\n\t\tFSDataOutputStream dataOut = fs.create(new Path(fs.getWorkingDirectory(), \"in/data\"));\n\t\tdataOut.write(\"some data\".getBytes());\n\t\tdataOut.sync();\n\t\tdataOut.flush();\n\t\tdataOut.close();\n\t\t\n\t\tFileInputFormat.setInputPaths(jobConf, new Path(\"in\"));\n\n\t\tlogger.info(\"out: we use a subdirectory of the 'out' directory as the output of your job. Therefor we create the out folder first.\");\n\t\tfs.delete(new Path(fs.getWorkingDirectory(), \"out\"), true);\n\t\tfs.mkdirs(new Path(fs.getWorkingDirectory(), \"out\"));\n\t\tPath out = null;\n\t\t\n\t\tif(args.length > 0 && \"1\".equals(args[0])) {\n\t\t\tlogger.info(\"using broken folder\");\n\t\t\tout = getBrokenOutFolder();\n\t\t} else {\n\t\t\tlogger.info(\"using working folder\");\n\t\t\tout = getWorkingOutFolder(fs);\n\t\t}\n\t\tlogger.info(\"setting output using path: \" + out.toString());\n\t\tTextOutputFormat.setOutputPath(jobConf, new Path(out, \"subfolder\"));\n\n\t\tjobConf.setMapperClass(TestMR.class);\n\t\tjobConf.setReducerClass(TestMR.class);\n\t\tjobConf.setJarByClass(TestMR.class);\n\t\t\n\t\tJobClient jobClient = new JobClient(jobConf);\n\t\tRunningJob runningJob = jobClient.submitJob(jobConf);\n\t\t\n\t\twhile(!runningJob.isComplete()) {\n\t\t\tlogger.info(\"still running...\");\n\t\t\ttry {\n\t\t\t\tThread.sleep(1000);\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate static Path getWorkingOutFolder(FileSystem fs) throws IOException {\n\t\t\n\t\tPath root = new Path(\"/test\");\n\t\tfor(FileStatus fileStatus : fs.listStatus(root)) {\n\t\t\tif(\"out\".equals(fileStatus.getPath().getName())) { return fileStatus.getPath(); }\n\t\t}\n\t\treturn null;\n\t}\n\n\tprivate static Path getBrokenOutFolder() throws IOException {\n\t\treturn new Path(\"out\");\n\t}\n\t\n}\n{code}\nIf I start the application like \"bin/hadoop jar /tmp/example.jar org.test.TestMR 1 hdfs://BOCK:9000/test/ /test/\" it works.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fuchsmi","name":"fuchsmi","key":"fuchsmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Fuchs","active":true,"timeZone":"Etc/UTC"},"created":"2009-01-21T14:13:04.842+0000","updated":"2009-01-21T14:13:04.842+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12412817/comment/12665866","id":"12665866","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=knoguchi","name":"knoguchi","key":"knoguchi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Koji Noguchi","active":true,"timeZone":"America/New_York"},"body":"Michael, could you try HADOOP-4717 patch? \n(or wait for 0.18.3)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=knoguchi","name":"knoguchi","key":"knoguchi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Koji Noguchi","active":true,"timeZone":"America/New_York"},"created":"2009-01-21T15:46:06.657+0000","updated":"2009-01-21T15:46:06.657+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12412817/comment/14068957","id":"14068957","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"I'm going to close this issue as stale.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2014-07-21T18:15:49.306+0000","updated":"2014-07-21T18:15:49.306+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-3753/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i07ipr:"}}