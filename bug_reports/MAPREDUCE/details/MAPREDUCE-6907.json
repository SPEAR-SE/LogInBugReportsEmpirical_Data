{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13082922","self":"https://issues.apache.org/jira/rest/api/2/issue/13082922","key":"MAPREDUCE-6907","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":null,"customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"2017-06-27 19:12:27.405","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-6907/watchers","watchCount":1,"isWatching":false},"created":"2017-06-27T19:12:27.405+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334007","id":"12334007","description":"2.7.3 release","name":"2.7.3","archived":false,"released":true,"releaseDate":"2016-08-25"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-06-27T19:12:27.405+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"When I run a MapReduce program which uses a custom ArrayWritable based on a custom data type, I get this error:\n\n    java.io.IOException: Spill failed\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1562)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1471)\n    \tat org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)\n    \tat org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:2019)\n    \tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:797)\n    \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n    \tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n    \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    \tat java.lang.Thread.run(Thread.java:748)\n    Caused by: java.lang.NegativeArraySizeException\n    \tat org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:93)\n    \tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:71)\n    \tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)\n    \tat org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:146)\n    \tat org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)\n    \tat org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302)\n    \tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)\n    \tat org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1688)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1637)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$900(MapTask.java:876)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1532)\n    17/06/26 16:41:54 INFO mapred.LocalJobRunner: map task executor complete.\n    17/06/26 16:41:54 WARN mapred.LocalJobRunner: job_local680262639_0001\n    java.lang.Exception: java.io.IOException: Spill failed\n    \tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n    \tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\n    Caused by: java.io.IOException: Spill failed\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.checkSpillException(MapTask.java:1562)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$300(MapTask.java:876)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1372)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1349)\n    \tat java.io.DataOutputStream.writeShort(DataOutputStream.java:167)\n    \tat edu.nyu.cusp.umg.mapreduce.houghplane.CellWritable.write(CellWritable.java:55)\n    \tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)\n    \tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)\n    \tat org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)\n    \tat org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)\n    \tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)\n    \tat edu.nyu.cusp.umg.mapreduce.houghplane.HoughTransformRasterCombiner$HoughTransformMapper.writeFull(HoughTransformRasterCombiner.java:253)\n    \tat edu.nyu.cusp.umg.mapreduce.houghplane.HoughTransformRasterCombiner$HoughTransformMapper.map(HoughTransformRasterCombiner.java:158)\n    \tat edu.nyu.cusp.umg.mapreduce.houghplane.HoughTransformRasterCombiner$HoughTransformMapper.map(HoughTransformRasterCombiner.java:59)\n    \tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\n    \tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n    \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n    \tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n    \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    \tat java.lang.Thread.run(Thread.java:748)\n    Caused by: java.lang.NegativeArraySizeException\n    \tat org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:93)\n    \tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:71)\n    \tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)\n    \tat org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:146)\n    \tat org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)\n    \tat org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302)\n    \tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)\n    \tat org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1688)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1637)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$900(MapTask.java:876)\n    \tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1532)\n\nWhen I used Google I saw this is a bug with MapReduce. It says this bug was \"fixed\" but apparently not for a custom ArrayWritable with a WritableComparable. Is this a problem with my code or with Hadoop?","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hadoop \"Spill failed\" with custom ArrayWritable and data type - \"java.lang.Exception: java.lang.NegativeArraySizeException\"","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=neelc","name":"neelc","key":"neelc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Neel Chauhan","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=neelc","name":"neelc","key":"neelc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Neel Chauhan","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Ubuntu 16.04 (GNOME Desktop) on Dell Inspiron 7352, i7 5500U, 8GB RAM, 512GB SSD","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10431","value":"Important","id":"10431"}],"progress":{"progress":0,"total":0},"comment":{"comments":[],"maxResults":0,"total":0,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-6907/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3gsx3:"}}