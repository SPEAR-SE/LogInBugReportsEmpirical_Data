{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12677498","self":"https://issues.apache.org/jira/rest/api/2/issue/12677498","key":"MAPREDUCE-5606","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/2","id":"2","description":"The problem described is an issue which will never be fixed.","name":"Won't Fix"},"customfield_12312322":null,"customfield_12310220":"2013-11-05T17:32:01.376+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sat May 09 00:27:51 UTC 2015","customfield_12310420":"356873","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_1208070360_*|*_5_*:*_2_*:*_61472945_*|*_4_*:*_1_*:*_46242044794","customfield_12312321":null,"resolutiondate":"2015-05-09T00:27:51.603+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-5606/watchers","watchCount":7,"isWatching":false},"created":"2013-11-05T02:48:03.553+0000","customfield_12310192":"jobTracker 's logs :\n\n2013-11-04 12:29:24,651 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #0 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1103)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1075)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.recoverBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)\n        at java.io.DataInputStream.read(DataInputStream.java:132)\n        at org.apache.hadoop.security.SaslInputStream.read(SaslInputStream.java:239)\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:749)\n2013-11-04 12:29:25,661 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #1 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:26,669 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #2 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:27,676 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #3 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:28,678 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:28,678 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 in pipeline xx.xx.24.11\n5:1004, xx.xx.23.48:1004, xx.xx.23.82:1004: bad datanode xx.xx.24.115:1004\n2013-11-04 12:29:28,683 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #4 from primary datanode xx.xx\n.23.48:1004\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: Block blk_4347580689772316942_201043980 is already being recovered,  ignoring this req\nuest to recover it.\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1070)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.recoverBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:28,683 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 failed  because recovery\n from primary datanode xx.xx.23.48:1004 failed 5 times.  Pipeline was xx.xx.24.115:1004, xx.xx.23.48:1004, xx.xx.23.82:1004. Will retry...\n2013-11-04 12:29:29,685 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:29,685 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 in pipeline xx.xx.24.115:1004, xx.xx.23.48:1004, xx.xx.23.82:1004: bad datanode xx.xx.24.115:1004\n2013-11-04 12:29:29,689 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #5 from primary datanode xx.xx.23.48:1004\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: Block blk_4347580689772316942_201043980 is already being recovered,  ignoring this request to recover it.\n                                                                                                                               \n2013-11-04 12:29:28,678 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:28,678 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 in pipeline xx.xx.24.115:1004, xx.xx.23.48:1004, xx.xx.23.82:1004: bad datanode xx.xx.24.115:1004\n2013-11-04 12:29:28,683 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #4 from primary datanode xx.xx.23.48:1004\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: Block blk_4347580689772316942_201043980 is already being recovered,  ignoring this request to recover it.\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1070)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.recoverBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:28,683 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 failed  because recovery f\nrom primary datanode xx.xx.23.48:1004 failed 5 times.  Pipeline was xx.xx.24.115:1004, xx.xx.23.48:1004, xx.xx.23.82:1004. Will retry...\n2013-11-04 12:29:29,685 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:29,685 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 in pipeline xx.xx.24.115:1004, xx.xx.23.48:1004, xx.xx.23.82:1004: bad datanode xx.xx.24.115:1004\n2013-11-04 12:29:29,689 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #5 from primary datanode xx.xx.23.48:1004\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: Block blk_4347580689772316942_201043980 is already being recovered,  ignoring this request to recover it.\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n                                                                                                                                \n2013-11-04 12:29:28,678 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:28,678 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 in pipeline xx.xx.24.115:1004, xx.xx.23.48:1004, xx.xx.23.82:1004: bad datanode xx.xx.24.115:1004\n2013-11-04 12:29:28,683 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #4 from primary datanode xx.xx.23.48:1004\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: Block blk_4347580689772316942_201043980 is already being recovered,  ignoring this request to recover it.\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1070)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.recoverBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:28,683 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 failed  because recovery from primary datanode xx.xx.23.48:1004 failed 5 times.  \nPipeline was xx.xx.24.115:1004, xx.xx.23.48:1004, xx.xx.23.82:1004. Will retry...\n2013-11-04 12:29:29,685 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:29,685 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 in pipeline xx.xx.24.115:1004, xx.xx.23.48:1004, xx.xx.23.82:1004: bad datanode xx.xx.24.115:1004\n2013-11-04 12:29:29,689 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #5 from primary datanode xx.xx.23.48:1004\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: Block blk_4347580689772316942_201043980 is already being recovered,  ignoring this request to recover it.\n2013-11-04 12:29:24,651 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #0 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1103)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1075)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)\n        at java.io.DataInputStream.read(DataInputStream.java:132)\n        at org.apache.hadoop.security.SaslInputStream.read(SaslInputStream.java:239)\n        at java.io.FilterInputStream.read(FilterInputStream.java:116)\n        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:342)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)\n        at java.io.DataInputStream.readInt(DataInputStream.java:370)\n        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:804)\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:749)\n2013-11-04 12:29:25,654 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:25,661 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #1 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1070)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.recoverBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:26,663 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:26,669 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #2 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1070)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.recoverBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:27,671 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:27,676 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #3 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1070)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.recoverBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:28,678 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n2013-11-04 12:29:28,683 WARN org.apache.hadoop.hdfs.DFSClient: Failed recovery attempt #4 from primary datanode xx.xx.23.48:1004\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1884)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2047)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1070)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.recoverBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2589)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2793)\n2013-11-04 12:29:29,685 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_4347580689772316942_201043980 bad datanode[0] xx.xx.24.115:1004\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1070)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3121)\n2013-11-04 12:30:22,883 INFO org.mortbay.log: index:34\n2013-11-04 12:30:59,778 INFO org.mortbay.log: index:34\n2013-11-04 12:31:16,161 INFO org.mortbay.log: index:34\n2013-11-04 12:30:30,692 WARN org.apache.hadoop.ipc.Client: Encountered java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be\nready for read. ch : java.nio.channels.SocketChannel[connected local=/xx.xx.23.162:30348 remote=/xx.xx.23.82:50020] while trying to establish SASL connecti\non to the server. Will retry SASL connection to server with principal hadoop/a082@DATA\n2013-11-04 12:30:33,375 INFO org.mortbay.log: index:34\n2013-11-04 12:30:33,771 INFO org.mortbay.log: index:34\n2013-11-04 12:30:59,778 INFO org.mortbay.log: index:34\n2013-11-04 12:31:09,851 INFO org.mortbay.log: index:40\n2013-11-04 12:31:15,345 INFO org.mortbay.log: index:34\n2013-11-04 12:31:16,161 INFO org.mortbay.log: index:34\n2013-11-04 12:31:16,635 INFO org.mortbay.log: index:34\n2013-11-04 12:31:26,504 INFO org.mortbay.log: index:40\n2013-11-04 12:31:26,521 INFO org.mortbay.log: index:40\n2013-11-04 12:31:34,674 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:blk_4347580689772316942_201043980 cause:java.net\n.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/xx.xx.23\n.162:30438 remote=/xx.xx.23.82:50020]\n2013-11-04 12:31:34,674 WARN org.apache.hadoop.ipc.Client: Encountered java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be\nready for read. ch : java.nio.channels.SocketChannel[connected local=/xx.xx.23.162:30438 remote=/xx.xx.23.82:50020] while trying to establish SASL connection to the server. Will retry SASL connection to server with principal hadoop/a082@DATA\n2013-11-04 12:32:37,666 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:blk_4347580689772316942_201043980 cause:java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/xx.xx.23.162:30529 remote=/xx.xx.23.82:50020]\n2013-11-04 12:32:37,666 WARN org.apache.hadoop.ipc.Client: Encountered java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to beready for read. ch : java.nio.channels.SocketChannel[connected local=/xx.xx.23.162:30529 remote=/xx.xx.23.82:50020] while trying to establish SASL connection to the server. Will retry SASL connection to server with principal hadoop/082@DATA","customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320250","id":"12320250","description":"maintenance release on branch-1.0","name":"1.0.3","archived":false,"released":true,"releaseDate":"2012-05-07"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-05-09T00:27:51.645+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312907","id":"12312907","name":"jobtracker"}],"timeoriginalestimate":null,"description":"when a  datanode was crash,the server can  ping ok,but can not  call rpc ,and also can not ssh login. and then jobTracker may be request a block on this datanode.\nit will happened ,the  JobTracker can not work,the webUI is also unwork,hadoop job -list also unwork,the jobTracker logs no other info .\n\nand then we need to restart the datanode.\nthen jobTraker can work too,but the taskTracker num come to zero,\nwe need run : hadoop mradmin -refreshNodes\nthen the JobTracker begin to add taskTraker ,but is very slowly.\n\nthis problem occur 5time  in 2weeks.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"357163","customfield_12312823":null,"summary":"JobTracker blocked for DFSClient: Failed recovery attempt","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"centos 5.8  jdk 1.7 ","customfield_12313520":null,"customfield_12311020":null,"duedate":"2013-11-04","customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13814061","id":"13814061","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Does this still happen when you upgrade to the most recent version of Hadoop 1.x (or better yet, Hadoop 2.2?)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2013-11-05T17:32:01.376+0000","updated":"2013-11-05T17:32:01.376+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13814471","id":"13814471","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"body":"thanks for your reply,  f we hava a plan upgrade the hadoop to 2.0.X next month,but there  now we need to do some test first.because the version on the production is modify base on 1.0.3.\n\ni think  1.0.3  version will last at least for  a month.\n\nbefore this time ，the server work very well more then 1 years.\n\nnow i can do is monitor the jobTracker log，when found some datanode was crash，then add this datanode to hostexclude.\n\ncan u give me some idea, or why it happened?\n\nneed  i turn down  the socket timeout  ,fail time?\n\nthanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-06T00:52:39.760+0000","updated":"2013-11-06T00:52:39.760+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13815261","id":"13815261","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"I've seen this happen in more recent 1.x versions too.  In my case, it happened while writing job history files to HDFS.  The problem is that this occurs while holding a global lock (inside a synchronized method of the {{JobTracker}} object).  This prevents the JT from getting other useful work done, like accepting new job submissions or displaying the web UI.  You might be able to confirm this by inspecting a thread dump of your JT process while this is happening.\n\nIf your investigation shows the same root cause (blocked writing history files to HDFS), then you can disable this and instead only write history to the local file system.  If the configuration parameter hadoop.job.history.location is set to a location on HDFS, then remove this.  (It will default to the standard Hadoop log directory on the local file system.)\n\nThere is also hadoop.job.history.user.location.  If unspecified, this will default to writing per-job history files in each job's output directory in HDFS.  You can disable these files by setting the value to none, like this:\n\n{code}\n<property>\n  <name>hadoop.job.history.user.location</name>\n  <final>true</final>\n  <value>none</value>\n</property>\n{code}\n\nTo fix this issue completely, we'd need to move the logic for writing history outside of the {{JobTracker}} monitor.  Really any kind of I/O performed while holding a global lock is problematic due to the risk of failure.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-06T20:31:25.916+0000","updated":"2013-11-06T20:31:25.916+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13825524","id":"13825524","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"I just noticed an old issue, MAPREDUCE-1144, which appears to be the same thing.  Is this a duplicate?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-18T17:51:27.737+0000","updated":"2013-11-18T17:51:27.737+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13825569","id":"13825569","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=viswaj","name":"viswaj","key":"viswaj","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"viswanathan","active":true,"timeZone":"Etc/UTC"},"body":"Hi Chris,\n\nBy adding the below property hope it will resolve temporarily, how to resolve this permanently.\n\n<property>\n  <name>hadoop.job.history.user.location</name>\n  <final>true</final>\n  <value>none</value>\n</property>\n\nEarlier we had used Hadoop 1.0.3 and 1.0.4 versions, but we never face this type of issue. Only we are facing in hadoop version 1.2.1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=viswaj","name":"viswaj","key":"viswaj","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"viswanathan","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-18T18:21:33.938+0000","updated":"2013-11-18T18:21:33.938+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13825590","id":"13825590","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"That config setting can be considered a permanent fix if you're not interested in saving user history to HDFS.  OTOH, if you are interested in saving user history to HDFS, then there is no permanent fix yet.  That's what this issue tracks.  :-)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-18T18:37:08.563+0000","updated":"2013-11-18T18:37:08.563+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13826079","id":"13826079","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"body":"Hi Chris,\n        Thanks for your help.we have set the property hadoop.job.history.user.location  to none  for a week  .   Just as you said，It now work  well.i closed   this jira.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-19T02:20:52.055+0000","updated":"2013-11-19T02:20:52.055+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13826081","id":"13826081","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"body":"<property>\n<name>hadoop.job.history.user.location</name>\n<final>true</final>\n<value>none</value>\n</property>","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-19T02:22:33.903+0000","updated":"2013-11-19T02:22:33.903+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13826085","id":"13826085","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=viswaj","name":"viswaj","key":"viswaj","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"viswanathan","active":true,"timeZone":"Etc/UTC"},"body":"Hi,\n\nDo I need to set this property only in jobtracker node mapred Conf or in\nwhole hdfs cluster.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=viswaj","name":"viswaj","key":"viswaj","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"viswanathan","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-19T02:31:20.001+0000","updated":"2013-11-19T02:31:20.001+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13826091","id":"13826091","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"body":"just set on jobTracker ,and need restart the jobTracker!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=firegun","name":"firegun","key":"firegun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"firegun","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-19T02:35:06.438+0000","updated":"2013-11-19T02:35:06.438+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/13826841","id":"13826841","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"I'm reopening this.  There is an actual bug here (holding a global lock in the JT while doing I/O).  Despite the config workaround I described, I don't think we can really call it resolved.\n\nWhat I'm not sure about is if this is a duplicate of MAPREDUCE-1144.  If anyone on that issue can tell, then we can close this as duplicate.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-19T19:27:06.852+0000","updated":"2013-11-19T19:27:06.852+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12677498/comment/14536022","id":"14536022","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"body":"I originally debugged the user.location bug with [~cnauroth].\n\nIt indeed is the same issue as MAPREDUCE-1144.\n\nWe haven't done a 1.x release in ~2 years. Unlikely this issue is going to be fixed/released. This shouldn't happen in 2.x, which was a complete rewrite. \n\nClosing this as Won't Fix.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-09T00:27:51.638+0000","updated":"2015-05-09T00:27:51.638+0000"}],"maxResults":12,"total":12,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-5606/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1pitb:"}}