{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12558847","self":"https://issues.apache.org/jira/rest/api/2/issue/12558847","key":"MAPREDUCE-4298","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2012-06-18T14:29:34.501+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Jun 18 14:29:34 UTC 2012","customfield_12310420":"257097","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_13239080_*|*_1_*:*_2_*:*_1524599346_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2012-06-18T14:29:34.451+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-4298/watchers","watchCount":6,"isWatching":false},"created":"2012-05-31T19:18:56.110+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320060","id":"12320060","description":"0.23.3","name":"0.23.3","archived":false,"released":true,"releaseDate":"2012-09-20"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12320354","id":"12320354","description":"hadoop-2.0.0-alpha release","name":"2.0.0-alpha","archived":false,"released":true,"releaseDate":"2012-05-23"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12335734","id":"12335734","description":"3.0.0-alpha1 release","name":"3.0.0-alpha1","archived":false,"released":true,"releaseDate":"2016-09-03"}],"issuelinks":[{"id":"12353761","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12353761","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12559884","key":"HADOOP-8495","self":"https://issues.apache.org/jira/rest/api/2/issue/12559884","fields":{"summary":"Update Netty to avoid leaking file descriptors during shuffle","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-05-12T18:23:45.766+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12314301","id":"12314301","name":"mrv2","description":"MR-279: Map Reduce Next."},{"self":"https://issues.apache.org/jira/rest/api/2/component/12315341","id":"12315341","name":"nodemanager"}],"timeoriginalestimate":null,"description":"A node on one of our clusters fell over because it ran out of open file descriptors.  Log details with stack traceback to follow.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320060","id":"12320060","description":"0.23.3","name":"0.23.3","archived":false,"released":true,"releaseDate":"2012-09-20"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12531406","id":"12531406","filename":"MAPREDUCE-4298.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2012-06-08T15:38:53.662+0000","size":388,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12531406/MAPREDUCE-4298.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"112818","customfield_12312823":null,"summary":"NodeManager crashed after running out of file descriptors","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12558847/comment/13286869","id":"13286869","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"The relevant portion of the NM logfile:\n\n{noformat}\n [New I/O server worker #1-2]2012-05-25 21:30:36,719 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/1/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000833_0/file.out not found\n [New I/O server worker #1-15]2012-05-25 21:30:36,719 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/1/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000833_0/file.out not found\n [New I/O server worker #1-23]2012-05-25 21:30:36,719 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/1/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000833_0/file.out not found\n [New I/O server worker #1-2]2012-05-25 21:30:36,720 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/9/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000831_0/file.out not found\n [New I/O server worker #1-17]2012-05-25 21:30:36,720 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/1/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000833_0/file.out not found\n [New I/O server worker #1-24]2012-05-25 21:30:36,721 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/10/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000030_0/file.out not found\n [New I/O server worker #1-30]2012-05-25 21:30:36,722 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/1/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000314_0/file.out not found\n [New I/O server worker #1-18]2012-05-25 21:30:36,722 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Container container_1337019383242_29334_01_000148 succeeded \n [ContainersLauncher #2677]2012-05-25 21:30:36,722 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1337019383242_29334_01_000148 transitioned from RUNNING to EXITED_WITH_SUCCESS\n [AsyncDispatcher event handler]2012-05-25 21:30:36,722 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1337019383242_29334_01_000148\n [AsyncDispatcher event handler]2012-05-25 21:30:36,727 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/9/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000831_0/file.out not found\n [New I/O server worker #1-7]2012-05-25 21:30:36,727 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/0/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000263_0/file.out not found\n [New I/O server worker #1-27]2012-05-25 21:30:36,728 INFO org.apache.hadoop.mapred.ShuffleHandler: /grid/0/tmp/yarn-local/usercache/xx/appcache/application_1337019383242_29334/output/attempt_1337019383242_29334_m_000263_0/file.out not found\n [New I/O server worker #1-13]2012-05-25 21:30:36,728 FATAL org.apache.hadoop.conf.Configuration: error parsing conf file: java.io.FileNotFoundException: /grid/0/Releases/conf-xx-0.23.3.1204240816-20120514-000/hadoop/datanode/core-site.xml (Too many open files)\n  [AsyncDispatcher event handler]2012-05-25 21:30:36,728 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread\n [AsyncDispatcher event handler]java.lang.RuntimeException: java.io.FileNotFoundException: /grid/0/Releases/conf-xx-0.23.3.1204240816-20120514-000/hadoop/datanode/core-site.xml (Too many open files)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1723)\n        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1588)\n        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1534)\n        at org.apache.hadoop.conf.Configuration.get(Configuration.java:581)\n        at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:598)\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1332)\n        at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:142)\n        at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:233)\n        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:315)\n        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:313)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n        at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:313)\n        at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:426)\n        at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:406)\n        at org.apache.hadoop.fs.FileContext.getLocalFSFileContext(FileContext.java:392)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:353)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:144)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:54)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:74)\n        at java.lang.Thread.run(Thread.java:619)\nCaused by: java.io.FileNotFoundException: /grid/0/Releases/conf-xx-0.23.3.1204240816-20120514-000/hadoop/datanode/core-site.xml (Too many open files)\n        at java.io.FileInputStream.open(Native Method)\n        at java.io.FileInputStream.<init>(FileInputStream.java:106)\n        at java.io.FileInputStream.<init>(FileInputStream.java:66)\n        at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)\n        at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)\n        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)\n        at com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:186)\n        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:772)\n        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)\n        at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)\n        at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:235)\n        at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)\n        at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:180)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1637)\n        ... 22 more\n2012-05-25 21:30:36,729 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..\n{noformat}\n\nIt's not clear yet exactly why the NM hit the open file ulimit which was set to 8192.  There were jobs running on the cluster with 4000+ reducers launched (potentially multiple of these types of jobs simultaneously).\n\nI checked the ShuffleHandler which uses Netty, and I didn't see any limit to the number of connections Netty would accept.  I wrote a quick Python script that simply connects to the shuffle port as fast as it can, keeping around all the sockets, and shortly afterwards the Netty boss thread had crashed with this in the NM logfile:\n\n{noformat}\nException in thread \"ShuffleHandler Netty Boss #0\" java.lang.InternalError\n        at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:755)\n        at sun.misc.URLClassPath.getResource(URLClassPath.java:169)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:194)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n        at sun.misc.Launcher$ExtClassLoader.findClass(Launcher.java:229)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:295)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n        at java.util.ResourceBundle$RBClassLoader.loadClass(ResourceBundle.java:435)\n        at java.util.ResourceBundle$Control.newBundle(ResourceBundle.java:2289)\n        at java.util.ResourceBundle.loadBundle(ResourceBundle.java:1364)\n        at java.util.ResourceBundle.findBundle(ResourceBundle.java:1328)\n        at java.util.ResourceBundle.findBundle(ResourceBundle.java:1282)\n        at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1224)\n        at java.util.ResourceBundle.getBundle(ResourceBundle.java:705)\n        at java.util.logging.Level.getLocalizedName(Level.java:223)\n        at java.util.logging.SimpleFormatter.format(SimpleFormatter.java:64)\n        at java.util.logging.StreamHandler.publish(StreamHandler.java:179)\n        at java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)\n        at java.util.logging.Logger.log(Logger.java:481)\n        at java.util.logging.Logger.doLog(Logger.java:503)\n        at java.util.logging.Logger.logp(Logger.java:703)\n        at org.jboss.netty.logging.JdkLogger.warn(JdkLogger.java:86)\n        at org.jboss.netty.logging.InternalLoggerFactory$1.warn(InternalLoggerFactory.java:133)\n        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink$Boss.run(NioServerSocketPipelineSink.java:260)\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\nCaused by: java.util.zip.ZipException: error in opening zip file\n        at java.util.zip.ZipFile.open(Native Method)\n        at java.util.zip.ZipFile.<init>(ZipFile.java:127)\n        at java.util.jar.JarFile.<init>(JarFile.java:135)\n        at java.util.jar.JarFile.<init>(JarFile.java:72)\n        at sun.misc.URLClassPath$JarLoader.getJarFile(URLClassPath.java:646)\n        at sun.misc.URLClassPath$JarLoader.access$600(URLClassPath.java:540)\n        at sun.misc.URLClassPath$JarLoader$1.run(URLClassPath.java:607)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at sun.misc.URLClassPath$JarLoader.ensureOpen(URLClassPath.java:599)\n        at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:753)\n        ... 31 more\n{noformat}\n\nSo in this case, Netty probably got an EMFILE error when trying to accept a new connection after hitting the socket limit, and when it tried to log a warning it ended up invoking the class loader which was unable to get a new file descriptor for the jar file.\n\nGiven there were a lot of reducers running, in theory these could all hit a single nodemanager near the same time when waiting for the last map to finish.  If for some reason the reducer shuffle connections lingered around long enough, we'd exhaust the file descriptors on the nodemanager given enough reducers within a short enough time span.  I can't tell from the logs if this is exactly what happened in this case, but it is concerning that there appears to be no limit to the number of connections the shuffle handler will accept before we hit hard OS limits that will crash the NM in various ways.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2012-05-31T19:44:18.675+0000","updated":"2012-05-31T19:44:18.675+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12558847/comment/13291492","id":"13291492","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"This occurred again on one of our clusters.  Turns out I was mistaken earlier, the file descriptor ulimit for our nodemanager daemons is set to 32768, not 8192.  Fortunately this time we were able to examine some nodemanagers that had leaked numerous file descriptors but had not fallen over yet.\n\nAlmost all of the file descriptors were referencing map outputs for the shuffle, often hundreds of file descriptors open to the same file.  Interestingly almost all of the map files corresponded to just one job.  Examining the NM log around the time that job ran, I found numerous exceptions in it showing things had not gone smoothly during the shuffle for that job.  For example:\n\n{noformat}\n [New I/O server worker #1-5]java.io.IOException: Connection reset by peer\n        at sun.nio.ch.FileDispatcher.write0(Native Method)\n        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:29)\n        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:100)\n        at sun.nio.ch.IOUtil.write(IOUtil.java:56)\n        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)\n        at org.jboss.netty.channel.socket.nio.SocketSendBufferPool$PooledSendBuffer.transferTo(SocketSendBufferPool.java:239)\n        at org.jboss.netty.channel.socket.nio.NioWorker.write0(NioWorker.java:470)\n        at org.jboss.netty.channel.socket.nio.NioWorker.writeFromUserCode(NioWorker.java:388)\n        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:137)\n        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)\n        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:68)\n        at org.jboss.netty.handler.stream.ChunkedWriteHandler.flush(ChunkedWriteHandler.java:253)\n        at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleDownstream(ChunkedWriteHandler.java:123)\n        at org.jboss.netty.channel.Channels.write(Channels.java:611)\n        at org.jboss.netty.channel.Channels.write(Channels.java:578)\n        at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:259)\n        at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:477)\n        at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)\n        at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:144)\n        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:523)\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:507)\n        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:444)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350)\n        at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)\n        at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n{noformat}\n\nLooking closer at the job, I could see that it had run with 15000 maps and 2000 reduces.  Hundreds of the reducers had failed running out of heap space during the shuffle phase, which lead to broken pipe and connection reset errors on the nodemanagers trying to serve up shuffle data to those reducers when they died.\n\nI was able to reproduce the broken pipe issue and step through the code with a debugger.  Normally the file descriptor is closed by adding a ChannelFuture after the map data is written, and that future's operationComplete() callback closes the file.  However when there is an I/O error sending the shuffle header, Netty closes down the channel automatically (plus we explicitly close it in a channel exception handler).  By the time we try to write the map file data to the channel, the channel is already closed.  And I was able to see that if we write to a closed channel, the ChannelFuture's operationComplete() callback is never invoked.  No operationComplete() callback means we leak the file descriptor for the map file.  If multiple map files are being sent to the reducer, we leak multiple file descriptors for the same error.\n\nI searched around and discovered this is a known issue in Netty 3.2.3.Final, the version we're currently using.  See https://issues.jboss.org/browse/NETTY-374.  It's fixed in version 3.2.4.Final.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2012-06-08T01:43:18.334+0000","updated":"2012-06-08T01:43:18.334+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12558847/comment/13291823","id":"13291823","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Patch to update Netty version from 3.2.3.Final to 3.2.4.Final.  I manually tested it on a single-node cluster to verify a broken pipe during the write of the shuffle header doesn't lead to a leaked file descriptor.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2012-06-08T15:38:53.827+0000","updated":"2012-06-08T15:38:53.827+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12558847/comment/13291830","id":"13291830","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Bumping priority since one misconfigured job can take down nodemanagers for a significant portion of the cluster.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2012-06-08T15:48:18.106+0000","updated":"2012-06-08T15:48:18.106+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12558847/comment/13291922","id":"13291922","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Patch for Netty version change is against Hadoop Common code, so I filed HADOOP-8495.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2012-06-08T19:20:00.536+0000","updated":"2012-06-08T19:20:00.536+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12558847/comment/13395918","id":"13395918","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tgraves","name":"tgraves","key":"tgraves","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Thomas Graves","active":true,"timeZone":"America/Chicago"},"body":"dup of HADOOP-8495","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tgraves","name":"tgraves","key":"tgraves","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Thomas Graves","active":true,"timeZone":"America/Chicago"},"created":"2012-06-18T14:29:34.501+0000","updated":"2012-06-18T14:29:34.501+0000"}],"maxResults":6,"total":6,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-4298/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0jnzz:"}}