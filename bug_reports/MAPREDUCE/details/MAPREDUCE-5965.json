{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12726479","self":"https://issues.apache.org/jira/rest/api/2/issue/12726479","key":"MAPREDUCE-5965","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329060","id":"12329060","description":"2.8.0 release","name":"2.8.0","archived":false,"released":true,"releaseDate":"2017-03-22"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12335734","id":"12335734","description":"3.0.0-alpha1 release","name":"3.0.0-alpha1","archived":false,"released":true,"releaseDate":"2016-09-03"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2014-07-11T18:49:20.698+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jun 04 15:57:53 UTC 2015","customfield_12310420":"404586","customfield_12312320":null,"customfield_12310222":"10002_*:*_4_*:*_21521295104_*|*_1_*:*_4_*:*_6910170797_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-06-04T01:46:20.919+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-5965/watchers","watchCount":9,"isWatching":false},"created":"2014-07-10T00:08:35.119+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-08-30T01:19:55.837+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Hadoop streaming exposes all the key values in job conf as environment variables when it forks a process for streaming code to run. Unfortunately the variable mapreduce_input_fileinputformat_inputdir contains the list of input files, and Linux has a limit on size of environment variables + arguments.\nBased on how long the list of files and their full path is this could be pretty huge. And given all of these variables are not even used it stops user from running hadoop job with large number of files, even though it could be run.\n\nLinux throws E2BIG if the size is greater than certain size which is error code 7. And java translates that to \"error=7, Argument list too long\". More: http://man7.org/linux/man-pages/man2/execve.2.html I suggest skipping variables if it is greater than certain length. That way if user code requires the environment variable it would fail. It should also introduce a config variable to skip long variables, and set it to false by default. That way user has to specifically set it to true to invoke this feature.\n\nHere is the exception:\n{code}\nError: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163) Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 9 more Caused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 17 more Caused by: java.lang.RuntimeException: configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:222) at org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66) ... 22 more Caused by: java.io.IOException: Cannot run program \"/data/hadoop/hadoop-yarn/cache/yarn/nm-local-dir/usercache/oo-analytics/appcache/application_1403599726264_13177/container_1403599726264_13177_01_000006/./rbenv_runner.sh\": error=7, Argument list too long at java.lang.ProcessBuilder.start(ProcessBuilder.java:1041) at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:209) ... 23 more Caused by: java.io.IOException: error=7, Argument list too long at java.lang.UNIXProcess.forkAndExec(Native Method) at java.lang.UNIXProcess.<init>(UNIXProcess.java:135) at java.lang.ProcessImpl.start(ProcessImpl.java:130) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1022) ... 24 more Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 Container exited with a non-zero exit code 143\n{code}\n\nHive does a similar trick: HIVE-2372 I have a patch for this, will soon submit a patch.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12696883","id":"12696883","filename":"MAPREDUCE-5965.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-05T22:04:15.900+0000","size":3212,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12696883/MAPREDUCE-5965.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12733519","id":"12733519","filename":"MAPREDUCE-5965.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-18T11:32:41.436+0000","size":1775,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12733519/MAPREDUCE-5965.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12735146","id":"12735146","filename":"MAPREDUCE-5965.3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-25T11:02:06.120+0000","size":5001,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12735146/MAPREDUCE-5965.3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12655256","id":"12655256","filename":"MAPREDUCE-5965.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-07-11T17:53:59.323+0000","size":2670,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12655256/MAPREDUCE-5965.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"404625","customfield_12312823":null,"summary":"Hadoop streaming throws error if list of input files is high. Error is: \"error=7, Argument list too long at if number of input file is high\"","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14059103","id":"14059103","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"body":"Attaching patch for trunk. \n\nWhile exporting job conf values as environment variables, it would look for configuration key: stream.drop.long.env.variable.value if it is set to true then it would do a length check on jobconf values and would drop it if it happens to be greater than 20KB.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-07-11T17:56:06.645+0000","updated":"2014-07-11T17:56:06.645+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14059187","id":"14059187","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12655256/MAPREDUCE-5965.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-tools/hadoop-streaming:\n\norg.apache.hadoop.streaming.TestFileArgs\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4728//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/4728//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-11T18:49:20.698+0000","updated":"2014-07-11T18:49:20.698+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14307851","id":"14307851","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12655256/MAPREDUCE-5965.patch\n  against trunk revision 276485e.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5165//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2015-02-05T19:41:33.280+0000","updated":"2015-02-05T19:41:33.280+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14307853","id":"14307853","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"Cancelling patch since it no longer applies.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2015-02-05T19:42:41.673+0000","updated":"2015-02-05T19:42:41.673+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14308081","id":"14308081","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"body":"Reattaching updated patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-02-05T22:04:15.909+0000","updated":"2015-02-05T22:04:15.909+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14308092","id":"14308092","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12696883/MAPREDUCE-5965.1.patch\n  against trunk revision e1990ab.\n\n    {color:red}-1 patch{color}.  The patch command could not apply the patch.\n\nConsole output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5168//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2015-02-05T22:10:22.518+0000","updated":"2015-02-05T22:10:22.518+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14342055","id":"14342055","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"Cancelling the patch... again.\n\nIt needs to be rebased for the current source tree.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2015-03-01T08:13:22.181+0000","updated":"2015-03-01T08:13:22.181+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14547891","id":"14547891","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"body":"Ran into the same issue. Re-based and cleaned up patch which does the same as the Hive patch (truncate the environment value)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-18T11:32:41.450+0000","updated":"2015-05-18T11:32:41.450+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14547929","id":"14547929","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"\\\\\n\\\\\n| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | pre-patch |  15m 10s | Pre-patch trunk compilation is healthy. |\n| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |\n| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |\n| {color:green}+1{color} | javac |   7m 48s | There were no new javac warning messages. |\n| {color:green}+1{color} | javadoc |   9m 47s | There were no new javadoc warning messages. |\n| {color:green}+1{color} | release audit |   0m 21s | The applied patch does not increase the total number of release audit warnings. |\n| {color:green}+1{color} | checkstyle |   0m 25s | There were no new checkstyle issues. |\n| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |\n| {color:green}+1{color} | install |   1m 34s | mvn install still works. |\n| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |\n| {color:green}+1{color} | findbugs |   0m 42s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |\n| {color:green}+1{color} | tools/hadoop tests |   6m 14s | Tests passed in hadoop-streaming. |\n| | |  42m 37s | |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Patch URL | http://issues.apache.org/jira/secure/attachment/12733519/MAPREDUCE-5965.2.patch |\n| Optional Tests | javadoc javac unit findbugs checkstyle |\n| git revision | trunk / 363c355 |\n| hadoop-streaming test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5741/artifact/patchprocess/testrun_hadoop-streaming.txt |\n| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5741/testReport/ |\n| Java | 1.7.0_55 |\n| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5741/console |\n\n\nThis message was automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2015-05-18T12:23:22.205+0000","updated":"2015-05-18T12:23:22.205+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14549114","id":"14549114","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rchiang","name":"rchiang","key":"rchiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ray Chiang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Wilfred.  I guess I'll comment on the meta issue first.  In general, I'm not sure whether it's  a good idea to filter based purely on size.  Would it better to have a more firm whitelist and/or blacklist capability for Hadoop streaming?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rchiang","name":"rchiang","key":"rchiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ray Chiang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-18T20:11:01.283+0000","updated":"2015-05-18T20:11:01.283+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14549128","id":"14549128","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rchiang","name":"rchiang","key":"rchiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ray Chiang","active":true,"timeZone":"America/Los_Angeles"},"body":"Making these comments assuming the current patch is an acceptable design approach, I have the following nitpicks:\n\n1) Can \"stream.truncate.long.jobconf.values\" be put in the appropriate *-default.xml file for documentation purposes?\n\n2) Can the lenLimit correspond to a Configuration variable?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rchiang","name":"rchiang","key":"rchiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ray Chiang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-18T20:15:36.957+0000","updated":"2015-05-18T20:15:36.957+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14549661","id":"14549661","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"body":"Arup: Do you mind if I assign the jira to me? Would like to get this fixed in an upcoming release.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-19T02:07:25.563+0000","updated":"2015-05-19T02:07:25.563+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14553180","id":"14553180","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"body":"[~wilfreds] sure. Just a comment in the patch I had submitted the check was inside a separate function with some comment on why we want to do it. As can be seen in: https://issues.apache.org/jira/secure/attachment/12696883/MAPREDUCE-5965.1.patch Is there a reason to remove those?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amalakar","name":"amalakar","key":"amalakar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=amalakar&avatarId=19604","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amalakar&avatarId=19604","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amalakar&avatarId=19604","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amalakar&avatarId=19604"},"displayName":"Arup Malakar","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-20T21:41:57.284+0000","updated":"2015-05-20T21:41:57.284+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14554197","id":"14554197","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"body":"[~amalakar] thank you for the assignment. The comment should be added back, I'll do that with an updated patch. The move to keep it in the same method was to make the change as simple as possible.\n\n[~rchiang] \nThe streaming configuration does not really have a *-default.xml file. There is documentation (markdown) that shows some of the settings and options: adding it to the FAQ would probably be the correct place. There is a help that is printed in the main StreamJob code that shows most of the options. I will update the two files and explain the setting that is available. I can upload a new patch with that added before I do lets get the other points finalised.\n\nA white list or black list is possible but what would we exclude or include? In the job configuration there could be any value which could be too long, a user could set something he wants. It will be really difficult to filter that consistently and be sure that we have a fix with limited impact.\n\nMaking the lenLimit configurable is possible. However I do not see what we would win with making the length configurable. The data is not used anywhere and lowering or increasing the size at which we cut it off will not give us anything extra. If you really want to make it configurable the easiest way would be to roll the two settings in one. We could make the stream.truncate.long.jobconf.values an integer: -1 do not truncate otherwise truncate at the length given.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-21T12:23:50.642+0000","updated":"2015-05-21T12:23:50.642+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14555957","id":"14555957","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=djp","name":"djp","key":"djp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=djp&avatarId=16954","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=djp&avatarId=16954","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=djp&avatarId=16954","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=djp&avatarId=16954"},"displayName":"Junping Du","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks guys for good discussions here. +1 on the overall solution here. Agree that we don't need to put new streaming configuration to *-default.xml as previous practices. \n\nbq. If you really want to make it configurable the easiest way would be to roll the two settings in one. We could make the stream.truncate.long.jobconf.values an integer: -1 do not truncate otherwise truncate at the length given.\nThat sounds better. May be we should rename \"stream.truncate.long.jobconf.values\" to something like: \"stream.jobconf.truncate.limit\" and document somewhere to say -1 is the default value which doesn't do any truncate and 20K is a proper value for most cases?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=djp","name":"djp","key":"djp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=djp&avatarId=16954","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=djp&avatarId=16954","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=djp&avatarId=16954","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=djp&avatarId=16954"},"displayName":"Junping Du","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-05-22T10:41:18.187+0000","updated":"2015-05-22T10:41:18.187+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14556415","id":"14556415","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rchiang","name":"rchiang","key":"rchiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ray Chiang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the clarification.  I'm still getting used to the non-core Hadoop parts and how those need or don't need to conform.\n\n+1 for the suggested property name and usage better.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rchiang","name":"rchiang","key":"rchiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ray Chiang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-22T16:39:35.310+0000","updated":"2015-05-22T16:39:35.310+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14558113","id":"14558113","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"body":"Updated the patch using the new name and made it an integer as [~djp] proposed. The documentation and the usage that is printed in the StreamJob have been updated to show the new option and the values.\n\nTo answer the 20000 question: it would be long enough to leave all but the problem value alone.\n\nThree values are documented:\n-1: do not truncate (default)\n0: only copy the key and not the value (side effect of using substring)\n20000: as a safe value which should prevent the \"error=7\" issue","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-25T11:02:06.129+0000","updated":"2015-05-25T11:02:06.129+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14558148","id":"14558148","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"\\\\\n\\\\\n| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | pre-patch |  17m 44s | Pre-patch trunk compilation is healthy. |\n| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |\n| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |\n| {color:green}+1{color} | javac |   7m 30s | There were no new javac warning messages. |\n| {color:green}+1{color} | javadoc |   9m 37s | There were no new javadoc warning messages. |\n| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |\n| {color:green}+1{color} | site |   2m 57s | Site still builds. |\n| {color:green}+1{color} | checkstyle |   0m 25s | There were no new checkstyle issues. |\n| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |\n| {color:green}+1{color} | install |   1m 33s | mvn install still works. |\n| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |\n| {color:green}+1{color} | findbugs |   0m 38s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |\n| {color:green}+1{color} | tools/hadoop tests |   6m  7s | Tests passed in hadoop-streaming. |\n| | |  47m 30s | |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Patch URL | http://issues.apache.org/jira/secure/attachment/12735146/MAPREDUCE-5965.3.patch |\n| Optional Tests | javadoc javac unit findbugs checkstyle site |\n| git revision | trunk / ada233b |\n| hadoop-streaming test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5753/artifact/patchprocess/testrun_hadoop-streaming.txt |\n| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5753/testReport/ |\n| Java | 1.7.0_55 |\n| uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5753/console |\n\n\nThis message was automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2015-05-25T11:58:14.855+0000","updated":"2015-05-25T11:58:14.855+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14563932","id":"14563932","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"body":"Can someone please review the latest patch and let me know if it is OK?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilfreds","name":"wilfreds","key":"wilfreds","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wilfred Spiegelenburg","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-28T23:54:42.098+0000","updated":"2015-05-28T23:54:42.098+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14571683","id":"14571683","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rkanter","name":"rkanter","key":"rkanter","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Kanter","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 LGTM.\n\nWill commit this later today if nobody has any other comments.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rkanter","name":"rkanter","key":"rkanter","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Kanter","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-03T21:28:07.058+0000","updated":"2015-06-03T21:28:07.058+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14571963","id":"14571963","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rkanter","name":"rkanter","key":"rkanter","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Kanter","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Wilfred.  Committed to trunk and branch-2!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rkanter","name":"rkanter","key":"rkanter","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Kanter","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-04T01:46:20.965+0000","updated":"2015-06-04T01:46:20.965+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14571969","id":"14571969","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-trunk-Commit #7959 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7959/])\nMAPREDUCE-5965. Hadoop streaming throws error if list of input files is high. Error is: \"error=7, Argument list too long at if number of input file is high\" (wilfreds via rkanter) (rkanter: rev cc70df98e74142331043a611a3bd8a53ff6a2242)\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java\n* hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm\n* hadoop-mapreduce-project/CHANGES.txt\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-04T01:53:30.948+0000","updated":"2015-06-04T01:53:30.948+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14572593","id":"14572593","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #218 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/218/])\nMAPREDUCE-5965. Hadoop streaming throws error if list of input files is high. Error is: \"error=7, Argument list too long at if number of input file is high\" (wilfreds via rkanter) (rkanter: rev cc70df98e74142331043a611a3bd8a53ff6a2242)\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java\n* hadoop-mapreduce-project/CHANGES.txt\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java\n* hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-04T11:37:24.881+0000","updated":"2015-06-04T11:37:24.881+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14572614","id":"14572614","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Yarn-trunk #948 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/948/])\nMAPREDUCE-5965. Hadoop streaming throws error if list of input files is high. Error is: \"error=7, Argument list too long at if number of input file is high\" (wilfreds via rkanter) (rkanter: rev cc70df98e74142331043a611a3bd8a53ff6a2242)\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java\n* hadoop-mapreduce-project/CHANGES.txt\n* hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-04T11:47:48.969+0000","updated":"2015-06-04T11:47:48.969+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14572863","id":"14572863","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Hadoop-Hdfs-trunk #2146 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2146/])\nMAPREDUCE-5965. Hadoop streaming throws error if list of input files is high. Error is: \"error=7, Argument list too long at if number of input file is high\" (wilfreds via rkanter) (rkanter: rev cc70df98e74142331043a611a3bd8a53ff6a2242)\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java\n* hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm\n* hadoop-mapreduce-project/CHANGES.txt\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-04T14:30:30.322+0000","updated":"2015-06-04T14:30:30.322+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14572891","id":"14572891","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Hadoop-Hdfs-trunk-Java8 #207 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/207/])\nMAPREDUCE-5965. Hadoop streaming throws error if list of input files is high. Error is: \"error=7, Argument list too long at if number of input file is high\" (wilfreds via rkanter) (rkanter: rev cc70df98e74142331043a611a3bd8a53ff6a2242)\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java\n* hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm\n* hadoop-mapreduce-project/CHANGES.txt\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-04T14:44:16.063+0000","updated":"2015-06-04T14:44:16.063+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14573008","id":"14573008","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #216 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/216/])\nMAPREDUCE-5965. Hadoop streaming throws error if list of input files is high. Error is: \"error=7, Argument list too long at if number of input file is high\" (wilfreds via rkanter) (rkanter: rev cc70df98e74142331043a611a3bd8a53ff6a2242)\n* hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java\n* hadoop-mapreduce-project/CHANGES.txt\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-04T15:49:27.937+0000","updated":"2015-06-04T15:49:27.937+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12726479/comment/14573036","id":"14573036","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Mapreduce-trunk #2164 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2164/])\nMAPREDUCE-5965. Hadoop streaming throws error if list of input files is high. Error is: \"error=7, Argument list too long at if number of input file is high\" (wilfreds via rkanter) (rkanter: rev cc70df98e74142331043a611a3bd8a53ff6a2242)\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java\n* hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm\n* hadoop-mapreduce-project/CHANGES.txt\n* hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-04T15:57:53.813+0000","updated":"2015-06-04T15:57:53.813+0000"}],"maxResults":28,"total":28,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-5965/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1xmp3:"}}