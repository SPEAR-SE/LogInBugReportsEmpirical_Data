{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12682650","self":"https://issues.apache.org/jira/rest/api/2/issue/12682650","key":"MAPREDUCE-5668","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310941","id":"12310941","key":"MAPREDUCE","name":"Hadoop Map/Reduce","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310941&avatarId=10096","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310941&avatarId=10096","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310941&avatarId=10096","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310941&avatarId=10096"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2013-12-04T14:17:09.508+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Dec 04 19:29:52 UTC 2013","customfield_12310420":"361907","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_17264200_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2013-12-04T14:38:46.812+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-5668/watchers","watchCount":3,"isWatching":false},"created":"2013-12-04T09:51:02.646+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2013-12-04T19:29:52.130+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"hi\n pl help\n\ni have wrote this code , at runtime i got this issue.\nException in thread \"main\" java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.JobContext, but interface was expected\n\tat org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:170)\n\tat org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:885)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:779)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:432)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447)\n\tat MultiFileWordCount.run(MultiFileWordCount.java:395)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat MultiFileWordCount.main(MultiFileWordCount.java:401)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nhduser@localhost:~$ \n\n\nI have attached the code.\n\nimport java.io.DataInput;  \n\nimport java.io.DataOutput;  \n\nimport java.io.IOException;  \n\nimport java.util.StringTokenizer;  \n\nimport org.apache.hadoop.conf.Configured;  \n\nimport org.apache.hadoop.fs.FSDataInputStream;  \n\nimport org.apache.hadoop.fs.FileSystem;  \n\nimport org.apache.hadoop.fs.Path;  \n\nimport org.apache.hadoop.io.IntWritable;  \n\nimport org.apache.hadoop.io.Text;  \n\nimport org.apache.hadoop.io.WritableComparable;  \n\nimport org.apache.hadoop.mapreduce.InputSplit;  \n\nimport org.apache.hadoop.mapreduce.Job;  \n\nimport org.apache.hadoop.mapreduce.Mapper;  \n\nimport org.apache.hadoop.mapreduce.RecordReader;  \n\nimport org.apache.hadoop.mapreduce.TaskAttemptContext;  \n\nimport org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat;  \n\nimport org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader;  \n\nimport org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;  \n\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  \n\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  \n\nimport org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;  \n\nimport org.apache.hadoop.util.LineReader;  \n\nimport org.apache.hadoop.util.Tool;  \n\nimport org.apache.hadoop.util.ToolRunner;  \n\n\n /**  \n\n  * MultiFileWordCount is an example to demonstrate the usage of   \n\n  * MultiFileInputFormat. This examples counts the occurrences of  \n\n  * words in the text files under the given input directory.  \n\n  */ \n\npublic class MultiFileWordCount extends Configured implements Tool {  \n\n   /**  \n\n    * This record keeps <filename,offset> pairs.  \n\n    */ \n\npublic static class WordOffset implements WritableComparable {  \n\n   private long offset;  \n\n   private String fileName;  \n  \n   public void readFields(DataInput in) throws IOException {  \n\n      this.offset = in.readLong();  \n\n      this.fileName = Text.readString(in);  \n\n     }  \n\n     public void write(DataOutput out) throws IOException {  \n\n       out.writeLong(offset);  \n\n       Text.writeString(out, fileName);  \n\n     }  \n\n      public int compareTo(Object o) {  \n\n       WordOffset that = (WordOffset)o;  \n\n       int f = this.fileName.compareTo(that.fileName);  \n\n       if(f == 0) {  \n\n         return (int)Math.signum((double)(this.offset - that.offset));  \n\n       }  \n\n       return f;  \n\n     }  \n\n     @Override \n\n     public boolean equals(Object obj) {  \n\n       if(obj instanceof WordOffset)  \n\n       return this.compareTo(obj) == 0;  \n\n       return false;  \n\n     }  \n\n     @Override \n\n     public int hashCode() {  \n\n     assert false : \"hashCode not designed\";  \n\n     return 42; //an arbitrary constant  \n\n     }  \n\n   }  \n\n   /**  \n\n    * To use {@link CombineFileInputFormat}, one should extend it, to return a   \n\n    * (custom) {@link RecordReader}. CombineFileInputFormat uses   \n\n    * {@link CombineFileSplit}s.   \n\n    */ \n\n   public static class MyInputFormat   \n\n     extends CombineFileInputFormat  {  \n\n     public RecordReader createRecordReader(InputSplit split,  \n\n      TaskAttemptContext context) throws IOException {  \n\n       return new CombineFileRecordReader(  \n\n         (CombineFileSplit)split, context, CombineFileLineRecordReader.class);  \n\n     }  \n\n   }  \n\n  \n   /**  \n\n    * RecordReader is responsible from extracting records from a chunk  \n\n    * of the CombineFileSplit.   \n\n    */ \n\n   public static class CombineFileLineRecordReader   \n\n     extends RecordReader {  \n\n     private long startOffset; //offset of the chunk;  \n\n     private long end; //end of the chunk;  \n\n     private long pos; // current pos   \n\n     private FileSystem fs;  \n\n     private Path path;  \n\n     private WordOffset key;  \n\n     private Text value;  \n\n     private FSDataInputStream fileIn;  \n\n     private LineReader reader;  \n\n     public CombineFileLineRecordReader(CombineFileSplit split,  \n\n         TaskAttemptContext context, Integer index) throws IOException {  \n      \n       this.path = split.getPath(index);  \n\n       fs = this.path.getFileSystem(context.getConfiguration());  \n\n       this.startOffset = split.getOffset(index);  \n\n       this.end = startOffset + split.getLength(index);  \n\n       boolean skipFirstLine = false;  \n\n       //open the file  \n\n       fileIn = fs.open(path);  \n\n       if (startOffset != 0) {  \n\n       skipFirstLine = true;  \n\n         --startOffset;  \n\n         fileIn.seek(startOffset);  \n\n       }  \n\n       reader = new LineReader(fileIn);  \n\n       if (skipFirstLine) {  // skip first line and re-establish \"startOffset\".  \n\n        startOffset += reader.readLine(new Text(), 0,  \n\n         (int)Math.min((long)Integer.MAX_VALUE, end - startOffset));  \n\n       }  \n\n       this.pos = startOffset;  \n\n     }  \n\n     public void initialize(InputSplit split, TaskAttemptContext context)  \n\n        throws IOException, InterruptedException {  \n\n     }\n     public void close() throws IOException { }  \n\n     public float getProgress() throws IOException {  \n\n       if (startOffset == end) {  \n\n        return 0.0f;  \n\n       } else {  \n\n       return Math.min(1.0f, (pos - startOffset) / (float)(end - startOffset));  \n\n       }  \n\n     }  \n\n    public boolean nextKeyValue() throws IOException {  \n\n       if (key == null) {  \n\n         key = new WordOffset();  \n\n         key.fileName = path.getName();  \n\n       }  \n\n       key.offset = pos;  \n\n       if (value == null) {  \n\n       value = new Text();  \n\n       }  \n\n       int newSize = 0;  \n\n       if (pos < end) {  \n\n        newSize = reader.readLine(value);  \n\n        pos += newSize;  \n\n       }  \n\n       if (newSize == 0) {  \n\n        key = null;  \n\n        value = null;  \n\n        return false;  \n\n       } else {  \n\n        return true;  \n\n       }  \n\n     }  \n\n         public WordOffset getCurrentKey()   \n\n         throws IOException, InterruptedException {  \n\n          return key;  \n\n     }  \n\n         public Text getCurrentValue() throws IOException, InterruptedException {  \n\n         return value;  \n\n     }  \n\n   }  \n\n  \n   /**  \n\n    * This Mapper is similar to the one in {@link WordCount.MapClass}.  \n\n    */ \n\n   public static class MapClass extends  \n\n       Mapper {  \n\n     private final static IntWritable one = new IntWritable(1);  \n\n     private Text word = new Text();  \n\n     public void map(WordOffset key, Text value, Context context)  \n\n         throws IOException, InterruptedException {  \n\n       String line = value.toString();  \n\n       StringTokenizer itr = new StringTokenizer(line);  \n\n       while (itr.hasMoreTokens()) {  \n\n         word.set(itr.nextToken());  \n\n         context.write(word, one);  \n\n       }  \n\n     }  \n\n   }  \n\n   private void printUsage() {  \n\n     System.out.println(\"Usage : multifilewc  \" );  \n\n   }  \n   public int run(String[] args) throws Exception {  \n\n     if(args.length < 2) {  \n\n       printUsage();  \n\n       return 2;  \n\n     }  \n\n     Job job = new Job(getConf());  \n\n     job.setJobName(\"MultiFileWordCount\");  \n\n     job.setJarByClass(MultiFileWordCount.class);  \n    \n     //set the InputFormat of the job to our InputFormat  \n\n     job.setInputFormatClass(MyInputFormat.class);  \n       \n     // the keys are words (strings)  \n\n     job.setOutputKeyClass(Text.class);  \n\n     // the values are counts (ints)  \n\n     job.setOutputValueClass(IntWritable.class);  \n \n     //use the defined mapper  \n\n     job.setMapperClass(MapClass.class);  \n\n     //use the WordCount Reducer  \n\n     job.setCombinerClass(IntSumReducer.class);  \n\n     job.setReducerClass(IntSumReducer.class);  \n   \n\n     FileInputFormat.addInputPaths(job, args[0]);  \n\n     FileOutputFormat.setOutputPath(job, new Path(args[1]));  \n\n     return job.waitForCompletion(true) ? 0 : 1;  \n\n   }  \n   \n   public static void main(String[] args) throws Exception {  \n\n     int ret = ToolRunner.run(new MultiFileWordCount(), args);  \n\n     System.exit(ret);  \n\n   }  \n\n \n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"362202","customfield_12312823":null,"summary":"Exception in thread \"main\" java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.mapreduce.JobContext, but interface was expected","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranjini+rathinam","name":"ranjini rathinam","key":"ranjini rathinam","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ranjini","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranjini+rathinam","name":"ranjini rathinam","key":"ranjini rathinam","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ranjini","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682650/comment/13838917","id":"13838917","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"This is something more appropriately asked on the [user@ mailing list|http://hadoop.apache.org/mailing_lists.html#User].  JIRAs are for reporting bugs against Hadoop, and this does not appear to be a bug.  It looks like the code has been compiled against a 2.x release but then run against a 1.x release, as org.apache.hadoop.mapreduce.JobContext changed from a class to an interface between the 1.x and 2.x releases.\n\nThe org.apache.hadoop.mapreduce.* API is only guaranteed to be source, not binary, compatible between the 1.x and 2.x releases.   See the [binary compatibility document|http://hadoop.apache.org/docs/r2.2.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html] for more details.  Also we cannot generally support compiling against a later release and then running on an earlier release, because new APIs could have been added and would not appear in the older release.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2013-12-04T14:17:09.508+0000","updated":"2013-12-04T14:17:09.508+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682650/comment/13838934","id":"13838934","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Closing this as invalid based on the evidence from MAPREDUCE-5667.  The code is being compiled against a later release but then run on an earlier release.  The code should be compiled against the Hadoop release being used or an earlier release, keeping in mind the binary compatibility document guidelines.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2013-12-04T14:38:46.837+0000","updated":"2013-12-04T14:38:46.837+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12682650/comment/13839249","id":"13839249","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"See also: http://wiki.apache.org/hadoop/InvalidJiraIssues\n\nWe aren't going to fix these problems as they aren't bugs in Hadoop, they are because you are running an out of date version and everytime something doesn't work coming to JIRA and asking for help. That help is not going to happen, all that will is that your issues will get ignored in future, which is dangerous if you ever come across a real bug *in an up to date version of Hadoop*\n\n# upgrade to Hadoop 1.2 or 2.2\n# ask the user group if you encounter problems on these versions\n# if that isn't enough, look at who offers support for Hadoop and consider whether it is something you are prepared to pay for. If not, the source code is there for you to debug your problems","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2013-12-04T19:29:52.130+0000","updated":"2013-12-04T19:29:52.130+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/MAPREDUCE-5668/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1qdun:"}}