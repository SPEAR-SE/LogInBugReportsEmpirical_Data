[If compression is on, the merge output would be compressed. The situation is better in 0.18 with compression working well. 

At this point of time, the only other option is to have a smaller io.sort.factor but this, in general, might increase the disk IO for the map output records, affecting performance in a negative way., Although compression will help (I will try to turn on compression with 0.18), would it make sense in general to make the merge algorithm adapt to available disk space and merge in smaller steps if necessary instead of just failing?, There was another instance of a reducer on a node with more than 280GB free space distributed over 4 disks and map input of 210 GB that could not find a disk with enough free space when it tried to create a merge file larger than 30 GB, because it included intermediate merged files into the new merge attempt although there were enough small non-merged files that could have been merged to bring the total number of input files below the fan factor., To get beyond this bottleneck, for such reduces requiring a lot disk space for merging, we deleted all map outputs on such nodes, getting back a lot of space. On one of these nodes with about 280GB reduce input we observed that one of the merged files was 75GB (a single file!), This seems to be fixed in hadoop-0.18. Could not reproduce.]