[The patch contains:

+    /** Hadoop Archive connections cannot be cached by (scheme, authority,
+     * username) -- the path is significant as well. Come to think of it, they
+     * probably don't need to be cached at all, since they wrap another
+     * connection that does the actual networking.
+     */
+    if (scheme.equals("har")) {
+      return createFileSystem(uri, conf);
+    }
+
    return CACHE.get(uri, conf);
  }

I don't think it's a good way. 

I'd like to suggest to introduce property fs.[fs-name].impl.disable.cache. And don't use cache is this property exists and set to true., Ben, for the caching part, 
HarFileSystem.java has a comment 
{quote}
   * the uri of Har is 
   * har://underlyingfsscheme-host:port/archivepath.
   * or 
   * har:///archivepath.
{quote}

So it already creates a cache for each harpath?
 , Koji, I'm not entirely sure what you're saying. When using the har filesystem, you create a connection for each harpath. This is not a network connection itself, but it wraps a network connection to the underlying filesystem, which is cached. I think this caching is sufficient, and it is not necessary to cache the wrapping har filesystem "connection"., Vladimir, I will implement that and submit a new patch., ben,
  koji is right. The caching is just a filesystem caching. The filesystem cache has a cache for each scheme cached. So for har filesystem its caching the scheme and harpath to create a cache for a filesystem meaning that a har filesystem is uniquely identified by a har:///archivepath. the connection caching has nothing to do with this filesystem cache.  The connection caching is done via the RPC layer and you cwould not be able to cache connections at the har filesystem layer. 

, Mahadev,

Yes, we are talking about the filesystem cache, not the connection cache. Sorry for the confusion. As you say, har filesystems are identified by har://authority/archivepath, but the filesystem cache does not index filesystems by path -- only by scheme, authority, and username. This leads directly to the misbehavior I described above. I worked around it by not caching har fielsystems. Another option is to cache by scheme, authority, username, AND path. I could submit a patch to do that if anyone has a strong preference for it., Ben, sorry about that. I was wrong.

I got confused since HarFileSystem.getUri() returns har://archivepath, 
I mistakenly thought FileSystem.CACHE would use the path as part of the hash key.

{noformat}
$ hadoop dfs -ls har:///user/knoguchi/test.har har:///user/knoguchi/test2.har                                
Found 1 items
drw-r--r--   - knoguchi users          0 2009-08-18 18:52 /user/knoguchi/test.har/user
ls: Invalid file name: /user/knoguchi/test2.har in har:///user/knoguchi/test.har


$ hadoop dfs -ls har:///user/knoguchi/test2.har har:///user/knoguchi/test.har
Found 1 items
drw-------   - knoguchi users          0 2009-08-17 19:15 /user/knoguchi/test2.har/user
ls: Invalid file name: /user/knoguchi/test.har in har:///user/knoguchi/test2.har
$ 
{noformat}, I agree with Vladimir; checking the scheme for "har" and dodging the cache is not a sufficiently general solution for FileSystem. As in the patch comment, the har filesystem isn't a great use case to justify a four-level cache, either.

bq. introduce property fs.[fs-name].impl.disable.cache. And don't use cache is this property exists and set to true

+1 However, I'd suggest that it be part of a separate issue., bq. I'd suggest that it be part of a separate issue.

I've opened HADOOP-6097 to add support for disabling the cache on a per-filesystem basis.

I've also regenerated the patch for this issue. It could really do with a unit test., I meant HADOOP-6231 for the cache issue., tom, ben,
 this jira has been marked for a fix for 0.20 branch. HADOOP-6231 has been committed only to trunk (i.e 0.21) .. should this jira be moved to 0.21? 


, The latest patch that I generated was actually for trunk, so it would need a 0.20 version if folks want a fix in that version., tom, the issue is that we would also need a backport for HADOOP-6231 which I dont think qualifies to get into a 0.20 branch, since its not a bug fix. We can attach a patch on HADOOP-6231 for 0.20 branch (that wouldnt be committed to the apache branch but others can download it if they want to include it). I can do that for now...  , Mahadev, HADOOP-6231 *is* a bug fix -- see Koji's last comment above. I attached a patch for the 0.20 branch there., ben, I did read through the comments... I think you are right... HADOOP-6231 can be treated as a bug fix for 0.20.*. I will try committing it to 0.20 branch after running the tests and others .... 

ill generate a 0.20 patch for this jira as well... it probably needs a test case.... , ant test passes with this patch except for TestHdfsProxy in contrib which fails due to the following unrelated reason:

{code}
Testcase: testHdfsProxyInterface took 6.078 sec
        Caused an ERROR
org/apache/commons/cli/ParseException
java.lang.NoClassDefFoundError: org/apache/commons/cli/ParseException
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:59)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface(TestHdfsProxy.java:222)
{code}, Filed  MAPREDUCE-1010 for adding the testcase., +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12418348/HADOOP-6097-v2.patch
  against trunk revision 816794.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/14/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/14/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/14/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/14/console

This message is automatically generated., patch for the 0.20 branch.., this patch includes the test in MAPREDUCE-1010., Do we want a test case of handling two har files at once?
Other than that, +1., since the core problem was fixed in HADOOP-6231 which already adds test to it, I dont think we need specific tests for har... , bq. since the core problem was fixed in HADOOP-6231 which already adds test to it, I dont think we need specific tests for har... 

I wanted a testcase so  that it would catch if someone take out 
{noformat}
<name>fs.har.impl.disable.cache</name>
<value>true</value>
{noformat} 
from core-default.xml 

, not a bad test to add at all... will add it .... , patch with changes corresponding to MAPREDUCE-1010 for hadoop-0.20 branch., +1

I committed this. Thanks, Ben, Tom, and Mahadev!

As part of the commit, I created a new 0.20.2 section and moved issues committed to 0.20.2 from trunk and into that section., Integrated in Hadoop-Common-trunk-Commit #64 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/64/])
    . Fix Path conversion in makeQualified and reset LineReader byte
count at the start of each block in Hadoop archives. Contributed by Ben Slusky,
Tom White, and Mahadev Konar
, Integrated in Hadoop-Mapreduce-trunk-Commit #88 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk-Commit/88/])
    . Fix Path conversion in makeQualified and reset LineReader byte
count at the start of each block in Hadoop archives. Contributed by Ben Slusky,
Tom White, and Mahadev Konar
, Integrated in Hadoop-Common-trunk #134 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/134/])
    . Fix Path conversion in makeQualified and reset LineReader byte
count at the start of each block in Hadoop archives. Contributed by Ben Slusky,
Tom White, and Mahadev Konar
, Integrated in Hadoop-Mapreduce-trunk #120 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Mapreduce-trunk/120/])
    ]