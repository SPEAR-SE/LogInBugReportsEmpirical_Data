[See [comment #1|https://issues.apache.org/jira/browse/HADOOP-10670?focusedCommentId=14381367&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14381367] and [comment #2|https://issues.apache.org/jira/browse/HADOOP-10670?focusedCommentId=14381379&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14381379] for more detail., An initial patch with more check, not only checking the signature file property, but also checking the file should be there available for reading., Hi [~sjlee0],
Sorry for causing this. As I discussed in HADOOP-10670, a possible safe fix would be the one I attached. How do you like it ? One thing not good in this way is, if the signature property file is configured, but as a mistake not prepared well, then such effor may be ignored., I'm not sure if that is the right thing to do. If security is enabled, the absence of the file should be a failure. It should not be silently accepted, or it would be a hole in security.

Since work in HADOOP-10670 was basically refactoring from the RM perspective, it would be good to preserve the existing RM behavior., A much better one, with warning log, if the file is configured but not exist., I'm afraid adding a warning logging statement is not going to be enough. Note that this changes behavior for *all use cases* to a more forgiving one if security is enabled. I'm not sure if that is OK., bq.If security is enabled, the absence of the file should be a failure.
And if security is not enabled, the absence of the file should be OK (better give a warning log here). The logic looks fine. I'm thinking about how to get a fix in such way, a difficulty is it's not easy to tell if security is enabled or not in the {{AuthenticationFilter}} class (in hadoop-auth module) as RM does., bq.it would be good to preserve the existing RM behavior.
It's possible to remain the existing RM behavior if we just remove the signature file property in RM when not in secure mode. In this way we don't have to change the current logic in {{AuthenticationFilter}}, meanwhile not having to bring the dirty codes in RM back. How do you like this way ? If sounds good I can try the change., bq. If security is enabled, the absence of the file should be a failure. It should not be silently accepted, or it would be a hole in security.

Agree. I have similar comments in HADOOP-10670.

bq. It's possible to remain the existing RM behavior if we just remove the signature file property in RM when not in secure mode. 

I don't believe that this is the right thing to do either. Initializing / binding filters in insecure mode breaks the contracts we have in HttpServer2. This should not be worked around., bq.Initializing / binding filters in insecure mode breaks the contracts we have in HttpServer2. 
Looking at RM codes, obviously it doesn't follow the contract. Removing the property if the configured file isn't there good in not secure mode keeps the current behavior in safe way for the release. It fixes the bad situation where the property is unintended there (originated from default configuration). Maybe we should avoid that at all, no default value for the property ? Indeed it should be explicitly prepared and configured., cc [~vinodkv]

bq. Maybe we should avoid that at all, no default value for the property ? Indeed it should be explicitly prepared and configured.

Wouldn't that break users that were simply relying on the default value for this property in the secure mode? I'm not sure if that qualifies as not backward compatible, but does sound like there will be user impact if that change is made.

How about reverting this change for now (at least for branch-2)? This is a blocker for the 2.7 release. Is there a strong reason that HADOOP-10670 must be part of the 2.7 release? If not, it may not be a bad idea to revert this for now and revise the patch for a later release. Thoughts?, bq.  If security is enabled, the absence of the file should be a failure. 

That's a definite change in behavior.  If a secret wasn't configured, the 2.6 and previous filters generated a random one since it was assumed that the serving system was a single host.

bq. Wouldn't that break users that were simply relying on the default value for this property in the secure mode? I'm not sure if that qualifies as not backward compatible, but does sound like there will be user impact if that change is made.

... which means that, yes, changing this behavior is most definitely not backward compatible., bq. How about reverting this change for now (at least for branch-2)? This is a blocker for the 2.7 release. Is there a strong reason that HADOOP-10670 must be part of the 2.7 release? If not, it may not be a bad idea to revert this for now and revise the patch for a later release. Thoughts?

I pull this into 2.7 because it is a build block for HDFS-5796, which is a blocker for 2.7 as well. :-( I'll take care of this today., bq. That's a definite change in behavior. If a secret wasn't configured, the 2.6 and previous filters generated a random one since it was assumed that the serving system was a single host.

Agree. This change is incompatible. It will break the current timeline server secure deployment., [~drankye] / [~wheat9],

I may still not have the full-picture but how about this? If all we want in HADOOP-10670 is for the webHDFS auth filter to be able to use file-based signer, why don't' we implement that functionality there similar to  RMAuthenticationFilterInitializer instead of changing AuthenticationFilter. That should get what you want but avoid this breakage, even if it isn't ideal?, BTW, that SIGNATURE_SECRET_FILE is not set and that SIGNATURE_SECRET_FILE is pointing to a non-existing file mean different. While the former case indicates using the default random secret, the latter one is regarded as the wrong configuration., The current state of things is SIGNATURE_SECRET_FILE is set by default in core-default.xml, so it's always set unless the user explicitly unsets it., bq.  If all we want in HADOOP-10670 is for the webHDFS auth filter to be able to use file-based signer, why don't' we implement that functionality there similar to RMAuthenticationFilterInitializer instead of changing AuthenticationFilter. That should get what you want but avoid this breakage, even if it isn't ideal?

... which is pretty much what [~rsasson]'s patch in HDFS-5796 does. :)

, For the long term, ideally, as desired and did in HADOOP-10670, the signature secret file setting stuff should be taken care of in the {{AuthenticationFilter}} so that it's possible for all the Hadoop web UIs (HDFS, YARN) can easily share the same and common configuration and logics, thus some advanced SSO effect can be achieved. The dirty things can all be handled in the common place, not needed in all the places. It's ideal.

For now and the release, to keep the original behavior, as I said before, we can have some logic in RM like this: if it's not in secure mode, and signature file property is set, then we may remove the property. If we want to be more careful, we can even check the specified file is originally the default file or not. As such change is only in RM and Timeline server, it won't affect other places.
, Oh bad, may I correct.
bq.we can have some logic in RM like this:...
I mean, the fix logic could be: if 1) it's not in secure mode, 2) **the signature file property is set but absent**, and optionally 3) it's the default property value (not set explicitly by user), then we may remove the property., Okay, then how about this?
 - In secure mode, we always had the signature file configured by default in the default config files. And if that file didn't exist, we failed the daemons (in HDFS as well as YARN). We should keep this behavior here.
 - In non-secure mode, before HADOOP-10670, RM didn't fail the daemon if the default signature file didn't exist but it starts failing after HADOOP-10670. We should fix this to not have RM fail. There are two ways to do this
    -- Not use the filter at all for the ResourceManager in non-secure mode - other daemons already do this. And so no cookies sent to RM clients. Which should be okay in non-secure mode.
    -- Use the filter in RM in non-secure mode also but fall back to the RandomSigner signed cookie as it is today. This can be done by keeping the signer choice code in each of the individual filter-initializers., It sounds complete ! Just note for the 2nd way, to allow RM to fall back to RandomSigner, we can unset or remove the file property. Current AuthenticationFilter will perform the fallback when not seeing the file property. We don't have to bring back the original specific codes in RM., bq. There are two ways to do this

Prefer the second way. We still want to load  auth filter with pseudo auth handler to accept "user.name=blah blah". Moreover, before HADOOP-10670, the semantics is falling back to random secret if no customized secret is given, no matter it's from config directly, or read from a configured secret file. After that jira, the semantics changed to also failing when error happens in reading the secret file. So previously if secret file is empty, it will work. Now even though no read failure happens, I'm afraid the empty secret file will still bring down the auth filter with null secret object.

, You're right. To be safer, we may also need to check if the file is empty or not, when deciding to unset the property or otherwise. Kinds of dirty.

Maybe we can have the 2nd way as a work around for the release to keep the original behavior, and the 1st way for the next release to clean up finally ?, Uploaded a patch to implement the second approach. In insecure mode, the {{AuthenticationFilerInitializer}} will fall back to {{RandomSignerSecretProvider}} when the secret file is unavailable. Note that the patch is based on HADOOP-11748., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707657/HADOOP-11754.001.patch
  against trunk revision 47782cb.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-auth:

                  org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/6007//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/6007//artifact/patchprocess/newPatchFindbugsWarningshadoop-auth.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/6007//console

This message is automatically generated., The logic looks good. It's smart to tell if security/Kerberos is enabled or not. I'm not sure why we change the tests, which caused the failures. Do we need an update or just trigger since the dep of HADOOP-11748 was already in ?, I checked the failures, they're caused by the patch. The cause is in {{TestKerberosAuthenticator}}, it's in secure mode, but no signature file property is set; in the patch, when in secure mode, it will use {{file}} type, without checking if the signature file property is set or not; so {{FileSignerSecretProvider}} will be used anyway, but in it, if no signature file property is set then no file reading will be tried thus no exception will happen. Therefore its {{getCurrentSecret()}} will return null even though its {{init()}} is successful., I'm not sure why we want to prevent using the random secret in the secure mode. As is mentioned above, it's an incompatible semantics change, which will break RM web interface and timeline server secure deployment. I don't think we have conveyed this secure setup requirement of secret file to the users (e.g., Ambari). [~vinodkv], any idea?
{code}
277	    // Fallback to RandomeSignerSecretProvider if the secret file is
278	    // unspecified in insecure mode
279	    if (!isSecurityEnabled && config.getProperty(SIGNATURE_SECRET_FILE) ==
280	        null) {
281	      name = "random";
282	    }
{code}

{code}
289	        if (!isSecurityEnabled) {
290	          LOG.info("The signature secret of the authentication filter is " +
291	                       "unspecified, falling back to use random secrets.");
292	          provider = new RandomSignerSecretProvider();
293	          provider.init(config, servletContext, validity);
294	        } else {
295	          throw e;
296	        }
{code}, To be specific, timeline server never has a default secret file before. This patch will forces it to have one., bq. I'm not sure why we want to prevent using the random secret in the secure mode. 

This is for fallback only. The behavior is consistent with the previous behavior. The authentication filter bails out when the secret is not found. This is true for both RM and other users of the authentication filters.

bq. As is mentioned above, it's an incompatible semantics change, which will break RM web interface and timeline server secure deployment. 

Can you be more specific? What are the behaviors before and after the changes?

bq. To be specific, timeline server never has a default secret file before. This patch will forces it to have one.

I'm confused. What does timeline server has to do with {{RMFilterInitializer}}? I think it is a separate issue and we can look at it in a separate jira., Before 2.7:

* {{AuthenticationFilterInitializer}}, {{RMAuthenticationFilterInitializer}} and {{TimelineAuthenticationFilterInitializer}} read the secret file, but behave a bit different. {{FileSignerSecretProvider}} seems to choose the behavior of {{RMAuthenticationFilterInitializer}}. However, unlike {{RMAuthenticationFilterInitializer}}, {{AuthenticationFilterInitializer}} doesn't allow null secret file path, while {{TimelineAuthenticationFilterInitializer}} DOESN'T have default secret file path.

* {{AuthenticationFilter}} check it customized secret exists (no matter it comes from secret file or directly put in the configuration) or not to decide failback to random secret no matter {{AuthenticationFilter}} is used in secure mode (Kerberos handler) or in insecure mode (Pseudo handler).

After these changes in 2.7.

* {{RMAuthenticationFilterInitializer}}'s behavior is chosen as the standard.

* {{AuthenticationFilter}} no longer accepts secret that is put inside the configuration file. It may not be the best practice, but it's a valid scenario before. {{AuthenticationFilter}} also forces the user to have the secret file in secure mode, and it's not able to failback to random secret.

Talking about timeline server specifically, in the case of starting timeline server in secure mode with the default secret config, the following logic will happen:

1. It tries to read the secret file, but it doesn't exists.
2. It checks and finds it's a secure mode, and throws the exception, and consequently timeline server fails to start.

bq.  think it is a separate issue and we can look at it in a separate jira.

I'm afraid it's not a separate issue. This change is going to break the timeline server secure deployment., bq. AuthenticationFilter check it customized secret exists (no matter it comes from secret file or directly put in the configuration) or not to decide failback to random secret no matter AuthenticationFilter is used in secure mode (Kerberos handler) or in insecure mode (Pseudo handler).

bq. AuthenticationFilter no longer accepts secret that is put inside the configuration file. It may not be the best practice, but it's a valid scenario before. AuthenticationFilter also forces the user to have the secret file in secure mode, and it's not able to failback to random secret.

We never support this use case. It is a misunderstanding of the code. See https://issues.apache.org/jira/browse/HADOOP-10670?focusedCommentId=14380372&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14380372

For Timeline / RM server, it looks like we have a lot of customized use cases here. Looks like the right fix is to move the some of the code in HttpServer2 and to allow customization., The v2 patch moves the instance of {{SignerSecretProvider}} to {{HttpServer2}}, which allows HDFS / RM / AM / Timeline server to customize their needs. The default is to allow falling back to random secret if the provider fails to read the file. All HDFS daemons will not fall back to random secret provider in secure mode, which is consistent with the existing behavior., bq.  All HDFS daemons will not fall back to random secret provider in secure mode, which is consistent with the existing behavior.

I don't think that's consistent with pre-2.7 behavior though., bq. I don't think that's consistent with pre-2.7 behavior though.

Can you elaborate what is the expected behavior?

From what I'm aware of this is the behavior since HADOOP-8857. HADOOP-10868 accidentally changes the behavior (which introduces a security vulnerability) and it was fixed in HADOOP-11748. You can check https://issues.apache.org/jira/browse/HADOOP-10670?focusedCommentId=14380372&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14380372 for more details., Haohui, thank for the latest patch. It looks good to me. I applied the patch and try RM in an insecure mode, it wont' crash again. I tried timeline server in a secure mode, it had fallback to use random secret. [~vinodkv], do you want to take a second look., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707912/HADOOP-11754.002.patch
  against trunk revision 3836ad6.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.qjournal.TestSecureNNWithQJM
                  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/6017//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/6017//artifact/patchprocess/newPatchFindbugsWarningshadoop-auth.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/6017//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12708006/HADOOP-11754.003.patch
  against trunk revision 3d9132d.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/6020//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/6020//artifact/patchprocess/newPatchFindbugsWarningshadoop-auth.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/6020//console

This message is automatically generated., The findbugs warnings are fixed in HADOOP-11761., The latest patch looks good to me. +1

You can fix the format of the following code while committing the patch.
{code}
       try {
-        secretProvider.init(config, filterConfig.getServletContext(), validity);
+      secretProvider = constructSecretProvider(filterConfig.getServletContext()
+            , config, false);
{code}, I've committed the patch to trunk, branch-2 and branch-2.7. Thanks Sangjin for reporting the issues, and many others for the reviews., Thanks [~wheat9] for the fix, and others for the reviews!, FAILURE: Integrated in Hadoop-trunk-Commit #7465 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7465/])
HADOOP-11754. RM fails to start in non-secure mode due to authentication filter failure. Contributed by Haohui Mai. (wheat9: rev 90e07d55ace7221081a58a90e54b360ad68fa1ef)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
* hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
, Quick comments
 - Why is disallowFallbackToRandomSecretProvider applicable only for file-based signers?
 - And why is the default the file-based signer?, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #149 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/149/])
HADOOP-11754. RM fails to start in non-secure mode due to authentication filter failure. Contributed by Haohui Mai. (wheat9: rev 90e07d55ace7221081a58a90e54b360ad68fa1ef)
* hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #883 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/883/])
HADOOP-11754. RM fails to start in non-secure mode due to authentication filter failure. Contributed by Haohui Mai. (wheat9: rev 90e07d55ace7221081a58a90e54b360ad68fa1ef)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java
* hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
* hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2081 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2081/])
HADOOP-11754. RM fails to start in non-secure mode due to authentication filter failure. Contributed by Haohui Mai. (wheat9: rev 90e07d55ace7221081a58a90e54b360ad68fa1ef)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
* hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #140 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/140/])
HADOOP-11754. RM fails to start in non-secure mode due to authentication filter failure. Contributed by Haohui Mai. (wheat9: rev 90e07d55ace7221081a58a90e54b360ad68fa1ef)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java
* hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #149 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/149/])
HADOOP-11754. RM fails to start in non-secure mode due to authentication filter failure. Contributed by Haohui Mai. (wheat9: rev 90e07d55ace7221081a58a90e54b360ad68fa1ef)
* hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2099 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2099/])
HADOOP-11754. RM fails to start in non-secure mode due to authentication filter failure. Contributed by Haohui Mai. (wheat9: rev 90e07d55ace7221081a58a90e54b360ad68fa1ef)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java
, Hi [~wheat9], seems like another issue related to this patch HADOOP-11815. Will you take a look at the quick fix there? Thanks!]