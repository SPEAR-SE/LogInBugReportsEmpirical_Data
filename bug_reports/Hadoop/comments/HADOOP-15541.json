[There are a bunch of subtle bugs that have lead to use recovering the way we do. Pinging [~stevel@apache.org] who has worked on a few of these: do you know what benefit we gain from draining the stream instead of simply aborting and starting a new stream?, Also filed an issue with the SDK: [https://github.com/aws/aws-sdk-java/issues/1630.] But like I said, I'm not sure what the point is or if there's anything wrong with just aborting on SdkClientExceptions since we'll have to fail at some point anyway., I've worried about something related to this for a while, precisely because we are using close() not abort. Assuming the error on read() is due to a network problem, breaking that whole TCP connection is the only way to guarantee that your followup GET isn't on the same HTTP1.1 stream.

I wasn't too worried, on the basis that nobody had complained...clearly that's not true any more. And my expectation of how things would fail was worse. 

Here's one possible strategy

# {{S3AInputStream.reopen()}}  adds a {{boolean forceAbort}} param; passes it in to {{closeStream}}; 
# {{S3AInputStream.onReadFailure()}} forces that abort.

Like you say, no real point in not aborting here.

Happy for a patch, I don't think we can test this easily so not expecting any tests in the patch..., {quote}Like you say, no real point in not aborting here.{quote}

Help me understand, though: when *do* we get a benefit from draining the stream instead of simply aborting?

{quote}Happy for a patch, I don't think we can test this easily so not expecting any tests in the patch...{quote}

Yeah. This was (at the time anyway) happening pretty repeatedly with a particular workload - I'm hoping that keeps up so I can be fairly confident that the end result here is correct handling of timeouts.

Instead of the forceAbort option, any objection to simple aborting when we catch IOExceptions AND SdkClientExceptions? If we're intended to close a previous stream and open a new one and draining the stream fails for any reason at all, I'd think we'd still want to force abort and proceed regardless of the option that led us to this point., Stream draining means the http1.1 connection can be returned to the pool and so save setup costs, which is why we like to do it on close()

But here, if we can conclude that the connection is in trouble, should we return it it? 

No objection to doing the abort for IOEs and SDKs, I was suggesting the arg because the reopen code already takes that param...requesting that forced abort after an exception on read() would be good.

though: are you suggesting for any IOE/SDK exception we don't try to reopen the call, just force the abort() before throwing up the exception? If so, yes, that also makes sense. We don't want a failing HTTP connection to be recycled

Make sure any metrics on forced aborts are incremented though, Thanks for the explanation [~stevel@apache.org]. Attaching a patch that uses the existing force-abort code in the event of a timeout. All tests continue to pass, and the workload that was consistently timing out before suddenly stopped upon applying this patch. I just saw your comment about incrementing metrics, though. Let me check for those and revise the patch if necessary., Although on first glance, it certainly seems that calling streamStatistics.streamClose takes care of all that, and we're doing that., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 27m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 30s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 19s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 59s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 19s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  4m 31s{color} | {color:green} hadoop-aws in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 22s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 61m 33s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:abb62dd |
| JIRA Issue | HADOOP-15541 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12930597/HADOOP-15541.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux e6408faeb955 3.13.0-137-generic #186-Ubuntu SMP Mon Dec 4 19:09:19 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ba68320 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_171 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14864/testReport/ |
| Max. process+thread count | 302 (vs. ulimit of 10000) |
| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14864/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Looks good. 
One issue: should we always force close on a read failure, rather than treat SocketTimeoutException as special? I guess there are some potential failure modes (source was deleted during the read) which could trigger IOEs during the GET (maybe? Do we test this with a large enough file to be sure there's no caching going on? If not, I could imagine adding it to the huge files test....). 

IF we say "every IOE -> forced abort', then its a simpler path on read. What you have here though is the core fix: on socket errors, don't try and recycle things.

What do you think? If you want this one as is, you've got my +1. I'm just wondering if the need to add a separate catch for SocketTimeoutException is needed, Thanks Steve, commited. I'd like to commit this right now to address the known issue. I wanna do a bit of searching around and see if I can find any cases of IOExceptions where it would make sense to reuse the stream before taking it further. I'll a separate JIRA for that before resolving..., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14550 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14550/])
HADOOP-15541. [s3a] Shouldn't try to drain stream before aborting (mackrorysd: rev d503f65b6689b19278ec2a0cf9da5a8762539de8)
* (edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java
, OK. cherry picked to branch-3.1; lets close this one.

Thank you for finding a new failure mode :)]