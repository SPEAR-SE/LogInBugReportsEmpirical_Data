[Dick,

EricW provided this replacement for hadoop::ICsvArchive::deserialize, which he said worked great for him. Can you try it out ?

{code}
void hadoop::ICsvArchive::deserialize(std::string& t, const char* tag)
{
   char c;
   if (1 != stream.read(&c, 1)) {
     throw new IOException("Error in deserialization.");
   }
   if (c != '\'') {
     throw new IOException("Errror deserializing string.");
   }
   while (1) {
     char c;
     if (1 != stream.read(&c, 1)) {
       throw new IOException("Error in deserialization.");
     }
     if (c == ',' || c == '\n' || c == '}') {
       if (c != ',') {
         stream.pushBack(c);
       }
       break;
     }
     else if (c == '%') {
       char d[2];
       if (2 != stream.read(&d, 2)) {
         throw new IOException("Error in deserialization.");
       }
       if (strncmp(d, "0D", 2) == 0) {
         t.push_back(0x0D);
       }
       else if (strncmp(d, "0A", 2) == 0) {
         t.push_back(0x0A);
       }
       else if (strncmp(d, "7D", 2) == 0) {
         t.push_back(0x7D);
       }
       else if (strncmp(d, "00", 2) == 0) {
         t.push_back(0x00);
       }
       else if (strncmp(d, "2C", 2) == 0) {
         t.push_back(0x2C);
       }
       else if (strncmp(d, "25", 2) == 0) {
         t.push_back(0x25);
       }
       else {
         t.push_back(c);
         t.push_back(d[1]);
         t.push_back(d[2]);
       }
     }
     else {
       t.push_back(c);
     }
   }
}
{code}, Note that this code is not only slow, it doesn't work. The string is passed in by value and then modified. *oops*

Note that since '%' is always escaped this can be done using sscanf to read the two digits and convert to a byte., I propose sticking to Dick's suggestion: after reading the string, we replace all the characters in one shot. That's the way the Java version of RecordIO does it too, and it makes sense to maintain that symmetry. , Attached patch (1758_01.patch). while this fix is much more efficient than the current code, it is not the most efficient. We read data from a stream into a temporary string, then append to our string. A more efficient solution would be as EricW provided in an earlier comment. However, the solution in the patch is only slightly more inefficient, but is a lot simpler to read and understand, and it also mirrors the Java code. These are both important, IMO. If ever an extra string copy does indeed cause performance problems, we can change the code. , 
Quoted from: 
http://www.nabble.com/-jira--Created%3A-%28HADOOP-1758%29-processing-escapes-in-a-jute-record-is-quadratic-tf4313169.html#a12400611

, +1

http://issues.apache.org/jira/secure/attachment/12364812/1758_01.patch applied and successfully tested against trunk revision r572826.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/688/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/688/console, I just committed this.  Thanks, Vivek!]