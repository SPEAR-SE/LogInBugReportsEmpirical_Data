[I see the point of this, and I can also see that this would be an optimisation as you'd skip the overhead of 1-3 HTTP GET requests,

But...we've effectively frozen all dev of s3n as it is stable enough that things work, and it gives us a fallback while s3a stabilises & takes on performance improvements.

Looking at the S3A code, the same-ish problem exists. S3A adds the fix for HADOOP-13188, not only doing the GET but failing if the destination path is a directory. That is, even if  overwrite==false, you want to make sure that you aren't unintentionally creating a file over a dir, so unintentionally losing all access to the data underneath (it'd break the listing code, see). If I were to go near S3n, I'd probably focus on that bug, rather than dealing with the intermittent caching of the 404 that S3 can do.

Your particular problem is really due to the fact that the output committer is doing a rename(), which from a performance perspective is the wrong thing to ask an object store what to do. In HADOOP-13345, S3Guard, we're planning to deal with this â€”anything you can do to help there would be welcome., This is an s3 list inconsistency surfacing on s3n job commits. It will be fixed in s3a with a specific committer for s3]