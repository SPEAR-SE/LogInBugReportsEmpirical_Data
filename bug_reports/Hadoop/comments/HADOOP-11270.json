[Hi Venkata,

The behavior of different file systems has been documented (quite recently). Have a look at [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/filesystem/index.html].

If you look at the contract for s3n (s3n.xml) you will find

  <property>
    <name>fs.contract.rejects-seek-past-eof</name>
    <value>true</value>
  </property>
, # which version of Hadoop is this; Hadoop 2.4 had a broken seek
# are you trying to do {{seek(len(file))}} or {{seek(len(file)-1)}}?
# is the file 0 bytes long.

I would recommend you test on Hadoop 2.5.1; the semantics of filesystem access has change, as Thomas pointed out. And {{seek()}} turned out to be the most inconsistent of them (exceptions, actions on a negative value, seeking on the current position, etc.)

Finally, yes, filesystems are different and that's a WONTFIX. Example: a native fileystem doesn't raise any exception on the seek, only on the following read(). HDFS and others do fail fast on the seek. Which is why I'm surprised you are seeing a difference between HDFS and S3N; both are going to reject on the seek(), Thanks for your inputs!

[~stevel@apache.org], my responses:
1. I am currently using Hadoop 2.5.1.
2. I am trying seek(len(file)).
3. No, the file size is more than 1MB.

I understand that behavior across file systems can be different. But I believe seek(<length of file>) should be supported by s3n as well.
I have noticed that seek() method in NativeS3FsInputStream creates a new input stream by performing a getObject() starting from seek position. This fails when seek position is length of file. Instead we could do this:-
a. If the new seek position is greater than the current position of the stream, skip the difference in the underlying input stream.
b. If the new seek position is less than the current position of the stream, get a new input stream starting from this position.
I tested this change and its working. Please let me know your thoughts on this.

One impact of current behavior is that Hive reads for RCFiles stored in S3 fail when it tries to skip columns by issuing skipBytes() on this input stream.
, 
the operation {{seek(len(file))}} is considered to be going one byte too far, according to [[our current filesystem specification|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/fsdatainputstream.md#seekableseeks]]

that's because offsets begin at 0.

regarding the forward skipping, that's what the swift:// fs client does, [[for short distances of forward seeks|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/SwiftNativeInputStream.java#L300]]. This is because its expensive to re-open an http connection â€”going forward is far more efficient for short blocks. Nobody has (yet) backported the feature to the s3 filesystems. On longer reads it becomes more efficient to close and re-open. (especially long haul, but even short haul as the IO bandwidth of a s3 read is ~constant irrespective of the #of readers. If you have lots of jobs reading the same s3 file, you want them closing and reopening the file).

to summarise
# yes, some forward seeking by chomping bytes is worth doing, but purely for efficiency.
# I dont' consider raising an exception on {{seek(len(file))}} to be an error. It's what HDFS does too,, [~stevel@apache.org], HDFS currently supports seek(len(file)), it doesn't throw an error. Code snippet from seek() in DFSInputStream.java:-

{quote}
    if (targetPos > getFileLength()) \{
      throw new EOFException("Cannot seek after EOF");
    \}
{quote}

EOF exception is thrown only when the seek position crosses the length of file.
Since our current filesystem spec doesn't mandate that an error should be thrown at seek(len(file)), I believe this is acceptable.
We would need similar behavior for NativeS3FileSystem so that the clients using FSDataInputStream will be able to seek irrespective of hdfs/s3n scheme.
I have submitted a patch that will ensure same behavior for NativeS3FileSystem in-case of seek(len(file)).

Can you please review?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12681635/HADOOP-11270.patch
  against trunk revision 49c3889.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-aws.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5087//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5087//artifact/patchprocess/newPatchFindbugsWarningshadoop-aws.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5087//console

This message is automatically generated., I am working on writing the testcase., Updated patch after adding test case and addressing findbugs warning., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12686743/HADOOP-11270.02.patch
  against trunk revision 5b9fced.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5250//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12686763/HADOOP-11270.03.patch
  against trunk revision b437f5e.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-aws.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5251//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5251//console

This message is automatically generated., [~stevel@apache.org], I resubmitted the patch with testcase.
Can you please review?, [~stevel@apache.org], can you please review?, Venkata, I've created a new issue, HADOOP-11417, to get me to work out exactly what HDFS does, correct the spec if needed, and add a test in the FS contract to see what the filesystems do.

If we have to change s3n/s3a, this will be the s3n patch, which should remap to s3a with relative ease., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12686763/HADOOP-11270.03.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6372/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12686763/HADOOP-11270.03.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6376/console |


This message was automatically generated., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 11s{color} | {color:orange} hadoop-tools/hadoop-aws: The patch generated 1 new + 30 unchanged - 5 fixed = 31 total (was 35) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 35s{color} | {color:red} hadoop-tools/hadoop-aws generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 13s{color} | {color:green} hadoop-aws in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 15s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 12m 47s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-tools/hadoop-aws |
|  |  org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.reopen(long) ignores result of java.io.InputStream.skip(long)  At NativeS3FileSystem.java: At NativeS3FileSystem.java:[line 207] |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:85209cc |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12815674/HADOOP-11270.04.patch |
| JIRA Issue | HADOOP-11270 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 4e0a6688e24b 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 846ada2 |
| Default Java | 1.8.0_91 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/9912/artifact/patchprocess/diff-checkstyle-hadoop-tools_hadoop-aws.txt |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/9912/artifact/patchprocess/new-findbugs-hadoop-tools_hadoop-aws.html |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/9912/testReport/ |
| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/9912/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Usual S3x patch question: which S3 installation have you run the full hadoop-aws test suite against? Jenkins doesn't test that bit, see.

Also: is there a seek test that we need? I've done a lot of extra work on seek tests on S3A, and actually hoped that I'd fixed this issue there. If S3n still has it, then the other S3 and object store clients may still have it too. 

Could you see what you can add to {{AbstractContractSeekTest}} in branch-2 or trunk to create the problem before your patch goes in, make it go away after. And, if s3a, s3, swift and azure have the issue, have their subclasses skip that test for now ... that'd be extra patches, +findbugs warning isn't to be ignored. Skip result should be used to check range skipped, consider some policy if != desired. Warn? repeat seek? {{S3AInputStream}} does a warning]