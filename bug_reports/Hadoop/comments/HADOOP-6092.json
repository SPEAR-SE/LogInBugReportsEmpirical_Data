[If your disk is out of space, of course you will see this exception. If you believe this to be an error in Hadoop, please reopen with an explanation., but there are a lot of hard disk space, mawanqiang, is this still an issue, RawLocalFileSystem.java is more of a wrapper on Java File I/O, Please let us know if you are still facing this., We've been seeing what we think is the same thing in Hadoop 0.20.1.

Here's what the dfs health page says:
Configured Capacity	 :	 77.22 TB
 DFS Used	 :	 58.18 TB
 Non DFS Used	 :	 5.69 TB
 DFS Remaining	 :	 13.34 TB
 DFS Used%	 :	 75.35 %
 DFS Remaining%	 :	 17.27 %
 Live Nodes 	 :	 45
 Dead Nodes 	 :	 0

Even the most occupied node has just under 80% used space.

For both Java and Streaming jobs, we've seen exceptions like the below, which is from a streaming run:
Failed to merge on the local FSorg.apache.hadoop.fs.FSError: java.io.IOException: No space left on device
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:192)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.mapred.IFileOutputStream.write(IFileOutputStream.java:84)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:218)
	at org.apache.hadoop.mapred.Merger.writeFile(Merger.java:157)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$LocalFSMerger.run(ReduceTask.java:2533)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:190)
	... 10 more

This job had about 100GB of input, and I'm guessing around that level of data being passed into the reducers. During the job execution, watching dfsnodelist.jsp?whatNodes=LIVE doesn't indicate much growth, certainly not to the point where a node hits 100%. Not that I know that kind of tracking of disk use is actually supported.

AFAIK, we have the physical disk space to match what the dfs pages indicate. 

What can we do to investigate how we're running out of space?, one more comment --
we have a nightly process composed of Java jobs, that runs over roughly the same data set size each time. One of the big upfront filtering jobs seems to currently the most at-risk for triggering a No space left error.
As noted above, we have about 13TB free. As this number shrinks past 10TB, the incidence of No space left errors rises. This is only empirical, but we try to keep more than 10TB free just to avoid it.

Without any knowledge of what's going on internally, it seems that there's certainly correlation between HDFS space left and likelihood of a job on roughly the same input size failing with No space left. What's puzzling us is why it's when there's still as much as 10TB free, for a job that's on the order of 100GB of input., Well tonight we had a failure with more than 12TB free. So the problem gets worse for us. Here is a sample of the errors we got:

java.io.IOException: Task: attempt_201005271420_14408_r_000000_0 - The reduce copier failed
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid local directory for file:/hadoop/hadoop-metadata/cache/mapred/local/taskTracker/jobcache/job_201005271420_14408/attempt_201005271420_14408_r_000000_0/output/map_1043.out
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:343)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:124)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$LocalFSMerger.run(ReduceTask.java:2513)


Error: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:190)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.mapred.IFileOutputStream.write(IFileOutputStream.java:84)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:217)
	at org.apache.hadoop.mapred.Merger.writeFile(Merger.java:157)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$LocalFSMerger.run(ReduceTask.java:2533)


Error: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:190)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.mapred.IFileOutputStream.write(IFileOutputStream.java:84)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:189)
	at org.apache.hadoop.mapred.Task$CombineOutputCollector.collect(Task.java:880)
	at org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorCombiner.reduce(ValueAggregatorCombiner.java:68)
	at com.visiblemeasures.overdrive.hadoop.metrics.OverdriveAggregator$OverdriveAggregatorCombiner.reduce(OverdriveAggregator.java:315)
	at com.visiblemeasures.overdrive.hadoop.metrics.OverdriveAggregator$OverdriveAggregatorCombiner.reduce(OverdriveAggregator.java:286)
	at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1148)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2642)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2580), Meng, from what I can guess looking at your cluster configuration and job that you are executing, Is that One of the nodes in your cluster is running out of space, and when map/reduce task tries to write out intermediate result data to its local disks (non HDFS disk space) it is running out of space causing job failures, Though your HDFS cluster had more than 12TB of free space, the temp space on the failed node should have fallen well below limits. just check the free space on the disks holding the temp paths configured for map/reduce., I see. Yes, our physical disks are definitely full to bursting. Looking on one of the nodes that recently reported several no disk space errors, our configured temp dir (/disk1/hadoop/hadoop-metadata/cache/mapred/local) has 62 GB of stuff in it. 
And only 16GB used by the current job.

There's about 1500 uncleaned attempt_ directories lying around in local/. The earliest have been around since the last restart of our grid. Is it safe to delete these? How do they accumulate?, I guess this is a known issue with Map/Reduce, even our clusters ran out of space after a few months due to these attempt files and cache files created by map/reduce. Ideally clean up of these files happen after successful job completion but in this case due to failures these are not getting cleaned up. I think its OK to purge these files when there are no Job's running, 

There are a few open bugs on Mapreduce relating to this issue, you might want to refer to a few of these fixes. 

https://issues.apache.org/jira/browse/MAPREDUCE-1693

If this is the case then you might want to close this against MAPREDUCE-1693., Already have jiras tracking this, see last comment.]