[
We still need to confirm, but I think what happened is that, {{out.write()}} failed because of lack of space on Datanodes and app did a {{out.close()}} after that. But inside {{clode()}}, FSOutputSummer ended up invoking lower level write() with stale data. The attached patch fixes that.
, 
Updated patch makes sure clean up is done in {{DFSOutputStream.close()}} even when there are other errors., +1 The patch looks good., Thanks Hairong., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12368704/HADOOP-2121.patch
against trunk revision r593665.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs -1.  The patch appears to introduce 1 new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1084/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1084/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1084/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1084/console

This message is automatically generated., fixed a findbugs warning., +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12369309/HADOOP-2121.patch
against trunk revision r593748.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1089/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1089/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1089/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1089/console

This message is automatically generated., I just committed this. Thanks Raghu!, Integrated in Hadoop-Nightly #302 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/302/]), Cluster running 0.15.3, linux, jdk 1.6

I just had this occur on a machine that had a lot of free disk space, in a cluster where no datanode was more than 40% full.

I was adding a new machine into a cluster that had a running job. Every job that was pushed on to the machine failed after a bit, with this error.
, Note that his is already fixed in 0.16.0. You could apply this patch to 0.15.x if you like. I think this is safe enough for 0.15., It looks like this is not the issue now. Now the machine is throwing to man open files. I think the patch allowed the file descriptor limit problem to become visible

as far as 0.16 goes, I will move forward soon but we have some production stuff running and getting the time to port and move the production app and cluster is difficult.]