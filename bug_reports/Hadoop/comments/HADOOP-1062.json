[On 2 attempt I got this addititonal error:
2007-03-05 16:05:57,883 WARN  mapred.TaskRunner - task_0001_r_000005_1 Final merge of the inmemory files threw an exception: org.apache.hadoop.fs.ChecksumException: Checksum error: /trank/nutch-0.9-dev/filesystem/mapred/local/task_0001_r_000005_1/map_2.out at 16776192
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.verifySum(ChecksumFileSystem.java:250)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:207)
        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:163)
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:57)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:91)
        at org.apache.hadoop.io.SequenceFile$Reader.readBuffer(SequenceFile.java:1300)
        at org.apache.hadoop.io.SequenceFile$Reader.seekToCurrentValue(SequenceFile.java:1363)
        at org.apache.hadoop.io.SequenceFile$Reader.nextRawValue(SequenceFile.java:1656)
        at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawValue(SequenceFile.java:2579)
        at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2351)
        at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2226)
        at org.apache.hadoop.mapred.ReduceTaskRunner$InMemFSMergeThread.run(ReduceTaskRunner.java:820)
, I think this has something to do with the changes that ChecksumFileSystem (HADOOP-928) introduced in the InMemoryFileSystem or something related to that. Need to take a closer look at the interaction of the InMemoryFileSystem and the ChecksumFileSystem., This should be investigated prior to 0.12.1 release., Hi Espen, is it possible that you send me a test case that can consistently reproduce this error? Thanks., Could this in fact be caused by a machine w/o ECC memory?  The Internet Archive had lots of problems in sort when it had a bad batch of memory., I looked at this issue, but I am not able to reproduce the error. I would suggest that we fix it in 0.13.0 when we get more inputs from the reporter., I haven't been able to reproduce this error even on the same hardware.]