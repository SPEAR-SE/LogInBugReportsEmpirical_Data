[If this is amazon EMR, then it's their own code talking to s3 and so can't be handled here. You'll need to take it with the EMR team via support channels.

If it is pure ASF EMR, then is this an s3:// s3n:// or s3a:// URL, Thanks Steve for responding. Yes its s3n:// URL however the same code works on hdfs so the code is not EMR specific.
, Hello [~jagdishk].

Is this referring to a directory or a file?  If it's a directory, then s3n always returns 0 for mtime.  This is also true of s3a.  I don't believe there are currently any plans in progress to change this behavior.

The expected atomicity semantics of implementing directory mtime are more challenging to implement against a blob store compared to a traditional file system or HDFS.  If a new file or sub-directory gets created under a directory, then users have an expectation that the corresponding update to mtime at the parent folder is atomic with respect to the file/directory creation operation.  On HDFS, we can take a central lock at the NameNode to do all of the metadata manipulations as a transaction.  For a blob store, this is multiple HTTP operations on different blob keys, and those multiple operations do not execute as an atomic transaction.

The Azure file system does provide mtime on directories, but it does not provide atomicity of the mtime updates.  (I just mention this to demonstrate that the behavior is not always consistent across different file system implementations.), Thanks [~cnauroth] for sharing the details. Yes, it is referring to a directory. Would it be possible for you to suggest some workaround since there are no plans to fix it as you mentioned. , [~jagdishk], can you provide more details about what exactly is not working for you due to the lack of directory mtime?  The description mentions something about a job.  Is that a MapReduce job?  If so, how does it fail?  Is there an error message or a stack trace?  If I see those details, I might be able to suggest a workaround., Hi [~cnauroth],

I have a path filter utility which takes Path as input and returns true if the modification time of the give path is less than a specified time. Here's a method snippet for reference.
{code}
  @Override
  public boolean accept(Path path) {
    try {
      FileStatus fs = filesystem.getFileStatus(path);
      if (fs.getModificationTime() < this.date.getMillis()) {
        return true;
      }
    } catch (IOException e) {
      LOG.error(e.getMessage());
    }
    return false;
  }
{code}

The actual job takes all the paths for whom this returns true. Since the modification time for S3 based paths is returned as 0 this method returns true for all the paths specified. This results in processing unwanted data. This job doesn't fail. It just produces undesired output.

Besides I have a use case where we create a backup of the directories by renaming them with the timestamp of the modification time.
Also here the *filesystem* could be S3 or HDFS so need to find a generic solution.

A probably workaround I can think of is writing some dummy file like _SUCCESS in each of these directories and then look for modification time of the file, however, that would be an added effort.

Thanks,
Jagdish
 , Afraid this isn't going to work for an object store, they're not real filesystems. HADOOP-9545 proposes exposing that and adding some object store specific stuff, but it's not in yet.

I would avoid doing a rename() against object store files too, that is often implemented client side..., Yeah, I'm afraid there really isn't a feasible workaround at the file system layer right now.  I think you're on the right track in trying to explore different filtering techniques on your data set so that it doesn't rely on mtime., Well, the above mentioned workaround of the _SUCCESS file works in my case since the content of the directory in question isn't expected to change after it is created. However in case of frequently updating directory contents that won't work. 
For that case one needs to dig deeper in the directory / sub-directories and determine mtime of each file and finally return the max value as mtime of the directory, however, that would be an expensive operation, particularly in case of huge directories / subdirectories.

For now the workaround seems to be working for me. You may want to keep this ticket in backlog if this happens to find priority. Feel free to close if otherwise.

Thanks guys for sharing your inputs.

Regards,
Jagdish , revisiting this. It's a wontfix I'm afraid. Sorry]