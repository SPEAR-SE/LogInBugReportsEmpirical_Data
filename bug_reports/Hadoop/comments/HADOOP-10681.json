[With sed -i ~ "s/synchronized //" snappy/*.java, the cpu fractions look more sane

!snappy-perf-unsync.png!, Added a SnappyCode impl into hive, which should come before in the classpath for hive+tez.

Tested out TPC-H Query5, which has a spill-merge on the JOIN.

Query times went from 539.8 seconds to 464.89 seconds, mostly from speedup to a single reducer stage., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650430/HADOOP-10681.1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4089//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4089//console

This message is automatically generated., The patch removes unsafe synchronized blocks from the individual methods in Snappy and Zlib codecs.

This synchronization is slow and when used in the most common pattern for CompressionCodec is still thread-unsafe for sharing streams

{code}
 while (!compressor.finished()) {
   compressor.compress(buffer, 0, buffer.length);
 }
{code}

with loops running across multiple threads and would ideally require explicit code

synchronized(compressor) {
 while (!compressor.finished()) {
   compressor.compress(buffer, 0, buffer.length);
 }
}

to get correct stateful behaviour.

As code exists today it is not thread-safe and does slow lock-prefix x86_64 instructions.

The JNI library below in SnappyCodec.c actually does its own locking mutexes for the actual critical sections within.

{code}
JNIEXPORT jint JNICALL Java_org_apache_hadoop_io_compress_snappy_SnappyDecompressor_decompressBytesDirect
(JNIEnv *env, jobject thisj){
....
  // Get the input direct buffer
  LOCK_CLASS(env, clazz, "SnappyDecompressor");
{code}, Since this bug has caused some alarm among those who looked at it, I will pessimize this a little.

The core open loop which needs per-stream sync is (the correct code version)

{code}
synchronized(compressor) {
   compressor.setInput();
   while (!compressor.finished()) {
       compressor.compress(buffer, 0, buffer.length); 
   }
}
{code}

I will make sure only this incorrect loop follows the required fast-path., Add test-case to test synchronization characteristics of the codec pool.

Only remove synchronization of functions which require external synchronization anyway - also remove synchronization flags for empty methods., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12653542/HADOOP-10681.2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 4 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4197//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/4197//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4197//console

This message is automatically generated., Address findbugs warnings, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12653663/HADOOP-10681.3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.io.file.tfile.TestTFile
                  org.apache.hadoop.io.TestSequenceFile
                  org.apache.hadoop.io.compress.zlib.TestZlibCompressorDecompressor
                  org.apache.hadoop.io.file.tfile.TestTFileSeqFileComparison
                  org.apache.hadoop.io.file.tfile.TestTFileSeek
                  org.apache.hadoop.io.compress.TestCodec
                  org.apache.hadoop.io.file.tfile.TestTFileSplit
                  org.apache.hadoop.io.TestSetFile
                  org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays
                  org.apache.hadoop.io.file.tfile.TestTFileByteArrays

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4200//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/4200//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4200//console

This message is automatically generated., Go back to the initial approach of unsynchronized streams, but retain the multi-threaded test-case.

The JNI locks remain in-place., The synchronized blocks would've made a lot of sense if setInput() or decompress/compress() was atomic.

Since it only reads part of the data (64kb or so) in for an invocation, the user has never been able to use this with multiple threads safely.

To make sure this was never used with threading in something like HBase, I cross-checked & HBase has an unsynchronized improved version of gzip which writes its own header/trailer chunks without synchronization.

https://github.com/apache/hbase/blob/c61cb7fb55124547a36a6ef56afaec43676039f8/hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/ReusableStreamGzipCodec.java#L100, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12655422/HADOOP-10681.4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ipc.TestRPCCallBenchmark
                  org.apache.hadoop.net.TestNetUtils
                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl
                  org.apache.hadoop.security.TestSecurityUtil
                  org.apache.hadoop.ha.TestZKFailoverControllerStress
                  org.apache.hadoop.ipc.TestRPC
                  org.apache.hadoop.security.TestDoAsEffectiveUser
                  org.apache.hadoop.conf.TestConfiguration
                  org.apache.hadoop.ipc.TestIPC

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4383//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4383//console

This message is automatically generated., FAILURE: Integrated in Hadoop-trunk-Commit #6194 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6194/])
HADOOP-10681. Remove unnecessary synchronization from Snappy & Zlib codecs. Contributed by Gopal Vijayaraghavan. (acmurthy: rev 8f9ab998e273259c1e7a3ed53ba37d767e02b6bb)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodecPool.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java
, The test failure are unrelated, verified.

I just committed this, sorry it took me so long. Thanks [~gopalv]!, FAILURE: Integrated in Hadoop-Yarn-trunk #703 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/703/])
HADOOP-10681. Remove unnecessary synchronization from Snappy & Zlib codecs. Contributed by Gopal Vijayaraghavan. (acmurthy: rev 8f9ab998e273259c1e7a3ed53ba37d767e02b6bb)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodecPool.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1893 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1893/])
HADOOP-10681. Remove unnecessary synchronization from Snappy & Zlib codecs. Contributed by Gopal Vijayaraghavan. (acmurthy: rev 8f9ab998e273259c1e7a3ed53ba37d767e02b6bb)
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodecPool.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1918 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1918/])
HADOOP-10681. Remove unnecessary synchronization from Snappy & Zlib codecs. Contributed by Gopal Vijayaraghavan. (acmurthy: rev 8f9ab998e273259c1e7a3ed53ba37d767e02b6bb)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodecPool.java
, Can we do the same for {{Lz4Compressor}} and {{Bzip2Compressor}}? I noticed a significant performance overhead using {{Lz4Compressor}} for {{hbase.client.rpc.compressor}} compared to {{SnappyCompressor}} due to the same problem.]