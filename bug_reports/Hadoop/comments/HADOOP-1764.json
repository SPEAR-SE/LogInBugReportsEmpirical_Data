[Srikanth, thanks for the detailed logs, appreciate it.

Looking at them, here is my version of events: 

a) task_200708210155_0003_m_002598_0 succeeds on node50 some time before 09:38:49 (actually before 09:28:49 since it takes 10mins to detect a lost tasktracker).
b) task_200708210155_0003_r_000006_2 is told that task_200708210155_0003_m_002598_0 is complete and it schedules map-output fetch.
c) At 09:38:49 the JobTracker detects that the TaskTracker on node50 is *lost*.
d) task_200708210155_0003_r_000006_2 tried (unsuccessfully) to fetch map-outputs from node50 (with backoffs i.e. at 09:47 and 09:57). *
e) task_200708210155_0003_m_002598_1 succeeded on node75 somewhere before 10:00:56.
f) task_200708210155_0003_r_000006_2 is told by JobTracker that task_200708210155_0003_m_002598_1 is complete, schedules the copy and successfully copies at 10:00:56.
g) At 10:02:17 task_200708210155_0003_r_000006_2 discarded the scheduled fetch from task_200708210155_0003_m_002598_0 since it had already copied the output from task_200708210155_0003_m_002598_1.
h) At 11:22:46 the JobTracker detects that the TaskTracker on node75 is *lost*, fails the completed map task task_200708210155_0003_m_002598_1, and re-executes the tip (task_200708210155_0003_m_002598_2) on node55.
i) The reduce task task_200708210155_0003_r_000006_2 doesn't copy output of task_200708210155_0003_m_002598_2, this could be due to:
   * task_200708210155_0003_r_000006_2 has already completed (success or failure).
   * Even if task_200708210155_0003_r_000006_2 is still running, it sees that task_200708210155_0003_m_002598_2 has completed but ignores that event since it already has the output of task_200708210155_0003_m_002598_1 which ran successfully. Clearly this is safe since the output of task_200708210155_0003_m_002598_1 & task_200708210155_0003_m_002598_2 are identical.


Rebuttals:

bq. 1. Even finally the reducer seems to fetch data from the incorrect TaskTracker, it is not checking with the job tracker for the final/correct map output
bq. 3. An obvious solution may be to go to the job tracker and directly get the correct map output (I was able to get the correct map output from node55 using http, without any errors)

This, as noted above, is due to the fact that the task_200708210155_0003_r_000006_2 doesn't reschedule a copy of output of task_200708210155_0003_m_002598_2 since it already successfully copied it from task_200708210155_0003_m_002598_1 (before that failed due to lost TaskTracker on node75).


bq. 2. It seems to retry more times and sleeps for longer time (looking at the interval of log messages)

The reduce task, on a failed output-fetch doesn't immediately schedule another fetch but backs-off for a period greater than 5mins.

The one minor issue here is that reduces do not process 'task failed' events, i.e. ignore them and continue to schedule copies from the failed map tasks, with backoffs (which is why you see the exceptions while trying to fetch from the failed task_200708210155_0003_m_002598_0). Devaraj can chime in on this, my take is that this is 'ok' since we back-off for a reasonably long period (5mins) and meanwhile we'd schedule the fetch from the other successful task-attempt and then proceed to ignore the failed one.

Overall, my take after looking at the logs is that there isn't anything that hits me as a *correctness issue* and follows what we've designed for and expect in such scenarios.

Given that I'm inclined to close this bug... thoughts?
, bq.  Given that I'm inclined to close this bug... thoughts?

+1, Srikanth - I'm closing this one, please feel free to open another issue if you see fit. Thanks!]