[I plan to fix this problem like this:
name the "side-effect output files" after the input Split rather than after the taskid:

/in/myfile
/sideout/myfile+200000-100000

And for the first split at offset zero, drop split info, 
i.e. name the output file as the unqaliified filename :
/sideout/myfile

That way, assuming your InputFormat only creates one split per file(*)
then the input filenames and partitioning is preserved (in a different output directory)

And in the case of speculative execution and failed / reexecuted tasks.
those output files are assigned the same name. 
Only one version of the file will remain in the end.
This solves the duplicate-output issue.

Only full output files are committed to HDFS on close. 
So there are no issues with concurrent writes / partial writes.

One question is what needs to be done to guarantee that the version from the last execution "wins" .
Is it a good idea to force an HDFS-delete of a target output file before opening the new version?

(*) for example inputs compressed with file-level compression, like .gzip. Could also be useful to let the user force a single-split.







, this issue is resolved as part of the patch submitted for HADOOP-542.
Please mark it as fixed.
Thanks,
Yoram, This was fixed as a part of HADOOP-542.]