[What version of Hadoop?  Can you get stack dumps of these processes by sending SIGQUIT?
, Pre HADOOP-5247, it was assumed that all the trackers will atleast report once before a job gets removed from the system. Unfortunately thats not true. HADOOP-5247 solves this issue by broadcasting a job level cleanup upon job completion. I think we need to port HADOOP-5247 to 0.19., Are you seeing Zombie processes around? Then, yes it is HADOOP-5247.
But if you are seeing no processes running and just tasktracker says it is running tasks, it is HADOOP-5269. Also a couple of race conditions got fixed in HADOOP-5233. All these will be ported to branch 0.19.2. , Nathan, now that HADOOP-5247, HADOOP-5269 and HADOOP-5233 are committed to 0.19.2, do you still see the problem?
If not, can you close the issue?, I am still seeing this issue. I am using the 0.19.2 code from around March 19th., Another note - these tasks are not taking up task slots in the cluster as jobs are still running normally like I thought they were before 0.19.2. That may have been a separate issue which I'm not seeing anymore. , Nathan, so what exactly do you see now? Do these tasks remain in PENDING state forever? Or is it some other state?, The tasks aren't actually shown in the jobtracker UI. They're processes with "attempt***" in the name running throughout the cluster. After a few weeks there are about a hundred of them in our 40 node cluster. The only way to get rid of them is to kill -9 them - a normal kill does not work., Do you know what happened to the jobs to which these attempts belonged? Also, if the task logs (syslog,stderr,stdout) still exist, what do they say? One suspicion is that the tasks got OutOfMemory errors and they got hung somewhere in the process of exiting (like in the task's finally block). The TaskTrackers marked those tasks as FAILED due to lack of ping messages from the tasks. , As it turns out, this was caused by an application problem. We were running embedded Solr instances in the tasks that were preventing the process from exiting. The fix was to close the Solr instances at task completion.]