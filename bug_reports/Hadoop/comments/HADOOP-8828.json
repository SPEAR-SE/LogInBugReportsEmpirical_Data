[Which versions are you observing this behavior?, I believe this was 2.0 or a trunk build around that time., Btw here's the current failure mode for secure to insecure using hftp and webhdfs (thanks to Stephen Chu).

Caused by: java.io.IOException: Couldn't setup connection for schu@HAL.CLOUDERA.COM to hdfs/c1204.hal.cloudera.com@HAL.CLOUDERA.COM
at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:540)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:512)
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:596)
at org.apache.hadoop.ipc.Client$Connection.access$1700(Client.java:220)
at org.apache.hadoop.ipc.Client.getConnection(Client.java:1213)
at org.apache.hadoop.ipc.Client.call(Client.java:1140)
... 22 more
Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database\
(7) - UNKNOWN_SERVER)]
at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)
at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:137)
at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:423)
at org.apache.hadoop.ipc.Client$Connection.access$1300(Client.java:220)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:589)
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:586)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:585)
... 25 more
Caused by: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)
at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:663)
at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:230)
at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)
at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)
... 34 more
Caused by: KrbException: Server not found in Kerberos database (7) - UNKNOWN_SERVER
at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:64)
at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:185)
at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:294)
at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:106)
at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:557)
at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:594)
... 37 more
Caused by: KrbException: Identifier doesn't match expected value (906)
at sun.security.krb5.internal.KDCRep.init(KDCRep.java:133)
at sun.security.krb5.internal.TGSRep.init(TGSRep.java:58)
at sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:53)
at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:46)
... 42 more, Owen and I were just discussing a few weeks ago if we should always attempt a kerberos connection if the user has kerberos credentials.  It's odd that we assume that all clusters are insecure if the local cluster is insecure.  It should be a simple and compatible change because an insecure NN's sasl will kick the user back to simple auth if presented with kerberos., To copy data from a secure cluster to insecure cluster, run the distcp on the secure cluster. It would be a security problem to run the distcp on the insecure cluster because you would need tokens for the secure cluster and they wouldn't be protected by the insecure cluster.

Of course that means you'll need to use webhdfs or hdfs rpc to perform the write, but that is far better than encouraging users to open up security holes in their secure clusters., FYI, this problem should be addressed by SASL changes in RPCv9.  The client won't attempt kerberos if the server doesn't support it., I'm reopening this JIRA as we found some subtle problems in HADOOP-10016 and HADOOP-10017.

I've tested the set up of copying from a secure Hadoop cluster to secure Hadoop 2 cluster using distcp, and have found some subtle problems:

1. There is a NPE in the trunk which fails the distcp job (addressed in HADOOP-10017).
2. When copying from a secure Hadoop 1 cluster to an insecure Hadoop 2 cluster, the Hadoop 1 cluster cannot handle this case because it lacks of negotiation and fallback mechanisms during authentications. (addressed in HADOOP-10016)., Any progress on this?

I ran into a variant of this recently when using HBase ExportSnapshot to copy from a secure to an insecure cluster. I was able to workaround using a custom WebHDFS implementation for the insecure cluster, that I used using {{-Dfs.webhdfs.impl=<classpath>.SimpleAuthWebHdfsFileSystem}}:

{code:java}
public class SimpleAuthWebHdfsFileSystem extends WebHdfsFileSystem {
  @Override public Token<DelegationTokenIdentifier> getDelegationToken(String renewer)
      throws IOException {
    return null;
  }
}
{code} 

Secure cluster is running Hadoop 2.3.0 / CDH 5.1.2
Insecure cluster is running Hadoop 2.0.0 / CDH 4.6.0, Hi, 
I am wondering if there is any updates on this before I invest time digging deeper for a solution. I to have seen this issue as described going from a Kerberized Secure Hadoop 2.7.3 Cluster to an insecure Hadoop 2.7.3 cluster. My use case is below, let me know if there is something more I can help with while I have the configuration available. 

hadoop distcp -D ipc.client.fallback-to-simple-auth-allowed=true hdfs://secure.com/tmp/tests/test.txt hdfs://insecure.com/tmp/tests/test-kerb-non-kerb-simple-auth.txt

The distcp command returns with an java.io.EOFException and the NN log has in it a "NoMatchingRule: No rules applied to..." error.
Thanks!]