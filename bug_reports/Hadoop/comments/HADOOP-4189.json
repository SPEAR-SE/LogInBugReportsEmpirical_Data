[Straight forward patch., Attaching a updated patch that solves the issue of stale job history analysis. Pre HADOOP-3245, missing job state meant that the job is running. HADOOP-3245 introduces the RUNNING state for jobs which indicates that the job meta-info is available in the history. {{loadhistory.jsp}} cached the analysis results and refreshed only if the job status was missing (which meant that the job is running). Fixed that to explicitly test the job runstate as RUNNING. Tested the patch with and without restarts. Also checked if the history analysis works after the job is complete. I apologize for not testing this before, my bad., There was a corner case in the previous patch where in if the history is accessed after the history file is created and before the job status is logged, then the cached result will never be refreshed. Now the check makes sure that the analysis is redone whenever the job is incomplete. Tested this on 20 nodes and the issue to do with stale values after the job is complete is addressed. Its difficult to test this corner case., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12390243/HADOOP-4189-v2.patch
  against trunk revision 696228.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3285/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3285/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3285/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3285/console

This message is automatically generated., In loadhistory.jsp, add a check for job.get(Keys.JOB_STATUS) == RUNNING, instead of check for not a SUCCESS.
Other than that patch looks good., Attaching a new patch that incorporates Amareshwari's comments., +1 Patch looks good, Amareshwari pointed out a major bug, thanks for that. Attaching a new patch with the changes., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12390362/HADOOP-4189-v3.1.patch
  against trunk revision 696777.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3306/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3306/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3306/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3306/console

This message is automatically generated., Null config values are not the right way to go. At best it should be "-1" in these cases to let the system pick the default values.

In these specific cases I'm not sure we even need to make them configurable... the system should just pick the right values?

Do we need to bump up TaskUmbilicalProtocol's version? If so, we at least should add a comment describing why the change was necessary., bq. Null config values are not the right way to go. At best it should be "-1" 
I am not sure if -1 internally selects the system defaults. If nothing is passed, by default the history code selects filesystem's default values for block size and buffer.

bq. In these specific cases I'm not sure we even need to make them configurable.
With the system defaults, the % recovery is very less as history info is not frequently flushed to the disk. So controlling these parameters, one can control the extent of recovery. Again there is a tradeoff between how much to recover versus the dfs performance w.r.t low block-size/buffer-size.

bq. Do we need to bump up TaskUmbilicalProtocol's version?
I have added the comment in HADOOP-3245 but forgot to bump up the version number. So fixing it in this patch.
, I'm arguing that that the jobhistory code should use the right blocksize and buffersize, I don't see the value in making this configurable., Arun,
I was not sure as to what the optimal values are. If we use default values from FileSystem then the amount of data recovered is very low. For long running jobs with less maps, these values should be low so that every update gets logged while for long running jobs with large maps, it is ok to use system defaults as the frequency of updates are high. So instead of using some heuristics and making it hardcoded, I thought its better to keep it (admin)configurable and then come up with some better heuristics or technique to set the defaults. Any thought on what the default values should be considering various types of jobs that can be run?, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12390362/HADOOP-4189-v3.1.patch
  against trunk revision 697306.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3351/console

This message is automatically generated., Ok, I think 

  * we should actually set the default in hadoop-default to the real value instead of 0, -1, or "".
  * we should mark it expert
  * if it isn't already, I think this should be only configured by the cluster, not the job.

As to the defaults, I guess the question is how many minutes of run are we willing to lose. Let's say 10 minutes, and let's say 1000 job events / minute at 100 bytes each. That would give a ballpark figure of 1mb, which seems reasonable., bq. we should actually set the default in hadoop-default to the real value instead of 0, -1, or "".
+1.

bq. we should mark it expert
+1. Will add this to the comment

bq. if it isn't already, I think this should be only configured by the cluster, not the job.
This value is used when the jobtracker starts. So its per cluster and not per job.

bq. As to the defaults ....
On an avg I found ~300-350 characters per line. Which makes 600-700 bytes per line. Hence with 1mb block size we risk ~1750 lines, i.e 875 task updates (as they come in pairs). Which I think should be ok. Thoughts?, {quote}
task updates (as they come in pairs)
{quote}

For restartability, we really should separate the start event from the finish event. The start event should be generated when the task starts, not when it ends. (Assuming that hasn't been changed already.)

{quote}
On an avg I found ~300-350 characters per line. Which makes 600-700 bytes per line
{quote}

Why does it double? Hopefully we are using UTF-8 encoding so it should be 1 byte/char.

If I assume it is actually 300 bytes per an event, I'd bump the default to 3mb., bq. For restartability, we really should separate the start event from the finish event.
I dont think it really matters. Any task-attempt that has started but not closed is probably useless w.r.t recovery. Hence task-attempt which has a _START_ and _END_ is really what we want and hence  we can keep it as it is. 

bq. Why does it double? Hopefully we are using UTF-8 encoding so it should be 1 byte/char.
I checked on my local box and its ASCII. Assuming 3mb as the default, we now risk 5.2k tasks which should be ok., Attaching a updated patch the sets the default job-history block size to 3MB., _test-patch_ and _ant test_ passed on my box., I think that we should address putting in the start event immediately, but I don't think that it should hold up this patch., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12390995/HADOOP-4189-v5.1.patch
  against trunk revision 699119.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3381/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3381/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3381/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3381/console

This message is automatically generated., +1 
, I just committed this. Thanks, Amar!, Integrated in Hadoop-trunk #618 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/618/])]