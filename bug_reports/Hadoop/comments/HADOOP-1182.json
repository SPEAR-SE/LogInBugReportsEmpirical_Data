[It would help us a lot if you can monitor the CPU usage on the namenode when this occurs. Was the CPU close to 100%? Also, do you see messages of the following type in the namenode log?

"Call queue overflow discarding oldest call", I am changing this to "dfs" because the stack trace shows that a connection to the namenode timed out and I suspecting that this is a DFS namenode issue., Indeed, namenode server is on 99.9 CPU.
top output during transition of job submittal:
                                                                                                                                             
 2143 crawler   16   0 2199m 175m 6248 S  4.0  4.3 278:13.64 java                                                                                                                                                 
 2143 crawler   16   0 2199m 172m 6248 S 79.5  4.3 278:16.03 java                                                                                                                                                 
 2143 crawler   16   0 2199m 176m 6248 S 99.9  4.4 278:19.52 java                                                                                                                                                 
 2143 crawler   16   0 2199m 184m 6248 S 99.9  4.5 278:22.99 java                                                                                                                                                 
 2143 crawler   16   0 2199m 188m 6248 S 99.9  4.7 278:26.42 java                                                                                                                                                 
 2143 crawler   16   0 2199m 188m 6248 S 99.9  4.7 278:29.84 java      

'Call queue overflow discarding oldest call' warn messsages in namenode log:

Typical exception in namenode log:
2007-03-30 09:29:08,490 WARN org.apache.hadoop.ipc.Server: handler output error
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:126)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:78)
        at java.io.DataOutputStream.writeByte(DataOutputStream.java:136)
        at org.apache.hadoop.io.UTF8.writeChars(UTF8.java:275)
        at org.apache.hadoop.io.UTF8.writeString(UTF8.java:247)
        at org.apache.hadoop.dfs.DatanodeID.write(DatanodeID.java:138)
        at org.apache.hadoop.dfs.DatanodeInfo.write(DatanodeInfo.java:248)
        at org.apache.hadoop.dfs.LocatedBlock.write(LocatedBlock.java:76)
        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:154)
        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:121)
        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)



, There are a few open issues that deal with reducing CPU usage on namenode. Hadoop-1155, hadoop-1149, hadoop-1079 and hadoop-1073. Some of these patches should improve your situation.

In the short-term,you could try increasing the number of Namenode threads. This, in turn, increases the call queue depth (100 calls per each additional server threads). The default number of server threads is 40. To make this change, you have to add the following to hadoop-site.xml:

 <property>
  <name>dfs.namenode.handler.count</name>
  <value>40</value>
  <description>The number of server threads for the namenode.</description>
</property>
, As part of this issue, should we make "calls waiting per thread"  configurable? , The calls waiting/thread is rarely relevant. Most of the time, it is the time since the call was received.  I believe the server-side call time out is calculated as 50% of the ipc time out., That's what i thought earlier. But when I investigated, it appears that most of the calls are very very recent but are getting dropped because there isn't any more free space in the incoming call queue. The message "Call queue overflow discarding oldest call"" is logged when the queue overflows.
, Increasing the number of handlers in namenode server seems to help a lot -- no ''Call queue overflow discarding oldest call' warn messages anymore.

CPU still going up to sustained 99.9% for a while at beginning of the job. We will test more.
, I created a follow-up issue Hadoop-1263. This current issue could be closed., A related issue is being tracked in hadoop-1263]