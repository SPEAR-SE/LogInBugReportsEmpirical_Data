[Should this be a blocker before enabling Trash on a large system with many active users?, If any throttling is required, it should be in the namenode.  The namenode should be capable of handling requests like 'rmr /' without running out of memory, no?  So perhaps file and directory deletions should be queued within the namenode, based on the size of RecentInvalidateSets, rather than altering the Trash mechanism to more slowly delete things., Shouldn't a file's deletion mostly just shuffle block ids from one list to another?  Deletion shouldn't raise the high-water-mark of memory usage, should it?, In the current code, the namenode sends out only 1000 block deletion requests per heartbeat. This means that the RecentInvalidateSets data structure could get bloated if tons of files get deleted at the same instant. Deleting a file causes the file to get removed from the FsDirectory, but blocks do not get removed from the blocksMap till the next block report from the datanode., In a recent test of block crc upgrade, Namenode deleted 5M .crc files in SafeMode without apparent increase in memory. The datanodes were asked to delete the blocks right after SafeMode was off. Each datanode was asked to delete 16k blocks in total. Even during this time there was no noticeable increase in memory (may be less than 5-10%).
, so is this a real issue, or can we close it?

Sounds like a hypothetical concern that has never been observed., I vote for closing it. Raghu's experiment with deleting 5M files showed no memory related problems., We have not been able to experimentally observe significant increase in memory usage when large number of files are deleted., The problem is that applications can also easily delete large chunks of the filesystem already.  So I don't see this as a Trash-specific issue.  Perhaps we can open a new issue generically about namenode memory use for large deletes?  I suppose the ultimate goal there would be to be able to delete the entire filesystem without growing the heap at all?  Would anything less satisfy folks?  Should we also file issues about adding or opening lots of files at once?  Seriously, we could, based on the available heap and our knowledge of the size of various data structures, try to limit activity to always stay within the heap, returning application exceptions (TooManyFiles) when these may be exceeded.  Short of that, I don't see how we can really address stuff like this without a major re-write of the namenode, so that it does not use single-host, memory-resident datastructures for each file and block.

+1 for closing it., I don't see the big deal of leaving this open.  Really.  Raghu's block crc upgrade code throttles the deletion of .crc files (so I suspect his comments above on this jira are irrelvant).  Why not just apply that throttling code to trash as well.  It would simply prevent a very large number of block related objects being added to the deletion queue in one go -- and again, we already do this for block crc upgrade., > Raghu's block crc upgrade code throttles the deletion of .crc files.

blockCrc upgrade does not throttle deletes explicitly. It deletes in a single thread and each deletiion results in editsLog entry.. these two naturally throttle the rate to around 700-800 deletes a second. Deleting a directory deletes whole tree with sigle editsLog entry. This is the difference. 

The similarity is that that none of these 5M blocks were removed from the namespace until after the upgrade is complete. In that sense memory overhead is same as deleting 5M files in one shot., > Why not just apply that throttling code to trash as well?

If this is really a problem, that wouldn't fix it when an application deletes a bunch of files.  So, if throttling is required for deletions, we should implement it generically, either on the namenode or in DFSClient.  But, as of yet, there's no evidence that throttling is required.]