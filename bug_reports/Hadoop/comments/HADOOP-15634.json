[#. EMR team gets to field their problems, as they've forked the code
# That said: yes. LocalDirAllocator uses local directories. They are used in many places in Hadoop, often in different config options for different bits of the code
# S3 is not a filesystem. You can't write directly to it, so the s3 connectors all save data into blocks before upload. Where to? The local HDD via the LocalDirAllocator. S3 isnt going to work

I'm afraid you will have to accept that when code wants a local FS directory it means it and that you have to provide enough local storage for running applications.

That said: if Tez isn't cleaning up, that's an issue to take up with the EMR and Tez teams. For the latter, re-open this JIRA, move it to the Tez project. But come up with some evidence that they don't clean up first.

Closing as INVALID. Sorry

, That makes sense, Steve. Will get in touch with AWS EMR support team.

Was wondering... if it is a simple pull (from an S3 location) and write (to an S3 location), then why do we need local disk.Â Liked your explanation. I will have to rethink the architecture/design of my application.

Thanks for your time and quick response., {quote}why do we need local disk.
{quote}
For future reference: check out the [documentation|https://hadoop.apache.org/docs/current3/hadoop-aws/tools/hadoop-aws/index.html] for S3A. See the section "How S3A Writes to S3". (The open source S3A connector also has support for in-memory buffering and the doc offers some numbers for calculating how much memory this would require--which is the main downside.) . As Steve said, this only applies to the Apache Hadoop code. EMR's S3 client is not open sourced., bq  if it is a simple pull (from an S3 location) and write (to an S3 location), then why do we need local disk

There's an S3 COPY command which can do in-store copy at ~6-10 MB/s. We don't expose that in the Hadoop FS APIs, though we use this operation for mimicing in-bucket renames

Otherwise, a copy from s3a://src to s3a://dest downloads locally and uploads; the upload code is: 
https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java

I belive AWS's s3 distcp does things direct, and perhaps CircusTrain from expedia does this too. If you are trying to use the open source distcp, there are opportunities to speedup using cloud storage as a destination, and between cloud stores of the same cloud infra. If you want to help, you are welcome to contribute!]