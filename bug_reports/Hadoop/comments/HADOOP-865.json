[Do you know if the files (inodes or blocks) got corrupted, or if a block didn't get written? If you still have the files on S3 then it would be really helpful if you could send an S3 directory listing using a regular S3 tool (e.g. http://www.hanzoarchives.com/development-projects/s3-tools/).

Thanks.

BTW nice use of S3FileSystem as an infinite disk!

Tom

, Looks like it might actually be .crc related.... but, I thought this file hadn't even been closed at the time.

Not that an -ls /backups/fon1 reflects:
/backups/fon1/backup.010507-1739.cpio.bz2.gpg   <r 1>   1048576
Yet, there are some .crc files that have been left from previous -rm operations, so there're probably some other middling problems around.

%2F
%2Fbackups
%2Fbackups%2Ffon1
%2Fbackups%2Ffon1%2F.backup.010507-1736.cpio.bz2.gpg.crc
%2Fbackups%2Ffon1%2F.backup.010807-1303.cpio.bz2.gpg.crc
%2Fbackups%2Ffon1%2Fbackup.010507-1739.cpio.bz2.gpg
block_-3795133870143584439
block_-8360567787439934597
block_8856210385271099486

I'll keep this data around for a little while, in case you think there are any patches that you'd like me to test.
, I think I've spotted the problem: the deleteRaw method throws an IOException if the inode doesn't exist - unlike the DFS or Local implementation. I'll produce a patch - thanks for the offer to test it.

Tom, Bryan,

The patch should be a simple fix for the problem. If you try "hadoop dfs -rm filename" it should now work.

Note that -rmr doesn't work yet (I will create another patch for this).

Thanks,

Tom, I just committed this.  Thanks, Tom!]