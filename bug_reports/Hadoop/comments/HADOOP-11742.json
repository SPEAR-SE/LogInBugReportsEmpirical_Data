[An empty key means a root directory instead of "Not Found". This is the same behavior as _NativeS3FileSystem#getFileStatus_.

{code}
# hadoop-2.7.0-SNAPSHOT/bin/hdfs dfs -ls s3a://s3a/
15/03/25 06:28:05 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/ ()
15/03/25 06:28:05 DEBUG s3a.S3AFileSystem: List status for path: s3a://s3a/
15/03/25 06:28:05 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/ ()
15/03/25 06:28:05 DEBUG s3a.S3AFileSystem: listStatus: doing listObjects for directory 
# hadoop-2.7.0-SNAPSHOT/bin/hdfs dfs -mkdir s3a://s3a/foo
15/03/25 06:28:22 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/foo (foo)
15/03/25 06:28:23 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3a/foo
15/03/25 06:28:23 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/ ()
15/03/25 06:28:23 DEBUG s3a.S3AFileSystem: Making directory: s3a://s3a/foo
15/03/25 06:28:23 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/foo (foo)
15/03/25 06:28:23 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3a/foo
15/03/25 06:28:23 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/foo (foo)
15/03/25 06:28:24 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3a/foo
15/03/25 06:28:24 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/ ()
# hadoop-2.7.0-SNAPSHOT/bin/hdfs dfs -ls s3a://s3a/
15/03/25 06:28:31 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/ ()
15/03/25 06:28:31 DEBUG s3a.S3AFileSystem: List status for path: s3a://s3a/
15/03/25 06:28:31 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3a/ ()
15/03/25 06:28:31 DEBUG s3a.S3AFileSystem: listStatus: doing listObjects for directory 
15/03/25 06:28:31 DEBUG s3a.S3AFileSystem: Adding: rd: s3a://s3a/foo
Found 1 items
drwxrwxrwx   -          0 1970-01-01 00:00 s3a://s3a/foo
{code}
, I checked how this is covered in test cases. _NativeS3FileSystemContractBaseTest#testListStatusForRoot_ looks like a relevant test for s3n. But something similar is not found for s3a.

But _TestS3AContractRootDir_ is supposed to test this scenario, correct?, sounds like there's a new test needed in {{AbstractContractRootDirectoryTest}}. Can you include that in the patch?, submitting patch for jenkins -though that does not run the AWS test suite; needs someone to run it themselves, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707152/HADOOP-11742-branch-2.7.001.patch
  against trunk revision c770df4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-aws.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5996//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5996//console

This message is automatically generated., OK, will do. But found the current s3a related unit tests won't finish successfully. Filed as HADOOP-11753. , I found _AbstractFSContractTestBase#setup_ always creates a test directory, which is removed at _teardown_. Thus, an empty directory was not tested in concrete test cases.

The problem here is not calling mkdir on an empty bucket. But when you call _S3AFileSystem#getFileStatus("/")_ on an empty bucket, it throws an exception.

To setup such a condition, I rather chose to remove the test directory at setup, then no-op at teardown.

Then, without this fix, TestS3AContractRootDir failed as follows.

{code}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir
Tests run: 5, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 8.027 sec <<< FAILURE! - in org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir
testRmEmptyRootDirNonRecursive(org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir)  Time elapsed: 2.82 sec  <<< ERROR!
java.io.FileNotFoundException: No such file or directory: /
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:995)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:464)
	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testRmEmptyRootDirNonRecursive(AbstractContractRootDirectoryTest.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

testRmRootRecursive(org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir)  Time elapsed: 0.475 sec  <<< ERROR!
java.io.FileNotFoundException: No such file or directory: /
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:995)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:464)
	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testRmRootRecursive(AbstractContractRootDirectoryTest.java:96)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

testCreateFileOverRoot(org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir)  Time elapsed: 2.922 sec  <<< ERROR!
com.amazonaws.services.s3.model.AmazonS3Exception: Status Code: 400, AWS Service: Amazon S3, AWS Request ID: 368CF290D38711E4, AWS Error Code: MalformedXML, AWS Error Message: The XML you provided was not well-formed or did not validate against our published schema.
	at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:798)
	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:421)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)
	at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1393)
	at com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:108)
	at com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:100)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.upload(UploadMonitor.java:192)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:150)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:50)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

testRmNonEmptyRootDirNonRecursive(org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir)  Time elapsed: 0.533 sec  <<< ERROR!
java.io.FileNotFoundException: No such file or directory: /
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:995)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:464)
	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testRmNonEmptyRootDirNonRecursive(AbstractContractRootDirectoryTest.java:88)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


Results :

Tests in error: 
  TestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmEmptyRootDirNonRecursive:63 » FileNotFound
  TestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmRootRecursive:96 » FileNotFound
  TestS3AContractRootDir.testCreateFileOverRoot » AmazonS3 Status Code: 400, AWS...
  TestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmNonEmptyRootDirNonRecursive:88 » FileNotFound

Tests run: 5, Failures: 0, Errors: 4, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
{code}


Also, I modified the fix so that it can tell if the root directory is empty or not., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12707452/HADOOP-11742-branch-2.7.002.patch
  against trunk revision b4b4fe9.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-aws.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/6003//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/6003//console

This message is automatically generated., {quote} Then, without this fix, TestS3AContractRootDir failed as follows.{quote}
At my end, without the fix, the test passes vs AWS. 

With the fix the test passes as well so what are you fixing? Can you elaborate? Will have a closer look then., [~tsato] : whose endpoints are you playing with there? Are these AWS or something of your own. As if its the latter, what's being shown up here is a difference between your impl & AWS. While we'll try and do our best to help, you do have to consider any variation in behaviour a variation between S3 API and your impl —which is really a bug on your end. , Thomas, Steve, yes, again this is against our own. I will check the difference. Let me close., Reopen to mark this as invalid., I confirmed mkdir fails on an empty bucket for AWS as follows:

1. make sure the bucket is empty, but get an exception

{code}
# hadoop-2.7.0-SNAPSHOT/bin/hdfs dfs -Dfs.s3a.access.key=ACCESS_KEY -Dfs.s3a.secret.key=SECRET_KEY -ls s3a://s3atest/
15/04/02 01:49:09 DEBUG http.wire: >> "HEAD / HTTP/1.1[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Host: s3atest.s3.amazonaws.com[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Authorization: AWS XXX=[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Date: Thu, 02 Apr 2015 01:49:08 GMT[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "User-Agent: aws-sdk-java/1.7.4 Linux/3.10.0-123.8.1.el7.centos.plus.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/24.75-b04/1.7.0_75[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Connection: Keep-Alive[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "HTTP/1.1 200 OK[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "x-amz-id-2: XXX[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "x-amz-request-id: XXX[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "Date: Thu, 02 Apr 2015 01:49:10 GMT[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "Content-Type: application/xml[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "Transfer-Encoding: chunked[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "Server: AmazonS3[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:09 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/ ()
15/04/02 01:49:09 DEBUG http.wire: >> "GET /?delimiter=%2F&max-keys=1&prefix= HTTP/1.1[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Host: s3atest.s3.amazonaws.com[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Authorization: AWS XXX=[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Date: Thu, 02 Apr 2015 01:49:09 GMT[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "User-Agent: aws-sdk-java/1.7.4 Linux/3.10.0-123.8.1.el7.centos.plus.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/24.75-b04/1.7.0_75[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "Connection: Keep-Alive[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: >> "[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "HTTP/1.1 200 OK[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "x-amz-id-2: XXX[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "x-amz-request-id: XXX[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "Date: Thu, 02 Apr 2015 01:49:10 GMT[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "Content-Type: application/xml[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "Transfer-Encoding: chunked[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "Server: AmazonS3[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "fe[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "<?xml version="1.0" encoding="UTF-8"?>[\n]"
15/04/02 01:49:09 DEBUG http.wire: << "<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Name>s3atest</Name><Prefix></Prefix><Marker></Marker><MaxKeys>1</MaxKeys><Delimiter>/</Delimiter><IsTruncated>false</IsTruncated></ListBucketResult>"
15/04/02 01:49:09 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "0[\r][\n]"
15/04/02 01:49:09 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:09 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3atest/
ls: `s3a://s3atest/': No such file or directory
{code}

2. create a directory, but get an exception

{code}
# hadoop-2.7.0-SNAPSHOT/bin/hdfs dfs -Dfs.s3a.access.key=ACCESS_KEY -Dfs.s3a.secret.key=SECRET_KEY -mkdir s3a://s3atest/root
15/04/02 01:49:41 DEBUG http.wire: >> "HEAD / HTTP/1.1[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Host: s3atest.s3.amazonaws.com[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Authorization: AWS XXX=[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Date: Thu, 02 Apr 2015 01:49:41 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "User-Agent: aws-sdk-java/1.7.4 Linux/3.10.0-123.8.1.el7.centos.plus.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/24.75-b04/1.7.0_75[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Connection: Keep-Alive[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "HTTP/1.1 200 OK[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-id-2: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-request-id: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Date: Thu, 02 Apr 2015 01:49:42 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Content-Type: application/xml[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Transfer-Encoding: chunked[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Server: AmazonS3[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/root (root)
15/04/02 01:49:41 DEBUG http.wire: >> "HEAD /root HTTP/1.1[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Host: s3atest.s3.amazonaws.com[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Authorization: AWS XXX=[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Date: Thu, 02 Apr 2015 01:49:41 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "User-Agent: aws-sdk-java/1.7.4 Linux/3.10.0-123.8.1.el7.centos.plus.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/24.75-b04/1.7.0_75[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Connection: Keep-Alive[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "HTTP/1.1 404 Not Found[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-request-id: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-id-2: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Content-Type: application/xml[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Transfer-Encoding: chunked[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Date: Thu, 02 Apr 2015 01:49:41 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Server: AmazonS3[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "HEAD /root/ HTTP/1.1[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Host: s3atest.s3.amazonaws.com[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Authorization: AWS XXX=[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Date: Thu, 02 Apr 2015 01:49:41 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "User-Agent: aws-sdk-java/1.7.4 Linux/3.10.0-123.8.1.el7.centos.plus.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/24.75-b04/1.7.0_75[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Connection: Keep-Alive[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "HTTP/1.1 404 Not Found[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-request-id: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-id-2: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Content-Type: application/xml[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Transfer-Encoding: chunked[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Date: Thu, 02 Apr 2015 01:49:41 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Server: AmazonS3[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "GET /?delimiter=%2F&max-keys=1&prefix=root%2F HTTP/1.1[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Host: s3atest.s3.amazonaws.com[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Authorization: AWS XXX=[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Date: Thu, 02 Apr 2015 01:49:41 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "User-Agent: aws-sdk-java/1.7.4 Linux/3.10.0-123.8.1.el7.centos.plus.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/24.75-b04/1.7.0_75[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Connection: Keep-Alive[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "HTTP/1.1 200 OK[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-id-2: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-request-id: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Date: Thu, 02 Apr 2015 01:49:42 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Content-Type: application/xml[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Transfer-Encoding: chunked[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Server: AmazonS3[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "103[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "<?xml version="1.0" encoding="UTF-8"?>[\n]"
15/04/02 01:49:41 DEBUG http.wire: << "<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Name>s3atest</Name><Prefix>root/</Prefix><Marker></Marker><MaxKeys>1</MaxKeys><Delimiter>/</Delimiter><IsTruncated>false</IsTruncated></ListBucketResult>"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "0[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3atest/root
15/04/02 01:49:41 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/ ()
15/04/02 01:49:41 DEBUG http.wire: >> "GET /?delimiter=%2F&max-keys=1&prefix= HTTP/1.1[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Host: s3atest.s3.amazonaws.com[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Authorization: AWS XXX=[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Date: Thu, 02 Apr 2015 01:49:41 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "User-Agent: aws-sdk-java/1.7.4 Linux/3.10.0-123.8.1.el7.centos.plus.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/24.75-b04/1.7.0_75[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Content-Type: application/x-www-form-urlencoded; charset=utf-8[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "Connection: Keep-Alive[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: >> "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "HTTP/1.1 200 OK[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-id-2: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "x-amz-request-id: XXX[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Date: Thu, 02 Apr 2015 01:49:42 GMT[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Content-Type: application/xml[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Transfer-Encoding: chunked[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "Server: AmazonS3[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "fe[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "<?xml version="1.0" encoding="UTF-8"?>[\n]"
15/04/02 01:49:41 DEBUG http.wire: << "<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Name>s3atest</Name><Prefix></Prefix><Marker></Marker><MaxKeys>1</MaxKeys><Delimiter>/</Delimiter><IsTruncated>false</IsTruncated></ListBucketResult>"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "0[\r][\n]"
15/04/02 01:49:41 DEBUG http.wire: << "[\r][\n]"
15/04/02 01:49:41 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3atest/
mkdir: `s3a://s3atest/root': No such file or directory
{code}

The fix for the unit test was wrong, but the fix for this problem in _S3AFileSystem#getFileStatus_ is still valid., This is the patch to fix _S3AFileSystem#getFileStatus_. The dedicated part to process a root directory was added, which is entered only when key.isEmpty() == true., This is the patch to fix the unit test, _AbstractContractRootDirectoryTest_.

Changes are:
# setup() prepares an empty directory
# assertion was added to make sure the root dir is empty in testRmEmptyRootDirNonRecursive()
# teardown() does nothing, Patches are verified as follows.

1. run TestS3AContractRootDir to see it succeeds

{code}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.855 sec - in org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10.341 s
[INFO] Finished at: 2015-04-02T05:41:48+00:00
[INFO] Final Memory: 28M/407M
[INFO] ------------------------------------------------------------------------
{code}

2. apply the test patch(003-2), and run TestS3AContractRootDir

{code}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir
Tests run: 5, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 21.296 sec <<< FAILURE! - in org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir
testRmEmptyRootDirNonRecursive(org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir)  Time elapsed: 4.608 sec  <<< ERROR!
java.io.FileNotFoundException: No such file or directory: /
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:996)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:464)
	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testRmEmptyRootDirNonRecursive(AbstractContractRootDirectoryTest.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

testRmRootRecursive(org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir)  Time elapsed: 2.509 sec  <<< ERROR!
java.io.FileNotFoundException: No such file or directory: /
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:996)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:464)
	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testRmRootRecursive(AbstractContractRootDirectoryTest.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

testCreateFileOverRoot(org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir)  Time elapsed: 3.006 sec  <<< ERROR!
com.amazonaws.services.s3.model.AmazonS3Exception: Status Code: 400, AWS Service: Amazon S3, AWS Request ID: 2B352694A5577C62, AWS Error Code: MalformedXML, AWS Error Message: The XML you provided was not well-formed or did not validate against our published schema
	at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:798)
	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:421)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)
	at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1393)
	at com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:108)
	at com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:100)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.upload(UploadMonitor.java:192)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:150)
	at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:50)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

testRmNonEmptyRootDirNonRecursive(org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir)  Time elapsed: 3.786 sec  <<< ERROR!
java.io.FileNotFoundException: No such file or directory: /
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:996)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:77)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:464)
	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testRmNonEmptyRootDirNonRecursive(AbstractContractRootDirectoryTest.java:96)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


Results :

Tests in error: 
  TestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmEmptyRootDirNonRecursive:70 » FileNotFound
  TestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmRootRecursive:104 » FileNotFound
  TestS3AContractRootDir.testCreateFileOverRoot » AmazonS3 Status Code: 400, AWS...
  TestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmNonEmptyRootDirNonRecursive:96 » FileNotFound

Tests run: 5, Failures: 0, Errors: 4, Skipped: 0
[INFO] Total time: 26.583 s
[INFO] Finished at: 2015-04-02T05:57:03+00:00
[INFO] Final Memory: 28M/407M
[INFO] ------------------------------------------------------------------------
{code}

*Note:* you may see a different error(??java.lang.AssertionError: root directory is not empty expected:<0> but was:<1>??) if you run this in US Standard region

3. apply the patch(003-1), and run TestS3AContractRootDir

{code}
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.857 sec - in org.apache.hadoop.fs.contract.s3a.TestS3AContractRootDir

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 27.970 s
[INFO] Finished at: 2015-04-02T06:01:00+00:00
[INFO] Final Memory: 29M/407M
[INFO] ------------------------------------------------------------------------
{code}, _mkdir_ and _ls_ worked as expected with the fix.

{code}
# hadoop-2.7.0-SNAPSHOT/bin/hdfs dfs -Dfs.s3a.access.key=ACCESS_KEY -Dfs.s3a.secret.key=SECRET_KEY -ls s3a://s3atest/
15/04/02 06:52:55 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/ ()
15/04/02 06:52:55 DEBUG s3a.S3AFileSystem: s3a://s3atest/ is empty? true
15/04/02 06:52:55 DEBUG s3a.S3AFileSystem: List status for path: s3a://s3atest/
15/04/02 06:52:55 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/ ()
15/04/02 06:52:55 DEBUG s3a.S3AFileSystem: s3a://s3atest/ is empty? true
15/04/02 06:52:55 DEBUG s3a.S3AFileSystem: listStatus: doing listObjects for directory 
# hadoop-2.7.0-SNAPSHOT/bin/hdfs dfs -Dfs.s3a.access.key=ACCESS_KEY -Dfs.s3a.secret.key=SECRET_KEY -mkdir s3a://s3atest/root
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/root (root)
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3atest/root
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/ ()
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: s3a://s3atest/ is empty? true
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Making directory: s3a://s3atest/root
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/root (root)
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3atest/root
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/root (root)
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Not Found: s3a://s3atest/root
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/ ()
15/04/02 06:53:20 DEBUG s3a.S3AFileSystem: s3a://s3atest/ is empty? true
# hadoop-2.7.0-SNAPSHOT/bin/hdfs dfs -Dfs.s3a.access.key=ACCESS_KEY -Dfs.s3a.secret.key=SECRET_KEY -ls s3a://s3atest/
15/04/02 06:53:26 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/ ()
15/04/02 06:53:26 DEBUG s3a.S3AFileSystem: s3a://s3atest/ is empty? false
15/04/02 06:53:26 DEBUG s3a.S3AFileSystem: List status for path: s3a://s3atest/
15/04/02 06:53:26 DEBUG s3a.S3AFileSystem: Getting path status for s3a://s3atest/ ()
15/04/02 06:53:26 DEBUG s3a.S3AFileSystem: s3a://s3atest/ is empty? false
15/04/02 06:53:26 DEBUG s3a.S3AFileSystem: listStatus: doing listObjects for directory 
15/04/02 06:53:26 DEBUG s3a.S3AFileSystem: Adding: rd: s3a://s3atest/root
Found 1 items
drwxrwxrwx   -          0 1970-01-01 00:00 s3a://s3atest/root 
{code}

The created directory didn't become visible immediately. But the successive _ls_ showed it was successful., I'm resolving this as a duplicate of HADOOP-11918.  Reading through the history on both issues, it appears that there is more recent activity on HADOOP-11918, and the patch there is closer to acceptance.  (If I'm mistaken, then please feel free to reopen this.)]