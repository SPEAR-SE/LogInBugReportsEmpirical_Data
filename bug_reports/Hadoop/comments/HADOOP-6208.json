[Test app that can reproduce truncated files in S3FS, This is the first time I am looking at S3FS. It is essentially a thin client managed FS layer over S3.. it would pretty hard to avoid such inconsistencies especially S3 itself can return stale info. isn't there an option for S3 client to get the latest copy (thinking Dynamo-like key-value reads)?

Between options (1) to (5) : (2) is probably the best. It is a bit analogous to what HDFS does : when close() completes successfully, it implies all the blocks have at least one replica reported.

Regd (6)-(7) :  a loosely managed FS will always such issues. I am not sure how much is S3FS is used in production, I would be curious to know.

For those familiar with HDFS: S3FS is a broadly similar where all the metadata is manipulated by S3FS client rather than a central server (e.g. rename moves metadata for each file under the tree to a new location). While writing, each block is written locally and stored as an S3Object when full., Thanks for the quick reply.  You're right; any use of S3FS is going to run into inconsistencies, no matter how clever the client is.  What I'm looking for is some way to make any failures due to inconsistency more visible, and less likely to lose data.  Of the ideas I threw out, I think I also like #2 the best - that way users like us who do a ton of S3 I/O can pay the extra performance hit to verify writes without impacting everyone.  I'll bounce this idea around with teammates and see if I can put together a patch., Regd option #2 : If one client gets the latest value, does it imply all subsequent reads (from any client) will? Otherwise one read might provide latest value, but very next read might return older value since it might read from a different replica (I don't know much about S3 internals). The client needs to somehow make sure all the replicas got the update., Option 2 seems like a reasonable approach. Since there is a single writer, do we only need to poll for consistency on file close?

Also, do we need to make this configurable? Always doing the polling seems acceptable since it guarantees correctness (at least for a given client).
, To answer the question about subsequent reads, no, unfortunately, getting one read of the latest version of an object does not guarantee that that object is fully propagated and that you won't get any more stale reads.  I ran a quick test yesterday to see how likely this scenario is (stale read after fresh read from the same client) and I was able to find one instance of a good read followed by a bad read over ~10,000 file writes (about 420,000 total reads).  For comparison, I found 7 instances in the same test where the very first read after a write was stale, but subsequent reads were all of the newest version.  The end result here is that with the current behavior, we'd have 7 truncated files of 10,000 written; with a patch to verify writes, we'd only get 1 truncated file out of 10,000.  Since the patch is simple and the verification step is cheap, I think it's worth the tradeoff; I'll let my script run a little longer and see if I can gather more significant data.

Tom: I think the safest thing to do is to verify writes (poll for consistency) after each INode write.  Looking through the code, this happens in 4 places: on directory creation, on file close, on file flush, and on rename.  Verifying file close and flush prevents truncated files as in this issue; verifying renames prevents a situation where a file can be renamed, then data can be written to the old filename (which should no longer exist), resulting in two copies of the file pointing to the same blocks (sort of like a weird hard link).  Verifying directory creates just prevents errors where a newly created directory doesn't show up for a little while; I don't think there are any data-loss issues possible in that scenario.

I have a patch and unit test for verifying in these places; I have a bit more testing to do, but I should be able to submit it to JIRA later today., Second iteration of consistency testing - this one keeps polling even after the file is consistent to see if it stays consistent, First attempt at a fix in this patch, Attempting to reupload patch - I don't think I hit the right sequence of buttons the first time around :-), +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12420945/HADOOP-6208.patch
  against trunk revision 820094.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 9 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/68/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/68/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/68/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/68/console

This message is automatically generated., This looks great.

> I was able to find one instance of a good read followed by a bad read over ~10,000 file writes (about 420,000 total reads).

One possibility is to strengthen the requirement to n consecutive good reads rather than just one, but that imposes extra S3 calls. Does the current patch bring the chance of a bad read down to an acceptable level for you?

> note that there's no sleep between polls; this is to avoid slowing down the unit tests, and assuming that the round-trip network latency is enough of a delay

Do you know how much of a delay this imposes in practice? I wonder whether we should have a delay in order to be nice to S3. You could do this by adding a private configuration parameter (e.g. fs.s3.verifyPollInterval) for the delay, which the tests set to zero.

Also, do you know about Jets3tS3FileSystemContractTest? It's a unit test that you run manually to test against S3, using your own credentials (in src/test/core-site.xml). It's worth running this as a regression test.

A couple of minor nits:

* I know some other classes in Hadoop use primes in their hash code calculations, but it isn't really necessary. Or-ing the id and length is probably sufficient. See HDFS-288.
* We generally put single line blocks in curly braces (e.g. line 76 of EventuallyConsistentInMemoryFileSystemStore).
, > One possibility is to strengthen the requirement to n consecutive good reads rather than just one, but that imposes extra S3 calls. Does the current patch bring the chance of a bad read down to an acceptable level for you?

Not sure yet; it's definitely an improvement.  We're just about to push this patch to our own cluster; I'll update the ticket with any information we gather.

> Do you know how much of a delay this imposes in practice? I wonder whether we should have a delay in order to be nice to S3. You could do this by adding a private configuration parameter (e.g. fs.s3.verifyPollInterval) for the delay, which the tests set to zero.

Done

> Also, do you know about Jets3tS3FileSystemContractTest? It's a unit test that you run manually to test against S3, using your own credentials (in src/test/core-site.xml). It's worth running this as a regression test.

I had seen it before but didn't run it with this patch.  Just ran it, everything looks good.

> I know some other classes in Hadoop use primes in their hash code calculations, but it isn't really necessary. Or-ing the id and length is probably sufficient. See HDFS-288.

Done

> We generally put single line blocks in curly braces (e.g. line 76 of EventuallyConsistentInMemoryFileSystemStore).

Done.  Thanks for the very prompt review, I think I've confused JIRA - can anyone direct me to the magic button to submit my new patch to Hudson?  Thx..., It's because you weren't assigned the issue (I've just added you to the JIRA list of contributors and assigned it to you). You should be able to cancel the patch then submit again to get Hudson to run., submitting new patch, +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421042/HADOOP-6208.patch
  against trunk revision 820094.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 12 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/70/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/70/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/70/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/70/console

This message is automatically generated., Changing to open while waiting for an update from Bradley., # what's the status of this?
# which s3 endpoint was being used to observe the problem -and to test that the changes worked? US default or something else?, Yes, an update would be good.

I suspect this is actually close-able now., I'm going to close this as a WONTFIX due to the imminent demise of the original S3FS]