[That problem happened in Hadoop 0.18, built on July 2.

The above entries continue flowing into the name node's log file.
The name node stopped responding
, 
Here is the stack trace of the infinite loop:

"org.apache.hadoop.dfs.LeaseManager$Monitor@228b677f" daemon prio=10 tid=0x0000002e3c516400 nid=0x763a runnable [0x000000004163e000..0x000000004163ebb0]
   java.lang.Thread.State: RUNNABLE
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:260)
        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:202)
        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:272)
        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:276)
        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:122)
        - locked <0x0000002b011b7068> (a java.io.OutputStreamWriter)
        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:212)
        at org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:57)
        at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:315)
        at org.apache.log4j.DailyRollingFileAppender.subAppend(DailyRollingFileAppender.java:358)
        at org.apache.log4j.WriterAppender.append(WriterAppender.java:159)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:230)
        - locked <0x0000002aa2d019d0> (a org.apache.log4j.DailyRollingFileAppender)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:65)
        at org.apache.log4j.Category.callAppenders(Category.java:203)
        - locked <0x0000002aa850d090> (a org.apache.log4j.spi.RootLogger)
        at org.apache.log4j.Category.forcedLog(Category.java:388)
        at org.apache.log4j.Category.log(Category.java:853)
        at org.apache.commons.logging.impl.Log4JLogger.info(Log4JLogger.java:133)
        at org.apache.hadoop.dfs.FSNamesystem.internalReleaseLease(FSNamesystem.java:1637)
        at org.apache.hadoop.dfs.LeaseManager$Monitor.run(LeaseManager.java:353)
        - locked <0x0000002aac9dbe78> (a org.apache.hadoop.dfs.LeaseManager)
        - locked <0x0000002ab1fe7760> (a org.apache.hadoop.dfs.FSNamesystem)
        at java.lang.Thread.run(Thread.java:619)

The monitor thread tried to release leases but failed, and loop around infinitely.
, This looks similar to HADOOP-3724. In that JIRA, we were using an old hadoop-0.18, not sure on which day it was built. But after moving to latest 0.18, I guess around 11 July, we didnt have any problems. Few JIRAs were checked in which fixed it. Could this the same case too?, Dhruba: What do you know about this? In particular, is this related to the infinite loop in Hadoop-3724 that you recently marked as resolved?

Does the lease monitor inner loop leave a faulty record at the top of the sorted leases map that is repeatedly tested?


, This is related to HADOOP-3724. Marking this for 0.18. This leads to a corrupted image, which results in namenode failure. , The issue would be solved by HADOOP-3724]