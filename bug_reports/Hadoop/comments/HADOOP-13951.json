[I think the right answer here is to 

# Stop having the jenkins git plugin do clean-on-checkout
# Add a step at the start of the job that does the find/chmod
# Add a step to run git-clean via cli after that but before the yetus test-patch invocation

alternatively, we could move the find/chmod bit to a post-build-action. I think that would handle the timeout case correctly, but I don't know about abort.

thoughts? ping [~aw], bq. , we could move the find/chmod bit to a post-build-action. I think that would handle the timeout case correctly, but I don't know about abort.

This is probably the best strategy.  Hadoop should probably have something that also shows which files have the wrong permissions in it's post too. I seem to recall that all builds gets the post action, regardless of the end result of the build., bq.  Hadoop should probably have something that also shows which files have the wrong permissions in it's post too.

output to console sufficient for this? or is an archived artifact needed?, well, really, unless it's part of the JIRA output, no one is going to look at it.  (Most committers ignore the qbt output, given how many errors it points out that get ignored. :( ), lemme see if there's an easy way to have a post-build action post to jira as well., just bounce through the yetus libraries., Updated the following jobs to do the find/chmod as a post-build step:

* PreCommit-HADOOP-Build
* PreCommit-HADOOP-OSX
* PreCommit-HDFS-Build
* PreCommit-MAPREDUCE-Build
* PreCommit-YARN-Build

Haven't updated them to do the "notice the problem and say something" part. Haven't updated the nightly builds either, for that matter.

(N.B. that this issue still isn't assigned to me because I don't know when I'll have time to do the above steps; anyone else with the time to work through the build changes should feel free to step up.), Thanks Sean. I think this is probably HDFS-only, since it's some dfs metadata directory.

I also see this at the end of some builds:

{noformat}
# See HADOOP-13951
find "${WORKSPACE}" -name target | xargs chmod -R u+w
[PreCommit-HDFS-Build] $ /bin/bash /tmp/hudson1505512985405565527.sh
find: `/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/hadoop-hdfs-project/hadoop-hdfs/target/test/data/1/dfs/data/data1/current': Permission denied
chmod: cannot read directory ‘/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/hadoop-hdfs-project/hadoop-hdfs/target/test/data/1/dfs/data/data1/current’: Permission denied
{noformat}

I was recursively chmod'ing the entire directory in my clean script, I'm going to modify the HDFS precommit to do that., FYI that I also parameterized and renamed my clean job, works with varying success when I tried to clean H6 and H0:

https://builds.apache.org/view/H-L/view/Hadoop/job/hadoop-clean-hdfs-workspace/]