[This issue is impossible to reproduce without Kerberos enabled, so I'm going to first upload the script I used to exercise this issue. This is obviously specific to my environment, but the gist should be pretty clear. You'll see that two separate {{Configuration}} objects are used to create two separate RPC connections, neither of which contains the principal name for the other service. Without the fix, the second RPC connection (to the YARN RM) will fail with a "Failed to specify server's Kerberos principal name" error, even though I did specify the principal name in the {{Configuration}} object for that connection. With the fix, both connections succeed., Here's a patch containing the actual fix for this issue., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12610224/HADOOP-10070.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3247//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3247//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12610224/HADOOP-10070.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3248//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3248//console

This message is automatically generated., As previously mentioned, no tests are included since doing so would require Kerberos to be enabled. I tested this manually using the TestKerberosClient program also attached to this JIRA., FYI, with hadoop-minikdc you can fire a kdc on the fly from a testcase. The hadoop-auth testcases are already doing this., +1 to the approach.  I wonder if it would make sense to remove the other "cached" conf values that are likely only there because the user provided conf to make the connection is/was not being used?  I always wondered why a small subset of values are being saved off., Good thinking, Daryn. How does this look to you?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12628146/HADOOP-10070.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3555//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3555//console

This message is automatically generated., Will this completely fix the issue?  Ex. Client1 uses a conf with a principal1 and establishes a connection.  Client2 uses an equivalent conf with a different principal.  Unless the connection from client1 has closed, won't client2 reuse the open connection?  If correct, is this valid behavior in your case?

Truly fixing the problem requires a calculating equivalence of all keys that affect the ipc client connection.  Ideally the connectionId would consider all "ipc.client.*" kvps predicated on the client only using values in that range which may not be true.  Extracting/calculating equivalence for just the subset is likely not performant...  Even serializing those keys back into another conf key as a cached value would help but it'd need to updated if one of the cached values changes which is back to re-checking all the keys.  Sigh., bq. Ex. Client1 uses a conf with a principal1 and establishes a connection. Client2 uses an equivalent conf with a different principal.

I don't follow this example. Could you perhaps be a bit more explicit? Are you suggesting in the above that "principal1" is the client or server principal? Note that connections are uniquely identified by (remote address, protocol, client UGI).

bq. Unless the connection from client1 has closed, won't client2 reuse the open connection? If correct, is this valid behavior in your case?

Take a look at the example repro case I provided in TestKerberosClient.java. With this fix, one need not close the first-opened connection before opening the second. The issue I'm trying to address is that, regardless of whether or not the first client is closed, the second client will use the Configuration of the first, which is clearly incorrect., [~daryn] - any comments on the above? I'd really like to get this resolved ASAP., +1 It's a good change, but I'm confused how it solves this problem.  The ConnectionId is based on addr, so the yarn connection should have already created a new connection and used the principal from the second config - although it would have continued using that principal for future connections even if it changed?  I think your patch solves the latter., Yes, that's correct - this fixes the latter issue.

I'm going to go ahead and commit this momentarily based on Daryn's +1., SUCCESS: Integrated in Hadoop-trunk-Commit #5210 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5210/])
HADOOP-10070. RPC client doesn't use per-connection conf to determine server's expected Kerberos principal name. Contributed by Aaron T. Myers. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1570776)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ClientCache.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #489 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/489/])
HADOOP-10070. RPC client doesn't use per-connection conf to determine server's expected Kerberos principal name. Contributed by Aaron T. Myers. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1570776)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ClientCache.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1681 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1681/])
HADOOP-10070. RPC client doesn't use per-connection conf to determine server's expected Kerberos principal name. Contributed by Aaron T. Myers. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1570776)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ClientCache.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1706 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1706/])
HADOOP-10070. RPC client doesn't use per-connection conf to determine server's expected Kerberos principal name. Contributed by Aaron T. Myers. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1570776)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ClientCache.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java
, Noticed this isn't resolved., I committed this to branch-2.4 to resolve a test failure after committing HADOOP-10221 (as investigated by [~benoyantony] [here|https://issues.apache.org/jira/browse/HADOOP-10221?focusedCommentId=13941170&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13941170]).

The fix version was already set to 2.4.]