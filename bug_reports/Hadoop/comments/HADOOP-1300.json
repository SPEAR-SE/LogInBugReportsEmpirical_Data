[The goal while deleting excess replicas should be to maximize the number of unique racks  on which replicas will remain.

For example, if a block has 5 replicas on racks R1, R1, R2, R2, R3 and the target replication factor is 3, then we can delete one of the replicas from rack R1 and another one from rack R2. This leaves us with replicas R1, R2 and R3.

If there are multiple replicas on a rack and we are supposed to delete a replica from that rack, then we select the replica from that datanode that has the least amount of available disk space.

, bq. The goal while deleting excess replicas should be to maximize the number of unique racks on which replicas will remain.

If this is for fault tolerance, 
is there any reasons to adopt different policies for allocation and deletion?

Is it because this event happens less often so we're willing to pay the extra overhead?, This patch deletes excess replicas in the following priority order:
1. Keep as many racks as possible
2. delete a replica with the least free space, HADOOP-1269 moves chooseTargets outside the FSNamesystem global lock. Is it possible to adopt a similar approach to chooseExcessReplicates too? This might be helpful because this algorithm might be heavy on CPU usage, especially in the (rare?) case when there are many excess replicas., I have a slightly different approach that uses the cluster map to delete excess replicas. Should work correctly, but probably needs more thought.

1. If all nodes are at the same height, then
    -- if any two nodes have the same parent, select one of the
       node (based on free space) as a candidate for deletion.
    -- replace each node by its parent node
2. If all nodes are not at the same height, then
    -- select node with max height, repce it with its parent node
    -- go to step 1


, > This might be helpful because this algorithm might be heavy on CPU usage
I do not think this algorithm is CPU heavy. Basically it scans all replicas of a block for at most 3 times. When there are excess replicas,  the most common case is 4 or 5 replicas per block. 

> is there any reasons to adopt different policies for allocation and deletion?
For allocation, where to place a replica has an effect on performance. But for deletion, the cost of deleting any replica is the same., > > is there any reasons to adopt different policies for allocation and deletion?
> For allocation, where to place a replica has an effect on performance. But for deletion, the cost of deleting any replica is the same.

I meant when replication is set to 4 or higher, are we "maximizing the number of unique racks" for the rest of the blocks?

Talking with Hairong, now I understand that we don't do this in allocation/replication to simplify the code.
Thanks! , +1 for the code. Looks good.

I still think that the algorithm is CPU-heavy in the case when there are many replicas to delete. But I do not have an alternative lighter-wright algorithm. HDFS allows an application to set the replication factor of a file. Even if dfs.replication.max is set to something reasonable (e.g. 40), the excess replica deletion could potentially need to get rid of 30 odd replicas. In this case, it could eat considerable CPU. This was the reason why I suggested that we can move this method outside the FSNamesystem lock.
, First of all I think removing 30 replicas is not a common case. Even there is such a case, my algorithm should not be more CPU heavier than the current algorithm. The only overhead is to split all replicas into two sets, but this is done once for all excessive replicas. When selecting a replica to remove, my algorithm scans only one of the two sets while the current algorithm scans all replicas. So in average my algorithm should cut the scanning cost by half. 

Alternatively I can build a heap to support replica removal when the number of excessive replicas is big. But since it is not a common case, I am not sure if it is worth doing. Please let me know what you think., +1 Sounds good to me. Thanks., +0, new Findbugs warnings

http://issues.apache.org/jira/secure/attachment/12358913/excessDel.patch
applied and successfully tested against trunk revision r546297,
but there appear to be new Findbugs warnings introduced by this patch.

New Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/267/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/267/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/267/console, The new patch fixed the performance bug found by findBug, +1

http://issues.apache.org/jira/secure/attachment/12359536/excessDel1.patch applied and successfully tested against trunk revision r546320.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/273/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/273/console, Why is this bug assigned to me? It doesn't seem relevant to HBase, which is the only code I am working on., I just committed this.  Thanks, Hairong!, Integrated in Hadoop-Nightly #123 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/123/])]