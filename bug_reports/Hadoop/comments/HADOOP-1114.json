[Here's my proposed patch:

--- bin/hadoop.orig
+++ bin/hadoop
@@ -74,7 +74,8 @@
 fi
 
 # CLASSPATH initially contains $HADOOP_CONF_DIR
-CLASSPATH="${HADOOP_CONF_DIR}"
+# respect previously set CLASSPATH
+CLASSPATH="$CLASSPATH:${HADOOP_CONF_DIR}"
 CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
 
 # for developers, add Hadoop classes to CLASSPATH, -1, because the patch command could not apply the latest attachment http://issues.apache.org as a patch to trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/525505. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch, Should this check to see if CLASSPATH has been previously set before including it in the new CLASSPATH? , I don't think it matters; if you set your classpath to ::::::::::/somejar.jar I think java will still do the right thing.

, Yes, I just tried it and this change works when the user's classpath variable is unset. 

I've just committed this. Thanks Michael!, Integrated in Hadoop-Nightly #53 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/53/), Sorry.  I'm coming to this rather late.  But, for what it's worth, the prior behavior was intentional.  The CLASSPATH environment variable is fragile to use.  Folks can end up with lots of crazy stuff on it (e.g., conflicting versions of libraries) that can break things in confusing ways.  Thus it's generally better to not rely on it.

Sun subtly discourages using the CLASSPATH environment variable:

http://java.sun.com/j2se/1.5.0/docs/tooldocs/windows/classpath.html#env%20var

Tomcat's startup scripts erase any pre-existing CLASSPATH values:

http://svn.apache.org/repos/asf/tomcat/container/tc5.5.x/catalina/src/bin/setclasspath.sh

However, Ant does respect it:

http://svn.apache.org/viewvc/ant/core/trunk/src/script/ant?view=markup

To add things to Hadoop's classpath one can simply add files to Hadoop's lib directory (as folks typically add junit's jar to ant's lib).  If that's insufficient, I'd rather we add a HADOOP_CLASSPATH environment variable than use the fragile, global CLASSPATH.

What do others think?, Currently, the hadoop runjar commands takes a single user jar as argument.  However, our jar depends on other (custom) java libraries.  So we get around this by this CLASSPATH hack.  We could drop dependent jars into the hadoop lib file, but I'd rather not mix shipped hadoop code with our user code.

We could package all the dependent code into the same jar, but that seems unnecessary.  A better alternative might be to set a CLASSPATH in the jar manifest, but I haven't thought very much about how that would work.

So unless there is another better yet simple method, we need some way to insert jar dependencies into hadoop.  Your example of Tomcat is a bit different, since there is a well-defined mechanism for getting dependent jars into servlet containers (using the WEB-INF/lib directory). 

It doesn't seem very worthwhile to rename CLASSPATH to HADOOP_CLASSPATH.  
, > We could package all the dependent code into the same jar, but that seems unnecessary.

Why is this unnecessary?  That's the intended mechanism.  Job jars may include a lib/ directory with other jar files, just like a war file.  Packaging things this way makes it much easier to update your code without restarting Hadoop daemons.
, We have a separate mechanism to distribute generic software; this is what we currently use to distribute jar dependencies.  Currently our hadoop jars also depend on non-java code which can't be distributed via jar.  So we could package all our java code into a single jar (we would just need to write a combine-jar tool).  But this wouldn't solve our problem, because some of our code wouldn't go in the jar and would still have to be distributed through a different mechanism.  I'd rather be supporting one software distribution system than two.
, > Currently our hadoop jars also depend on non-java code which can't be distributed via jar.

Job jar files are unpacked in the directory where the job jvm is run, so non-java data and code in the jar file can be easily accessed via relative paths.

> we would just need to write a combine-jar tool

Ant has good tools for constructing jar files.

In any case, if you like to install all your software in a separate directory, and want parts of it to appear on the classpath of the Hadoop daemons, that's fine.  I just think our scripts should not rely on the overused CLASSPATH environment variable, but would be less fragile if they use HADOOP_CLASSPATH.
, While I accept that using CLASSPATH for dependencies is fragile and hacky, I'm not sure why HADOOP_CLASSPATH is any different.  It seems both environment variables are equally abusable.  Using CLASSPATH is much more intuitive than the nonstandard HADOOP_CLASSPATH.  The only problem that HADOOP_CLASSPATH seems to solve is that a caller script might leave inherited garbage in CLASSPATH before adding more jars.  But that should be the responsibility of the caller script, I think., > I accept that using CLASSPATH for dependencies is fragile and hacky, I'm not sure why HADOOP_CLASSPATH is any different.

It's a bit less fragile, since it won't conflict with every other Java application in the world, only with other Hadoop-based applications.  That's all.  For example, I've seen Windows installers that set CLASSPATH to include lots of stuff that we wouldn't want on Hadoop's class path., I would vote for HADOOP_CLASSPATH over plain CLASSPATH. This solves Michael's problem, while avoiding hard-to-diagnose support issues that might arise in the future from junk on the standard CLASSPATH., Here's a patch that implements HADOOP_CLASSPATH.

Michael, is this acceptable to you?, Yes, that works for me., I just committed this., -1, could not apply patch.

The patch command could not apply the latest attachment http://issues.apache.org/jira/secure/attachment/12355371/HADOOP-1114.patch as a patch to trunk revision r527711.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/30/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/30/console

Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., Integrated in Hadoop-Nightly #55 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/55/)]