[Works for me, although the paths need to renamed a bit.

If I unpack the attached, and move the directory named 'ndfs' to be /tmp/hadoop/dfs, then I can find the file /user/dcutting/foo/test.
, As I understand the failure, we brought up a new nameserver on an old cluster (so no file information).  The nameserver saw all the blocks of the old FS and decided they must all belong to deleted files.  It then proceeded to remove them!!

This is bad.  We should have enough interlocks in place that in this situation the name server haults and asks for help.  This is not only a risk during upgrades!  Other operator failures could cause huge data lose., I see two cases:

1. A namenode has been misconfigured, and comes up knowing about no files.  When datanodes report their blocks, the namenode tells them those blocks belong to no files, so the datanode removes them.  This is bad.

2. A datanode has been moved to a new cluster, or is rebooted after a long downtime.  It comes up and tells the namenode about the blocks that it has.  The namenode tells it that those blocks belong to no files, so the datanode removes them.  This is good.

So perhaps the thing to fix is that the namenode should not automatically create an empty filesystem if it is misconfigured.  Perhaps instead we should have a separate "format" command that creates an empty filesystem, and one must run this the first time before starting dfs.  How does that sound?  It makes the out-of-box experience a little more awkward, but I think it might be warranted in this case., I'd perhaps request one more feature: tag each block somehow with a GUID related to the DFS filesystem it is associated with.  So, if we're adding a filesystem "format" can assign the GUID to the newly-created image. And a datanode can find out what the GUID is of the nameserver it's working with before it enumerates blocks, merely ignoring any blocks that have the wrong GUID.

This solves the problem of multiple folks writing with different DFS installs to the same drive, as well as the misconfigured namenode problem, above., A format command sounds useful.

GUID and other extensions to block meta data to allow more error checking also sound like a good idea, but I feel like we need to thrash out the use cases  Not sure I follow the intended behavior in bryan's example.  Maybe we should fork that discussion into an enhancement proposal?
, I agree with Eric: a filesystem GUID could be a useful long-term feature, and may help solve this problem, but we need a simpler solution short-term.  I think adding a format command would address this issue, and also still be useful if we later add a filesystem guid., Format is definitely useful.
But I don't think it is reasonable to add GUID or any user information anywhere except
file description on the namenode. Otherwise it will be hard to support this information.
- The problem of different DFS installs writing to the same drive can be solved by introducing
DFS cluster name, and including it as a final component of dfs.name.dir and dfs.data.dir
- DFS probably needs an application version number, which should be compared when the
file system starts uploading information from the check pointed image.
And the conversion method, which if specified would provide conversion between the versions,
and if not the system should refuse loading the image. Sooner or later conversion of data will
become an issue, it would be good to have the mechanism in place by that time.
, The advantage of the GUID over enhancing the configuration is that it prevents anyone from "screwing up" - the defaults will never cause data loss. Without it, even with "format", it's possible for someone to bring up a set of datanodes on a shared set of machines and start clobbering data. With a GUID, this is no longer an accident that can happen. The original issue that spawned this discussion was with this kind of situation - settings in a config file lead to data "cleanup" that wasn't desired. With a GUID, this risk goes away. Without it, especially with default data directories like /tmp/, it's very easy for two different people to clobber each others data by running datanode instances on the same machine(s).

I understand there's a big push back against complexity. But hadoop is a component likely to be used in a lot of situations, by users who might or might not have complete control of their cluster. The DFS layer is supposed to provide data reliability, so it seems appropriate to put in guards against bad end-user behavior/misconfigurations, if it's not going to be a big cost in performance (it shouldn't - what, an extra string during the initial chat between namenode/datanode?), or storage (it shouldn't add more than a few extra bytes to the filename of each block - or a whole GUID subdir, rather than/in addition to the suggested named paths).

An much weaker alternative to prevent only the one worst-case I'm highlighting, would be for a datanode to shutdown with an error if *none* of the blocks on in a datanode's storage directory are from live files in the DFS. I think that is a far less powerful fix, with the only benefit being that it doesn't require changing the behavior of virtually any of the existing code., Sorry Bryan, I misinterpreted GUID as a group/user id.
So, yes a global unique id is for identifying particular data instances of the file system.
It can be generated by "format". And there shouldn't be any substantial overhead to support this.

What I am trying to propose is to use the version number of the file system (as a software application),
hard coded (rather than configured) and accessible via the getVersion() method, as a part of GUID.
The version alone would have been sufficient to avoid the original problem
with data loss, but GUID provides even better consistency.
An additional advantage of versioning is that when the internal data layout for the name
or data nodes is changed, one can provide conversion methods from one version to another.
, Sorry -  yes, I meant a globally-unique identifier, not a group/user id.

Sticking a GUID+version into the namenode metadata sounds helpful. I don't really see a reason to stick the version into the block filename, though. Especially if all "upgrade" related commands are going to be issued by the namenode., Duplicated by HADOOP-19]