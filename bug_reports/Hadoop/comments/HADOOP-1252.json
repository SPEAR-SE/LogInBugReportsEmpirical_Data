[Some points:
1) In the shuffle code, if the map output file was writing data to a disk and that disk has some problem (currently) leading to write failures, the reduce task won't realise it and will keep on fetching/writing, thus, hanging on that (much similar to the issue HADOOP-1246)
2) Checks for disk space sufficiency can be done before trying to copy a map output on the reduces since we know the length of the map output file apriori. There are some catches here though. If multiple entities are using the disk for writing (maps and reduces), then we might hit the problem of insufficient disk space a while down the line (we don't know apriori what the size of map output would be). But in any case this check will not worsen things. If a task hits the problem later we kill it.
3) getLocalPath needs to change to accomodate the (2) point (currently, it computes a hash of the path that we want to read-from/write-to, and maps it to a specific disk; checks mentioned in (2) have to be introduced for both read/write).

Comments? Did I miss out anything?, 4) Better modelling of disk-space where-in the TaskTracker notifies the JobTracker of the amount of local disk available... HADOOP-657, Clarification:
"If multiple entities are using the disk for writing (maps and reduces), then we might hit the problem of insufficient disk space a while down the line (we don't know apriori what the size of map output would be). "
This should read as :
"If multiple entities are using the disk for writing (maps and reduces), then we might hit the problem of insufficient disk space a while down the line (we don't know apriori what the map output size of a currently executing map would be). ", Adopted a simple strategy for this:
Basically, the strategy is to do round-robin for disk selection during "write" and return the first one that satisfies the space requirements.  When we are given a path to "read" we check each disk to locate the path. New APIs have been added in FileUtil called getLocalPathForWrite and getLocalPathForRead. The write API takes an optional size argument. If the disk that a Map task is currently using happens to run out of disk space (for e.g., if two tasks are using the same disk) then the Map task fails as usual. In the case of Reduce tasks a particular instance of a fetch may fail but the task itself won't fail unless it doesn't find any available disk. However, for merges, the reduce task would fail if while spilling the data to the returned disk, the disk runs out of space (similar to the map tasks case).
Also added APIs in MapOutputFile for creating map/shuffle files and equivalent APIs for reading those. Also replaced calls to Configuration.getLocalPath to use the new MapOutputFile APIs wherever possible and in some cases to use the new FileUtil APIs.

Comments?, Changed the patch to have the disk logic in Configuration.java. That way things remain mostly as they already are (in terms of uses of the getLocalPath API). So now Configuration.java has a new class called DiskAllocator that exposes public methods getLocalPathToRead(path), getLocalPathForWrite(path, size), getLocalPathForWrite(path). I haven't touched the part of the code that does caching and downloads jar files from the dfs, etc. This patch is meant to handle the disk failures better for the map outputs, shuffle, merge, and final reduce inputs. Making changes to the caching part to tolerate disk failures can probably be a separate jira issue., +1

http://issues.apache.org/jira/secure/attachment/12356420/1252.patch applied and successfully tested against trunk revision r533013.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/91/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/91/console, This looks good, but DirAllocator should not be nested in Configuration.java.  Rather it should be a standalone utility class in the fs package., Attached is the patch with Doug's comment incorporated., +1

http://issues.apache.org/jira/secure/attachment/12356719/1252.new.patch applied and successfully tested against trunk revision r534624.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/110/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/110/console, Why does LocalDirAllocator need to be accessed from a Configuration?  LocalDirAllocator.java should be dependent on Configuration.java, not vice-versa.  Sorry to be picky about this, but we don't want Configuration to become a grab-bag of application state.  Rather it should just be a way to access configuration parameters.  And LocalDirAllocator is not a configuration parameter, it's a stateful class.  I should have been clearer about this concern earlier..., One more red flag in this patch for me is that the accessor method for the the configuration parameter "mapred.local.dir" is not in the mapred package.  If this is mapred-specific, then it belongs in the mapred package, no?  So if we think that LocalDirAllocator might be useful to non-mapred applications, then it should go in the fs package, but the code that creates a LocalDirAllocator based on the value of "mapred.local.dir" surely belongs in the mapred package., Attached is a patch that addresses (hopefully) Doug's concerns. , Had attached the wrong patch earlier .. attaching the correct one this time .., +1

http://issues.apache.org/jira/secure/attachment/12356859/1252.may7.patch applied and successfully tested against trunk revision r534975.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/121/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/121/console, I just committed this.  Thanks, Devaraj!, Integrated in Hadoop-Nightly #82 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/82/)]