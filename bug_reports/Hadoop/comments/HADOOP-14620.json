[Attaching log file with 403 error.
You can compare String to sign in debug output with string to sign returned by amazon s3 in HEX bytes. Decode using https://aws.amazon.com/code/199, This is odd: we do test against other endpoints, as that's the only way to test v4 auth & so connect to buckets in frankfurt or london.

, Here's the XML options I'm using to talk to the bucket {{s3a://hwdev-steve-london}} hosted in London, which is v4 only. try talking to the London bucket yourself, to see if its the special case of s3 ireland at play.

{code}

  <property>
    <name>fs.s3a.bucket.hwdev-steve-london.endpoint</name>
    <value>${london.endpoint}</value>
  </property>
  
  <property>
    <name>london.endpoint</name>
    <value>s3.eu-west-2.amazonaws.com</value>
  </property>
{code}

, That's extremely interesting.
So 
{code}
hadoop  fs -D fs.s3a.endpoint=s3.eu-west-1.amazonaws.com -ls s3a://dshbasebackup/
{code}
fails for me with 403 as described above

however if I use format as proposed by [~stevel@apache.org]
{code}
hadoop  fs -D fs.s3a.bucket.dshbasebackup.endpoint=s3.eu-west-1.amazonaws.com -ls s3a://dshbasebackup/
{code}
it works as expected. Now, what's the difference between those 2 formats? 
, the sole difference is that in the second case the per-bucket option is being copied in on top of  the default fs.s3a.endpoint option...we've added that precisely so you can define things like different endpoints for different buckets. The default endpoint value in {{fs.s3a.endpoint}} is the one which gets used when there isn't a per bucket override going on,

If you've got the time, stepping through what's going on in S3A would be useful. I suspect maybe there's a default value somewhere in your site configs, or indeed, the core-default one, which is not letting the one you've set on the classpath through. Of course, you know have an immediate fix to your problem...


, Upon further investigation it turned out that setting  fs.s3a.bucket.dshbasebackup.endpoint=s3.eu-west-1.amazonaws.com seems to have no effect as hadoop was going through default endpoint s3.amazonaws.com. I'm on 2.7.3.
However it turns out that using default endpoint actually works for buckets hosted in eu-west-1. And authentication succeeds for them.

Going through region specific endpoint s3.eu-west-1.amazonaws.com fails with 403

, Looks like you are on a fairly old version.  Can you retest with hadoop trunk?  I'm guessing it will work., I've now got a diagnostics tool to help diagnose these config problems: https://github.com/steveloughran/cloudstore/releases

Grab the latest one there, and run it against your bucket. It will look at what values are coming in, attempt to make a basic http/https call first to see what happens, then spins up an FS instance. Please can you use that to see what's up with your config.

closing as a WORKSFORME, Removing 3.1.0 fix-version from all JIRAs which are Invalid / Won't Fix / Duplicate / Cannot Reproduce.]