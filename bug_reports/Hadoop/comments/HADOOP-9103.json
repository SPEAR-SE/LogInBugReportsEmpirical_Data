[I  only change the writeString method  to make it  the same with INode  stringToBytes method,and then the problem disappears., Rather than change the code to not use UTF8, I think we should figure out why the UTF8 writeString function is writing the wrong data. Is "ä¹±ç " the string that causes the problem? I tried to reproduce using this string, but it works fine here.

(I did "hadoop fs -put /etc/issue 'ä¹±ç '", then successfully restarted and catted the file), dear  todd:
   "ä¹±ç " is not the string that causes the problem,it is chinese I don't how do describe, I has place the string that causes the  problem and the test code  in attachments . wish for your reply,  best wishes!, I am try to figure out the problem of UTF8 ~_~, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12523758/TestUTF8AndStringGetBytes.java
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2312//console

This message is automatically generated., I ran TestUTF8AndStringGetBytes.java with the provided ProblemString.txt and got this output:
{code}
49
49
30
30
{code}

This seems expected, however-- the {{String#getBytes}} method returns UTF-8 with the non-BMP code points encoded using UTF-8.  {{hadoop.io.UTF8}} returns UTF-8 with the non-BMP code points encoded as supplementary pairs.

I don't see what any of this has to do with the FSImage or block leases-- since we always encode/decode using {{hadoop.io.UTF8}}, and never anything else, there should be no problem., We had a customer run into a similar issue today, and I figured out the problem -- UTF8.readChars() only handles UTF8 sequences up to 3 bytes. After some spelunking, Colin, Andy, and I came to the following conclusions with regard to characters outside the basic multi-lingual plane. Given the unicode character ðŸ± (CAT-FACE U+1F431 http://www.fileformat.info/info/unicode/char/1f431/index.htm):

- DFSUtil.string2Bytes calls through to Java's encoding, which yields the following 4-byte sequence: f09f90b1
- UTF8.writeString does our own encoding, which yields the following 6-byte sequence: eda0bdedb0b1
- If you read back the 6-byte sequence using new String(bytes, "UTF-8"), it properly decodes back into the cat-face
- If you read back the 6-byte sequence using UTF8.readChars, it also gets back the cat-face
- If you read back the 4-byte (proper UTF8) sequence using UTF8.readChars, you get back some nonsense ("ßÑ°")

It turns out that UTF8.java doesn't actually encode in UTF8 - but rather it encodes in "CESU-8" (thanks Colin for finding this!). Both Java and UTF8.java can properly decode that back into the correct String. However, if you encode a non-BMP character using Java (into proper UTF8 instead of CESU-8), then UTF8.java can't properly decode it.

The issue is that the code in UTF8.readChars doesn't handle any UTF8 sequences longer than 3 bytes. By extending the code to handle longer byte sequences, we were able to fix the issue.

I'll upload a patch with a unit test either later tonight or tomorrow., Todd thank you for your reply,you are so kind., Attached patch should fix this issue. I also amended the javadoc for UTF8 to indicate that it encodes CESU-8, I said:

bq. since we always encode/decode using hadoop.io.UTF8, and never anything else, there should be no problem...

I take this back; looks like we don't always encode/decode using {{hadoop.io.UTF8}}.  D'oh!

bq. Attached patch should fix this issue.

Nice.  Should we test for rejecting 5-byte and 6-byte sequences, since I notice you added some code to do that?

I'm also a little scared by the idea that we have differently-encoded byte[] running around for the same file name strings.  We have to be very careful about this.  Unfortunately, we can't change the decoder to emit real UTF-8 (rather than CESU-8) without making a backwards-incompatible change, since as INode.java reminds us, 

{code}
   *  The name in HdfsFileStatus should keep the same encoding as this.
   *  if this encoding is changed, implicitly getFileInfo and listStatus in
   *  clientProtocol are changed; The decoding at the client
   *  side should change accordingly.
{code}

I also wonder if this means that we need to hunt down all the places not using CESU-8.  Otherwise older clients are just not going to work with astral plane code points, even after this fix... However, we could do that in a separate JIRA, not here., bq. Nice. Should we test for rejecting 5-byte and 6-byte sequences, since I notice you added some code to do that?

I added a test for an invalid sequence. I didn't think it was necessary to add a separate test for a 5-byte sequence, since it would trigger the same "invalid" code path. Got an example hex sequence you think we should test against?

bq. I'm also a little scared by the idea that we have differently-encoded byte[] running around for the same file name strings. We have to be very careful about this. 
bq. ...However, we could do that in a separate JIRA, not here
Agreed. Let's open a separate HDFS JIRA and use this for the Common-side fix. This patch alone was enough to successfully restart a NN which had an open file with a 4-byte codepoint., bq. Got an example hex sequence you think we should test against?

Here is a 5-byte sequence that used to be valid UTF-8, before the 4-byte max rule was put into place:

{{0xF8 0x88 0x80 0x80 0x80}}

Source: http://www.cl.cam.ac.uk/~mgk25/ucs/examples/UTF-8-test.txt, Attached patch includes the test sequence Colin provided above., bq. +   * This is a regression est for HDFS-3307.

test, not est.  Since this jira has moved to HADOOP-9103, update the reference.

{code}
+ * Note that this decodes UTF-8 but actually encodes CESU-8, a variant of
+ * UTF-8: see http://en.wikipedia.org/wiki/CESU-8
{code}
Rather than adding a comment saying "this code is buggy", how about we fix the bug?  Outputting proper 4-byte UTF8 sequences for a given UTF-16 surrogate pair is a much better solution than the current behavior.

So as far as it goes the patch looks good.  I'll look into the surrogate pair stuff., bq. Rather than adding a comment saying "this code is buggy", how about we fix the bug? Outputting proper 4-byte UTF8 sequences for a given UTF-16 surrogate pair is a much better solution than the current behavior.

That would be an incompatible change.  Consider what happens when the server hands back 4-byte UTF-8 sequences to existing DFSClients.  Boom, they fall over., Fixed typo in the test javadoc, bq. Rather than adding a comment saying "this code is buggy", how about we fix the bug? Outputting proper 4-byte UTF8 sequences for a given UTF-16 surrogate pair is a much better solution than the current behavior.

It's not "buggy" it's just "different" (reminds me of something my elementary school teachers used to say). But on a serious note, yea, what Colin said above -- it could break existing clients of the code who are using the old code to _decode_, and were relying on the fact that we are able to round-trip non-BMP characters through UTF8.java.

, bq. It's not "buggy" it's just "different" 

It's buggy if we ever end up writing a CESU-8 bytestream where someone else expects UTF-8.  For example, {{dfs -ls}} writing CESU-8 to stdout wouldn't work properly, because other programs such as {{xterm}} or {{putty}} don't implement the CESU-8 decoding rules.  (This example doesn't happen currently, because the CESU-8 filename is deserialized into a String, where it's interpreted as a surrogate pair, which is then written, and the correct surrogate pair -> UTF-8 encoding happens on the output side.)  Hopefully we haven't overlooked any such existing bugs and nobody accidentally uses UTF8.java in the future.  (At least it's marked @Deprecated.)

Agreed that as long as UTF8.java is the thing that reads the bytestream, we can continue to implement CESU-8 and it can remain partially backwards compatible with previous versions of UTF8.java., bq. Hopefully we haven't overlooked any such existing bugs and nobody accidentally uses UTF8.java in the future. (At least it's marked @Deprecated.)

Right. That's my thinking.

bq. Agreed that as long as UTF8.java is the thing that reads the bytestream, we can continue to implement CESU-8 and it can remain partially backwards compatible with previous versions of UTF8.java.

Java String can also properly decode the CESU-8, so we can move forward by getting rid of usage of UTF8.readString ASAP, and then some day later kill all the paths that use the write path (once we're sure the read path is dead).

Could a committer please review this? Thanks., +1, the patch looks good to me.

Thanks a lot for looking into this tricky issue, Todd, Colin, and Andy., Committed to trunk and branch-2. Thanks for reporting this, yixiaohua, and thanks everyone for the reviews., Integrated in Hadoop-trunk-Commit #3088 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3088/])
    HADOOP-9103. UTF8 class does not properly decode Unicode characters outside the basic multilingual plane. Contributed by Todd Lipcon. (Revision 1417649)

     Result = SUCCESS
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1417649
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/UTF8.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestUTF8.java
, Integrated in Hadoop-Hdfs-trunk #1246 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1246/])
    HADOOP-9103. UTF8 class does not properly decode Unicode characters outside the basic multilingual plane. Contributed by Todd Lipcon. (Revision 1417649)

     Result = FAILURE
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1417649
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/UTF8.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestUTF8.java
, Integrated in Hadoop-Mapreduce-trunk #1277 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1277/])
    HADOOP-9103. UTF8 class does not properly decode Unicode characters outside the basic multilingual plane. Contributed by Todd Lipcon. (Revision 1417649)

     Result = SUCCESS
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1417649
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/UTF8.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestUTF8.java
]