[Here's a patch that truncates the job's name in the filename (but not the job history file) to 50 characters., The current strategy for naming a history file is as follows
_job-history-start-time___job-id___job-name___user-name_. 
The reason for using this technique is to make sure that the job history filename remains unique across jobtrackers and across restarts. Making it 50 characters might cause the filenames to clash. We should probably come up with a strategy of giving shorter unique filenames to the job history. Thoughts?, Isn't the combination of start time and job ID enough? Job ID's are already unique within a JobTracker. The start time also includes the name of the machine the JobTracker ran on. Therefore the only way you could have a conflict is if you start a JobTracker twice in the same millisecond on the same machine and actually run jobs on both of the instances, which should be impossible. Before Hadoop 0.17, I think we only used <i>job-history-start-time_job-id</i> and this worked fine (the job name and username were added so as not to have to create an index file)., I am planning to remove {{job-history-start-time}} from the job history filename as a part of HADOOP-3245. Look [here|https://issues.apache.org/jira/browse/HADOOP-3245?focusedCommentId=12620042#action_12620042] for more details. This was introduced by HADOOP-239 since the job names were not unique when the patch was written. The reason why username and jobname are part of the history filename is because JobHistoryViewer uses  history filename to find out the username and jobname. The need is to detect the username and the jobname without reading the history file. We could very well use the jobid as the history filename and maintain an _index_ file to map job-id to _{username,jobname}_. Thoughts?, If the history start time has to go, then the best thing to do is to ensure that JobID's are globally unique rather than hoping that the (jobid, job name, user) combination will be unique. First of all, are JobID's actually unique right now? My impression was that they weren't - they include jobtracker start time and job number, but two trackers could give the same JobID. However, if we keep the machine name in there, then the (machine, jobid) combination could be unique. If you want to make sure JobID's are unique however, just use a GUID generator, such as http://java.sun.com/j2se/1.5.0/docs/api/java/util/UUID.html#randomUUID()., bq. _job-history-start-time___job-id___job-name___user-name_
I missed the jobtracker's hostname in this. So the history filename looks like 
_job-history-start-time___jobtracker-hostname___job-id___job-name___user-name_

Job's id is unique within a jobtracker but not across jobtrackers although its less probable that two tracker will start at the same time. Since running two jobtrackers on a same node is even less probable, I think its safe to assume that _jobtracker-hostname___job-id_ should be unique across clusters. One simple way to achieve short and unique history filenames would be to have something like _job-id___*f*(jobtracker-hostname)_, where _*f*_(s) is something like a hash. One can maintain the mapping of _*f*_(s) to _jobtracker-hostname_ in some _index_ file along with the username and jobname information. Thoughts?
, Earlier we had trimming of the file name to MAX_FILENAME_SIZE, but that is removed by HADOOP-3263.

bq. One can maintain the mapping of f(s) to jobtracker-hostname in some index file along with the username and jobname information.
When DFS supports *appends*, we can have master-index file for job history that  was there earlier (till 0.16). Then user name, job name  will be in master index file, instead of part of filename.
Thoughts?
, DFS appends sounds reasonable, but we'll have to make sure that each jobtracker uses a different file, because there can be only one appender. We could just name the file index-<tracker machine> to achieve this if we're going to assume that no machine is running two jobtrackers., I should also add that some people probably run MapReduce without DFS (e.g. those using S3 or Kosmos) so we should make sure we support that., Until we have HADOOP-3245, it seems reasonable to put an upper limit on job names. I'm +1 on committing this one. , pushing through hudson., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12388093/HADOOP-3937.patch
  against trunk revision 687868.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3078/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3078/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3078/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3078/console

This message is automatically generated., There seems to be an issue with Hudson that's causing it to show warnings generated by some other patches. The Javadoc warning and tests failing are also things I got on HADOOP-3746, but neither this patch nor that one does anything to the tests in question or to the commons logging package that generates the Javadoc warning., I just committed this. Thanks, Matei!, Integrated in Hadoop-trunk #589 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/589/])]