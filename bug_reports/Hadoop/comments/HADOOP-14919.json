[Add patch for the unit test., Adding the test bz2 file (The bz2 file that the attached unit test generates), Upgrading the priority since this involves silent data loss., A lot of the troubles with split handling in this codec are related to these issues:
1) It seeks _backwards_ from the split start looking for a possible bzip2 block header and sometimes miscomputes where that should start.
2) It is reporting stream positions that can skip data that has not been processed yet (as in this case).

So here's my pitch at fixing this for the Nth time:
- No seeking backwards.  Any block whose start marker appears before the split is not the responsibility of this split's reader.  Split processing should never require reading data just before the split offset, as that data is entirely the responsibility of the previous split's reader.
- Stream position reports the byte where the block start marker begins rather than the byte after it ends.  That way we're always consistent about who is responsible for "consuming" a block start header and never report byte positions that may have skipped some data bits.

This is probably going to break _something_ given how many attempts there have been at fixing this, so I greatly appreciate any and all eyes willing to take a look.  Alternative proposals are also welcome.

Attaching a patch that implements the approach described above.  Thanks to [~tanakahda] for the unit test.  I extended it to test all the split positions around the block start marker for this test case. Speaking of tests, Jenkins won't run all the related tests, so I also manually ran the TestLineRecordReader tests in hadoop-mapreduce-client-core and they passed.
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 29s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 49s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 15s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 21s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m  6s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m  6s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m 19s{color} | {color:orange} root: The patch generated 2 new + 118 unchanged - 9 fixed = 120 total (was 127) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 57s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 40s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  9m 22s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}101m 37s{color} | {color:green} hadoop-mapreduce-client-jobclient in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 45s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}216m 49s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestKDiag |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:71bbb86 |
| JIRA Issue | HADOOP-14919 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12890447/HADOOP-14919.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 081a67763b93 3.13.0-117-generic #164-Ubuntu SMP Fri Apr 7 11:05:26 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / cae1c73 |
| Default Java | 1.8.0_144 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/13454/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/13454/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/13454/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13454/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thank you for the patch. I tested the patch and confirmed that the patch can fix the issues we saw in our production environment.
As far as I tested, I did not see any regressions or new issues., Thanks for taking the patch for a test drive!  Glad to hear it fixes the problem and doesn't seem to regress anything so far., Thanks [~jlowe] for fixing this and [~tanakahda] for validating the fix. I assume since it has been tested, it'll be committed in time for 2.9?, If it can get a thorough review from someone then yes. ;-)  Still looking for a committer who knows a thing or two about splittable codecs and the pitfalls therein to find time to give this a review.
, [~lewuathe] [~ajisakaa] this is a bug related to the changes in HADOOP-13270.  Would you have time to take a look?, LGTM, +1. I ran all the bzip2-related tests locally and all the tests passed., +1 for removing the seek backward. The text reader also had bugs from that. Ironically, they were discovered/fixed as part of adding splittable codecs.

Looking at the code, would this support concatenated bzip files? The reader handling the previous block will detect the end of its stream, and a split following it should find the block delimiter after the header of the next file. However, if the text splits are around the concat point, the {{BZh9}} bytes may not be unaccounted for. The codec skips these at the beginning of the file and updates {{reportedBytesReadFromCompressedStream}}, but I didn't see handling for this within the stream. Similarly, if splits are arranged like this:
{noformat}
file.txt.bz2: [BZh93141592659xxxxxxxx3141592659xxxxxxxx0x177245385090BZh93141592659ooooooo3141592659xxxxxxxx...]
               ^split0                                                               ^split1
{noformat}

Would split0 pick up the {{ooooooo}} bytes?

It doesn't look like the unit tests cover a combination of multi-byte delimiters and splittable codecs. I don't know how thoroughly we can test that, without getting too deep into bzip2..., [~jlowe], is this on track for 2.9.0? I see that [~ajisakaa]'s signed off and [~chris.douglas] also has reviewed it and has couple of outstanding questions. Thanks., FWIW, I'd be +1 on the patch as-is, Thanks for the reviews, Akira and Chris!

bq. is this on track for 2.9.0?

I'd like to dig into the potential issues Chris brought up, and I was hoping to carve out some time early next week to do so.  So if we have time I'd rather wait, but I'm also OK if this goes in and the investigation is done in a followup JIRA.

Concatenated bzip2 files may already be problematic according to HADOOP-6852 but I haven't had a chance to verify yet.
, Thanks [~jlowe] for the clarification. We can wait if it's early next week. If it gets delayed or you hit complications, we can fall back to committing this and having a followup JIRA., Had some more time to look into this today.  HADOOP-6852 is legit.  Concatenated bz2 files don't work at all in Hadoop:
{noformat}
$ echo hey | bzip2 -c > foo.bz2
$ echo there | bzip2 -c >> foo.bz2
$ bzcat foo.bz2
hey
there
$ hadoop fs -put foo.bz2
$ hadoop fs -text foo.bz2     
2017-10-30 13:18:08,083 INFO  [main] compress.CodecPool (CodecPool.java:getDecompressor(184)) - Got brand-new decompressor [.bz2]
hey
text: bad block header
{noformat}

I don't think it would be very difficult to add support for concatenation.  IIUC all it needs to do is account for the possibility that 'BZh9' could appear before the block marker.  We should _not_ updated the reported position when skipping just the 'BZh9' bytes and only when we move from block mark to block mark.  The existing behavior of skipping at the file offset 0 is benign, but I don't think we want/need to update reported position when skipping these extra bytes mid-stream.

bq. The reader handling the previous block will detect the end of its stream, and a split following it should find the block delimiter after the header of the next file. However, if the text splits are around the concat point, the BZh9 bytes may not be unaccounted for.

Assuming we add the ability to silently skip 'BZh9' we should still be OK.  The compression input stream will only report the position moving when the next block starts to be read.  Whether we have 'BZh9' bytes or not doesn't change that.  We either read the whole block header and marker or none of it.  The upper layer reader will continue reading until the reported position changes, so the upper layer semantics don't change based on the presence of the extra header bytes.  Therefore I argue we're either OK or already screwed whether there's an extra header there or not.

bq. Would split0 pick up the ooooooo bytes?

I had a little trouble following the example and knowing what was a record delimiter.  In general the split reader is responsible for reading until a record ends in the next split because the next reader will always toss away the first record.  "Ends in the next split" means the entire delimiter appears in the next split, since the next split reader will toss all bytes up to and including the first record delimiter found.  There's some complicated logic in LineRecordReader and SplitLineReader to account for buffering occurring at both the codec and line reader levels along with the games codecs can play with reported byte position in the stream.

bq. It doesn't look like the unit tests cover a combination of multi-byte delimiters and splittable codecs.

See TestLineRecordReader#testBzipWithMultibyteDelimiter and compressedMultibyteDelimiter.txt.bz2.  I doubt it is exhaustive of all the corner cases, but there is at least one test there.

At this point I think we're good to go with committing this and addressing concatenated bz2 in HADOOP-6852.  As such I'll commit this tomorrow if there are no objections.
, bq. We should not updated the reported position when skipping just the 'BZh9' bytes and only when we move from block mark to block mark. The existing behavior of skipping at the file offset 0 is benign, but I don't think we want/need to update reported position when skipping these extra bytes mid-stream.
+1 I saw this was skipped from the codec, and wanted to be sure (if concatenation is supported) that your fix worked in that case. But as you say, it's moot if it doesn't support concatenated bz2 files.

bq. I had a little trouble following the example and knowing what was a record delimiter
Sorry. If split0 stopped at the end of stream and split1 skipped to the next delimiter, then the {{oooooo}} bytes would be skipped.

bq.  See TestLineRecordReader#testBzipWithMultibyteDelimiter
Thanks, I'd missed that.

+1 for committing this. Thanks for the detailed fix and followup, Jason., Thanks to [~chris.douglas], [~ajisakaa], and [~tanakahda] for reviews!  I committed this to trunk, branch-3.0, branch-2, branch-2.8, and branch-2.7.
, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13166 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13166/])
HADOOP-14919. BZip2 drops records when reading data in splits. (jlowe: rev 2fae63aa60c43b62bd908a9499562fe528603185)
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestTextInputFormat.java
, Hello,

Regarding this issue, I found another corner case of BZip2 codec/input stream behavior. Created HADOOP-15206 .

I'd like someone in this thread look at HADOOP-15206 too. Thank you.]