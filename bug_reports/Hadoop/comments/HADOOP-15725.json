[I believe this is a non-Kerberos setup issue.  In non-Kerberos cluster, UserGroupInformation.isSecurityEnabled()==false, and it will by pass all security check in Hadoop code base.  Therefore, if you use UserGroupInformation.createRemoteUser(), there is no security enforcing impersonation check and allow the operation to be carried out as root user.  Try this on a Kerberos setup, and see if you get the same result.  Same operation shouldn't work on Kerberos enabled cluster., Thank you [~eyang] for your comment.
The same behavior on a kerberos cluster:
{noformat}
bash-4.1# hadoop fs -put testfile /tmp/
bash-4.1# hadoop fs -chmod 700 /tmp/testfile
bash-4.1# hadoop fs -ls /tmp
Found 2 items
drwxrwx--- - root root 0 2018-09-07 06:16 /tmp/hadoop-yarn
-rwx------ 1 root root 0 2018-09-07 09:16 /tmp/testfile
bash-4.1# java -jar testDeleteOther.jar
DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_2134263987_1, ugi=root@EXAMPLE.COM (auth:KERBEROS)]]
/tmp/testfile
hive (auth:KERBEROS)
bash-4.1# hadoop fs -ls /tmp
Found 1 items
drwxrwx--- - root root 0 2018-09-07 06:16 /tmp/hadoop-yarn
bash-4.1#

{noformat}
{code:java}
final FileSystem fs;
Configuration conf = new Configuration();
conf.set("hadoop.security.authentication", "kerberos");
conf.set("dfs.namenode.kerberos.principal.pattern", "*");
conf.set("hadoop.rpc.protection", "privacy");
conf.set("fs.default.name", "hdfs://hadoop.docker.com:9000");
conf.set("fs.hdfs.impl",
org.apache.hadoop.hdfs.DistributedFileSystem.class.getName()
);
UserGroupInformation.setConfiguration(conf);
fs = FileSystem.get(conf);
System.out.println(fs);

Path f = new Path("/tmp/testfile");
System.out.println(f);

UserGroupInformation hive = UserGroupInformation.createRemoteUser("hive", SaslRpcServer.AuthMethod.KERBEROS);
System.out.println(hive);

hive.doAs((PrivilegedExceptionAction<Boolean>) () -> fs.deleteOnExit(f));

fs.close();


{code}
Do you think this is correct behavior for non-Kerberos cluster? I believe this is not acceptable for insecure cluster too since we do not honor file permissions. Please, correct me if I missed something.
Appreciate your help., [~oshevchenko] Thank you for the example, and I confirmed that if the program is running as root user, and HDFS is also running as root user.  The delete call will carry out.  If the java code is running as a different user than hdfs super user, then the file doesn't get deleted.  It does look like a bug that using super user to impersonate a less privileged user to perform operation, and the operation still carry out with super user privilege is a concerning problem and needs to be addressed.  Both secure and insecure cluster seems to have the same bug that super user impersonate a lesser privileged user doesn't prevent invocation of super user power.  

You are correct that this is not acceptable for insecure cluster as well.  Before this bug is fixed, the rule of thumb is to prevent running program with hdfs super user.  It is too mighty powerful.  At least, kerberos enabled cluster, user must have login to KDC with hdfs super user to exploit this bug.  It is not easily carry out by unauthorized user.]