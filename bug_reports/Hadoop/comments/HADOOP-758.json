[
Looking around the code around the stack trace, it is hard to see how this could have happened. We should update the bug if we notice this again. I checked a large sorter benchmark run there does not seem to be anything close to this exception.
, 
One possible culprit suggested in this case is lack of space under tmp directory. I will try with a very small tmp directory.
, 
Running 'dfs -copyFromLoccal' with a very small tmp directory correctly fails with 'No space left on the device'. Should run this in a map reduce job., 
The exception in the bug is that last exception that that occurred. It masks the first exception that would be a better indicator of the problem.

ReduceTask.java (around line 313)Looks like 

try { /* run reducer */ } finally { /* close some streams */  }

The above trace and the one in HADOOP-757 both are in finally {} and mask the exception in try {}. I will submit a patch that prints the exception  thrown in try {} if finally block throws one. 

While trying reproduce the above trace I managed to produce "Bad File Descriptor" exception in HADOOP-757.

In summary, it looks like these failures are possible with low tmp spaces but we don't log the exceptions that were triggered initially. , 
Attached patch removes finally block. This results in first exception thrown from ReduceTask.
, Rather than ignore all those exceptions, wouldn't it be better to at least log them?  Also, I'm not sure we need to proceed with all cleanups if any fail.  And we shouldn't replicate the cleanup code.

The problem is that exceptions in cleanups are masking the exception thrown in the body.  Wouldn't something like the following work?

IOException ioe = null;
try {
  ... body ...
} catch (IOException e) {
   ioe = e;
   throw e;
} finally {
  try {
    ... cleanups...
  } catch (IOException e) {
    if (ioe != null) LOG.warn(e) else throw e;
  }
  if (ioe != null) throw ioe;
}
    , -1, because 1 attempts failed to build and test the latest attachment (http://issues.apache.org/jira/secure/attachment/12348608/HADOOP-758.patch) against trunk revision r494676. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., 
> Wouldn't something like the following work?
> IOException ioe = null;
> try {
> ... body ... 
...
This is exactly what I had in my devel code. The method ('double code path') in the patch was Owen's preferred approach.

If we don't need to do all the cleanups (may be because this will actually close the Java process and open a new one for the new task), then we don't need finally at all.
, I think we should make a good-faith effort to cleanup: user-errors can be common, and this also runs under LocalRunner, not always as a separate process.  If there are errors in the cleanups we should log these as warnings, since they should not occur.

Which raises the related question: why did the cleanup fail?  Closing an open file shouldn't throw an exception.  That looks like a bug in DFSClient, no?, > Which raises the related question: why did the cleanup fail? Closing an open file shouldn't throw an exception. That looks like a bug in DFSClient, no?

Yes. FSDataOutputStream does not store its failure condition. so initial flush() fails and subsequent close() also result in flush() calls, which fail again. Do we want to fix that?  I am not sure if this will require trapping exceptions in multiple places. will look into.

Since there are multiple java filters in the stream, close() actually results in many 
FSDataOutputStream.flush() calls.  Java FilterOutputStream ignores exceptions from flush() during close.
, > so initial flush() fails and subsequent close() also result in flush() calls, which fail again

Neither of these should fail, should they?, 
First time flush/write can fail, common reason is lack of space. After that what should the subsequent flush() do? Should they produce the exception again (current behavior) or should the silently ignore the call()?, If the second flush() is under close() then its exception should be ignored, I think., Sure.  I will make sure FSDataOutputStream.close() that is simple., > Sure. I will make sure FSDataOutputStream.close() that is simple.

I meant to say : Sure. I will make sure FSDataOutputStream.close() ignores or logs the exception but does not throw it. That is simple.
, 
hmm...  DFSOutputStream.close() does a lot apart from flush(). I need to understand it better to make the changes.
, 
Owen brought up another important point. We want close() to throw exceptions since many times the last flush might occur only in close() and we should know about any failure.
, 
Based on the above discussion, shall we retain the patch that is submitted? It ignores exception in clean up only when regular code path has already thrown one. 

HADOOP-757 has another patch the fixes 'BadFileDescriptor' issue in DFSClient. It also resets 'blockWritenToBlock' before opening a new block file instead of after. This removes repeated attempts by DFSClient to write.
, The phased file system should not be committed when there is an exception. Other than that change, this looks reasonable., Thanks Owen. attached 858_2.patch moves PhasedFileSystem.Commit() out of clean up. 
, +1
, I just committed this.  Thanks, Raghu!]