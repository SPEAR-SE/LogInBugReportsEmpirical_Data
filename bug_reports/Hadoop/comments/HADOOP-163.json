[Alternately, the datanode daemon should simply exit if it cannot write to its configured data directories., 
Exiting is an option. However, the datanode may still be able to read, thus to serve the existing blocks.
, Good point.  So perhaps a read-only node should report itself at 100% of capacity?  Then the namenode should never allocate blocks to it., I envision 24x7 systems where the datanode is automatically restarted upon failure by init or another HA component. When a partition/FS fails, it will likely remain in a failed state after restart, so reporting up and stopping to serve would be better than simply exitting, which would lead to thrashing. At the end of the day, outside intervention would be required, so the most important part is diagnosing the error and reporting it as such. reporting 100% full would not generate the same kind of attention by a correction system/person., But should a datanode with no disk or a read-only disk still send heartbeats to the namenode?  I think not.  So it should exit the datanode daemon loop.  I think more than that is hard to specify at this point.  You're talking about what we should do when we have a system that automatically restarts, and a system that monitors, etc.  We don't have those systems in Hadoop today, so they're hard to code to!  In the meantime, do you think it would be better to enter some zombie state, not sending heartbeats or otherwise participating in namenode network protocols, but looping sending out SOS over channels TBD?, what I'm suggesting is close to your suggestion:
if you're read-only, behave as though you're 100% full, serve only read requests, but don't mislead: report you're read-only, not that you're 100% full. Namenode will avoid new block allocations to the node, but its log will contain an error that could trigger external corrective action.
, I plan to take a simple approach to this problem. If a data node finds out that it can not write to its disk. It reports to the name node and aborts. The name node logs the error and alerts it on the http UI.

I will use the existing data node protocol "errorReport" for the error reporting but with a minor change. In addition to the parameter error message, the rpc will also send an error code so that the name node does not need to parse the error message to figure out which action to take., Sounds good - except, what "aborts"? The idea of the datanode staying operating, but reporting error and not accepting further blocks is probably better, but maybe you meant "abort the block write". The node's blocks should probably not be counted by the namenode, but still available as a source for replication. Also, staying up means that there are fewer timeouts - it used to be that, when writing large volumes into DFS, if one or more of your nodes was full, your writer would hit a periodic timeout as connections to the (full and constantly restarting) datanode were refused. Hitting a timeout because some fraction of all resources is overused is, of course, much *much* slower than continuing to stream. Further - if the datanode periodically re-tests if the error condition has lifted, it can more immediately begin contributing to the cluster productivity again., We are trying to deal with the case that the node is misconfigured / broken.  Trying to operate in these situations is hard.  Simpler to fail fast, IMO.  This leverages the designed strengths of HDFS.  Our goal is to get the information to the operator so they can diagnose and fix the problem and seal the problem off from the cluster.

This is distinct from the case that the node is simply full.  That would not trigger this condition., In this patch, if a data node finds that its data directory becomes not readable or writable, it logs the error and reports the problem to its namen ode and shut down itself. When the name node receives the error report, it lots the error and removes the data node info.

A data node detects disk problem at startup time, when it receives a r/w request, after it receives a command from its name node, and before it sends out a block report. A data node will not start up if its data dir is not readable or writable. , This looks great!  I just committed it.  Thanks, Hairong!]