[Thanks for reporting this, [~axenol].

Could you provider more details on this? Some questions I have in mind:
- Was there any ZK restart etc when the disconnection happened?
- Was the network stable?
- Did this affect any client-side operations? Ideally there should be multiple KMS instances running, hence {{LoadBalancingKMSClientProvider}} would failover the kms client to the next KMS instance on a failure. The client op fails only if it retried all available KMS instances and all failed.

bq. I didn't see any issues on the ZK server side, so the issue must reside on client side.
I'm no expert on ZK, but if ZK doesn't have a quorum, it will not serve client requests. Client will see this as connection timeout.

I agree it looks odd though that the connection always succeeded on the next retry., [~xiaochen], thank you for your comments. The zookeeper (ZK) cluster seemed healthy, and nothing in any of the zookeeper logs indicated loss of quorum or random disconnects.
Instead, it seems ZK connections became unstable after an accumulation of a significant number of delegation tokens for KMS (>160,000). I'm not sure how this caused the issue, but once we manually deleted the tokens, the disconnects stopped. Once we apply the patch you provided for [HADOOP-13487|https://issues.apache.org/jira/browse/HADOOP-13487] (thank you!), I expect we'll be able to better manage the number of dtokens in ZK.

I do wish we were able to control some of the parameters for curator, so that we can adjust the timeouts for our needs, and curtail the repetitive error logging when a disconnect happens - these logs have taken up to 70GB of space per day, which turns a single log viewing into a big data problem.

On a different note, in a situation with multiple KMS instances, you pointed out how the {{LoadBalancingKMSClientProvider}} will try to find a working KMS. The problem I've seen is the KMS client timeout seems quite long, so in the case of one failed KMS, it takes a long time to talk to KMS from a client perspective. Do you know how we can configure this behavior and have a shorter timeout?, Thanks for the explanations Alex.
Interesting theory about the huge number of DTs resulting in such disconnect. I'll see if I can find anything out from that direction.
Glad HADOOP-13487 helped on alleviating the problem.

bq. Do you know how we can configure this behavior and have a shorter timeout?
I guess HADOOP-13318 would help?, [~xiaochen], it seems there is already a property to set the connection timeout, {{hadoop.security.kms.client.timeout}}, looking at {{KMSClientProvider.java}}. If that is indeed the case, perhaps you can close the referenced jira., Thanks [~axenol], you are right. I just closed HADOOP-13318 as a dup., [~xiaochen], I wonder if the issue we've seen is related to the curator version used by KMS. I see a very similar exception stack to the one here: [CURATOR-209|https://issues.apache.org/jira/browse/CURATOR-209]. Do you see any problem with having KMS use curator 2.10 instead?]