[This is an edge-case that I discussed with Yahoo!'s HDFS team a long time back that lead me to the conclusion that one is still better off specifying a max size rather than trying to guess capacity and do negative math.  Needless to say, I lost.  

In this particular edge-case, I think the fix would work. I'd still rate it as risky since there are likely other filesystems (especially pool based) that have similar df outputs, however where capacity is not used+avail.

Although I'm curious about one thing.

Why not just create another filesystem in this ZFS pool rather than using the root filesystem?  A  ZFS file system is significantly faster for Hadoop operations than using UFS. [... yes, I've tested it.] As an added bonus, you avoid this issue. :), I had the same problem on Solaris and I did exactly what Allen mentioned. More detailed informations what I did:

1) Create a zfs filesystem in the global zone 

zfs create rpool/srv/hadoop 

2) set the mountpoint to legacy and set a quota

zfs set mountpoint=legacy rpool/srv/hadoop

zfs set quota=50G rpool/srv/hadoop

3) Add the dataset to the hadoop zone 

zonecfg -z <hadoopzone>
zonecfg:hadoopzone> add dataset
zonecfg:hadoopzone:dataset> set name=rpool/srv/hadoop
zonecfg:hadoopzone:dataset> end
zonecfg:hadoopzone>verify
zonecfg:hadoopzone>commit
zonecfg:hadoopzone>exit

4) login to your hadoop zone

zlogin hadoopzone

5) set mountpoint for the zfs filesystem

zfs set mountpoint=/srv/hadoop rpool/srv/hadoop

Than hadoop recognizes the 50G as capacity. Remember to configure hadoop so that it uses a data dir in /srv/hadoop

I set in core-site.xml 

<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/srv/hadoop/tmp/hadoop-${user.name}</value>
  </property>
</configuration>

I hope these informations may help other solaris users.

Martin]