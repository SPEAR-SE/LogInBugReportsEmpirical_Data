[There are the following options we have:
- Not have this feature. HDFS and Map/Reduce teams will submit patches to the common project to add deprecated keys in the Configuration.java class. Simple, reliable, but inconvenient if this list keeps changing. Also, dependency on the hadoop common jars will exist w.r.to. project releases etc.
- Have a static final variable in Configuration to define the names of classes to load statically. Attempt to load these classes in the static block of Configuration.java. This makes it possible for projects to define one entry point class to add deprecated keys to configuration. We feel making it a variable as opposed to a configuration item is simpler, because we would then not have to create an instance of Configuration to add the deprecated keys from sub projects. There is a need though to make a one time change to Configuration to include a class for the HDFS and M/R projects.
- Same as above, but make the contract more rigourous - for e.g. define something along these lines:
{code}
package org.apache.hadoop.conf;

public abstract class DeprecatedKeyProvider {
  public abstract DeprecatedKeyInfo[] getDeprecatedKeys();
}
{code}

Classes in M/R and HDFS can extend this class. In getDeprecatedKeys, they can construct and return deprecated keys to add to the configuration class. Configuration will create an instance of these classes in it's static block and call getDeprecatedKeys() on each of them.

Thoughts ?
, It might be better if subprojects could push deprecated keys into the registry, rather than have the registry pull them in.

Perhaps we could have a public static method, Configuration.addDeprecatedKeys(String[]), and an indispensable classes in each subproject with a static block that calls this.  The trick is finding the indispensable class(es).  HDFS and mapreduce could each have a single class that adds all keys for the subproject.  This class can then be referenced statically from a key client class (e.g. DistributedFileSystem and Job) and each of the daemons.  Could something like this work?, Doug, the API you suggest was already part of HADOOP-6105. So, it looks like we need no work. I also confirmed with Amareshwari working on MAPREDUCE-849, that using the technique Doug suggested works for including deprecated keys from other projects. So, I think we can close this JIRA., While I see the advantage of push over pull, this approach would require that HDFS has its own config initialization that pushes the deprecated keys.
HDFS does not have such a thing. Not a big deal to add it to the HDFS daemons but we would also have to add it to all the tests. ( I guess one could argue 
that tests should have moved from old keys to new keys anyway.)
It would have been better to have the deprecated keys initialized via config files - not sure what this would entail.]