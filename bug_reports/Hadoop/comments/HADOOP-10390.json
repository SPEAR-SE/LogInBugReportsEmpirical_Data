[Looks like the problem is caused by version miss match in libhdfs CMakefile and DFSIOTest.HDFS_LIB_VERSION. One of them needs to be changed.
The patch changes DFSIOTest.HDFS_LIB_VERSION, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12633093/HADOOP-10390.v1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3634//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3634//console

This message is automatically generated., +1, Actually, it seems that the problem is different. There's an attempt to load the lib from
 {{/usr/lib/hadoop/libhdfs}} which doesn't exist in any of Bigtop based Hadoop installations (e.g. all of them, really), Interesting... it words on CDH cluster, but I don't see the fix anywhere in the Hadoop JIRAs., s/words/works/, bq. Actually, it seems that the problem is different. There's an attempt to load the lib from /usr/lib/hadoop/libhdfs which doesn't exist in any of Bigtop based Hadoop installations (e.g. all of them, really)

Java has a property named {{java.library.path}} which determines where it will look for native libraries.  If bigtop is not setting this correctly, let's open a bigtop JIRA to address that.

It's a separate issue from the one described here, which relates to {{HDFS_LIB_VERSION}}.

I'll commit this tomorrow if there are no more comments.  Thanks!, Bigtop's hadoop-config.sh - in fact the very same script that is used by _any_ commercial Hadoop distro - are setting {{java.library.path}} correctly. I have validated it.

Yet, the test is failing., Perhaps try setting the version to 0.0.0.

strace may be helpful in showing you what files the test is actually trying to open., Looks like there is no libhdfs.so.0:
{code}
[root@namenode native]# ll
total 2184
-rw-r--r-- 1 67974 users 621326 Feb 11 08:55 libhadoop.a
-rw-r--r-- 1 67974 users 534024 Feb 11 08:55 libhadooppipes.a
lrwxrwxrwx 1 67974 users     18 Feb 17 22:37 libhadoop.so -> libhadoop.so.1.0.0
-rwxr-xr-x 1 67974 users 446741 Feb 11 08:55 libhadoop.so.1.0.0
-rw-r--r-- 1 67974 users 226360 Feb 11 08:55 libhadooputils.a
-rw-r--r-- 1 67974 users 204586 Feb 11 08:55 libhdfs.a
lrwxrwxrwx 1 67974 users     16 Feb 17 22:37 libhdfs.so -> libhdfs.so.0.0.0
-rwxr-xr-x 1 67974 users 167760 Feb 11 08:55 libhdfs.so.0.0.0
{code}, I reviewed code carefully, related code:
{code}
        fs.copyFromLocalFile(new Path(hadoopHome + "/libhdfs/libhdfs.so." + HDFS_LIB_VERSION),
                             HDFS_SHLIB);
        fs.copyFromLocalFile(new Path(hadoopHome + "/libhdfs/hdfs_read"), HDFS_READ);
        fs.copyFromLocalFile(new Path(hadoopHome + "/libhdfs/hdfs_write"), HDFS_WRITE);
{code}
the program try to copy hadoopHome/libhdfs/libhdfs.so to HDFS and failed, because file doesn't exists.
Actually the whole path is not right, I suspect the path(HADOOP_HOME/libhdfs/libhdfs.so|hdfs_read|hdfs_write) is already out-of-date now(maybe they exist in hadoop-v1 test environments), so in theory the test should fail for a long time ago.
, Currently I think there are 3 issues we need to fix: 
1. libhdfs has a different path and version now, they should be updated
2. hdfs_read/hdfs_write no longer exists(at least not in hadoop distribution), but it is used for DFSCIOTest, should they also go into distribution?
3. DFSCIOTest on exception only prints error message:
      System.err.print(e.getLocalizedMessage());
    it is so confusing that I took a long time to find the root cause

, I think it makes sense to re-purpose this ticket and fix all the test issues at once instead of stretching the process into a few different issues., In the interest of full disclosure: the same fix (as in the patch) exists in Cloudera's source tarball since 5/9/2012. Things that make you go 'hmmm', bq. In the interest of full disclosure: the same fix (as in the patch) exists in Cloudera's source tarball since 5/9/2012. Things that make you go 'hmmm'

I'm not sure what you're talking about.  I looked at DFSCIOTest.java in the latest version of CDH5.  It still has:

{code}
  private static String HDFS_LIB_VERSION = System.getProperty("libhdfs.version", "1");
{code}

And the test has an {{\@Ignore}} before it., sorry, I had the patch applied in my workspace. Yet, on CDH4 DFSCIO is working while it doesn't on open source installation starting back from 2.0.6-alpha, bq. hdfs_read/hdfs_write no longer exists(at least not in hadoop distribution), but it is used for DFSCIOTest, should they also go into distribution?
I was trying to update the patch to solve all the issues in the jira, but have not found a proper place to put test_libhdfs_read/test_libhdfs_write, HADOOP_HOME/bin does not feel like a right place for test programs. 
, I think the tests should be relative to where libhdfs* is. Maybe it should go into HADOOP_HOME/lib/native/test directory?
, Actually, keep in mind that in a normal deployment all these directories are owned by root, hence test would be have some impossible requirements. I'd pick a neutral location for these binaries., Attach new version of the patch addressing all 3 issues, I put test_libhdfs_read/write to HADOOP_HOME/lib/native as suggested.
, [~decster]],  {{HADOOP_HOME/lib/native}} is root owned in an deployed cluster. However, running tests as root is a big no-no, normally (as I commented above). Also, this location is managed by a package when the cluster is installed. Adding/removing stuff into a package realm is - again - is a bad idea. Is it possible to keep these elsewhere?, Which position would you suggest? Do you mean we do not put those into package distribution?
, I am sorry - I misread the change, I think. Please disregard my comment about the permissions. However, the binary executables should belong to {{bin/}} rather than {{lib/}} directory. Any particular reasons to expect them to be under {{lib/native}} ? , The reason why I suggested the lib/native is because previously hdfs_read and hdfs_write were under the same directory as libhdfs.so*
And now libhdfs.so* has moved lib/native. , bq. The reason why I suggested the lib/native is because previously hdfs_read and hdfs_write were under the same directory as libhdfs.so*
Sorry, I am confused - were where? in the tarball or the packaged versions of Hadoop?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12634183/HADOOP-10390.v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:

                  org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3665//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3665//console

This message is automatically generated., Oh, I was looking at the wrong location. You can put the binary files under <HADOOP_HOME>/bin., update patch, change test_libhdfs_read/write to HADOOP_HOME/bin
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12634404/HADOOP-10390.v3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-assemblies hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3671//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3671//console

This message is automatically generated., [~decster]Were you able to run the test successfully with your change?, After apply the patch, libhdfs.so.0.0.0, test_libhdfs_read/write are correctly copied to dest location, but the DFSCIOTest job failed because the task failed due to libhdfs internal error, maybe it is my environment's problem, have not found the root cause yet, unlikely it is caused by the issue here., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12634404/HADOOP-10390.v3.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6325/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  1s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12634404/HADOOP-10390.v3.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6333/console |


This message was automatically generated.]