[cc [~stevel@apache.org], how about this "not sure that v2 task recovery is correct"

Specifically: v2 task commit renames from attempt dir into destination. If that fails partway through, then both MR and spark assume that the entire task can be retried. Which has the following flaws

* a rerun task may generate files with different names. If this holds, those files from the first attempt which are copied into place will still be there. Outcome: output of two attempts may be in the destination.
* if the created filenames are the same, if the first attempt hasn't actually failed, but instead paused for some time, but then resumes (GC pauses, VM hangs etc), then the first attempt will continue its rename, and potentially then overwriting 1+ file of the previous attempts output. Outcome: the data may be a mix of two attempts.

If each attempt creates precisely one file, and the name of the file is the same on both, then these problems don't arise. There's no partial commit of files; the second attempt will overwrite the first completely, and, if a paused attempt resumes, then it will completely overwrite the output of the latter. Provided that doesn't happen partway through ongoing work (deletion of task attempt dirs will cause the rename to fail, obviously), then the requirement for speculative/retriable tasks "output from either attempt is valid" will be met  —downstream code will have to deal with it.

That said, happy to be wrong, if I've misunderstood something —that commit code is complex enough that I had to step through with a debugger taking notes to understand what was going on.

Right now I don't trust v2. it's worse on object stores as time-to-rename is potentially much longer, so probability of task failure during rename is higher.

See also [a zero rename committer|https://github.com/steveloughran/zero-rename-committer/releases/download/tag_draft_003/a_zero_rename_committer.pdf]; review & corrections welcome there., BTW, HADOOP-15107 updated that committer docs a lot; if you haven't looked at the 3.1.2+ version of the docs, start there, PS: spark recovery is, AFAIK, always by rerunning entire job; it's only task recovery it attempts —and there it assumes that task attempts can always be retried. I now believe that in the absence of atomic/recoverable task commits, it & MR need to treat failures during task commit as special from failures during task execution., Thanks for the comments [~stevel@apache.org]. I was looking at committers.md in trunk.  I mostly agree. v2 was designed with atomic rename semantics in mind, and any FileSystem implementation that does not provide it will be vulnerable to inconsistencies. However, the section on hadoop committers is called "Background" which is the good old atomic rename world on HDFS executed by MapReduce . 
{quote}a rerun task may generate files with different names. If this holds, those files from the first attempt which are copied into place will still be there. Outcome: output of two attempts may be in the destination.
{quote}
Correct, but this is a user error to have any kind non-determinism between task attempts. I always remind my coworkers to set seeds for Random based on a hash of something repeatable like task id excluding attempt Id  and not use anything like the default currentTimeMillis. Let alone generate files with different names. Rereading this a few times I realize that you might also include the case of multi-file outputs which makes even the task commit non-atomic.  
{quote}if the created filenames are the same, if the first attempt hasn't actually failed, but instead paused for some time, but then resumes (GC pauses, VM hangs etc), then the first attempt will continue its rename, and potentially then overwriting 1+ file of the previous attempts output. Outcome: the data may be a mix of two attempts.
{quote}
Only one attempt is allowed to commit via canCommit and with deterministic output + atomic rename works anyway.
{quote}Right now I don't trust v2. it's worse on object stores as time-to-rename is potentially much longer, so probability of task failure during rename is higher.
{quote}
I understand, but for previously typical cases (HDFS, single output file per attempt) it is robust. Once you realize that even on HDFS v1 was noticeably non-atomic for large jobs and you need to check for _SUCCESS or have another service recording completion, v2 was a big improvement for Twitter.

I am just about to familiarize myself with Spark's use of FOC , bq. Only one attempt is allowed to commit via canCommit and with deterministic output + atomic rename works anyway.

yes, but if that attempt is considered to have failed, an alternative attempt may commit. At least, that's my reading of the code in Hadoop and spark. Now, provided that only one file is generated by a task attempt, and the output of any attempt can be accepted, then a long-delayed task commit *shouldnt* be harmful, at least if it happens while the job is in progress. If it happens after the job has completed, well, that's "unusual"

I've never seen that happening, it is a failure mode to be considered if you want to be able to show that your algorithm is robust. The MR job committer explicitly checks before job commit that it's had a recent heartbeat with the YARN RM to avoid this and cluster partition problems at the job level; nothing worries about it for individual tasks.

bq. even on HDFS v1 was noticeably non-atomic for large jobs and you need to check for _SUCCESS or have another service recording completion, v2 was a big improvement for Twitter.

Yes: neither committer is fully atomic at some phases in its operation

The S3A ones don't have atomic job commit either; just O(files) POST requests which can be done in parallel. They do at least deliver atomic task commit (at least I believe so...).


bq. I am just about to familiarize myself with Spark's use of FOC 

it doesn't need to worry about job restart, so life is simpler. Still uses the MRv1 APIs though, which they should be weaned off (And in Hadoop MR: deprecated)


, BTW, do you have a patch for this?]