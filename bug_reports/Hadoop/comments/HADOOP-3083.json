[
Actually I think you should repair the leases so that the existing lease holders can continue writing to their files.

Here is  a use case.

say a client is writing to a file /xyz/parent_dir/dub_dir/file

Somebody performs a command like "hadoop dfs -rename  /xyz/parent /xyz/new_parent"

This operation should not cause the client to fail to continue writing the the file.
Otherwise, "hadoop dfs -rename dir1 dir2" will be a very dangerous operations , The HDFS client talks to the namenode using the pathname as the identifier. For example, if the client needs to allocate a new block, it sends a block-allocation request to the namenode. The namenode looks up the inode using the pathname and associates a new block with the file. So, a file is renamed, all new request for block allocations using the old pathname fails. So, just fixing the lease is not sufficient to support writing to files across renames., Here is a patch that does not throw an exception if a dangling lease is found while saving an fsimage. This is a stop-gap solution so that existing hdfs installs continue to work. A better solution to avoid dangling leases have to be worked out., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12378542/leaseIgnoreExceptions.patch
against trunk revision 619744.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2046/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2046/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2046/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2046/console

This message is automatically generated., A rename of directory entry traverses all the leases and fixes dangling leases if any. It does a sequential scan of all leases, so there is some overhead in renaming a file., Promoted after observing a stuck name node., Each lease record has a pointer to the inode. This is the most effective solution because rename does not require that all leases be searched., # FSEditLog. Redundant:
import java.util.List;
import java.util.Iterator;
# FSDirectory.addFile() should return INodeFileUnderConstruction instead of INode.
# INode.getAbsoluteName() is deprecated and should not be used.
The full path for leases is necessary only when writing the INodeUnderConstruction to fsimage. 
Otherwise, it is used in the current code and in the patch only for logging.
During saveImage() the full path of an INode is known so it should be remembered
next to the Lease and used in writeINodeUnderConstruction().
# I just noticed that we store each INodeFileUnderConstruction in fsimage twice, first as a regular inode in the 
directory tree, and then in the file-under-construction section with additional file-under-construction specific fields.
This is probably for a different issue.
# We also talked about how to efficiently implement the new file-under-construction 
collection that replaces current TreeSet of leases. Dhruba has a solution for that.
# File delete() is very inefficient because it uses removeLeases(), which traverses the whole 
tree underneath current INode and collects all nodes in one array in order then to sort out those that are being constructed.
In fact the traversal happens two times in collectSubtreeBlocks() and then in removeLeases().
The subtree should be traversed only once. We can introduce a method say
{code}
  collectSubtreeBlocksAndLeases(List<Block> v, List<INodeFileUnderConstruction> u)
{code}
which would collect both blocks and files under construction at the same time., Do not save leases into the image. This needs to be fixed in the next release., Keep the lease section at the end of the fsimage but do not save any records in this section., +1
I also think that it is safer to postpone the persistent lease feature for the next release.
As it could be seen Dhruba tried several approaches to fix the issue but all of them turned out to be problematic in one aspect or another.
, -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12379167/doNotSaveLease.patch
against trunk revision 643282.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    patch -1.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2144/console

This message is automatically generated., merged patch with latest trunk., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12379326/doNotSaveLease.patch
against trunk revision 643282.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests -1.  The patch failed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2156/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2156/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2156/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2156/console

This message is automatically generated., Changed the unit test that was checking to make sure that leases are persisted across namenode restarts., +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12379418/doNotSaveLease.patch
against trunk revision 643282.

    @author +1.  The patch does not contain any @author tags.

    tests included +1.  The patch appears to include 3 new or modified tests.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2167/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2167/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2167/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2167/console

This message is automatically generated., I juts committed this., Integrated in Hadoop-trunk #451 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/451/])]