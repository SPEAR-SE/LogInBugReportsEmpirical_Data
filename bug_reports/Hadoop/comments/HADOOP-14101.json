[[~Thomas Demoor] what do you think? I think we're seeing a partially deleted message due to concurrent deletes, but that this isn't actually harmful. That is, provided the final message just entries which weren't there when the request was made, in which case, given we want an idempotent call and all, all is well

I don't want to try and be clever about reissuing delete requests, precisely because of race conditions if a another caller is creating things. Its bad enough with a blob-by-blob delete as it is. Instead I think I'll catch and build up a report to WARN on.

Luke, while I look at this, can I highlight that you should not be using S3 as a direct destination of spark work, not unless you've got something like s3mper or s3guard (HADOOP-13345) providing a consistency layer. Because lists are not consistent with the actual state of the store, there's a risk that the final commit-by-rename protocol won't actually enumerate all files to move into the right place. , This is what HADOOP-11572 covered; looks like it is surfacing here. Restarting work on that, closing this as a duplicate., Thanks for explaining the problem with writing directly to s3. Is it okay to read from s3 or is it better to do that from hdfs as well (at least until s3guard is in place)., Read from s3 is fine, and there are lots of speedups coming in Hadoop 2.8. It's just that problem of committing the data is dangerous right now. It might work fine during development, but in production, with larger datasets, less good

see: https://www.slideshare.net/steve_l/spark-summit-east-2017-apache-spark-and-object-stores]