[With such big input files the default logic should split things into dfs block-sized splits.  Smaller splits should only be used if this would result in fewer than mapred.map.tasks splits.  What value do you have for mapred.map.tasks in your mapred-default.xml?  Let's make sure that is working before we add a new min.split.size feature.  I don't oppose the feature, but it should be generating 356*30G/32M splits, not 356*30G/2K splits as you claim.  That's still a lot of splits.  If it is too many then we should add the feature you're adding.

Note that, as a workaround, it is also easy to implement this w/o patching by defining an InputFormat that subclasses InputFormatBase and specifies a different minSplitSize.  But making that a long is a good idea.

So, in summary, can you please confirm that the actual number of splits that you object to is 356*30G/32M splits, not 356*30G/2K?  Thanks., From what I've seen, it is always 32M fragments, but that is still 300k input splits/maps, which is a lot. We'd like to be able to drop that by an order of magnitude. (I think in this case that the input splitter never finished, so we don't know.), Doug, you are right. The number of splits we got was 356*30G/32M, but still too many., Updated patch, Okay, I have applied this.

For the record, patches are easier to apply if they are made from the root of the project.  Also, new config properties should generally be added to hadoop-default.xml.  Finally, the cast added in SequenceFileInputFormat was not required.]