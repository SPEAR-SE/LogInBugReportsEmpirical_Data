[[~stevel@apache.org] FYI. Incidentally, we're trying to move away from direct writes to s3, but are stuck for now:(, well, you know my stance on directs to s3, ":dont". Wait gfor HADOOP-13786.

That message is being thrown because rename returned false.
{code}
      if (!fs.rename(from.getPath(), to)) {
        throw new IOException("Failed to rename " + from + " to " + to);
      }
{code}

The fact that rename() doesn't return anything meaningfui on a failure other than return a "worked/didn't work" flag is just one of the many issues with rename, the other being "nobody quite knows what it is meant to do". There's an open jira on making the rename ext call public, which does throw exceptions: I'd like to do that and switch to it within the Hadoop codebase at the very least.

I suspect what's happening is that rename is failing because there is something at the far end, and even after a DELETE call, a HEAD request is finding it, so the operation rejected. 

Try cranking up the debug logging in the s3a module and see what it says...attach it here and I'll see what can be done. Renaming JIRA to describe problem better
, & downrating to minor as you are trying to do something I've already told you not to :), [~stevel@apache.org] What is the alternative though, write to HDFS and then move stuff across to S3? , Right now, if you are using Spark, use https://github.com/rdblue/s3committer/ ; this will be in Hadoop 3.1, I'm closing this as a WONTFIX because the classic FileOutputFormat committer isn't the right way to work with data in S3. It should work with HADOOP-13345 and the consistent listings there, but performance will still suffer. 

# Short term (Hadoop 2.9+): use S3Guard for the consistency you need
# Longer term: Hadoop 3.1+: use the S3A Committers for the performance you want
, Removing 3.1.0 fix-version from all JIRAs which are Invalid / Won't Fix / Duplicate / Cannot Reproduce.]