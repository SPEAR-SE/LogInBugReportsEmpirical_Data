[following things will remove the duplicate VM args.

1. Make following changes in hadoop-env.sh
{noformat}
export HADOOP_OPTS=""

export HADOOP_NAMENODE_OPTS="-Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}"
export HADOOP_DATANODE_OPTS=""
export HADOOP_SECONDARYNAMENODE_OPTS="-Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}"

export HADOOP_CLIENT_OPTS="-Xmx128m"
{noformat}

Above change is because, from any script hadoop-config.sh will be the first script called, which inturn call hadoop-env.sh. So appending not required.
Anyway remaining args will be added inside hadoop-config.sh. 

Also hadoop-config.sh will be called again by hdfs-config.sh and yarn-config.sh. 
So args will be appending again and again.

2. Remove calling hadoop-env.sh explicitly from hadoop-daemon.sh

I will post patch in some time.

Any comments on this.. ?
, Attaching the patch which includes
Above mentioned changes
along with slaves.sh, and addition of HADOOP_ZKFC_OPTS from templates/conf/hadoop-env.sh, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12530809/HADOOP-8476.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.fs.viewfs.TestViewFsTrash

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1119//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1119//console

This message is automatically generated., Attaching the correct patch to remove duplicate VM args, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12541972/HADOOP-8476.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1345//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1345//console

This message is automatically generated., Vinay could you regenerate the patch, it does not apply on trunk.

Also some comments/questions based on your patch file

You have added an option to the hadoop-config.sh script to skip hadoop opts "--skip_hadoop_opts" and you are passing that in all the various places hadoop-config.sh is called and this skipping the setting of HADOOP_OPTS.

I dont think we should make the assumption that people will have the appropriate values set in the env by the hadoop-env.sh config file. People change this config based on what their needs are and we cannot force them to have all of these defined. hadoop-config.sh made sure certain defaults are set., I also went wrong with duplicate values for mx !! 

Vinay, can you update the patch ? , These are already handled as part of script re-write.]