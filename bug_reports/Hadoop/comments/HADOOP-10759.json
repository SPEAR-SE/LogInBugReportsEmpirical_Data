[{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12652744/HADOOP-10759.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4183//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12653100/HADOOP-10759.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4189//console

This message is automatically generated., SUCCESS: Integrated in Hadoop-trunk-Commit #6009 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6009/])
HADOOP-10759. Remove hardcoded JAVA_HEAP_MAX. (Eric Yang) (eyang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1615700)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh
, +1 I just committed this., FAILURE: Integrated in Hadoop-trunk-Commit #6010 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6010/])
HADOOP-10759. Remove hardcoded JAVA_HEAP_MAX. (Sam Liu via Eric Yang) (eyang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1615707)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #634 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/634/])
HADOOP-10759. Remove hardcoded JAVA_HEAP_MAX. (Sam Liu via Eric Yang) (eyang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1615707)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
HADOOP-10759. Remove hardcoded JAVA_HEAP_MAX. (Eric Yang) (eyang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1615700)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1828 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1828/])
HADOOP-10759. Remove hardcoded JAVA_HEAP_MAX. (Sam Liu via Eric Yang) (eyang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1615707)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
HADOOP-10759. Remove hardcoded JAVA_HEAP_MAX. (Eric Yang) (eyang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1615700)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1853 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1853/])
HADOOP-10759. Remove hardcoded JAVA_HEAP_MAX. (Sam Liu via Eric Yang) (eyang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1615707)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
HADOOP-10759. Remove hardcoded JAVA_HEAP_MAX. (Eric Yang) (eyang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1615700)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh
, [~eyang] - thanks for reviewing and committing this patch. However, we should avoid committing anything short of "Critical" past branch-x. 

Given we are in the middle of RCs for 2.5, I am reverting this from branch-2.5.0 and branch-2.5. , Updated CHANGES.txt to move it 2.6.0 and also add it at the end of the section instead of the beginning, so we can preserve chronological order. , BTW, people are aware this removes the default 1000m setting if HADOOP_HEAPSIZE or *_OPT isn't set in *-env.sh, right?, Thanks Allen. I was planning to follow up on this after taking care of 2.5.0 branching.

[~eyang]/[~sam liu] - is there a particular reason for doing this? , Take a look at ZOOKEEPER-1670. We noticed that if no default heap was provided java can end up taking upto 1/4th of machine mem., FAILURE: Integrated in Hadoop-trunk-Commit #6019 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6019/])
Edit CHANGES.txt files to move HADOOP-10759 and HDFS-6717 from 2.5.0 to 2.6.0 (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1616036)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, The information in ZOOKEEPER-1670 is not entirely accurate.  Java does a good job on calculate initial heap size, and it will use 1/4th of machine memory up to 1GB.  See:

http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#par_gc.ergonomics.default_size

Therefore, without this being specified, it may use up to 1GB for heap on machine that has greater than 4GB physical memory.  However, for smaller machine such as a virtual machine, it would be nicer if it can scale dynamically.  Another benefit of removing this hard coded value is to make sure that the Hadoop command line is not capped to 1GB for trivial operation such as GetConf, or dfs client operation to reduce memory starvation in executing too many cli operations in parallel to map reduce tasks. We have notice that while the machine may already hand out most memory to map reduce tasks, and some amount of cli command happens in parallel, may trigger excessively allocating memory and causes JVMs to aggressively running garbage collection and increases chances of dead lock in highly fragmented memory pages.  It is somewhat a serious bug that I think it is worth while to be included in 2.x releases., FAILURE: Integrated in Hadoop-Yarn-trunk #635 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/635/])
Edit CHANGES.txt files to move HADOOP-10759 and HDFS-6717 from 2.5.0 to 2.6.0 (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1616036)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1829 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1829/])
Edit CHANGES.txt files to move HADOOP-10759 and HDFS-6717 from 2.5.0 to 2.6.0 (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1616036)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1855 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1855/])
Edit CHANGES.txt files to move HADOOP-10759 and HDFS-6717 from 2.5.0 to 2.6.0 (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1616036)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, I'm -1 this going into branch-2.  This is an incompatible change that will break users reliant on the existing behavior.

, Also, before I forget to mention it, while I can appreciate this feature:

bq. However, for smaller machine such as a virtual machine, it would be nicer if it can scale dynamically.

The rest of isn't true.

bq.  it may use up to 1GB for heap on machine that has greater than 4GB physical memory. 

directly contradicts

bq. Another benefit of removing this hard coded value is to make sure that the Hadoop command line is not capped to 1GB for trivial operation 

If the algorithm as reported in the Oracle documentation is true, removing the default JAVA_HEAP_MAX will *still* limit the dynamic value to 1GB max.... thus accomplishing nothing. Additionally, HADOOP_CLIENT_OPTS and HADOOP_HEAPSIZE can both be used to override the 1GB max from the command line.

One other thing:  JAVA_HEAP_MAX is set in a few more places in the current shell code base (e.g., bin/yarn).  Just removing it from hadoop-config.sh is incomplete., How is removing a hard coded mistake a feature?  This is a defect.  There is no need to run with 1GB heap for trivial command line 90% of the time.  There is no contradiction in my statement, it is quote out of context.  The 1GB max will automatically get applied, if no JAVA_HEAP_MAX is defined.  However, it should not cap cli to 1GB in case if an administrator wants to run cli operation that may require more memory, he can do:

export JAVA_HEAP_MAX=4096m

Then run the cli.  distcp and other utilities can directly benefit from this.

YARN hard code is not described in any part of this JIRA, I did not look at YARN cli that may have the need to remove capped JAVA_HEAP_MAX.  If that is required, it can be filed as another JIRA., Defaults are *always* hard-coded.  That's what makes them defaults...

bq. export JAVA_HEAP_MAX=4096m then run the CLI

Except this is incorrect usage.  Users should be doing either:

HADOOP_CLIENT_OPTS=-Xmx (which, granted, results in multiple Xmx settings...) or using the preferred HADOOP_HEAPSIZE=xxx setting.

You'll note that the lines below the removed one overrides JAVA_HEAP_MAX by the setting of HADOOP_HEAPSIZE...

For all intents and purposes, JAVA_HEAP_MAX isn't really meant to be set by users in Hadoop 2.x and previous releases.  It's an *internal* environment variable.  The big tip off is that it doesn't begin with HADOOP_ ., BTW, just so folks don't have look it up, this is what hadoop-env.sh says:

{code}
# The maximum amount of heap to use, in MB. Default is 1000.
#export HADOOP_HEAPSIZE=
#export HADOOP_NAMENODE_INIT_HEAPSIZE=""
{code}

Removing the JAVA_HEAP_MAX *breaks* that default.  There is also *no* mention of  JAVA_HEAP_MAX in anything user facing in the source. HADOOP_HEAPSIZE, of course, is defined right up there., Allen, you are correct in picking on my latest statement.  I meant to say that HEAP can override it with HADOOP_HEAPSIZE in either hadoop-env.sh or from environment when hadoop-env.sh does not explicitly define it in running environment.  However, the hard code value is unnecessary since JDK already cap to 1GB by default, if user's machine has more than 4GB.  For smaller machines, it is better that we don't use the hard coded cap but let Java decide the right value for it., As I said before, I agree that it's a better behavior. But it's still a feature, not a bug fix, and it's still incompatible with the existing expectations of the system.  It breaks the principal of least surprise.... so I'm still -1 for branch-2.

For trunk, this patch is very much incomplete if we want to change how this works:
* All hard-coded values to JAVA_HEAP_MAX need to be found and removed (hadoop-config.sh, yarn, yarn-env.sh)
* The content in hadoop-env.sh needs to get rewritten
* The content in yarn-env.sh needs to get rewritten (and a good chunk removed) ... oh yarn, you make our lives so hard by doing the wrong thing 90% of the time... :(  On the plus side, it perfectly demonstrates my point about the confusion with having two vars that do the same thing....

So even with this change, JAVA_HEAP_MAX really shouldn't be exposed to users.  It's a terrible variable name (there is no JAVA_HEAP_MIN which sets Xms, for example).  It also doesn't begin with HADOOP_ which means it could conflict with other software.

HADOOP_HEAPSIZE has really awful semantics (forced into m's) and one really should be able to specify your own unit.  But my concern is that it might be "too late" to fix it unless we're very willing to break the universe.  That would probably be a common-dev@ level decision.  (My own vote would be yes, let's break users on this one, because everyone I've ever talked to hates the semantics here too.), After a bit more thinking about this idea, I have a proposal to make that I think will accomplish the ultimate goal of this JIRA while also giving the users much more control than they have now (a win-win):

# We introduce HADOOP_HEAPSIZE_MAX and HADOOP_HEAPSIZE_MIN, both default to empty.  They will set the Xmx and Xms values, respectively.  Both require units to be provided, such that e.g. HADOOP_HEAPSIZE_MAX="10g" is valid.
# We deprecate HADOOP_HEAPSIZE and JAVA_HEAP_MAX and completely remove them from the hadoop-env.sh, etc. documentation.  The code should honor them if set (thereby providing backwards compatibility), but the HADOOP_HEAPSIZE_xyz vars take precedence in case of a conflict.
# I'm willing to do this work as either part of HADOOP-9902 (which rewrites all of this code anyway) or as part of the series of follow-on patches to that change, depending upon the review for 9902.

Thoughts?, Allen, this JIRA is filed for Hadoop Common, YARN problems can be addressed in YARN JIRA.  The hard coded value was introduced in HADOOP-5212.  Before 0.21, the behavior of JVM heap size control is exactly same as this patch.  It does not look like a new feature if we are just rollback to existing Hadoop behavior in 2009.  We don't need to spread fear on this patch is going to the universe because it doesn't.  This has been tested in stress test, and our clusters have been running this patch for almost a year before it was contributed to the community., A veto is valid, even if the code is recently committed. [~eyang], could you please revert the change in branch-2 while this is discussed?, [~eyang]

At least with JDK1.6 we saw zookeeper taking p around 4GB of heap on a 16GB machine thus we filed ZOOKEEPER-1670, I reverted this from branch-2, Removed the fix version since this was reverted.

Additionally, the trunk, post-9902 version of this is part of HADOOP-10950 .  It likely needs to get rebased though., ---^^ this was reverted, so this wasn't fixed in branch-2.  It was replaced by similar code in another JIRA for trunk.]