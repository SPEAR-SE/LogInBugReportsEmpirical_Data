[This could be a known issue described in HADOOP-10768, [~hanm]
Yes, looks like a duplicate. , Duplicate of HADOOP-10768.

Problem here is that the java encryption APIs allocate new byte buffers, get expensive fast.

If it were to be fixed, the obvious solution would be to have an option for native encryption and a private API which would encrypt either in-place or to a preallocated buffer, the latter being able to compensate for the fact that encrypted blocks often get their sizes rounded up.

looks the like HADOOP-10768 patch could be viable, so its a matter of marshalling all the IPC reviewers needed to be happy with it. It's a big patch to a core piece of the code, so will no doubt face inertia â€”but would be invaluable, I've optimized the common code path for non-privacy.  It's true that the current impl creates too many copies of requests.  I've been meaning to fix that but it's been a very low prio.  I'll review the linked jira., [~stevel@apache.org] [~daryn]

Thank you for the analysis and comments. 
For applications with fast moving reloading block information for new data quickly becomes a bottleneck puts significantly more pressure on the NN. , We did custom AES encryption on HBase RPC. It could be adapted to Hadoop RPC. Please see HBASE-16414]