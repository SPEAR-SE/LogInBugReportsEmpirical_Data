[This is a simple map-reduce that uses the default identity mapper / reducer.   To run this, you need to give it two input arguments
   - input file/directory
   - output directory
Next attachment slashr33.txt contains the input file to run on.  I'll document more on the skipping behavior there., 
It turned out not to be trivial to generate a small input file that breaks both the 0.13.0 release and head of the source tree.   It probably is related to buffer size settings...etc.  The only input file I got that consistently breaks all the config setups, is the original 52MB file.  This is not so ideal.   So, here is a 45k input file that breaks 0.13.0 release with the following hadoop-site.xml settings.

{quote}
dfs.replication:  1 
hadoop.tmp.dir 
fs.default.name 
dfs.name.dir 
mapred.job.tracker 
 {quote}

Other config settings may not break on this small input file.

Running MRIdentity on this, you get
{quote}
07/06/13 17:33:28 INFO mapred.JobClient:     Map input bytes=26870
{quote}

Since the file is 45KB,  about 20k bytes didn't get read.

I want to emphasize that different buffersize setup in the config may result in different outcome.     Please let me know if there difficulties reproducing this.

, Does this problem exist in previous release, e.g. 0.12.3? It will be good to know whether this was introduced in 0.13 release. Thanks., It exists in prev releases also., So, can it be related to HADOOP-1491? In that case, distcp copies files with a buffer size specified by "copy.buf.size". It has a default value of 4K.
, > So, can it be related to HADOOP-1491? In that case, distcp copies files with a buffer size specified by "copy.buf.size". It has a default value of 4K.

I don't think so. This bug only truncates and only occurs when user uses 'mark()' on inputstream. I don't hink distcp uses mark().  In the case of HADOOP-1491 one file (or block) seems to get overwritten., This is a good one and very subtle too.

The problem is caused by using an external buffer for checksum verification. So FSInputChecker requires the amount of data read should be at least bytesPerCheccksum, which makes sure that no checksum verified data are read.

We use Java's BufferedInputStream to implement the external buffer and guranteens that the buffer size is at least bytePerChecksum. However, in order to support mark() in BufferedInputStream, BufferedInputStream sometimes may expand its buffer size and issue read that asks for bytes which are less than bytesPerChecksum. Therefore, we get the problem described in this jira.

The proposal in HADOOP-1470 is to verify checksum using an internal buffer.  So the buffer size restriction will no longer be required.  Hence a patch to HADOOP-1470 will also resolve this issue.

An alternative quick fix is to overide markSupported() in FSDataInputStream.Buffer so that it returns false.

, > An alternative quick fix is to overide markSupported() in FSDataInputStream.Buffer so that it returns false.

+1 I think we should do that in the short-term to fix this.
, +1. I think in short term even HADOOP-1470 will not support mark().
, > An alternative quick fix is to overide markSupported() in FSDataInputStream.Buffer so that it returns false.

If you do this, please also fix LineRecordReader().  Otherwise, the initial seek will fail.
My current work around (before i understood what was happening) was delaying
the initial seek till BufferedInputStream() is created.  (The orig code directly reads
using FSDataInputStream)

{code}
    boolean skipFirstLine = false; 
    ...
    } else if (start != 0) { 
      --start; 
      fileIn.seek(start); 
      skipFirstLine = true; 
    } 
 
    this.in = new BufferedInputStream(in); 
    if (skipFirstLine) { 
      start += LineRecordReader.readLine(this.in, null); 
    } 
{code}

Having an extra buffer on top means that the external buffer being used by the FSInputChecker will never be called with mark() or reset().

bwolen
, by "initial seek" I mean it's search for newline in
{code}
  LineRecordReader.readLine(fileIn, null);
{code}
, btw, if people agrees with these, I would be comfortable to submitting a patch, including a check on FSInputChecker.read()'s assumption.
, +1 on the fix to the use of LineRecordReader.readLine. Shall we enforce that LineRecordReader.readLine to take BufferedInputStream as an input?

Bwolen, you are more than welcome to submit a patch., > Shall we enforce that LineRecordReader.readLine to take BufferedInputStream as an input?

would It be too restrictive to force BufferedInputStream on future extensions of
LineRecordReader?

The only requirement is that InputStream needs to support mark/reset.  
This would be easy to check if a LineReader class got broken out of LineRecordReader.
(though maybe an overkill till someone else needs a LineReader outside of LineRecordReader).

Another thing is that reading BufferedReader.nextLine, its implementation does not depend on mark/reset as it remembers seeing \r means that when people request to read the next line, and if the first character is \n, skip it.   It is too bad BufferedReader.nextLine returns a String instead of allowing people to pass it an OutputStream to write to....   Would have been nice to use it to simlify code and avoid a copy.
, We need to mark only because readLine tries to let '\r' alone to be a valid end of line character. I think most other software don't define that way. We could require a '\n' to end a line. Of course readLine should still swallow a '\r' if it immediately followed by '\n'. Then we don't need to mark or 'put back' a byte.
, {noformat}
bash-3.2$ printf "1 \n 2 \n 3 \r 4 \n" | wc
      3       4      14
{noformat}
But current our readLine would count 4 lines.
, BufferedReader.nextLine() will also treat \r alone (without \n afterwards) as a newline by itself.
It basically returns a line when it see a \r or a \n.
On the \r case, it sets a variable called skipLF so that on the next "nextLine()" call,
it will skip the next character if it is \n.

, Then we should fix BufferedReader.nextLine() too.
, 
BufferedReader came from java.io

{quote}
http://java.sun.com/j2se/1.3/docs/api/java/io/BufferedReader.html
{quote}

The semantics of treating \r without \n afterwards as a end of a line seems to be a correct semantics 
since \r is the carriage return character.   

I am only saying what BufferedReader implements basically is what Hadoop's LineRecordReader does
without using mark()/reset().   Had it's nextLine() allows writes to OutputStream, we could simplify 
LineRecordReader to use it instead of implement yet another line reader.

, I like the idea of using the same logic as BufferedReader.  Can we simply maintain a flag instead of using mark() and reset()?
, This function is a stateless static function. Bwolen's work around to create a BufferedInputSream() by before calling ReadLine() should work ok now ( because the caller anyway created BufferedInputStream()) or using BufferedReader.readLine() will also work I think.
, Here is a patch for this bug.   Please review.

This patch "fixes" this bug by
- making FSDataInputStream.Buffer not support mark()/reset()
- make sure all read() in LineRecordReader is done thru a top level InputStreamBuffer.

It also adds checks to FSInputChecker to ensure that whenever read() wants to return -1, it better be not due to the request is asking for 0 bytes.   The check is somewhat aggressive to find potential violations early.   Please comment.

Finally, I added a unittest.  I ended up reading BufferedInputStream to make this test possible.  Current head of tree will fail with an assertion failure.   While it is a good test for testing the existence of current bug, it is not a good test for whether the fix is correct.  i.e., 
this test works with the patch simply because "mark()" does nothing when markSupported() is disabled (modelled after InputStream).    Suggestions?, Bwolen,
regd changes under hadoop/fs/, It will heavily conflict with HADOOP-1470. For hadoop/fs, would it be enough to implement markSupported()  and mark() as you did. 
, 
> changes under hadoop/fs/, It will heavily conflict with HADOOP-1470. 

are you referring to  3 changes in 
   hadoop/fs/ChecksumFileSystem.java
two in callers to readBuffer(), and one in readBuffer() itself.

These 3 changes are checking for 2 assumptions:
1. read() assumes "len >= bytesPerSum"
2. readBuffer() assumes if 0 bytes read, then it must be end-of-file.

- The changes for 1 is 11 lines in 1 code block.
- The changes for 2 is 11 lines in 2 code blocks.

Generally, it would be nice for unusual assumptions to be verified at run time if it doesn't have much performance impacts.
Though if people think it would generate too much integration pain, I will remove the checks.   Please let me know which
of these 3 changes would impact integration badly. 

A separate question, would either of these assumptions go away for HADOOP-1470?
If not, could the implementer please add the checks in his/her version?
Alternatively, will HADOOP-1470 be ready soon?  If so, I would be happy to add those checks back in after 1470 get submitted.

let me know.  thanks
, I am fine either way. I am not opposed to having those checks in hadoop/fs. I think readBuffer() itself will go away in 1470, so both of the assumptions, I think will go way.
, ok.  let's deal with the checks when HADOOP-1470 is done.
Here is a HADOOP-1489.2.patch without those 3 checks.
Are there other issues I should consider before submitting this patch for review?
also, currently the build is broken by hbase.  does patch submission need to wait for the build to be fixed first?

thanks, +1. Changes look good. I am not sure if implementing reset() is necessary though. Default implementation is supposed to check markSupported().
, > Default implementation is supposed to check markSupported().

I just grep thru Hadoop code base, I didn't find any code that checks for this.  It is better to be safer... 
Let me know which function's default implementation is suppose to have checked this.  I will verify whether or not it does.

finally, how does "submit patch" work?  I click on "submit patch" link on the left, but it doesn't allow me to select which patch I am submitting.  does it just assume the newest patch?
, I was thinking of original contract of reset http://java.sun.com/j2se/1.4.2/docs/api/java/io/InputStream.html#reset(). But what you have is fine.

'Submit patch" is just a flag, it does not actually commit anything. Usually the commiter selects the latest patch. Also in the case of Hadoop, this flag triggers automated build and runs unit tests.
, marking for sumbit., > original contract of reset 

Looks in in the entire java source code, only one function actually bothered to check for this...
{code}
   net/URLConnection.java
   static public String guessContentTypeFromStream(InputStream is)
                        throws IOException {
        // If we can't read ahead safely, just give up on guessing
        if (!is.markSupported())
{code}

Let's play it safe just in case people forgets...
thanks for the input.
, +1

http://issues.apache.org/jira/secure/attachment/12360131/HADOOP-1489.2.patch applied and successfully tested against trunk revision r548794.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/310/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/310/console, I just committed this.  Thanks, Bwolen!, Integrated in Hadoop-Nightly #131 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/131/])]