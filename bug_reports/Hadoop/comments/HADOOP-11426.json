[{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12688082/HADOOP-11426.patch
  against trunk revision 389f881.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 3 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ha.TestZKFailoverControllerStress

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5302//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5302//artifact/patchprocess/newPatchFindbugsWarningshadoop-common.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5302//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12688082/HADOOP-11426.patch
  against trunk revision ef1fc51.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5304//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5304//artifact/patchprocess/newPatchFindbugsWarningshadoop-common.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5304//console

This message is automatically generated., I really would prefer not to do this.  What's the problem with just installing the bzip2 development files?

edit:  I guess I should go into more detail about the reasons why I wouldn't like to do this:
* Consistency.  For all the other stuff we {{dlopen}}, we search for both the libraries and the headers.  Why should bzip2 behave differently than everything else?
* "Least surprise" to developers.  By searching for the library, we ensure that developers who successfully create a hadoop build with bzip2 can almost certainly run it on that same machine.
* Lack of a use-case.  Linux distributions usually supply two kinds of packages: development packges with both the header and the libraries, or production packages with just the library.  As far as I know, nobody supplies packages with just header files.

let me know if I missed anything or if there's a use-case I forgot!, There is no rule for build_env == runtime_env, but I agree that is atypical in hadoop. Won't fix.]