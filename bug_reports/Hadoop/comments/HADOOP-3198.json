[
Add simple re-try logic.
, Some comments.
1) Declare a _private static final_ for {{MAX_DFS_RETRIES}} and initialize it to 10. Use this in the for loop. 
2) Remove extra spaces after {{reporter}} (line 14 of the patch)
3) Sleeping for 1 sec needs to be argued. Btw a log message is required before waiting. 
4) Some extra code slipped in (regarding the log message). 
5) After 10 retries we should throw the exception rather than silently coming out of the loop (leading to null pointer exception).

+_Points to ponder_+
Can we do a timeout based stuff where we wait for _shuffle-run-time / 2_ before bailing out and having multiple retries within this timeout. This will somehow make sure that we dont kill the reducer too early., This is the wrong place to do this.

In particular, I believe the HDFS client already does a retry on exists. If it doesn't, it should. If the rpc timeout is coming out of the record writer creation times out, it means that it already failed several times.

-1, 
HDFS client has a retry on exists.
It is likely that it tried and failed the several times. 
That is perhaps fine for exists call in general.

However, for this particular call in getRecordWriter in reduce rask, the cost of failure is too expensive.
Thus, reduce task has to do something special.
In this sense, I think it is reduce task's responsibility to further re-try.

I am open for any suggestions to fix the problem.
However, I am not convenced that re-try at rpc level is the right answer.

, BTW, why the output format class bothers to check the existence of the output dir (see https://issues.apache.org/jira/browse/HADOOP-3218)?
, This will lead to very unmaintainable code. We absolutely do not want to have nested retries for different contexts., 
I saw a jos, 13% of its reducers were failed due to this rpc timeout problem.
That is seriously flawed.

 ]