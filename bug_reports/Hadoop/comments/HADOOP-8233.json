[Patch skips CRC on 0 byte size files and when blocksize between source and target do not match., this should probably have a test., Hey Allen,  fwiw, that attachment is not the patch fix for this ticket.  Hope you weren't thinking otherwise prior to it being in PA state.

Regarding tests, I've been unit testing by creating different blocksize objects from the system default.  Something along the lines of:

 hdfs dfs -Ddfs.blocksize=33554432 -put testData /user/davet/testDataBS32MB

Likewise for zero length:
touch bla
hdfs dfs -put bla /user/davet/bla

distcp is run on the above data with system defaults.   The above tests will fail prior to this patch, and will succeed when complete., Decided it best to split these two issues out.   I created HADOOP-8703 to deal with skip CRC on 0 byte aspect.   , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12520685/HADOOP-8233-branch-0.23.2.patch
  against trunk revision fb06c00.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5604//console

This message is automatically generated., Cancelling patch as this no longer applies., * 0-byte length can be skipped
* blocksize is trouble, as for filesystems != HDFS, there's no guarantee that different blocksize ==> Different checksum.]