[Bug fix was backported to AWS SDK 1.7.14.  Officially, only last two releases are supported by Amazon.  Currently this is 1.10.x and 1.9.x.

I suggest 1.7.14 SDK jar for 2.6.x and 2.7.x, and then moving to latest/greatest 1.10.x for trunk.  Adding patches.

I tested the patches with some basic hdfs fs s3a:// commands.

 , Hi Aaron,

in HADOOP-11684 I have bumped to 1.9.x (we have been testing this for a month now and all is well). Note that other bugs fixed in the aws-sdk (multi-part threshold from int -> long ) require some code changes in s3a. 

You will see in the comments that [~stevel@apache.org] requested to pull out the aws-sdk upgrade to a separate patch. I am doing that today, will link to the new issue then.

Another main benefit of 1.9+ is that s3 is a separate library. We no longer need to pull in the entire sdk., I have isolated the aws-sdk bump in HADOOP-12269, v2 patch for branch-2.7, v2 patch for branch-2.6, Tested these two v2 patches with S3, ensuring behavior is the same around the 2GB-1 boundary for fs.s3a.multipart.threshold.

[~Thomas Demoor] already started on patches for trunk, which will use latest-greatest aws-java-sdk.

I think we should move forward with these patches for the 2.6.x and 2.7.x branches (fixes bugs for existing customers who can't upgrade to trunk).
, Attached v3 patch that leaves DEFAULT_MIN_MULTIPART_THRESHOLD as the same 2^31-1 value., Submitting single patch for branch-2 instead [~eddyxu], Hi, [~fabbri]. The patch itself looks good to me. Would you cooperate with [~Thomas Demoor] and [~stevel@apache.org] regarding the version of aws-sdk to pull in?

Thanks a lot for the efforts., Hi, [~fabbri] As we discussed offline, we will close this as duplicated to HADOOP-12269. Lets bump aws-sdk version in both trunk and branch-2 in HADOOP-12269. 

Thanks again for this effort.]