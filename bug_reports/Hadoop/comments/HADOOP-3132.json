[Random writer should be able to re-produce this problem.

Try  a map only job with 500 mappers, each writing 4GB data.
, Runping, Dhruba and I have been looking at this. findings till now are strange. It is strange enough to be a kernel bug. Here a client is writing a block to three datanodes : DN-1, DN-2, and DN-3 in this order.

In brief, what happens is that DN-3 gets stuck while reading data from DN-2. But netstat on DN-3 shows that recv buffer for socket from DN-2 is full. The stacktrace of the stuck thread on DN-3 is shown below (*). So we suspected may be there is a bug in JRE with non-blocking sockets or kernel. 

Even though these sockets are running with large timeout values (not the default 10min), at around 16min after the pipeline is stuck like this,   DN-2 gets "Connection timeout out" error while it is reading from the socket for ack for the last packet from DN-3. DN-3 keeps sending the 'keep-alive" every 30 seconds and DN-2 receives them every 30 seconds till then. If there is no problem at the kernel or jre, there is no reason for this read to get such an error (unless there is some TCP timeout we don't know), note that the block receive thread at DN-2 w. At this point write pipeline gets broken. This pipeline should have stayed stuck forever ("keep-alive" messages keep coming).

Another option tied is to use regular sockets on DataNodes using patch from HADOOP-3124. The same thing happened again. (This time the pipeline did not recover properly and the task failed.. I will file another jira for it).

Next thing to try is to select() on the write socket with 10 second timeout at a time (user specified timeout will still be obeyed.. but the thread wakes up every 10 seconds) and hope for better.

(*) : Stacktrace on DN-3 : {noformat}
"org.apache.hadoop.dfs.DataNode$DataXceiver@16fcc4" daemon prio=10 tid=0x08567400 nid=0x7d7 runnable [0xb1072000..0xb10730a0]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:184)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0xb64d54d8> (a sun.nio.ch.Util$1)
        - locked <0xb64d54c8> (a java.util.Collections$UnmodifiableSet)
        - locked <0xb64d52c0> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:237)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:155)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:149)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:122)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        - locked <0xb66b3858> (a java.io.BufferedInputStream)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveChunk(DataNode.java:2257)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2398)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2463)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1192)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:964)
        at java.lang.Thread.run(Thread.java:619)
{noformat}




 


, Next time we could also tcpdump traffic for socket between DN2 and DN3 until DN2 gets "Connection timeout out" error., I could not reproduce this problem on another cluster of the same size. will try on the old cluster., Making this a non-blocker and moving it to 0.18 because :
# It is not fatal. DFS writes, tasks recover from it.
# happens very very rarely. Till now we know only one cluster where this happens.
# mostly looks like a bug outside Hadoop and JRE (so may not be present on different kernel versions, hardware or os, switches).

Why delay in diagnosis :
hard to reproduce and requires a specific 500 node cluster., 
tcpdump on the sender (second datanode in the pipeline) shows the TCP connection was stuck because of a missing packet. Retransmission of the missing packet does not seem to be accepted by the receiver (might be because of wrong checksum, did not capture traffic on the receiver, will try next time).

I got the traffic for last 3-4 minutes on the sender before connection was broken. This explains all the observations :
# sender has lot of data in its 'sendbuf'
# receiver has a lot of data in its 'recvbuf', but DataNode is blocked in this socket's read.
# after 16 minutes or so sender's write fails with 'connect timeout' exception.

The missing packet is also confirmed by the fact that every packet from the remote side has (tcp option) SACK data with "1448-31332 (relative values)". This implies the receiver is missing first 1448 bytes from the acked seqno.

There are two retransmissions  of this missing packets in the capture (2 min apart). ethereal says that checksum is incorrect (not sure if this is dependable since we are not sure if checksum is offloaded etc). But in both cases the packet has same wrong value though it needs to be different because of different TCP headers. Traffic on receiver would make this more clear.

In any case, this is not an application bug.

, I tried couple of more times to reproduce this case but could not. The cluster was rebooted and bios settings were changed a little bit (for unrelated reasons). Not sure if reboots helped. We can open it again if we notice this again.]