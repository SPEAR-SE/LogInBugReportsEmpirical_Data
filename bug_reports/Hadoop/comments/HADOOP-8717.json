[If use non-array assignment of JAVA_HOME it works.

diff --git a/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh b/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh
index aa971f9..02e5f15 100644
--- a/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh
+++ b/hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh
@@ -136,9 +136,9 @@ if [[ -z $JAVA_HOME ]]; then
   # On OSX use java_home (or /Library for older versions)
   if [ "Darwin" == "$(uname -s)" ]; then
     if [ -x /usr/libexec/java_home ]; then
-      export JAVA_HOME=($(/usr/libexec/java_home))
+      export JAVA_HOME=$(/usr/libexec/java_home)
     else
-      export JAVA_HOME=(/Library/Java/Home)
+      export JAVA_HOME=/Library/Java/Home
     fi
   fi, used non-array to set the JAVA_HOME under mac, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12542647/HADOOP-8717.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ha.TestZKFailoverController

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1366//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1366//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12542666/HADOOP-8717.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1368//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1368//console

This message is automatically generated., After the patch, I start dfs and yarn and rerun the job successfully.


âš¡ bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar wordcount /file.txt output3
12/08/27 14:52:34 INFO input.FileInputFormat: Total input paths to process : 1
12/08/27 14:52:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12/08/27 14:52:34 WARN snappy.LoadSnappy: Snappy native library not loaded
12/08/27 14:52:34 INFO mapreduce.JobSubmitter: number of splits:1
12/08/27 14:52:34 WARN conf.Configuration: mapred.jar is deprecated. Instead, use mapreduce.job.jar
12/08/27 14:52:34 WARN conf.Configuration: mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
12/08/27 14:52:34 WARN conf.Configuration: mapreduce.combine.class is deprecated. Instead, use mapreduce.job.combine.class
12/08/27 14:52:34 WARN conf.Configuration: mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class
12/08/27 14:52:34 WARN conf.Configuration: mapred.job.name is deprecated. Instead, use mapreduce.job.name
12/08/27 14:52:34 WARN conf.Configuration: mapreduce.reduce.class is deprecated. Instead, use mapreduce.job.reduce.class
12/08/27 14:52:34 WARN conf.Configuration: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
12/08/27 14:52:34 WARN conf.Configuration: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
12/08/27 14:52:34 WARN conf.Configuration: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
12/08/27 14:52:34 WARN conf.Configuration: mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
12/08/27 14:52:34 WARN conf.Configuration: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
12/08/27 14:52:35 INFO mapred.ResourceMgrDelegate: Submitted application application_1346104327473_0002 to ResourceManager at /0.0.0.0:8032
12/08/27 14:52:35 INFO mapreduce.Job: The url to track the job: http://LM-SJN-00714134:8088/proxy/application_1346104327473_0002/
12/08/27 14:52:35 INFO mapreduce.Job: Running job: job_1346104327473_0002
12/08/27 14:52:42 INFO mapreduce.Job: Job job_1346104327473_0002 running in uber mode : false
12/08/27 14:52:42 INFO mapreduce.Job:  map 0% reduce 0%
12/08/27 14:52:46 INFO mapreduce.Job:  map 100% reduce 0%
12/08/27 14:52:50 INFO mapreduce.Job:  map 100% reduce 100%
12/08/27 14:52:50 INFO mapreduce.Job: Job job_1346104327473_0002 completed successfully
12/08/27 14:52:50 INFO mapreduce.Job: Counters: 43
	File System Counters
		FILE: Number of bytes read=3267
		FILE: Number of bytes written=131819
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=2767
		HDFS: Number of bytes written=2279
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=15752
		Total time spent by all reduces in occupied slots (ms)=18328
	Map-Reduce Framework
		Map input records=87
		Map output records=317
		Map output bytes=3881
		Map output materialized bytes=3027
		Input split bytes=95
		Combine input records=317
		Combine output records=186
		Reduce input groups=186
		Reduce shuffle bytes=3027
		Reduce input records=186
		Reduce output records=186
		Spilled Records=372
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=15
		CPU time spent (ms)=0
		Physical memory (bytes) snapshot=0
		Virtual memory (bytes) snapshot=0
		Total committed heap usage (bytes)=282660864
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=2672
	File Output Format Counters 
		Bytes Written=2279
, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12546458/HADOOP-8717.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1513//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1513//console

This message is automatically generated., I just ran into this same issue with Hadoop 2.0.4-alpha on OS X 10.8.3., The attached patch fixed things for me, but I just thought I'd mention that this affects that Hadoop version, too., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12546458/HADOOP-8717.patch
  against trunk revision 98588cf.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4714//console

This message is automatically generated., This is already fixed in trunk., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12546458/HADOOP-8717.patch
  against trunk revision 9112f09.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5598//console

This message is automatically generated., I doubt if this patch is committed. I faced same issue on hadoop 2.6.3 on my Mac. I corrected the "hadoop-config.sh" manually, then it started working., Using spring-hadoop-test version 2.3.0 suite on mac with hadoop 2.6 - facing the same issue., Starting a pseudo-distributed cluster on Mac OS X El Capitan using the sbin/start-yarn.sh script encounters the JAVA_HOME issue on Hadoop 2.7.3. The culprit is the slaves.sh script, which is called by yarn-daemons.sh in a for loop over all of the slave nodes. slaves.sh ssh's into each slave machine and then runs the sbin/yarn-daemon.sh script to start the NM. In Pseudo-distributed mode, this will just be localhost. Running sbin/yarn-daemon.sh manually to start the NM on localhost works fine, but running it indirectly through start-yarn.sh -> yarn-daemons.sh -> slaves.sh seems to unset or clear JAVA_HOME. I have not dug too far into the NM code, but it would make sense for JAVA_HOME to be unset/cleared, since the NM fails when starting containers because it can't find "/bin/java" instead of "/somepath/bin/java". 

Steps to reproduce failure. This will fail with exit code 127 due to the container failing to start.
{noformat}
$HADOOP_PREFIX/bin/hdfs namenode -format;
$HADOOP_PREFIX/sbin/start-dfs.sh;
$HADOOP_PREFIX/sbin/start-yarn.sh;
{noformat}

Running The following sleep job will give an error that the container failed to start.
{noformat}
$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 1 -rt 1
{noformat}

{noformat}
2016-02-22 10:55:58,142 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1449)) - Job job_1456160109510_0001 failed with state FAILED due to: Application application_1456160109510_0001 failed 3 times due to AM Container for appattempt_1456160109510_0001_000003 exited with  exitCode: 127
For more detailed output, check application tracking page:http://localhost:8088/cluster/app/application_1456160109510_0001Then, click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_e53_1456160109510_0001_03_000001
Exit code: 127
Stack trace: ExitCodeException exitCode=127: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)
	at org.apache.hadoop.util.Shell.run(Shell.java:456)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 127
Failing this attempt. Failing the application.
2016-02-22 10:55:58,158 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1454)) - Counters: 0
{noformat}

Steps that set up pseudo-distributed correctly. Running a sleep job using this setup will succeed. 
{noformat}
$HADOOP_PREFIX/bin/hdfs namenode -format;
$HADOOP_PREFIX/sbin/start-dfs.sh;
$HADOOP_PREFIX/sbin/yarn-daemon.sh start resourcemanager;
$HADOOP_PREFIX/sbin/yarn-daemon.sh start nodemanager;
{noformat}, [~aw] I'm still seeing the extra parens in hadoop-config.sh in the bin artifact of 2.7.3 even though you mentioned this was fixed in trunk by Feb '15; am I missing something, or is there anything I can do to help get this into a release?, 3.0.0-alpha1 is the first release with the fix. (that was trunk when I wrote that.), Got it, thank you.]