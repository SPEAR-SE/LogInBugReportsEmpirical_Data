[Attached a test case class that opens and closes a stream in a loop:
{code:title=Bzip2MemoryTester.java}
for (int i = 0; i < iterations; i++) {
	try (InputStream stream = codec.createInputStream(fileSystem.open(inputFile))) {
		System.out.println(stream.read());
	}
}
{code}

Running the loop for 100000 times causes the process to be killed by the OS on my machine before reaching 100000 lines of output. Monitoring the process's RSS shows that it grows significantly.

After placing the attached {{Bzip2MemoryTester.java}} and {{log4j.properties}} files in an arbitrary folder and setting the {{HADOOP_HOME}} environment variable, the following can be used to run the test case:

{noformat}
echo 'a' > test && bzip2 test

javac -cp $HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/common/lib/* Bzip2MemoryTester.java

java -Xmx128m -cp .:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/common/lib/* -Djava.library.path=$HADOOP_HOME/lib/native Bzip2MemoryTester test.bz2 100000 > out.txt 2> err.txt &

export PID=$(jps | grep Bzip2MemoryTester | cut -d' ' -f1); while [ -a /proc/${PID} ]; do grep VmRSS /proc/${PID}/status; sleep 2; done

grep '^97$' out.txt | wc -l out.txt
{noformat}, Test case, Thanks for the report, [~eliac]!  This problem isn't specific to bzip2, as I was able to reproduce the problem with both the gzip and zstandard codecs.  I updated the summary accordingly.

This looks like it may have been an accidental oversight when HADOOP-10591 was added.  Before that change the DecompressorStream close method was a superset of what CompressionInputStream did.

It looks like LineRecordReader and some other users of codecs aren't susceptible to this because they explicitly get the decompressor from the codec pool, create the input stream, then explicitly return the decompressor to the pool afterwards.  I believe it's safe to try to return the same decompressor to the pool multiple times, so we should be able to safely update the DecompressorStream to call super.close() rather than in.close().  Also should be straightforward to write a unit test, using CodecPool.getLeasedDecompressorsCount to verify the codec is not being returned to the pool before the change and is afterwards.

[~eliac] are you interested in taking a crack at the patch?  If not then I should be able to put up something later this week., Thanks [~jlowe]! Absolutely, I'll prepare a patch. I wasn't sure how to write a unit test that checks off-heap memory for a leak, but using CodecPool.getLeasedDecompressorsCount is much simpler., Patch attached. First time contributor, I hope I followed the guidelines correctly.

For testing, I enhanced an existing unit test - TestCodec.codecTest(), since it's already invoked for different types of native and pure-Java codecs. I added an assertion that the number of leased decompressors after the test equals to the one before it. This exposed a similar bug in BZip2Codec.BZip2CompressionInputStream.close(), which also doesn't call its super.close() method, and thus doesn't return the decompressor to the pool.

Adding an assertion for compressors as well as decompressors uncovered a similar issue in CompressorStream.close(), GzipCodec.GzipOutputStream.close(), and BZip2Codec.BZip2CompressionOutputStream.close(), which I attempted to fix as well.

Regarding BZip2Codec.BZip2CompressionOutputStream.close(), I removed the overriding method altogether, because the superclass's close() method invokes finish(). The finish() method handles internalReset() if needed, and also calls output.finish(), which eliminates the need to call output.flush() or output.close().

Testing GzipCodec without native libraries showed that CodecPool erroneously calls updateLeaseCounts even for compressors/decompressors that are null, or ones with the @DoNotPool annotation. I added a condition that checks for that.

The memory leak only manifests when using the native libraries. In Eclipse I achieved this by setting java.library.path in the unit test launcher. Seeing the usage of assumeTrue(isNative*Loaded()), I understand that native-related tests are covered in Maven builds as well.

Looking forward to a code review., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 25s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 19m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 20s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 49s{color} | {color:red} hadoop-common-project/hadoop-common in trunk has 19 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 16m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 42s{color} | {color:green} hadoop-common-project/hadoop-common: The patch generated 0 new + 112 unchanged - 6 fixed = 112 total (was 118) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m  1s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 34s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 76m 45s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.net.TestDNS |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HADOOP-14376 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12866684/HADOOP-14376.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 642da560dc51 3.13.0-116-generic #163-Ubuntu SMP Fri Mar 31 14:13:22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / cef2815 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/12267/artifact/patchprocess/branch-findbugs-hadoop-common-project_hadoop-common-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/12267/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12267/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12267/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks for the patch!  The test failure is unrelated.

bq. Regarding BZip2Codec.BZip2CompressionOutputStream.close(), I removed the overriding method altogether, because the superclass's close() method invokes finish(). The finish() method handles internalReset() if needed, and also calls output.finish(), which eliminates the need to call output.flush() or output.close().

I'm not sure this is a net good change.  Other maintainers will come along and see that BZip2CompressionOutputStream never calls close() on one of its private member streams which is usually a bug.  Even if the close() ends up being redundant today that doesn't mean it always will.  The root cause for this JIRA is a great example.  Also I'm not seeing how the superclass's finish() method ever ends up closing the out stream.  I see it write some final bytes and sets it to null, which in turn _prevents_ the close() method from trying to call out.close(), so I'm wondering how the output stream normally gets closed.

For the BZip2CompressionInputStream change, if input.close() throws then we won't call super.close() and we could leak some resources and won't return the codec to the pool.

For the CompressorStream patch:
{code}
   public void close() throws IOException {
     if (!closed) {
       try {
-        finish();
+        super.close();
       }
       finally {
-        out.close();
         closed = true;
       }
     }
{code}

This is subtly different that the previous code because finish() can throw.  In the old code, finish() could throw and out.close() would still be called but now we'll skip calling out.close() but still set closed=true so we can't retry the close.  This change was done by HADOOP-10526 but it looks like they missed the same change for DecompressorStream.  Similarly the CompressionInputStream/CompressionOutputStream code won't return the codec to the pool if finish() throws or the underlying stream's close() throws., Thanks [~jlowe]. Those are excellent points, I completely agree that the patch introduced subtle differences if some of the streams throw exceptions upon close(). My previous reasoning was that in this case something's probably gone horribly and irrevocably wrong anyway. But following your comments, I prepared a more defensive patch, in which even if some of the close() or finish() methods throw exceptions we still try to close/recover what we can. The price of this is assuming that it's okay to call the close() method of a stream multiple times.

*BZip2CompressionOutputStream*:
bq. Other maintainers will come along and see that BZip2CompressionOutputStream never calls close() on one of its private member streams which is usually a bug. Even if the close() ends up being redundant today that doesn't mean it always will. The root cause for this JIRA is a great example.
In patch 002 I put back the BZip2CompressionOutputStream.close() method with a call to super.close() and some explanatory documentation. It still seems to me that calling super.close() should be sufficient, let me try to explain why.

bq. I'm not seeing how the superclass's finish() method ever ends up closing the out stream. I see it write some final bytes and sets it to null, which in turn prevents the close() method from trying to call out.close(), so I'm wondering how the output stream normally gets closed.
My understanding is that the output stream _does_ get closed, thanks to the out.close() call in CompressionOutputStream.close(). The {{out}} data member of CBZip2OutputStream is indeed nullified, but the {{out}} data member of CompressionOutputStream should still reference the actual stream object.

My reasoning was: the only difference between BZip2CompressionOutputStream.finish() and close() is that BZip2CompressionOutputStream.finish() calls output.finish(), whereas BZip2CompressionOutputStream.close() calls output.flush() and output.close(). Changing BZip2CompressionOutputStream.close() to super.close() will mean that we invoke finish() only instead of flush() and close(). Looking at CBZip2OutputStream (which can be the only class of the {{output}} data member in the current implementation), it seems to me that it's okay to invoke finish() instead of flush() + close(), because the only difference between them is calling out.flush() + out.close(). As I said above, out.close() will be called anyway by CompressionOutputStream.close(), and I'm assuming that any reasonable stream calls flush() internally on close().

*BZip2CompressionInputStream*:
In BZip2CompressionInputStream, patch 002 puts the call to super.close() in a finally block. This preserves the previous logic (set needsReset to true only if input.close() didn't throw) while ensuring that super.close() will unconditionally close the {{in}} stream and return the trackedDecompressor to the pool.

*CompressorStream/CompressionInput/OutputStream*:
bq. This is subtly different that the previous code because finish() can throw. In the old code, finish() could throw and out.close() would still be called but now we'll skip calling out.close() but still set closed=true so we can't retry the close. (...) Similarly the CompressionInputStream/CompressionOutputStream code won't return the codec to the pool if finish() throws or the underlying stream's close() throws.
In patch 002 I wrapped each of CompressionInput/OutputStream.close()'s internal steps in try/finally. (For CompressionOutputStream.close() this leaves the corner case of both finish() _and_ out.close() throwing an exception each, but I think it's reasonable that only one of them will be propagated since it's a doomed stream anyway.) This brings the behavior of the patched CompressorStream.close() to what it was before my changes: if finish() throws, out.close() is still called., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} mvnsite {color} | {color:red}  0m 27s{color} | {color:red} hadoop-common in trunk failed. {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 41s{color} | {color:red} hadoop-common-project/hadoop-common in trunk has 19 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 49s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 34s{color} | {color:orange} hadoop-common-project/hadoop-common: The patch generated 1 new + 118 unchanged - 6 fixed = 119 total (was 124) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 16s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch 3 line(s) with tabs. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  6m 50s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 25s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 58m 34s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.net.TestDNS |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HADOOP-14376 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12867001/HADOOP-14376.002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux e35f1d93bd5c 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 1769b12 |
| Default Java | 1.8.0_121 |
| mvnsite | https://builds.apache.org/job/PreCommit-HADOOP-Build/12272/artifact/patchprocess/branch-mvnsite-hadoop-common-project_hadoop-common.txt |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/12272/artifact/patchprocess/branch-findbugs-hadoop-common-project_hadoop-common-warnings.html |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/12272/artifact/patchprocess/diff-checkstyle-hadoop-common-project_hadoop-common.txt |
| whitespace | https://builds.apache.org/job/PreCommit-HADOOP-Build/12272/artifact/patchprocess/whitespace-tabs.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/12272/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12272/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12272/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Patch 003 is the same as 002 with tabs converted to spaces., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 20s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 23s{color} | {color:red} hadoop-common-project/hadoop-common in trunk has 19 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 14m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 37s{color} | {color:green} hadoop-common-project/hadoop-common: The patch generated 0 new + 119 unchanged - 6 fixed = 119 total (was 125) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 20s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 11s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 35s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 65m 16s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HADOOP-14376 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12867034/HADOOP-14376.003.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux e28c1350b9d7 3.13.0-116-generic #163-Ubuntu SMP Fri Mar 31 14:13:22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 749e5c0 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/12275/artifact/patchprocess/branch-findbugs-hadoop-common-project_hadoop-common-warnings.html |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12275/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12275/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks for updating the patch!

I still think BZip2CompressionOutputStream.close() should be doing more than just calling super.close().  BZip2CompressionOutputStream has an "output" field that is private and instantiated by the class, yet it never calls the close() method on it.  While it's true that _today_ calling output.close() won't do anything useful because underlying resources are closed/freed by other entities, that may not always be the case in the _future_.  Someone could come along later and update CBZip2OutputStream such that it becomes critical to call its close() method, and failure to do so means we start leaking at that point.

The following:
{code}
  @Override
  public void close() throws IOException {
    if (!closed) {
      try {
        super.close();
      }
      finally {
        closed = true;
      }
    }
  }
{code}
can be simplified to:
{code}
  @Override
  public void close() throws IOException {
    if (!closed) {
      closed = true;
      super.close();
    }
  }
{code}
although even that has a code smell.  Why are we protecting the parent's close method from being idempotent on redundant close?  The parent's method should already be doing that, which precludes the need to have an override at all since there's nothing else to do in the close method other than call the parent's version.  The closed check logic should be moved into the parent rather than having the child do it on behalf of the parent.
, I see what you mean, Jason. Thanks for your comments!

*BZip2CompressionOutputStream:*
Putting back output.close() brings us to the following:
{code:java}
  @Override
  public void close() throws IOException {
    try {
      super.close();
    } finally {
      output.close();
    }
  }
{code}

*CompressorStream:*
I was attempting to change the current implementation as little as possible. Switching the order of closed = true and super.close() may affect subclasses, especially user-supplied ones (e.g. if they rely on the state of the closed flag in their finish() method). So what would be the best course of action here? Switch the order to simplify the method? Move the closed check logic into the parent (which also affects subclasses)? If so, should a separate "finished" flag be added to keep track of whether finish() was completed successfully? Similarly, should the closed check logic of DecompressorStream be moved to _its_ parent? Also, in DecompressorStream the closed flag is set to true only if super.close() doesn't throw - which I also haven't changed so far., Yeah, I see what you mean if derived classes are looking at the closed flag.  Let's leave the closed flag logic as-is for now in CompressorStream, although I do think we should make the DecompressorStream logic consistent with how it's done in CompressorStream., Great. Attaching patch 004, which includes adding output.close() to BZip2CompressionOutputStream.close(), and aligning DecompressorStream.close() to the same try/finally structure as CompressorStream.close()., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 18s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 24s{color} | {color:red} hadoop-common-project/hadoop-common in trunk has 19 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 32s{color} | {color:green} hadoop-common-project/hadoop-common: The patch generated 0 new + 118 unchanged - 6 fixed = 118 total (was 124) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  7m 25s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 59m 41s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HADOOP-14376 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12867410/HADOOP-14376.004.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux ef6ec2450546 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ad1e3e4 |
| Default Java | 1.8.0_121 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/12290/artifact/patchprocess/branch-findbugs-hadoop-common-project_hadoop-common-warnings.html |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12290/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12290/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, +1 latest patch ltgm.  I'll commit this later today if there are no objections., Thanks, Eli!  I committed this to trunk, branch-2, branch-2.8, and branch-2.7., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11731 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11731/])
HADOOP-14376. Memory leak when reading a compressed file using the (jlowe: rev 7bc217224891b7f7f0a2e35e37e46b36d8c5309d)
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionOutputStream.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DecompressorStream.java
* (edit) hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodec.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionInputStream.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CodecPool.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressorStream.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java
, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14057 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14057/])
HADOOP-14376. Memory leak when reading a compressed file using the (xyao: rev 192f1e63180d4ddfc7fa204090a3341190f1b0df)
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionOutputStream.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DecompressorStream.java
* (edit) hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodec.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CodecPool.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressorStream.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionInputStream.java
]