[13/10/03 20:21:22 INFO client.RMProxy: Connecting to ResourceManager at namenode/68.142.246.96:8032
Running on 3 nodes to sort from har://... hdfs://namenode:8020/user/name/archive_tests/dst/archive_11 with 5 reduces.
Job started: Thu Oct 03 20:21:22 UTC 2013
13/10/03 20:21:23 INFO client.RMProxy: Connecting to ResourceManager at namenode/68.142.246.96:8032
13/10/03 20:21:23 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 174 for name on 68.142.246.96:8020
13/10/03 20:21:23 INFO security.TokenCache: Got dt for hdfs://namenode:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 68.142.246.96:8020, Ident: (HDFS_DELEGATION_TOKEN token 174 for name)
13/10/03 20:21:23 INFO mapreduce.JobSubmitter: Cleaning up the staging area /user/name/.staging/job_1380795041307_0008
java.lang.IllegalArgumentException: java.net.UnknownHostException: hdfs-namenode
        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:418)
        at org.apache.hadoop.security.SecurityUtil.buildDTServiceName(SecurityUtil.java:299)
        at org.apache.hadoop.fs.FileSystem.getCanonicalServiceName(FileSystem.java:298)
        at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:520)
        at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:504)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:235)
        at org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:59)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:340)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:491)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:508)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1268)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1265)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1265)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1286)
        at org.apache.hadoop.examples.Sort.run(Sort.java:180)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.examples.Sort.main(Sort.java:191)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: java.net.UnknownHostException: hdfs-namenode
, Some more methods that should be overridden in HarFileSystem. This issue is mainly due to not overriding FileSystem#getChildFileSystems()., BTW couple observation related to this issue:
# From the stack trace the URI corresponding to the service is print as hdfs-<namenode host>
# This is due to the following code where getUri() from HarFileSystem is being invoked:
{code}
  public String getCanonicalServiceName() {
   return (getChildFileSystems() == null)
     ? SecurityUtil.buildDTServiceName(getUri(), getDefaultPort())
     : null;
 }
{code}

With this change, FileSystem.collectDelegationTokens will correctly work., Updated patch that adds tests along the lines of FilterFileSystem to catch or ignore FileSystem methods that should be overridden., Updated patch with some more cleanup., *  resolvePath(). Not sure what is correct here. Resolve is suppose to follow though symlinks/mount points and resolve the path. One possibility is to make it the same as the default implementation of FileSystem (it calls fileStatus.getPath)?
* Add comment why getCanonicalUri calls the underlying file system (due to tokens).
* I would make ALL the copyFromXX and moveFromXX variants  throw the exception rather than rely on the fact that FileSystem's default implementation calls one of copyFromXX that HarFileSystem implements and throws exception. , bq. I would make ALL the copyFromXX and moveFromXX variants throw the exception rather than rely on the fact that FileSystem's default implementation calls one of copyFromXX that HarFileSystem implements and throws exception.
There are many other methods that rely on this such as create(), createNonRecursive() etc. We will be adding all these implementations. At some point in time, we should make the internal method used in FileSystem implementation of xyz() as xyzInternal() to establish this pattern.

bq. resolvePath(). Not sure what is correct here....
I am just restoring the previous behavior here (which was changed by HADOOP-10003). I will create a separate jira to discuss and address this.

Other comments taken care of., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607702/HADOOP-10029.3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1535 javac compiler warnings (more than the trunk's current 1525 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3198//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/3198//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3198//console

This message is automatically generated., There are compiler warnings. Otherwise +1., Updated patch that adds suppress warning for deprecation, The javac warnings are due to use of deprecated methods. I have @SuppressWarnings annotation to suppress the warnings., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607837/HADOOP-10029.4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1529 javac compiler warnings (more than the trunk's current 1525 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3199//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/3199//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3199//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607838/HADOOP-10029.4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1535 javac compiler warnings (more than the trunk's current 1525 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3200//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/3200//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3200//console

This message is automatically generated., Updated patch. For some reason @SuppressWarnings for deprecation is not working for TestHarFileSystem. I removed explicitly declaring the deprecated AccessControlException in the interface to work around it., Renamed the interface DoNoCheck to MustNotImplement., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607875/HADOOP-10029.5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3201//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3201//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607879/HADOOP-10029.6.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3203//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3203//console

This message is automatically generated., +1 , SUCCESS: Integrated in Hadoop-trunk-Commit #4580 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4580/])
HADOOP-10029. Specifying har file to MR job fails in secure cluster. Contributed by Suresh Srinivas. (suresh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1531125)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestHarFileSystem.java
, I have committed the change to branch-2.2, branch-2 and trunk. [~sanjay.radia], thank you for the review., SUCCESS: Integrated in Hadoop-Yarn-trunk #359 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/359/])
HADOOP-10029. Specifying har file to MR job fails in secure cluster. Contributed by Suresh Srinivas. (suresh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1531125)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestHarFileSystem.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1549 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1549/])
HADOOP-10029. Specifying har file to MR job fails in secure cluster. Contributed by Suresh Srinivas. (suresh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1531125)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestHarFileSystem.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1575 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1575/])
HADOOP-10029. Specifying har file to MR job fails in secure cluster. Contributed by Suresh Srinivas. (suresh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1531125)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestHarFileSystem.java
]