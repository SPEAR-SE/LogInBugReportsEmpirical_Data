[Can you pl post the offending stack trace? Thanks., The trunk I had was not the latest and was not entirely clean. Since this is so easy to reproduce, I didn't attach initially. Now I cleaned up my source and updated to latest trunk. The trace from the latest trunk :

{noformat}
java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.hadoop.dfs.FSNamesystem.getBlockLocations(FSNamesystem.java:596)
	at org.apache.hadoop.dfs.FSNamesystem.getBlockLocations(FSNamesystem.java:563)
	at org.apache.hadoop.dfs.NameNode.getBlockLocations(NameNode.java:280)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)
{noformat}, Here is a patch that attaches a zero-size block when a file has not data in it. This prevents the Exception that you are seeing. It also fixes the problem where a zero size file get magically converted into directory when the namenode is restarted., This adds a block to the system that does not exist on any datanode. When I tested the patch, when I restarted my Namenode it never came out of safemode because of this mismatch.
, The directory entry has the number of blocks set to -1., This patch passes all unit tests and is ready for code review, Did you test the case described in the description of the jira?

I am still getting errors. Though different ones. Namenode log has the following exception when goto /user/rangadi on webui :
{noformat}
2007-09-14 18:04:04,193 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 7020, 
call getFileInfo(/user/rangadi/xxx0) from 192.168.0.118:60339: error: java.io.IOException: 
File does not exist
java.io.IOException: File does not exist
        at org.apache.hadoop.dfs.FSDirectory.getFileInfo(FSDirectory.java:418)
        at org.apache.hadoop.dfs.FSNamesystem.getFileInfo(FSNamesystem.java:1285)
        at org.apache.hadoop.dfs.NameNode.getFileInfo(NameNode.java:459)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)
{noformat}

and webpage shows (not sure why it does not show the exception):
HTTP ERROR: 500
Index: 0, Size: 0
RequestURI=/browseDirectory.jsp
Powered by Jetty://
, The earlier patch caused the browseDirectory jsp to encounter an error. Attaching a new patch that fixes the jsp related problem. It appears that we do not have any unit tests for testing jsp functionality. I will open a separate JIRA to create unit tests for hdfs jsps.

The FileNotFound exception in the namenode log is expected. A client is invoking isDir() to determine if the path is a directory. The deprecated isDir() call is invoke getFileStatus but the file does not exist yet, hence the exception in the namenode log., bq. The FileNotFound exception in the namenode log is expected. A client is invoking isDir() to determine if the path is a directory. The deprecated isDir() call is invoke getFileStatus but the file does not exist yet, hence the exception in the namenode log.

File does exist, right? Otherwise, how does client even know the path to use for isDir()? , I am using trunk version, and my namenode servers spilling out messages like

2007-09-17 22:41:01,525 INFO org.apache.hadoop.ipc.Server: IPC Server handler 29 on 9000, call blockReport(<ipaddress>:50010, [Lorg.apache.hadoop.dfs.Block;@af7fab) from <ipaddress>:40393: error: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException
java.io.IOException: java.lang.ArrayIndexOutOfBoundsException

Mulitple messages per second from multiple servers.

Is this related to this issue?

This patch changes the layout version. Is there anything else than the LAYOUT_VERSION constant in dfs/FSConstants.java that determines the layout version? My first attempt to use this patch failed for some reason even when I used the -upgrade option for starting the namenode.
, > 40393: error: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException

This should be HADOOP-1904.

As far I know you do not need this patch. The bug here is not fatal., Thanks, Raghu.

I searched for ArrayIndexOutOfBoundsException and did not get HADOOP-1904 because of missing s in ArrayIndexOutOfBoundException :), Removed redundant assert., getBlockLocations() can return null.
So you should either check for null value after calling it or return an empty array if the file is empty.
By the way the same problem happens with fsck, when it examines empty files.
{code}
07/09/17 17:07:37 WARN /: /fsck?path=%2F: 
java.lang.NullPointerException
	at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:156)
	at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:147)
	at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:147)
	at org.apache.hadoop.dfs.NamenodeFsck.check(NamenodeFsck.java:147)
	at org.apache.hadoop.dfs.NamenodeFsck.fsck(NamenodeFsck.java:123)
	at org.apache.hadoop.dfs.FsckServlet.doGet(FsckServlet.java:49)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
	at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
	at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
	at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
	at org.mortbay.http.HttpServer.service(HttpServer.java:954)
	at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
	at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
	at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
	at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
	at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
	at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
{code}, The crash in NamenodeFsck is not directly related to this fix. getBlockLocations returns null in the case of error. It continues to do so. I will open a separate jira issue for that one., +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12366051/zeroSizeFile5.patch
against trunk revision r577010.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/783/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/783/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/783/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/783/console

This message is automatically generated., I just committed this., Integrated in Hadoop-Nightly #243 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/243/])]