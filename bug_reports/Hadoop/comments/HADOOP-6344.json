[Steps to Reproduce:
1) Make sure fs.trash.interval in core-site.xml is set to some positive number
2) Copy a large file to your hdfs
3) Set a low size quota for your hdfs
4) Do a rm of the large file

Result: 
A message comes up saying, that the directory could not be created in the trash, but that the file was deleted.  

Expected:
If the file fails to move to trash, then it should not be deleted., After much deserved ridicule...  I fixed the title., Duplicate of HDFS-677?, Yea, I think Rob is right.  It turns out the build I am testing, does not have it in., Actually, I take that back.  Turns out my code did have the fixe for HDFS-677., It looks like Gary did his test slightly differently: in his test the Trash directory doesn't exist beforehand and there isn't quota available to even create it.  At which point we hit:
{code}    for (int i = 0; i < 2; i++) {
      try {
        if (!fs.mkdirs(baseTrashPath, PERMISSION)) {      // create current
          LOG.warn("Can't create trash directory: "+baseTrashPath);
          return false;
        }
      } catch (IOException e) {
        LOG.warn("Can't create trash directory: "+baseTrashPath);
        return false;
      }
{code}
This false gets percolated up to FsShell:delete which interprets the failure as instruction to go ahead with the hard delete:
{code}      if (trashTmp.moveToTrash(src)) {
        System.out.println("Moved to trash: " + src);
        return;
      }
    }
    
if (srcFs.delete(src, true)) {{code}  It would probably be better to throw and exception rather than return false, so that the deletion doesn't go ahead.
This is not new behavior, it's been around since at least January (the farthest back into the repo I went).  Looks like it just hasn't been tested., A quick instrumentation of code and Gary able to prove mkdir is throwing an exception, but it's an odd one:
{noformat}
an IOException from mkdirs: org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /user/hadoopqa is exceeded: quota=1 diskspace consumed=19.6g
{noformat}
I didn't think that creating a directory would count against your DSQuota.  Still poking around.
At the very least this code should be fixed no to silently catch the exception.  After Boris' fix (HADOOP-6203), the log message at least indicates there was an exception rather than a false return value, but still swallows the exception and doesn't log what it actually was., Moved to Common, since that is where the affected code resides post-split., Attaching patches to fix this code path for trunk and Yahoo! 20 branch.  The problem is as described, rather than stopping the delete, we swallow the exception and proceed.  These patches fix that and toss the exception received from mkdirs up to FsShell.  After patch, the file is not remains, the user is notified of the exception and the exception is logged.

Ran tests (all fine) and test-patch on trunk version: 
{noformat}[exec] +1 overall.  
[exec] 
[exec]     +1 @author.  The patch does not contain any @author tags.
[exec] 
[exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
[exec]                         Please justify why no new tests are needed for this patch.
[exec]                         Also please list what manual steps were performed to verify this patch.
[exec] 
[exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
[exec] 
[exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
[exec] 
[exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
[exec] 
[exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.{noformat}
Tests and test-patch for Y! 20 release still pending.  Will update.

Reason for no tests: This is very difficult code path to test and we're doing it manually.  This entire method should be refactored: there are like 10 different ways to get out of the method and its tests re-written, since this has been the source of several bugs.  I'll open a JIRA shortly to do this.  For now, I believe a manual test will suffice and drive the next set of automatic tests.
, submitting patch, I tested manually using the steps above.  I got the following results:

[hadoopqa@gsbl20230 hadoop-0.20.1.3092118000]$ bin/hadoop fs -put ../hadoop-0.20.1.3092118002/test.bin gmurry/test1.bin
put: org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /user/hadoopqa is exceeded: quota=1 diskspace consumed=11.2g
[hadoopqa@gsbl20230 hadoop-0.20.1.3092118000]$ bin/hadoop fs -rm gmurry/test2.bin
09/10/29 22:28:07 WARN fs.Trash: Can't create trash directory: hdfs://gsbl20230.blue.ygrid.yahoo.com/user/hadoopqa/.Trash/Current/user/hadoopqa/gmurry
rm: Failed to move to trash: hdfs://gsbl20230.blue.ygrid.yahoo.com/user/hadoopqa/gmurry/test2.bin


And the files were still present.  
+1

Also, we need to have an attached Jira for the fixing of the unit tests to find this sort of thing.

Thanks, +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12423635/HDFS-740.patch
  against trunk revision 831070.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/116/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/116/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/116/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/116/console

This message is automatically generated., I am not sure whether this is bug:
The trash behavior to me is a best-effort trash, i.e. the files being deleted will be moved to trash if possible.  Otherwise, the files will be deleted permanently.

The doc seems not defining clearly the trash behavior.  We should first clarify trash behavior before changing it., There are some questionable things going on in moveToTrash(), which should be addressed in HADOOP-6345.

As for the patch I think it does what it is intended for, namely not removing silently the directory if the quota exceeded or any other exception is thrown while creating a trash directory. I first thought that the directory will still be removed in FsShell.delete() if mkDirs() returned false. But it turns out that HDFS mkDirs() never returns false: the result is either true or an exception.

+1 one for the patch., +1 for the patch., Of note, tests passed for Y! branch and test-patch was fine (well, it warned about the class path, but I reject that as false)., Committed the patch to trunk and branch 0.21. Thank you Jakob., Integrated in Hadoop-Common-trunk-Commit #78 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/78/])
    . Fix rm and rmr immediately delete files rather than sending to trash, if a user is over-quota. Contributed by Jakob Homan.
, Integrated in Hadoop-Common-trunk #144 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/144/])
    . Fix rm and rmr immediately delete files rather than sending to trash, if a user is over-quota. Contributed by Jakob Homan.
, I could reproduce it on the version of 0.20.2.
# bin/hadoop dfs -rmr input
13/05/14 02:37:12 WARN fs.Trash: Can't create trash directory: file:/C:/Documents and Settings/XU Zheng/.Trash/Current/C:/cygwin/usr/hadoop-0.20.2
Deleted file:/C:/cygwin/usr/hadoop-0.20.2/input
, It could be reproduced in the Hadoop-1.0.4

# bin/hadoop dfs -rmr input
13/05/15 00:14:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/05/15 00:14:15 WARN fs.Trash: Can't create trash directory: file:/C:/Documents and Settings/XU Zheng/.Trash/Current/C:/cygwin/usr/hadoop-1.0.4
Problem with Trash.Failed to set permissions of path: C:\Documents and Settings\XU Zheng\.Trash\Current\C:\cygwin\usr\hadoop-1.0.4 to 0700. Consider using -skipTrash option
rmr: Failed to move to trash: file:/C:/cygwin/usr/hadoop-1.0.4/input

]