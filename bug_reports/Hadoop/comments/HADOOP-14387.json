[{code}
- Check Hadoop version *** FAILED ***
  java.lang.RuntimeException: java.io.IOException: Fetch fail on include with no fallback while loading 'core-site.xml'
  at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2872)
  at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2657)
  at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)
  at org.apache.hadoop.conf.Configuration.get(Configuration.java:1325)
  at com.hortonworks.spark.cloud.CloudSuite$.loadConfiguration(CloudSuite.scala:353)
  at com.hortonworks.spark.cloud.common.HadoopVersionSuite$$anonfun$1.apply$mcV$sp(HadoopVersionSuite.scala:32)
  at com.hortonworks.spark.cloud.common.HadoopVersionSuite$$anonfun$1.apply(HadoopVersionSuite.scala:31)
  at com.hortonworks.spark.cloud.common.HadoopVersionSuite$$anonfun$1.apply(HadoopVersionSuite.scala:31)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  ...
  Cause: java.io.IOException: Fetch fail on include with no fallback while loading 'core-site.xml'
  at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2831)
  at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2657)
  at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)
  at org.apache.hadoop.conf.Configuration.get(Configuration.java:1325)
  at com.hortonworks.spark.cloud.CloudSuite$.loadConfiguration(CloudSuite.scala:353)
  at com.hortonworks.spark.cloud.common.HadoopVersionSuite$$anonfun$1.apply$mcV$sp(HadoopVersionSuite.scala:32)
  at com.hortonworks.spark.cloud.common.HadoopVersionSuite$$anonfun$1.apply(HadoopVersionSuite.scala:31)
  at com.hortonworks.spark.cloud.common.HadoopVersionSuite$$anonfun$1.apply(HadoopVersionSuite.scala:31)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
{code}, I'm assuming this is HADOOP-14216 related

we don't see this in hadoop's own tests as we do tend to have a core-site in test/resources; this is one of mine [which doesnt|https://github.com/steveloughran/spark-cloud-examples/tree/master/cloud-examples], cc [~jeagles], I think may the story here would be to declare that some resources could be absent, with the -site.xml on the list; that way if there's no hdfs-default, core-default things would fail fast, but the site ones were considered optional, Thanks for filing this jira and bringing this to my attention, [~stevel@apache.org]. Will post a patch shortly., {code}
java.lang.RuntimeException: java.io.IOException: Fetch fail on include with no fallback while loading 'core-site.xml'
{code}
This message looks to be saying that while parsing core-site.xml, it was unable to xinclude a file and there was no fallback. Can you confirm this case, [~stevel@apache.org]?, [~stevel@apache.org], included a patch that includes a better error message as well as allowing for URL includes which is as known problem. Can you comment on whether this fixes the issue?, The problem I'm having is

I'm trying to create a configuration object with: {{new Configuration(true)}} in a test suite downstream. That's all.

The problem arises as the new code fails fast if any resource is missing, and this test deployment doesn't have a core-site as it's not a live Hadoop cluster.
{code}
  static {
    // Add default resources
    addDefaultResource("core-default.xml");
    addDefaultResource("core-site.xml");
   ...+ check for hadoop-site.xml & load iff present
  }
{code}

And the default configuration option requires not just the classpath resource {{/core-default.xml}} (which I want) but {{/core-site.xml}}, which isn't here. I suspect if I'd got past that and asked for an HdfsConfiguration or YarnConfiguration I'd hit the same on yarn-site and hdfs-site XML files mapred-site, httpfs-site.xml, kms-site.xml, ...

Essentially: we must not treat the absence of these resource files as a reason to fail the construction. They are optional. 

I like the patch and reporting, but it isn't going address this as the problem isn't that the XML file isn't loading: it's that we're treating the failure to find the -site.xml files as a failure

I think we need to move to making the -site.xml files optional, presumably by adding one option for a mandatory resource, and one for an optional one. And for backwards compatibility, probably making addDefaultResource() be for optional., addMandatoryDefaultResource for the mandatory one, so that downstream apps don't suffer the same fate (hbase-site.xml, ....)


, I'm also seeing problems when I'm trying to XInclude resources from different JARs, specifically hadoop-common-test referencing anything in hadoop common JAR.

I don't actually know if this is a regression or not because it's something I've never tested before

{code}

<configuration>
  <include xmlns="http://www.w3.org/2001/XInclude"
    href="/core-default.xml">
  </include>
...
{code}

Here's the stack trace with the 001 patch applied. Its extra diagnostics are valuable

{code}
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.io.IOException: Fetch fail on include for '/core-default.xml' with no fallback while loading 'contract/localfs.xml'
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2840)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2663)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2546)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1326)
	at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:240)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:471)
	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:439)
	at org.apache.hadoop.fs.contract.localfs.LocalFSContract.getLocalFS(LocalFSContract.java:89)
	at org.apache.hadoop.fs.contract.localfs.LocalFSContract.init(LocalFSContract.java:63)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:177)
	at org.apache.hadoop.fs.contract.AbstractContractSetTimesTest.setup(AbstractContractSetTimesTest.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}
, Followup: that last failure isn't a regression, it surfaces on branch-2.8 too, if you try to xinclude across JARs
{code}
testSetTimesNonexistentFile(org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSetTimes)  Time elapsed: 0.128 sec  <<< ERROR!
java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/Users/stevel/Projects/hadoop-trunk/hadoop-common-project/hadoop-common/target/test-classes/contract/localfs.xml; lineNumber: 22; columnNumber: 13; An include with href 'unknown'failed, and no fallback element was found.
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2713)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2570)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2443)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1257)
	at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:189)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:377)
	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:360)
	at org.apache.hadoop.fs.contract.localfs.LocalFSContract.getLocalFS(LocalFSContract.java:89)
	at org.apache.hadoop.fs.contract.localfs.LocalFSContract.init(LocalFSContract.java:63)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:177)
	at org.apache.hadoop.fs.contract.AbstractContractSetTimesTest.setup(AbstractContractSetTimesTest.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: org.xml.sax.SAXParseException: An include with href 'unknown'failed, and no fallback element was found.
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:257)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:339)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2548)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2536)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2607)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2570)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2443)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1257)
	at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:189)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:377)
	at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:360)
	at org.apache.hadoop.fs.contract.localfs.LocalFSContract.getLocalFS(LocalFSContract.java:89)
	at org.apache.hadoop.fs.contract.localfs.LocalFSContract.init(LocalFSContract.java:63)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:177)
	at org.apache.hadoop.fs.contract.AbstractContractSetTimesTest.setup(AbstractContractSetTimesTest.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}, [~stevel@apache.org], Could use some help reproducing the case you are seeing. Some background. The original patch tries to maintain the policy that all default resources are optional by checking the reader. 

{code:title=Configuration#loadResource}
      ...
      if (reader == null) {
        if (quiet) {
          return null;
        }
        throw new RuntimeException(resource + " not found");
      }
      ...
{code}

Where I am having confusion is the stack trace provided doesn't match the description of how to reproduce. I verified the bug in the stack trace is fixed. I have verified in my setup that core-site.xml is optional using the below test case. Can you help me understand how to reproduce the case you are having? 

{code:title=FailConf.java}
import org.apache.hadoop.conf.Configuration;
public class FailConf {
  public static void main(String[] args) {
    Configuration conf = new Configuration();
    System.out.println(conf.get("hadoop.common.configuration.version"));
    System.out.println(conf.toString());
  }
}
{code}

{code:title=bash}
# build hadoop 2.9
$ mvn clean install package -Pdist -Dtar -DskipTests -Dmaven.javadoc.skip
# build FailConf.java
$ javac -cp ./:./hadoop-client/target/hadoop-client-2.9.0-SNAPSHOT/share/hadoop/client/lib/* FailConf.java
# run FailConf with core-default.xml and no core-site.xml
$ java -cp ./:./hadoop-client/target/hadoop-client-2.9.0-SNAPSHOT/share/hadoop/client/lib/* FailConf
# Success
# 0.23.0
# Configuration: core-default.xml, core-site.xml
{code}, My last comment was posted before reading your previous post., [~stevel@apache.org], still eager to get to the bottom of this but I still need help trying to reproduce the failure as stated in the summary. Can you please provide a stack trace or reproducible case that can cause this to fail. I have tried to reproduce this case with the test code above. As you can see from my results, a core-site.xml missing from the classpath is treated as optional based on these steps., OK, let me try again; I'd made things go away by adding a core-site.xml

the project I see this is actually: https://github.com/steveloughran/spark-cloud-examples/tree/master/cloud-examples

I could try something much more minimal though, I've created a simple standalone project and I'm not seeing this, so I'll go back to my other one. That's a bit trickier to work with as it uses spark built downstream of hadoop; you'll need to build all of it. I'll list the details if I can show the problem there consistenty, I'm not seeing this any more; don't know what is up.

I'm closing as a cannot reproduce for now. The one about file:// not working is still a serious issue, Thanks for following up with this [~stevel@apache.org]. My only last thought is that there was a jar with a core-site.xml bundled or a core-site.xml in the classpath by accident. I this does occur, please reopen and I'll be happy to take another look. I'll re-post the attached patch as HADOOP-14399 to handle the file:// case correctly. Thanks, again.]