[jstack output showing processes waiting on ackQueue lock, From the Child java process's jstack info, the "SIGTERM handler" thread call the org.apache.hadoop.fs.FileSystem$ClientFinalizer, but it is blocked for the thread naming "Thread-5".
And the "Thread-5" is blocked at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal called by the "main" thread.
The "main" thread locked the org.apache.hadoop.hdfs.DFSClient$DFSOutputStream at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(), and the dataQueue hangs waiting some thread to notified.

In normal case, when close the DFSOutputStream, it calls the flushInternal(), and the dataQueue will be notified by the DataStreamer when DataStreamer get the first packet from dataQueue.
The DFSOutputStream will be unlocked.

Sometimes when the Child java process get the "SIGTERM", and it will call the ClientFinalizer to FileSystem.closeAll(); => CACHE.closeAll(); => DistributedFileSystem.close(); => clientRunning=false;
The DataStreamer thread will exit, because the clientRunning=false, And the DFSOutputStream now also goes in the closeInternal()=>flushInternal() and wait for notifying the dataQueue.
But now the DataStreamer has exited and now the closed flag is false(because the closed flag would be set true after flushInternal() in closeInternal()), so the flushInternal will be hang.

In deed we shoud judge clientRunning to the flushInternal() function., https://issues.apache.org/jira/browse/HADOOP-3998

this bug has been fixed?, I am closing this one because I think this is a duplicate of HADOOP-3998. Please re-open if you think otherwise.]