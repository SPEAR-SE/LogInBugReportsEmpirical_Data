[This jira is a natural extension of HADOOP-4659., +1 for this change 

Currently, waitForProxy(...) waits forever if there are ConnectException. It seems not right since if the server in the other side is down, the client cannot detect it but keep waiting., +1 for this. It's useful for development and  easy to test (start these nodes with nothing to bond to), 

* What property names to use?
* What are reasonable defaults for production systems? 

For minidfs we can run with a configuration that times out much faster; the default timeout should be adequate for people setting up basic clusters on  real/virtual machines without any assumptions about NTP working :), >  What property names to use?
Should we have one property per kind of ipc client? Something like datanode.connect.timeout, tasktracker.connect.timeout, dfsclient.connect.timeout...

>  What are reasonable defaults for production systems?
I am thinking to start with a large number like 1 hour or 1 day. It is at least backwards compatible. , >Something like datanode.connect.timeout, tasktracker.connect.timeout, dfsclient.connect.timeout...

Maybe include the fact that this is for IPC timeouts, not say http

datanode.ipc.connect.timeout
tasktracker.ipc.connect.timeout
dfsclient.ipc.connect.timeout

>I am thinking to start with a large number like 1 hour or 1 day. It is at least backwards compatible.

24 hours would be good. It lets you handle the kind of outage that has the team paged in from home and removes the "fix this in 15 minutes before the nodes start giving up" crisis, With HADOOP-6435 fixed, this is now possible. 

There is also the problem that JobTracker now spins on startup waiting for the filesystem to go live, that needs timeouts too. I'd like to move that code down to the worker thread, but that's a separate issue]