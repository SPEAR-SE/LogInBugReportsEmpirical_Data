[Lohit, Koji and I looked at the logs an this is what looks to have happened.

client :{code}
//...
    try {
      out.write(buf);
    } finally {
      out.close(); //<== stuck here
    }
{code}
Thats a valid way to close. This is what happened :
 - Namenode allocates block b_x at t and datanodes report the block completed at t+4sec (128 MB block).
 - Namenode allocates next block b_y two minutes later. No datanodes reported this block written.
 - Namenode allocates another block b_z 100 millisecs later.

So on the client side write() failed when client trying to get b_y. client proxy throws SocketTimeoutException. Then app does out.close(), which again tries to write the block, and this time it is b_z. Client does not know about b_y at all.

The client side bug is that DFSClient's OutputStream() does not remember that there was an error. It should.
, patch attached applies on 0.15. I haven't checked it on trunk yet., +1 looks like this will handle the failure and client will not hang

Meanwhile
{code}
       if (closed) {
-        throw new IOException("Stream closed");
+        return; // closing multiple times is ok.
       }
{code}

in close(), can stream be closed due to other reason?, > can stream be closed due to other reason?
yes, e.g. when there is an exception while getting a new block (this is the exception that triggered this jira).

Also user should be able able to close a stream multiple times. most Java streams allow closing multiple times., 
Where should this go? 0.15.x, 0.16.x or 0.17?
, On 0.16 (and trunk), the same error does not make 'dfs -put' hang. User would see :
{noformat}
08/02/01 04:06:54 WARN fs.DFSClient: DataStreamer Exception: SocketTimeoutException [...]
08/02/01 04:06:54 WARN fs.DFSClient: Error Recovery for block null bad datanode[0]
put: Could not get block locations. Aborting...
{noformat}

This is probably ok. Let me know if we want to change the error message or exception thrown to be the one that caused the problem etc... I am still finding my way around new DFSOutputStream., Maybe we can improve the error message for trunk. Also, how about putting this fix in 0.15.x and a better error report in trunk? My vote would be to do nothing on 0.16., > My vote would be to do nothing on 0.16.
+1. 

We can close this jira. Error message etc, could be changed later as part of some other jira. I think there are no plans to fix this for 0.15.
, Changing the fix version to 0.15.4. Note that this bug and fix apply only to 0.15. No need to propagate the patch up to 0.16 or trunk., I understand that this will fail the dfs -put command. 

But are there any ways for the client to catch this sockettimeout and retry from there? 
I believe whenever the namenode is in a fullGC,  most of the clients will timeout., Maybe we can do better than this. In the current code, if the client encounters an RPC error while fetching a new block id from the namenode, it does not retry. It throws an exception to the application. This becomes especially bad if the namenode is in the middle of a GC and does not respond in time. The reason the client throws an exception is because it does not know whether the namenode successfully allocated a block for this file.

One possible enhancement would be to make the client retry the addBlock RPC if needed. The client can send the block list that it currently has. The namenode can match the block list send by the client with what it has in its own metadata and then send back a new blockid (or a previously allocated blockid that the client had not yet received because the earlier RPC timedout). This will make the client more robust!, > One possible enhancement would be to make the client retry the addBlock RPC if needed.
>
Dhruba, that sounds great. 
Could you open a new Jira and maybe have it for 0.17? 

For 0.15.4,  failing the dfs -put should be fine.  

Thanks Raghu, Dhruba!, retrying addBlock works fine 17. Not sure if that would work with appends.

Its a pretty bad experience for users to see write fail just because Namenode is busy even in 0.15 and 0.16. Koji, could you try ipc.client.timeout set to 10 or 15 min? Its good step in the right direction since we are going to essentially remove this timeout in 0.17.
, Given that we're trying to eliminate the timeout in 0.17, does retrying still make sense?, This patch is meant only for 0.15.4. Hudson will fail since this won't apply to trunk., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12374430/HADOOP-2647.patch
against trunk revision 619499.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified JUnit tests.
                        Please justify why no unit tests are needed for this patch.

    patch -1.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1759/console

This message is automatically generated., Hudson failure is expected and is OK., This patch is for earlier branches and is not expected to cleanly merge with current trunk., I just committed this to branch-0.15.]