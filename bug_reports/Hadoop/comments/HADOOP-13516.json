[HADOOP-11694 fixes only the case of empty root directory, however listing a empty non-root directory still throws FileNonFoundException.
{code}
key = maybeAddTrailingSlash(key); //If this line is commented, then objects.getCommonPrefixes().size is greater than 1 for valid empty directories. with nextMarker like dir/empty-ids-1449644026143/
      ListObjectsRequest request = new ListObjectsRequest();
      request.setBucketName(bucket);
      request.setPrefix(key);
      request.setDelimiter("/");
      request.setMaxKeys(1);
{code}, sprry, race condition confusion;re-opending.  And, given that LoC, my code. 

Which version of the hadoop source tree are you looking at. As that sounds like you are working with Branch 2.8+?

, I am unable to repro this with a fresh trunk build.  [~shaik.idris], I noticed what appears to be a typo in your example: "emtyp" instead of "empty".  Is it possible that you ran the ls command on a directory that doesn't exist?

{code}
> hadoop version
Hadoop 3.0.0-alpha2-SNAPSHOT
Source code repository https://git-wip-us.apache.org/repos/asf/hadoop.git -r ae4db2544346370404826d5b55b2678f5f92fe1f
Compiled by chris on 2016-08-18T17:56Z
Compiled with protoc 2.5.0
From source with checksum 4de746edbf65719fec787db317e866a
This command was run using /Users/chris/hadoop-deploy-trunk/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-alpha2-SNAPSHOT.jar

> hadoop fs -ls s3a://cnauroth-test-aws-s3a/

> hadoop fs -mkdir s3a://cnauroth-test-aws-s3a/test-empty-dir
[chris@Chriss-MacBook-Pro-2:ttys002] hadoop-deploy-trunk                                                            

> hadoop fs -ls s3a://cnauroth-test-aws-s3a/
Found 1 items
drwxrwxrwx   - chris          0 2016-08-18 11:44 s3a://cnauroth-test-aws-s3a/test-empty-dir

> hadoop fs -ls s3a://cnauroth-test-aws-s3a/test-empty-dir

> hadoop fs -ls s3a://cnauroth-test-aws-s3a/test-emtpy-dir
ls: `s3a://cnauroth-test-aws-s3a/test-emtpy-dir': No such file or directory
{code}
, Hi [~stevel@apache.org], [~cnauroth],
Thanks for checking. Chris, it is typo in copy paste, sorry for the confusion.

STEP 1:
Here is the environment setup:
Hadoop-version: 2.7.2 Release

Here is the stack-trace while calling getFileStatus from test-code programatically:
java.io.FileNotFoundException: No such file or directory: 
{code}
s3a://<mask>:<mask>@dataplatform-prod-store/dev/empty
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:996)
	at com.olacabs.dp.utils.commonutils.io.S3ACustomFileSystem.main(S3ACustomFileSystem.java:29)
{code}

STEP 2: Checked out Trunk
I checked out the code and tried to work with the Trunk. However, I am hitting into build another issue,:
{code}
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: F8394B728D82563E), S3 Extended Request ID: AXVEW+4p4f2XaByJyPPhSsSlnRVvmsvCjEgKju4NJ7Y4yenIus3IQreW8RwEjqK7tnPt6lHeK70=
	at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)
	at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)
	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)
	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)
	at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:902)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1414)
{code}

STEP 3: Checkout 2.7.2 and tried to patch with the "else fix" 
{code}
} else if (key.isEmpty()) {
        LOG.debug("Found root directory");
        return new S3AFileStatus(true, true, path);
      }
{code}
But looks like since the key is not empty, it is not helping here. However when I changed to code to without this line //key = maybeAddTrailingSlash(key); getCommonPrefixes() is non-empty.




, I'll see if I can replicate this.

All work is going to have to be against branch-2 or trunk; there's been a lot of changes to s3a there and things won't work.

Interesting that you were getting a 403 there; could be a sign of some regression —we've been doing a lot with provider plugin, session providers, etc.

Can you get set up for running the hadoop-aws test run? Ask for help on the common-dev list if you need it —hopefully everything you need is written up in https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md


if , I'm not seeing this in branch-2
{code}
$ ./hadoop fs -mkdir  s3a://hwdev-steve-ireland/emptydir
16/09/28 15:29:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
$ hadoop fs -ls s3a://hwdev-steve-ireland/emptydir
fish: Unknown command 'hadoop'
$ ./hadoop fs -ls s3a://hwdev-steve-ireland/emptydir
16/09/28 15:29:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
$ ./hadoop fs -ls s3a://hwdev-steve-ireland/emptydir/
16/09/28 15:29:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
{code}, closing as cannot reproduce. If this surfaces, test against the latest Hadoop release, then, if it occurs there, please re-open]