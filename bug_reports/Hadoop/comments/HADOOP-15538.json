[Hm, I noticed that in [https://bugs.openjdk.java.net/browse/JDK-8007476], The "which is held by UNKNOWN_owner_addr" shows two different values for the two threads involved in the deadlock, one corresponds to each of these two threads.
{code:java}
Found one Java-level deadlock:
=============================
"Worker-1":
  waiting to lock monitor 0x02f60e54 (object 0x1026ce00, a java.lang.Object),
  which is held by UNKNOWN_owner_addr=0x0352fc54
......
Found one Java-level deadlock:
=============================
"Worker-0":
  waiting to lock monitor 0x02f601bc (object 0x1026ce08, a java.lang.Object),
  which is held by UNKNOWN_owner_addr=0x0357fbd4
{code}
However, in the case I reported here, they point to the same value:
{code:java}
Found one Java-level deadlock:
=============================
"IPC Parameter Sending Thread #294":
  waiting to lock monitor 0x00007f68f21f3188 (object 0x0000000621745390, a java.lang.Object),
  which is held by UNKNOWN_owner_addr=0x00007f68332e2800
......
Found one Java-level deadlock:
=============================
"IPC Client (297602875) connection to x.y.z.p:8020 from impala":
  waiting to lock monitor 0x00007f68f21f3188 (object 0x0000000621745390, a java.lang.Object),
  which is held by UNKNOWN_owner_addr=0x00007f68332e2800
{code}
and this value could just be a third thread, thus two deadlock pairs <thread1, thread3>, <thread2, thread3>. This explains issue#2. However, I wish java stack dump could be more clear about why UNKNOWN_owner_addr here, what thread this really is., All other 6 BLOCKED threads (there are 8 BLOCKED threads in total in each jstack frame) look like
{code:java}
"Thread-52" #82 prio=5 os_prio=0 tid=0x000000001a2c1000 nid=0xf189f waiting for monitor entry [0x00007f697a77f000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1053)
        - waiting to lock <0x00000006215c1e08> (a java.lang.Object)
        at org.apache.hadoop.ipc.Client.call(Client.java:1483)
        at org.apache.hadoop.ipc.Client.call(Client.java:1441)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:266)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy11.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1323)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1310)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1298)
        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)
        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:275)
        - locked <0x0000000618947838> (a java.lang.Object)
        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:267)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1629)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:338)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:334)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:334)
{code}
These threads all blocked due to RPC serialization at
{code:java}
    public void sendRpcRequest(final Call call)
        throws InterruptedException, IOException {
      if (shouldCloseConnection.get()) {
        return;
      }

      // Serialize the call to be sent. This is done from the actual
      // caller thread, rather than the sendParamsExecutor thread,
      
      // so that if the serialization throws an error, it is reported
      // properly. This also parallelizes the serialization.
      //
      // Format of a call on the wire:
      // 0) Length of rest below (1 + 2)
      // 1) RpcRequestHeader  - is serialized Delimited hence contains length
      // 2) RpcRequest
      //
      // Items '1' and '2' are prepared here. 
      final DataOutputBuffer d = new DataOutputBuffer();
      RpcRequestHeaderProto header = ProtoUtil.makeRpcRequestHeader(
          call.rpcKind, OperationProto.RPC_FINAL_PACKET, call.id, call.retry,
          clientId);
      header.writeDelimitedTo(d);
      call.rpcRequest.write(d);

      synchronized (sendRpcRequestLock) { <===================Client.java, line 1053
        Future<?> senderFuture = sendParamsExecutor.submit(new Runnable() {
{code}

These 6 threads are all blocked due to the single RPC thread holding the sendRpcRequestLock ("IPC Parameter Sending Thread #294" in the jira description), which is blocked due to the reported deadlock.

So what is the "UNKNOWN_owner_addr=0x00007f68332e2800" thread that caused the "deadlock" is hidden.

 , The JDK-8007476 bug appears to have simply obscured the threads in the deadlock output.  Notice that the stacktraces still include "\- waiting to lock <monitor>" and "\- locked <monitor>" which makes it easy to see the deadlock offenders despite the "UNKNOWN_owner_addr".

Do you have the full stack trace that you can link as an attachment?, [~yzhangal] It may be easier to use jhat to do analyze the heapdump. Since we object address like below. Can you get java heapdump?

 
 waiting to lock monitor 0x02f60e54 (object 0x1026ce00, a java.lang.Object),
  which is held by UNKNOWN_owner_addr=0x0352fc54, Thanks a lot [~daryn] and [~billyean].

Please see attached two jstack files, one is at the begining t1.jstack, the other is 13 minutes later: t1+13min.jstack.

My read is, JDK-8007476 fixed a jdk internal error that crash the jstack dump itself, by introducing "which is held by UNKNOWN_owner_addr=" to make it dump reasonable value of the other guy who hold the lock, so it can dump out the deadlock threads pair instead of crashing. In the example provided there, the two threads point to each other because they are the two threads involved in the same deadlock.

I think the java version here has the JDK-8007476  fix, that's why we also have "which is held by UNKNOWN_owner_addr=" reported; However, the other party of deadlocks is not reported here.

Problem here:
 # Did the deadlock finder miss some threads involved in the deadlock?
 # If it did not miss, why the two threads listed are blocked at the same lock (stateLock)? And these two threads point to the same culprit addr reported as "which is held by UNKNOWN_owner_addr=".
 # Who holds the stateLock?

Haibo's suggestion of having heapdump seems a good direction to dive into. Unfortunately the problem is intermittent and it takes time to see it again. But I will look into.

If any of you have more comments and insight to share, I would really appreciate.

Thanks., The IPC client and parameter sending threads should be the only threads using the socket channel.  Note there's no other parameter sending threads other than the one reported in the deadlock.  This implies a dead thread is holding the monitor which would be an insidious jvm bug.

Here's what produces the UNKNOWN_owner_addr output.  Note the comment "blocked permanently".
{code:java}
      currentThread = Threads::owning_thread_from_monitor_owner(
                        (address)waitingToLockMonitor->owner(),
                        false /* no locking needed */);
      if (currentThread == NULL) {
        // The deadlock was detected at a safepoint so the JavaThread
        // that owns waitingToLockMonitor should be findable, but
        // if it is not findable, then the previous currentThread is
        // blocked permanently.
        st->print("%s UNKNOWN_owner_addr=" PTR_FORMAT, owner_desc,
                  (address)waitingToLockMonitor->owner());
        continue;
      }
{code}

Are there any log lines indicating an OOM or other serious problem occurred?  Maybe native code somehow killed the thread directly or by mangling memory?

Next time it happens, run pstack to check for {{SharedRuntime::handle_wrong_method}}.  There is a (supposedly) fixed jvm bug that causes a process to go into an infinite loop which eats 100% cpu.  That said, was the process idle or consuming cpu?, Just noticed a possible clue:
{noformat}
"main" #1 prio=5 os_prio=0 tid=0x0000000004cd8000 nid=0xf1491 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

"VM Thread" os_prio=0 tid=0x0000000004f39000 nid=0xf163e runnable
{noformat}

The main thread has no stack trace.  Did you clip it out?  If no, that's indicative of the jvm trying to shutdown.  It may have detected a thread death or other nasty issue but is stuck waiting for all the non-daemon threads to exit.  Look for a hs_err_pid file., Thanks a lot [~daryn]! somehow I did not see your update in time. I did not clip anything out from the jstack logs I posted. I am not aware any OOM, good question about whether the process is idle or consuming cpu, not sure if we have that info but let me try. We do some have pstack too, I had been studying that and was not very successful. I will dig further with your hint.  Thanks again!

 , pstack for thread
{code:java}
"IPC Parameter Sending Thread #294" #142281 daemon prio=5 os_prio=0 tid=0x00007f676e2b7800 nid=0x1bfe75 waiting for monitor entry [0x00007f6920ed4000]
{code}
is
{code:java}
Thread 6499 (Thread 0x7f6920ed5700 (LWP 1834613)):
#0 0x000000396ce0b68c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1 0x00007f69f16c3c23 in os::PlatformEvent::park() () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#2 0x00007f69f16b2bad in ObjectMonitor::EnterI(Thread*) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#3 0x00007f69f16b4b81 in ObjectMonitor::enter(Thread*) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#4 0x00007f69f1769430 in SharedRuntime::complete_monitor_locking_C(oopDesc*, BasicLock*, JavaThread*) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#5 0x00007f69dedaadc8 in ?? ()
#6 0x0000000630aa0dd0 in ?? ()
#7 0x00007f69dfa61464 in ?? ()
#8 0x00000006217452d0 in ?? ()
#9 0xc42e8a70000294c8 in ?? ()
#10 0x0000000621745380 in ?? ()
#11 0x0000000621745390 in ?? ()
#12 0x00007f676e2b7800 in ?? ()
#13 0x00007f69f1d83790 in Universe::_heap_used_at_last_gc () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#14 0x00007f67c42e8a70 in ?? ()
#15 0x00000000c42e8a72 in ?? ()
#16 0x0000000630aa0dd0 in ?? ()
#17 0x00007f69f17601b0 in OptoRuntime::new_instance_C(Klass*, JavaThread*) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
{code}
 
 The pstack for 
  
{code:java}
"IPC Client (297602875) connection to x.y.z.p:8020 from impala" #142248 daemon prio=5 os_prio=0 tid=0x00007f6920ed6000 nid=0x1bc4ee waiting for monitor entry [0x00007f6961246000]
{code}
is
{code:java}
Thread 9778 (Thread 0x7f6961247700 (LWP 1819886)):
#0  0x000000396ce0ba5e in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f69f16c8dbf in os::PlatformEvent::park(long) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#2  0x00007f69f16b2a85 in ObjectMonitor::EnterI(Thread*) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#3  0x00007f69f16b4b81 in ObjectMonitor::enter(Thread*) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#4  0x00007f69f1769430 in SharedRuntime::complete_monitor_locking_C(oopDesc*, BasicLock*, JavaThread*) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
#5  0x00007f69dedaadc8 in ?? ()
#6  0x00000006217452d0 in ?? ()
#7  0x00007f69e010080c in ?? ()
#8  0x0000060600000606 in ?? ()
#9  0xc42e8a72c42e8a72 in ?? ()
#10 0x0000000621745390 in ?? ()
#11 0x0000000621745370 in ?? ()
#12 0x0000000621745390 in ?? ()
#13 0xc42e8a6e00000606 in ?? ()
#14 0x0000000600000606 in ?? ()
#15 0x000000062d2c5b80 in ?? ()
#16 0x00000000806f8230 in ?? ()
#17 0x0000000000000007 in ?? ()
#18 0x0000000000000001 in ?? ()
#19 0x0000000000000001 in ?? ()
#20 0x0000000000000001 in ?? ()
#21 0x0000000000000003 in ?? ()
#22 0x0000000000000003 in ?? ()
#23 0x0000000000045044 in ?? ()
#24 0x00000006217455c0 in ?? ()
#25 0x00007f69dfa7cd68 in ?? ()
#26 0x00000006217455c0 in ?? ()
#27 0x0000000100000001 in ?? ()
#28 0x000000062d2c5b80 in ?? ()
#29 0x00000001002a6c60 in ?? ()
#30 0x000000062d2c5bb0 in ?? ()
#31 0x00000001100df044 in ?? ()
#32 0x00000000806f8220 in ?? ()
#33 0x0000000000000007 in ?? ()
#34 0x00000163ce93a3ba in ?? ()
#35 0x00000006217452d0 in ?? ()
#36 0x000000062d2c5bb0 in ?? ()
#37 0x00007f69f122422a in CollectedHeap::fill_with_object(HeapWord*, unsigned long, bool) () from /usr/java/jdk1.8.0_162/jre/lib/amd64/server/libjvm.so
{code}, Only the two threads above refers to 0x0000000621745390 in the pthread dump. In LWP 1819886 it looks like
{code}
#10 0x0000000621745390 in ?? ()
#11 0x0000000621745370 in ?? ()
#12 0x0000000621745390 in ?? ()
{code} 
and this one has deeper stack,
vs 
{code}
#11 0x0000000621745390 in ?? ()
{code} 
in LWP 1834613, which has less deep stack.

1819886 is in:
{code}
#0  0x000000396ce0ba5e in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
{code}

1834613 is in
{code}
#0  0x000000396ce0b68c in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
{code}
, FYI [~daryn], the CPU usage is pretty low.

 , Expanding the code snippet that [~daryn] shared, the following method searched all threads for the owner of the lock, and failed to find.
{code}
    // refer to Threads::owning_thread_from_monitor_owner
    public JavaThread owningThreadFromMonitor(Address o) {
        if (o == null) return null;
        for (JavaThread thread = first(); thread != null; thread = thread.next()) {
            if (o.equals(thread.threadObjectAddress())) {
                return thread;
            }
        }

        for (JavaThread thread = first(); thread != null; thread = thread.next()) {
          if (thread.isLockOwned(o))
            return thread;
        }
        return null;
    }
{code}

There are several possibilities:
1. The thread the used to hold the lock was removed somehow
2. The thread is still there, but the above method did not find.

It's more likely 1; It could be 2 if it were the threads that have empty dump in the logs I attached, but the chance seems low there.

, There is also the other thread reported in HADOOP-15530 blocked at:

[https://insight.io/github.com/AdoptOpenJDK/openjdk-jdk8u/blob/dev/jdk/src/share/classes/java/util/concurrent/FutureTask.java?line=396]

There is a chance that this thread is in play but I have not figured out how. 

I wish the comment here 

[https://insight.io/github.com/JetBrains/jdk8u_hotspot/blob/master/src/share/vm/services/threadService.cpp?line=906]

could give some explanation what are the possible scenarios that the lock owner is not findable. 

This is likely a jdk bug: a lock has an associated owner thread address, but the thread can not be found. It seems to me that the only hope now is to  get heapdump and coredump to find clue. 

Any comment/insight is very much appreciated., So, as far as I understand, we have two threads that try to grab the same lock (which is an instance of java.lang.Object referened by SocketChannelImpl.stateLock). When the JVM's deadlock detection mechanism tries to find who currently holds this lock, it cannot find any Java thread responsible for this. Such a situation is considered a variation of a deadlock, though it's not a classical one with two threads and two locks. Rather, it's a single lock, but it can never be grabbed by the waiting threads, because the only thread that can unlock it somehow disappeared. Note that the JVM's message, as well as the comments in the JVM code, are somewhat cryptic, and it took me some head-scratching and guessing before I understood what they try to say.

I don't think that some normal Java thread threw an exception and exited, but didn't clean up one of the locks that it was holding. At least I've never seen such a situation in the past. Probably such a bug would be relatively easy to reproduce, and thus would have been fixed long ago. So I think here we have something really non-standard in play, and therefore the following exotic scenarios are more likely here:
 # The lock is still being held by some thread that the JVM doesn't know about, e.g. one started from native code.
 # The thread that was holding the lock exited in some non-standard, non-graceful way, perhaps because of a failure in native code. I am not sure what happens in such a case, and my theory is that if the thread is terminated by the OS and the JVM doesn't have a chance to interfere, all the Java locks that such a thread holds won't be unlocked. So we really have an "orphaned" lock.
 # Native code generally doing something bad. Note that in the JDK bug mentioned above, one of the JDK guys gave the following possible reason for running into this condition: "My point is that the reason for the assert condition not holding (the owner of a monitor apparently not being in the Threadslist) may not be due to any inherent bug in deadlock detection or monitor management but may due to some other problem induced by the testcase - e.g. a memory stomp due to native code failing to check for errors after invoking JNI functions."

The bottom line is that I think we should really sniff for the native code in the app that runs this HDFS Client, and then check if that native code is doing something unusual., Thanks a lot [~misha@cloudera.com]. Good inputs!

The java lock of interest here is a java object (stateLock in SocketChannelImpl), and it's used in synchronized block, example below. Assume the unknown thread is an external thread in the C++ world that reached java via JNI (should be in our case since it's impalad).

The lock is acquired/released looks like the following:
{code:java}
    private void ensureWriteOpen() throws ClosedChannelException {
        synchronized (stateLock) {
            if (!isOpen())
                throw new ClosedChannelException();
            if (!isOutputOpen)
                throw new ClosedChannelException();
            if (!isConnected())
                throw new NotYetConnectedException();
        }
    }
{code}
so the speculated scenario is: an external thread unknown to java acquired the lock upon entering the synchronized block, then the thread disappeared abnormally without the lock being released.

In java synchronized block semantics, even if the execution of this thread is to exit, with or without exception being thrown inside the block, it would release the lock before getting out of the block;

However, it doesn't release the lock in our case. This means the execution of the thread is essentially terminated inside the synchronized block, without handling any error, without even going out of the synchronized block, or the thread is "frozen" inside the synchronized block. Memory stomp might be a reason, but I wonder how.

Discussions in
 [https://bugs.openjdk.java.net/browse/JDK-8007476]

stated
{quote}The VM should never encounter a monitor with an owner thread that doesn't exist. Seems to me this may be a symptom rather than the actual problem.

Looking at the NFC.c code it seems to me that the code is fragile because it has sequences of the form:

clazz = (*env)->FindClass(env, "cvm/jit/share/TestException");
 mid = (*env)->GetMethodID(env, clazz, "<init>", "()V");
 throwObj = (jthrowable) (*env)->NewObject(env, clazz, mid);

but there is no checking to see if any exceptions occurred. Not saying that is the case here but if for some reason there is a problem who knows what the failure mode might be
{quote}
and
{quote}My point is that the reason for the assert condition not holding (the owner of a monitor apparently not being in the Threadslist) may not be due to any inherent bug in deadlock detection or monitor management but may due to some other problem induced by the testcase - eg a memory stomp due to native code failing to check for errors after invoking JNI functions.
{quote}, BTW, I agree very much with 
{quote}
The bottom line is that I think we should really sniff for the native code in the app that runs this HDFS Client, and then check if that native code is doing something unusual.
{quote}, Did you ever look for an hs_err file per my earlier comment:

"The main thread has no stack trace. Did you clip it out? If no, that's indicative of the jvm trying to shutdown. It may have detected a thread death or other nasty issue but is stuck waiting for all the non-daemon threads to exit. Look for a hs_err_pid file.", Hi [~daryn]

At this point we (+[~yzhangal]) think this symptom most likely pertains to OOM in a native + JVM hybrid process (such as an Impala Daemon). Somehow a thread exited without releasing the lock, and other threads had to wait for the lock and can't terminate. 

 

We don't have a definite evidence to prove this theory, but after we bumped up the heap settings of the Impala daemons on the cluster, we've not observed the same issue for over a month (previously it was seen at least once per week).

So to conclude, it is likely a JVM bug rather than a Hadoop bug.]