[See previous description of this problem here:
https://issues.apache.org/jira/browse/HADOOP-2012?focusedCommentId=12548478#action_12548478
, The datanode has a new directory per volume called "detachDir". This directory is used to do temporary copy-on-write for data blocks that are part of a snapshot.

 When a client writes a block that is linked to a snapshot, it does the following:

1. Create a copy of the original file into a file in the detachDir.
2. Rename the newly created file in detachDir into the original file. This breaks the hardlink and creates two copies of the block atomically.

Point 2 works perfectly on Linux platform. The following are some caveats on Windows platform.

On Windows platform,  the rename fails because the target file already exists. Thus, the code issues a delete followed by a rename. This means that there is a window of opportunity (on Windows) when the block does not exist in the right place. If a read request for the block occurs precisely in that window, then the client will get an exception and will try to read that block from an alternate location. (When a datanode restarts, it recovers blocks that are exist in detachDir but do not exist in the original data directory.) I am proposing that this is an acceptable solution.


, - I think it is ok to have a short window during "unlinking" a block, when it is not visible to the data-node.
I believe it should be a pretty rare event.
- Relying on mod time is not such a good idea. It's not the time itself, but the way you will have to find another hard link.
You will need rely on that the other link to the blockFile is in ".../previous/blockFile".
And if an administrator renamed previous directory in order to preserve the snapshot for longer time storage then
your approach will not recognize that the blockFile has another link.
- We should probably use stat instead
{code}
stat -c%h blockFile
{code}
I checked it works both on Linux and CygWin., This patch does a copy-on-write for blocks that need modification but were linked to an existing snapshot. As part of this patch, the two maps in the datanode (volumeMap and blockMap) are combined into one single map., Merged patch with latest trunk., # DatanodeBlockInfo.getVol() rename to getVolume().
# org.apache.hadoop.fs.*  is imported twice.
# FSDataset.detachFile(v, f, b)
#- It is better to use /** JavaDoc comments */
#- I would place this method into DatanodeBlockInfo.detachFile(b)
# FSDataset.detachBlock().
#- Should @inheritDoc from FSDatasetInterface.
#- When do we need numLinks parameter?
#- The whole part below the comment starting with
{code}
// If this block does not have a linked snapshot,
{code}
should be a method of DatanodeBlockInfo.detachBlock()
# In general I'd reorganize methods and place most of the functionality related to copy-on-write 
into DatanodeBlockInfo rather than exposing it in the high level class like FSDataset.
# I have a doubt that detachBlock() should be a public interface of FSDatasetInterface.
My understanding is that detachBlock() is intended to be used in append(), and is going to be specific
for real data-nodes, because simulated data-nodes do not have any data that require to be copied on write.
I think it should be just a method of FSDataset and TestAppend should directly call it because
it is testing the real data-nodes rather than the simulated ones.
# FSVolume.createTmpFile()
#- should probably be a static method.
#- There is an old System.out in it. Could you please remove it.
#- The whole try catch statement seams to be redundant here.
# FSDataset.replaceFile().
#- We are using the same code for renames because of the Windows semantics.
And this is yet another variant of that code. I think we should introduce
a FSUtil.replaceFile() method and call it where appropriate.
#- Are you sure we should wait 5 seconds before failing to replace?
Do we expect something to change during that period?
# HardLink.getLinkCount -> getLinkCountCommand;
 and you can set it once outside the switch{} because it is the same for both OSs.
# TestFileAppend has 3 warnings:
#- import org.apache.hadoop.dfs.FSConstants.DatanodeReportType;
#- private void checkFile() is never used.
#- long len = fs.getFileStatus(file1).getLen(); is never used.
#- We should systematically use /** create file ... */ instead of // create file ... for method title comments., 1. Moved most of detachBlock to DatanodeBlockInfo.java.
2. It is not a public interface anymore. Removed it from FSDatasetInterface.java
3. numLinks is needed to drive the unit tests. It is also needed to make the method handle the case when there are possibly multiple snapshots in the future.
4. I did not make createDetachFile() static, especially because it refers to the detachDir variable.
5. Moved replaceFile to FileUtil. It is intended to handle OS specific cases when the rename will fail if somebody else has a handle to the target file. The current setting of the retry time-limit is adhoc. During this period, it is likely that another thread that was reading that block file will be done reading it., Triggering Hudson tests., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12376078/copyOnWrite2.patch
against trunk revision 619744.

    @author +1.  The patch does not contain any @author tags.

    tests included +1.  The patch appears to include 7 new or modified tests.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs -1.  The patch appears to introduce 3 new Findbugs warnings.

    core tests -1.  The patch failed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1822/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1822/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1822/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1822/console

This message is automatically generated., Findbugs warnings., The "stat" command is not available on solaris. Use "ls -l" on solaris.
Fixed findbugs warnings., +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12376169/copyOnWrite3.patch
against trunk revision 619744.

    @author +1.  The patch does not contain any @author tags.

    tests included +1.  The patch appears to include 7 new or modified tests.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1831/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1831/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1831/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1831/console

This message is automatically generated., - FSDataset.detachBlock()
Variables "file" and "vol" are not used anymore.
- TestFileAppend: redundant import DSDataImputStream
- +1 other than that., I just committed this. Thanks Konstantin., Integrated in Hadoop-trunk #412 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/412/])]