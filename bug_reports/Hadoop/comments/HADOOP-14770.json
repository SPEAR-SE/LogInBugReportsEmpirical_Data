[# add the Hadoop version to the JIRA, thanks
# What is the file format? simple or columnar (ORC, Parquet)
# Looks like the connection is being closed on every seek, which is a sign of HADOOP-13203 not engaging (random IO), or on a sequential read, forward reads aborting/reopening rather than skipping forward.

Make sure you are using the Hadoop 2.8.x JARS, then:

For columnar data: enabling random IO.

{code}
spark.hadoop.fs.s3a.experimental.fadvise=random
{code}

For sequential data with big forward skips

{code}
spark.hadoop.fs.s3a.readahead.range = 768K
{code}

If this fixes it, close as a duplicate of HADOOP-13203
If this doesn't fix it, you can print both the input stream and s3a FS, as their toString() ops print all their stats.

Oh, one more possible cause: split calculation isn't getting it write. Look at your s3a block size, and the format itself.

, also, remember to set component as fs/s3. We need this categorisation to know whether to begin looking at the problem and where (i.e if it's just Hadoop 2.7, the fix is upgrade, if its 2.8+ then its a real issue), Thanks Steve, the application running on Hadoop 2.7.3 and against ORC file format. I will upgrade to Hadoop 2.8.0 to verify., Does moving to 2.8 fix this? If so, close as a duplicate of HADOOP-13202, thanks, Sorry, not yet. I am working with multiple partners on our big data cluster, so it's not easy to move to 2.8.  But I will complete it ASAP., Thanks Steve. when we apply the read input policy with random in our workload after upgrade to Hadoop 2.8.1, it works as my expect, connections not destroyed every time but reused., Good to hear. You should find IO numbers much better too]