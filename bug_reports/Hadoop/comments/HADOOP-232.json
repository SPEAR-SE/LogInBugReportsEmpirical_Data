[Patch attached., This is a security issue.  Typically there should not be multiple versions of a resource on the classpath.  If there are, what's to be trusted, the one local to the machine, or the one included in the job?  We don't want folks to be able to, e.g. include new versions of Hadoop's internal classes in a job.  Nor do we wish them to be able to override non-overrideable job settings, which this would permit.

Can you elaborate on the case where you needed this feature?, debugging the hadoop distcp command required it.
the command is executed by the dfs client, which is in this case a map task.
the class is part of the hadoop release however, so it already resided on the tasktracker's nodes. We wanted to actually override it using a newly sent jar, without upgrading and restarting the map-reduce cluster.
we saw that as a general enough case to warrant a change to the task tracker.

I guess that an alternate solution would be to unbundle the 'external' programs from the main jar file.
, Yes, if you think this cp command needs to be updatable separately from the core then its code should not be in the core build, but rather in the examples, contrib or perhaps a (new) apps build., If the CopyFiles class is not put into core hadoop, I think it should not be part of the bin/hadoop command as well. Because all the classes invoked from bin/hadoop are in core hadoop. We should provide a separate script called distcp (which was my original proposal) in a top-level opt (denoting optional) directory. The source and class file should go to examples. Comments ?, No longer necessary.]