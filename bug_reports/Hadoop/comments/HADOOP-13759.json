[either under hadoop-tools/, or the proposed new cloud-storage/ section. It doesn't quite fit in there, but its closer, isn't it?, I'd prefer hadoop-tools, not sure why a separate cloud-storage module would be preferable., is there no other dependency on the lib? i only ask, because i didn't see that being added as a dependency in the patches for it in hadoop-5732 and jsch is a dependency..

in fact i played around with a mvn project and rolled back the version of hadoop-common i depended on all the way to 2.2.0 and it still pulls in jsch.

i only am posting because i've been troubleshooting some issues with the code in git, and are working through some odd behavior so i was looking at sftp jira items.
, Sorry for the late response.
[~andrew.wang] We cannot get rid of jsch by splitting SFTP into a module of hadoop-tools, because {{SshFenceByTcpPort.java}} in {{hadoop-common}} is dependent on jsch.
[~coleferrier] You're right, jsch is committed before SFTP.
But I like the idea that we need to spilit SFTP and FTP from {{hadoop-common}} to {{hadoop-tools}}, [~andrew.wang], if you agree, I'd like to close this JIRA and open another JIRA to track the migration of SFTP and FTP., This JIRA can cover both, just change the title to include FTP. It'd move FTP off as a dependency.

I recall from writing the FS contract tests for FTP That it was pretty under tested, and didn't really work as a destination of data. I expect SFTP to be similar. We should really document that. The fact that nobody else has noticed is probably a metric of its use, [~stevel@apache.org] Thanks for your response.
{quote}
The fact that nobody else has noticed is probably a metric of its use
{quote}
Probably the best way to let people notice FTP and SFTP file system is just deleting them :)

Anyway I will try to split them into hadoop-tools/hadoop-ftp since you agree to do so. But we should know that splitting them cannot really save jsch dependency in Hadoop Common, so both the title and description need to be changed if we want to keep this JIRA., At Cloudera, we've stopped using SSH fencing. We had only a small handful of customers who were using a custom fencer at all (meaning something besides /bin/true), it's essentially a relic from the NFS HA days. So we could work on getting rid of SSH fencing, and then doing this split to move out jsch., [~andrew.wang] Thanks for your response.
{quote}
So we could work on getting rid of SSH fencing, and then doing this split to move out jsch
{quote}
Not sure the background of SSH fencing, but when I google hadoop ssh fencing, there are still some fresh discussions about this topic showing up. If we need to get rid of this legacy code, I'd like to raise another JIRA to discuss it, any thoughts?, Certainly something to discuss â€”or it could be moved here. Discussion should be on hdfs-dev tho'

FWIW, ssh fencing isn't that reliable, as it only stops the remote service if somehow it is considered to have failed, but the remote server is still reachable via SSH. STONITH fencing via power supplies is generally state of the art, as per [RHAT docs|https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/5/html/Configuration_Example_-_Fence_Devices/index.html]. (Also there's VM-infra hosting with API calls, that's what we used for HA HDFS in VMWare), bq.  If we need to get rid of this legacy code, I'd like to raise another JIRA to discuss it, any thoughts?

Don't bother.  I know of people that are using it., Dropping 3.0.0 target version since this is an incompatible change and beta1 is up, this will have to wait for the next major release., I disagree about moving stuff in JARs being incompatible on minor releases. We have done it before (s3n -> hadoop-aws).  There's also a proposal for a whole new sftp client which will go into a new JAR from the outset.

we can add a new JAR as part of the downstream hadoop client, Adding new artifacts is fine, moving code to a new artifact is not okay since downstreams will need to make pom changes to retain their dependency.

Related, I remember chatting with [~busbey] about how it's totally undocumented which of our Maven artifacts are public or private. We talk about hadoop-client being public and hadoop-hdfs private, but even this is only documented in JIRA, and there's no easy reference location for downstream developers.]