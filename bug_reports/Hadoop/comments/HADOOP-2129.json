[BTW, when the failures happen, the distcp log directory (/logdir) is empty, and the map-reduce log files do not have any error messages except the ones given above., Copy from A to B, by running distcp on B with -i (ignore read failures) generated the following exception trace (prolifically):

{noformat}
FAIL hdfs://namenode-of-B:8600/targetdir/targetfile : org.apache.hadoop.ipc.RemoteException: java.io.IOException: Cannot open filename /targetdir/targetfile
        at org.apache.hadoop.dfs.NameNode.open(NameNode.java:238)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)

        at org.apache.hadoop.ipc.Client.call(Client.java:482)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy1.open(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:848)
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.<init>(DFSClient.java:840)
        at org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:285)
        at org.apache.hadoop.dfs.DistributedFileSystem.open(DistributedFileSystem.java:114)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:244)
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.copy(CopyFiles.java:289)
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:367)
        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:218)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)
{noformat}

All the directories were created successfully- so the src file list is readable at the destination- but none of the files could be opened at src.

The trace through o.a.h.u.CopyFiles doesn't match what's in the repository, though. Is this running with any custom patches?, I experienced the same problem.  Adding some logging revealed that the source paths no longer had the "hdfs://host:port" portion of the URI when they were used in the copy() method of the mapper.  This caused the source file-system to be incorrectly identified (it was set to the default file system, which was what it found in the config file)., It looks like o.a.h.dfs.DistributedFileSystem::getPathName discards the scheme, authority, and port from the URI here:

{code}
    String result = makeAbsolute(file).toUri().getPath();
{code}

This is called from o.a.h.dfs.DistributedFileSystem::listStatus(Path), used to build the source list. The source is written as a FileStatus object, not a Path, so this information is never restored., > It looks like o.a.h.dfs.DistributedFileSystem::getPathName discards the scheme, authority, and port [...]

That's actually appropriate: getPathName is supposed to do that.  I think the bug is that DistributedFileSystem#listStatus() does not qualify the paths, as does DistributedFileSystem#listPaths(), by using the DfsPath whose constructor qualifies., Here's a patch that should cause DFS#listStatus() to return fully-qualified paths.  Does this fix the distcp issue?, Sorry, I didn't mean to imply that getPathName was doing the wrong thing, only that the information was discarded there.

I'd been picturing something closer to Path::makeQualified. Since the method on FileStatus must be public, it might as well be restricted to adding information instead of changing it., Here's a version that doesn't add a new public FileStatus#getPath(), but rather fixes this entirely in DistributedFileSystem.java.  Is that better?

It's important that, in addition to returning a fully-qualified path, we return an instance of DfsPath, so that future RPCs with that path are cached.
, +1, -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12370250/HADOOP-2129-2.patch
against trunk revision r598469.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs -1.  The patch appears to cause Findbugs to fail.

    core tests -1.  The patch failed core unit tests.

    contrib tests -1.  The patch failed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1166/testReport/
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1166/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1166/console

This message is automatically generated., Sigh.  I renamed a variable after testing this, and then submitted a broken, untested, patch..., New version that actually compiles!, -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12370335/HADOOP-2129-3.patch
against trunk revision r598699.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests -1.  The patch failed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1177/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1177/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1177/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1177/console

This message is automatically generated., I committed this., Integrated in Hadoop-Nightly #316 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/316/]), When/if we roll a 0.15.2 should this be included?, > When/if we roll a 0.15.2 should this be included?

+1  I'll merge it to the branch.

]