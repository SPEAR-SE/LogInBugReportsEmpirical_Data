[Patch for the dfs file-writing progress reporting., After svn update, the earlier patch failed to compile.
I have attached a new patch now., This looks good, except it is not back-compatible.  Any user code that implements an OutputFormat will no longer compile after this change is made.  Sigh.  I don't see an easy way around this..., The only way around it that I can see is if we had:

FSDataOutputStream:
    setProgressable(Progressable prog)

RecordWriter:
   setProgressable(Progressable prog)

which just pushes the problem down a level., I don't see how this can be done without breaking backward-compatibility. Therefore I have made changes so that with minimum porting any other output formats could be incorporated. An additional getRecordWriter method needs to be implemented that takes an additional parameter. This parameter can be passed to fs.create (or even ignored as in the case of local filesystem.)
, This is an updated patch for this issue that does not have any errors "task reported no progress for 600 seconds" even if there is progress. In fact it is a datanode allocation patch. Each datanode sends an additional load data to namenode that indicates how many bllocks it is currently writing or reading. The namenode, when choosing datanodes for new block takes this load into consideration, and discards datanodes whose load is more than twice that of average.

Thiss is in addition to the requirement that the datanode has enough space to store min_num_blocks.

With this patch, I never see the "no progress for 600 seconds, killing task" error. Therefore, on my 240 node cluster, the randomwriter times went down from 3997 seconds to 2404 seconds.

This patch includes the file-writing progress patch as well. So, please discard  the two patches I submitted earlier., Sigh, I also don't see a compatible way to make this change.  So we'll have to upgrade some Nutch InputFormat implementations to define the new method.  Could you please construct a patch for Nutch too?  That would make my life easier.  Thanks., Sure. I will provide a patch for nutch tomorrow., I have attached the patch for nutch to NUTCH-312.
Affter the recent commmits, there are conflicts (espcecially in datanodeInfo etc). I am resolving those issues, and will provide  a new patch for this issue soon. In the meanwhile, I am deleting the currrently attached patches., this is the correct patch after resolving issues arising from conflicts due to recent commits., Sigh.  Now, with HADOOP-321 reverted, this no longer applies., I will submit the correct patch again., After factoring out the changes introduced by recalling patch for hadoop-320, I have not attached a new patch for this bug. Hopefully this is the last patch., I meant reverting hadoop-321, not hadoop-320., Any reason not to delete the old getRecordWriter() method?, Did not even think about it earlier. But  if compiles, go ahead., Doug. I have removed the extra getRecordWriter method, added progressable documentation, and param doc, and here is the patch. I have also uploaded corresponding patch to nutch-312., I just committed this.  Thanks, Milind.]