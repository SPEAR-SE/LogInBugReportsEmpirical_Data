[To be clear, deletion is called from rename() as well. It makes more frequent to encounter this issue..., GitHub user kazuyukitanimura opened a pull request:

    https://github.com/apache/hadoop/pull/208

    HADOOP-14239. S3A Retry Multiple S3 Key Deletion

    Hi @steveloughran 
    
    Sorry for sending may requests.
    
    I explained the problem here https://issues.apache.org/jira/browse/HADOOP-14239
    
    This pull requests recursively retries to delete only S3 keys that are previously failed to delete during the multiple object deletion because aws-java-sdk retry does not help. If it still fails, it will fall back to the single deletion.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/bloomreach/hadoop HADOOP-14239

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hadoop/pull/208.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #208
    
----
commit 707773b6e14b61b31ecd5473eaafa75dd5217707
Author: kazu <kazu@bloomreach.com>
Date:   2017-03-25T01:29:19Z

    HADOOP-14239. S3A Retry Multiple S3 Key Deletion

----
, I'm going to mark this up as a dupe of HADOOP-11572, which is where I first sat down to look at this problem, or at least the related one of concurrent object delete failure.

I stopped there as I concluded I didn't know enough about how things fail. It may be someone else deleted the object â€”race condition, that being the likeliest. Or its permission related, auth related, etc etc. Without knowing all the failure modes, I wasn't confident I could implement the right policy.

If you have more insight there, that'd be good. As usual, the tests are as important as the production code; the landsat-pds repo the read only one to look at., I went over HADOOP-11572 and saw what you worked on.

I see the cases that partial transient deletion errors happen other than race condition, permission, auth cases. Particularly when a Hadoop job creates thousands of output part files, only a few files (like part-00003) fails out of part-00000 to part-0xxxx. The majority of the files are deleted without issues, there shouldn't be reasons to target particular part files that cannot be deleted. 

aws-java-sdk is populating the partial failure file list here
https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L2097

These are the insights that I have. If the above case makes you more confident that the retrying on failed S3 keys is the right strategy, I will start adding tests., Assuming its just race conditions we need to retry on, and we can identify them (how?) then retry makes sense,  In which case, this JIRA should be closed as a duplicate of HADOOP-11572, and you take on that work.

1. If we can identify 404-failures without making calls on each one, handling is easy: strip them out.
2. Otherwise, yes, individual failures should be queued for 1 by 1 attempts
3. If any of the individual attempts fails with something other than 404, then all queued work must be halted, operation raises an exception.

Note that s3guard is going to add complexity here, just because of new codepaths/deployment situations, as well as enough diffs in the files to make merge hard. I'm not going to be reviewing any work on this patch until s3guard is merged into trunk and/or branch 2. If you start coding atop the HADOOP-11345 branch, then you may have less merge pain, I don't have a good way to identify 404 failures out of the all failed keys.
I think we can still try to use the multi-delete on failed keys if there are multiple instead of deleting one by one. If the number of failed keys reduced, it means the retry was a right thing to do.

Anyway, HADOOP-11572 covers the same topic. 
I am closing this ticket as duplication., Github user kazuyukitanimura commented on the issue:

    https://github.com/apache/hadoop/pull/208
  
    Closing PR as stated in Jira
, Github user kazuyukitanimura closed the pull request at:

    https://github.com/apache/hadoop/pull/208
]