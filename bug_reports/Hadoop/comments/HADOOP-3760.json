[Here is the complete error
{code}
08/07/15 10:21:04 INFO mapred.FileInputFormat: Total input paths to process : 100
java.io.IOException: Stream closed.
       at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.isClosed(DFSClient.java:2240)
       at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:2744)
       at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:2652)
       at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:59)
       at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:79)
       at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:726)
       at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:966)
       at org.apache.hadoop.examples.Sort.run(Sort.java:147)
       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
       at org.apache.hadoop.examples.Sort.main(Sort.java:158)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
       at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
       at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:59)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
Exception closing file system.dir/job/job.xml
java.io.IOException: Stream closed.
       at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.isClosed(DFSClient.java:2240)
       at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:2687)
       at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:2652)
       at org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:220)
       at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:236)
       at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:1379)
       at org.apache.hadoop.fs.FileSystem.closeAll(FileSystem.java:230)
       at org.apache.hadoop.fs.FileSystem$ClientFinalizer.run(FileSystem.java:215)

{code}
The way this can be reproduced is 
1) Run random-writer 
2) Run sort ... here the job submission fails with the above mentioned error.


Also reduce tasks fail with the error 
{code}
2008-07-15 09:31:28,631 INFO org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.ipc.RemoteException: 
org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException: Not replicated yet:tmp-dir/part-00000
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1115)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:340)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)

	at org.apache.hadoop.ipc.Client.call(Client.java:707)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
	at $Proxy1.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at $Proxy1.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2445)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2328)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1800(DFSClient.java:1740)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1917)

{code}, Recently DFSClient was changed for a bug related to infinite loop while writing in HADOOP-3681. This could be related to that. The second exception is when the reduce fails instead of hanging for ever as seen before the bug fix. Trying to reproduce the first case. , I see the comment that job submission fails, which isnt acceptable. Promoting this for 0.18 until we know what is the cause. , I was able to reproduce this by commenting out call to file complete, a request to namenode. The changes in HADOOP-3681 was waiting for a while, retrying 10 times for file to complete and then check for isClose(). By that time closed is already set to true so, it would throw a stream closed exception. isClosed() call after flushInternal() should do the job needed for HADOOP-3681. I tested both the cases and it seems to fix it. , Here is test-patch output. I ran tests on both 0.18 and trunk. All tests pass.
     [exec] 
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
, +1 patch looks good, +1. Looks good.

It will be better to comment why isClosed() is callled and {{closed}} is set to true right after after {{flushInternal()}}. It will save some head-scratching multiple times in future when we look at it., Thanks Raghu. Updated patch with comments. , I just committed this. Thanks Lohit!, Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])]