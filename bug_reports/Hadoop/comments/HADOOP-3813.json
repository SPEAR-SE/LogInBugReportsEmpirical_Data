[Christian, is this the first time you noticed this? Is it reproducible? Thanks., Arun, this was first time with a large job.

We just had another job completed with a similar number of maps, but only a single reducer, and the JobTracker exhibited this problem just for a couple of seconds (57 discarded heartbeats vs 130,000 discarded rpc calls in the previous job). But from this point of view, yes it is reproducible.

In my first comment I probably should not have said 'Promotion', but 'Removal of completed tasks', because the first job had a lot of failed and speculatively executed tasks with a lot of temporary output in dfs, making the cleanup operation more intense. , Assigning to Amareshwari for investigation, The code in JobInProgress.garbageCollect uses FileUtil.fullyDelete to delete the temporary directory, which is consuming time, during which Jobtracker is locked.
{code}
        FileSystem fileSys = tmpDir.getFileSystem(conf);
        if (fileSys.exists(tmpDir)) {
          FileUtil.fullyDelete(fileSys, tmpDir);
        }
{code}

I performed a delete on a directory with 1000 directories inside. It took few milli seconds with fs.delete(dir, true) and it took 2 minutes with FileUtil.fullDelete(FileSystem, dir).

The Jira HADOOP-3202 to deprecate FileUtil.fullyDelete is still open. , Here is patch changing the code in JobInProgress to call FileSystem.delete insteadof FileUtil.fullyDelete., +1. FullyDelete has lots of overhead compared to fs.delete(true). Good catch!, It would be great if we can make this patch into 0.17, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12386711/patch-3813.txt
  against trunk revision 679202.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2929/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2929/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2929/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2929/console

This message is automatically generated., Here is a patch for  branch-0.17. Earlier patch applies to both trunk and branch-0.18, TestCLI failure is not related to the patch., +1. This patch looks fine, the question is whether we need to do more to help ease Christian's pain? 

Christian - do you think you can use this patch/build and re-run this? If you cannot do it right-away I propose we move it to hadoop-0.19. I'm ok committing this as-is too. Thoughts?

TestCLI failure is unrelated to this patch - HADOOP-3809., It would be nice if this patch gets into 0.17.2. The patch looks very simple but its impact could be large on long-lived jobtrackers that serve plenty of jobs., Unfortunately, I cannot apply the patch right away., Same patch, after removing the now unused import of org.apache.hadoop.fs.FileUtil., I just committed this. Thanks, Amareshwari!, Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])]