[Ping [~aw], [~eyang] based on involvement in initial metrics2 JIRAs (HADOOP-6919, HADOOP-6728), HI [~xkrogen],

Can we keep both sinks using the same refresh rate, like 10 seconds?  I would not recommend to have different refresh rate, this is comparing data samples at different frequency.  The resulting graph will not look the same.  Total is a high watermark and it will eventually overflow.  This is the reason that Hadoop community favored gauge system to minimize compute and interested to monitor metrics at real time only during the development phase.

If we want to produce high fidelity data samples.  Time stamp, previous count, current count, and Time passed since last sample (or refresh rate) are the essential information to record for high fidelity data samples, but post processing is more expensive.  Gauge and average are only good for measuring velocity of the metrics for a point in time.  Most monitoring system can only handle time precision at second or minute scale.  Hence, MutableRate is heavily dependent on time precision that the down stream can consume.  One important limitation is JMX cache reset requires JMX sink to be the last one in the chain with slowest refresh rate to avoid accurate problem like you described.  JMX sink should not have a lower refresh rate than FileSink to avoid destroying samples before data is sent.
, Thank you for the comments [~eyang]! You actually made me realize I had a bit of a misunderstanding after digging into the code further. Let me try again:
* The problem I described is definitely an issue if you specify multiple refresh rates. I agree there's not a great way around this issue but I think we should, at minimum, put something in the documentation indicating that it is not a good idea. Right now the behavior I describe when dealing with MutableRate values is not documented and would come as a surprise to an operator.
* Specifying only a single refresh rate does not solve the JMX issue. The single-point collection of metrics for all sinks occurs in {{MetricsSystemImpl}}, specifically {{sampleMetrics()}}, which then passes off the single {{MetricsBuffer}} to all sinks. This is great. However, JMX avoids the {{MetricsSystemImpl}} code altogether, instead directly calling {{getMetrics()}} on each {{MetricsSourceAdapter}}. Thus JMX cache refills can destroy metrics values even if you correctly configure only one period. I have attached a patch, [^HADOOP-14989.test.patch], which demonstrates this issue - it's hacky but it should get the point across.

It seems to me the best way to fix this is to save the output values each time {{getMetrics()}} is called and use those for the cache. We can either
* Call {{updateJmxCache()}} at the end of {{getMetrics()}} with the computed values
* Store the return value of {{getMetrics()}} and use it as the input for {{updateJmxCache()}} next it is called, assuming that value is fresh enough.

The second is considerably more complex. It avoids some potential performance penalty of the {{updateAttrCache()}} and {{updateInfoCache()}} calls, which do create a bunch of objects. Not sure if it would be enough to be worth the extra complexity.

While digging / testing I also noticed another bug which occurs if you have multiple sink periods set; see HADOOP-15008, One more comment:
{quote}
Total is a high watermark and it will eventually overflow.
{quote}
Sure it is a high watermark, but so are all of the {{MutableCounter}} metrics in Hadoop. These all rise indefinitely; I fail to see how this situation is any different., [~xkrogen] Line 233, The test patch is comparing double to int, and val is value of 10.00.  If it did not reset, shouldn't it be 100000010?
As far as I know, JMX mbean calls resets internally, and there is no need to call it externally.  However, if multiple people are pulling from JMX, I don't know how the reset is managed.  Let me know if I misunderstood the test patch.

{code}
Sure it is a high watermark, but so are all of the MutableCounter metrics in Hadoop. These all rise indefinitely; I fail to see how this situation is any different.
{code}

True, this is less of a concern on 64 bits system.  Overflow happens less frequently.  , Assuming nothing else was submitting block reports, then {{val}} with the current code would be 10, but it should be 50000005 (it is an average so {{= 100000010/2}}). Since it is taking metrics from a minicluster there are also some real block reports that skew things; that's why I used a big value and a comparison rather than equal. Like I said, hacky. But the test will definitively pass if you omit the JMX call and definitely fail if you include it. I'll try to put together a real unit test for this.

I am not sure what you mean about JMX mbean calling reset internally. Are you talking here about the metrics2 level reset ({{MetricsSourceAdapter#updateJmxCache()}}) or something at a JVM level? I explained how the cache reset is managed at the metrics2 level; let me know if there's something about my explanation that was not clear., Hi [~xkrogen] Your observation is correct.  However, {{MetricsSourceAdapter}} can not call {{updateJmxCache}} at end of {{getMetrics}}.  It will just deadlock because {{updateJmxCache}} calls {{getMetrics}}.  {{MetricsSystemImpl}} was not used by JMX to avoid a dead lock in the timer thread where  {{MetricsSourceAdapter}} lock and is trying to grab the {{MetricsSystemImpl}} lock. The locking order isn't consistent in the "push and pull" part of {{MetricsSourceAdapter}} so it can deadlocked.

In your second suggestion, store the return value of {{getMetrics}} and use that to populate jmx cache, this is the correct logic, in a push vs pull system.  We need to be careful in the synchronization of cache value to MBean or it can cause mbean to fail with null value.  HADOOP-11361 has some of the background information of how the system arrived at the current state.  There is a new ReentrantLock utility in Java 7 which might help to reduce the deadlock in publishing metrics and retrieved cache by JMX.  This might be one way to solve the race condition and produce more accurate data for JMX.  HADOOP-12594 had an attempt in removing the deadlock, and it might be useful background information on how to solve this the proper way., Perfect, thank you [~eyang]! That information & the pointers to previous JIRAs is very helpful. I will plan further and update..., Moving target version to 2.7.6 due to 2.7.5 release.]