[Here is the error message I'm getting:

2008-12-14 18:43:37,884 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=blk_-1466661706312681205_91866, newgenerationstamp=0, newlength=0, newtargets=[])
2008-12-14 18:43:37,884 INFO org.apache.hadoop.ipc.Server: IPC Server handler 33 on 9000, call commitBlockSynchronization(blk_-1466661706312681205_91866, 0, 0, false, true, [Lorg.apache.hadoop.hdfs.protocol.DatanodeID;@b4e602c) from 172.16.1.162:44797: error: java.io.IOException: Block (=blk_-1466661706312681205_91866) not found
java.io.IOException: Block (=blk_-1466661706312681205_91866) not found
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:1898)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.commitBlockSynchronization(NameNode.java:410)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)
, Could you please post the whole stack trace for the exception.
I am tracing this to {{DFSClient.DFSOutputStream.processDatanodeError()}}. But it would be good to have a confirmation.
Do you know whether anybody was trying to append to this file?, I meant that there should be the same exception on the other side. If you go to your data-node 172.16.1.162 and check it in the logs there., Hi Brian, do you have some instructions to reproduce the problem?  I checked the codes when you have reported it in the mailing list.  I could not see why a datanode could call commitBlockSynchronization(..) with newgenerationstamp=0, newlength=0, newtargets=[]., Checked the codes again, the primary datanode may call commitBlockSynchronization(..) with newgenerationstamp=0, newlength=0, newtargets=[] when syncList.isEmpty() is true.  The problem may be due to the block is deleted after block recovery begins., 4866_20081215.patch: In FSNamesystem.commitBlockSynchronization(..), return silently if the lastblock is not found and deleteblock is true., Hey all,

Below is the exception which is happening on the datanode side (not for the exact same block, but I believe it's the same problem).

We have been growing our cluster constantly, meaning that almost every day we take out nodes (to install new drives and re-kickstart them) or put in new nodes.  This has caused a lot of churn through the decommissioning process and the balancer.  We also have continuous external transfer load test which delete files within seconds after they transfer successfully.  I'd believe you if you claimed we were pushing the boundaries :)

I have a few other patches to apply to the namenode today; I'll try getting to the one Nicholas posted and see if that solves it.

Brian

2008-12-16 08:31:46,084 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 50020, call recoverBlock(blk_5492981093339503298_94099, false, [Lorg.apache.hadoop.hdfs.protocol.DatanodeInfo;@6325950d) from 129.93.239.144:39774: error: org.apache.hadoop.ipc.RemoteException: java.io.IOException: Block (=blk_5492981093339503298_94099) not found
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:1898)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.commitBlockSynchronization(NameNode.java:410)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)

org.apache.hadoop.ipc.RemoteException: java.io.IOException: Block (=blk_5492981093339503298_94099) not found
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:1898)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.commitBlockSynchronization(NameNode.java:410)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)

	at org.apache.hadoop.ipc.Client.call(Client.java:696)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
	at $Proxy4.commitBlockSynchronization(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:1461)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1442)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1508)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)
, No dice.  Now we just get the commitBlockSynchronization message continuously:

2008-12-16 09:46:17,990 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=blk_5492981093339503298_94099, newgenerationstamp=0, newlength=0, newtargets=[])
2008-12-16 09:46:17,991 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=blk_5492981093339503298_94099, newgenerationstamp=0, newlength=0, newtargets=[])
2008-12-16 09:46:17,992 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=blk_5492981093339503298_94099, newgenerationstamp=0, newlength=0, newtargets=[])
2008-12-16 09:46:17,993 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=blk_5492981093339503298_94099, newgenerationstamp=0, newlength=0, newtargets=[])

That's happening at a rate of about 1 KHZ!  Help!

Brian, If I filter out these commitBlockSync messages, I also see the following error which may or may not be related.  This is from the datanode side, but the first portion is printed out at the NameNode.

Brian

2008-12-16 09:52:19,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Client calls recoverBlock(block=blk_-2476019851465028189_97924, targets=[172.16.1.99:50010])
2008-12-16 09:52:19,179 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 50020, call recoverBlock(blk_-2476019851465028189_97924, false, [Lorg.apache.hadoop.hdfs.protocol.DatanodeInfo;@f749036) from 129.93.239.151:58727: error: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-2476019851465028189_97924 is already commited, storedBlock == null.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:4556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:402)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)

org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-2476019851465028189_97924 is already commited, storedBlock == null.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:4556)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:402)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)

	at org.apache.hadoop.ipc.Client.call(Client.java:696)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
	at $Proxy4.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:1468)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1442)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1508)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)
, Checked your exceptions messages.  Since the line numbers do not match, it seems that you are using 0.19 release + some patches.  Do you have HADOOP-4257?  It is a bug fix committed after 0.19.0 release., 4866_20081216.patch: terminate block recovery nicely when the block is already committed., I do have HADOOP-4257 applied.

Second patch does not work; it's trying to match an exception's string, and it's looking for the wrong string.  The correct line should be:

      if (ioe.getMessage().startsWith("java.io.IOException: " + block + " is already commited")) {

With that fix, the DataNode gets the following message first at a high rate, then settles to around 1HZ:

2008-12-16 22:32:55,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Client calls recoverBlock(block=blk_8418325005865976265_90286, targets=[172.16.1.124:50010])

, Even with the second patch, the extremely high rate of the log messages in the NN still appear., > Even with the second patch, the extremely high rate of the log messages in the NN still appear.

I think it will stable down.  Does it?

> 2008-12-16 22:32:55,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Client calls recoverBlock(block=blk_8418325005865976265_90286, targets=[172.16.1.124:50010])

There is a DFSClient keeps calling recoverBlock(..).  Could you find out the corresponding DFSClient log messages to see why it keeps doing that?

Thanks a lot for your help on debugging this!

4866_20081217.patch: use contains(..) instead of startsWith(..)., Hey Nicholas,

Things didn't stabilize.

Looking at the lsof output, there were some very long-lived clients (>10 days old?).  Oddly enough, these clients seem to have survived cluster reboots, namenode restarts, the files they were writing were deleted a long time ago.  I killed the clients and the datanodes shut up (i.e., problem stabilized).

This particular client which uses libhdfs has had problems with infinite loops before (due to our problems, not libhdfs problems).  However, I'd claim that if the file fails, there shouldn't be any way repeated reads should cause problems.

That is, if read is called on DFSClient repeatedly even after errors, it shouldn't cause any issues on the DN side.

Brian, > Oddly enough, these clients seem to have survived cluster reboots, namenode restarts, ...

One of our HDFS design is to let client survives namenode restart.  And you just have shown that it works.  :)

> However, I'd claim that if the file fails, there shouldn't be any way repeated reads should cause problems.
>
> That is, if read is called on DFSClient repeatedly even after errors, it shouldn't cause any issues on the DN side.

I agree that we should protect our servers against denied of service attack.  However, we are not in that stage yet.

Do you think the current patch is good?, After reading the comments on this JIRA and aftrer some discussion with Nicholas, we gathered that the infinite loop was probably created by a client application that continuously tried to open/append to the file. A code walk-through revealed that the DFSClient will attempt lease recovery 5 times per replica for that node. After that, it throws an IO error to the application. We also determined that the current patch is not required.

I am closing this issue for now, please feel free to reopen if you see it occur again.]