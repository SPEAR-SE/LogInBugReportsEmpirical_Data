[can you provide a sample code that cause you the issue, hi, Sudharsan, sure, you can use the data and scripts I uploaded(later) to do the check. 

I used the "org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner" instead of the newer "org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner" in streaming command cause in 0.21.0, the org.apache.hadoop.mapred.JobConf.setPartitionerClass() is still expecting an Object extends from "org.apache.hadoop.mapred.Partitioner":

public void setPartitionerClass(Class<? extends Partitioner> theClass) {
'      setClass("mapred.partitioner.class", theClass, Partitioner.class);
}

and why is KeyFieldBasedPartitioner configured twice here:
  cause the "o.a.h.mapred.lib.KeyFieldBasedPartitioner" class now extends from "o.a.h.mapreduce.lib.partition.KeyFieldBasedPartitioner" to use potential newer features(which's not appeared yet), which is Configurable, so in ReflectionUtils.setConf, the line :

"if (theObject instanceof Configurable) { ((Configurable) theObject).setConf(conf); }" 

will be executed, configure the partitioner once, add a KeyDescription to the KeyDescription list.
And later, no matter the above line is executed or what, this line:

"setJobConf(theObject, conf);"

will be executed anyway, configure the Partitioner twice, add another KeyDescription to the list, even if we just configured one.
, use the scripts in this tar ball to test the KeyFieldBasedPartitioner.
, Hi Steven,

Yes. You are right. This issue is seen on the trunk too. But to me, configuring twice should not be an issue but processing the configured property without realizing that the same configuration has been processed already could be avoided.

Will it be ok if instead of maintaining a List, we maintain a Set of keyspecs in the KeyFieldHelper and provide an equals method that returns true if its params match exactly as the passed in. The downside is that if some job wants this kind of double hashing done on the same keyspec that would be difficult asking for a custom partitioner. But I doubt if there would be such a scenario.

This would also ensure we get the correct partition and increase performance as we avoid a hash invoke for every record. , hi Sudharsan, 
    I think the KeyFieldHelper has done its job, it is the setConf method of ReflectionUtils makes the partitioner configured twice.
    I wonder why the logic in ReflectionUtils.setConf first call setConf() then call configure() on the same object? can we write it as:


      if (theObject instanceof Configurable) {

        ((Configurable) theObject).setConf(conf);

      }
      else { 
        
        setJobConf(theObject, conf);

      }    
can we do this?
, Made changes to set configuration only once for the old/new api, Changes to set configuration only once for the new/old api, changes to apply configuration only once , -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12486973/Hadoop-7425.patch
  against trunk revision 1147971.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/745//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/745//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/745//console

This message is automatically generated., thanks Sudharsan, you are doing great job here. will we see this modification in new release of hadoop?, Added test case to the patch, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12487111/HADOOP-7425.patch
  against trunk revision 1147971.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The patch appears to cause tar ant target to fail.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:


    -1 system test framework.  The patch failed system test framework compile.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/752//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/752//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/752//console

This message is automatically generated., Updated with proper test case. We should add a test case on JobConfigurable or KeyFieldBasedPartitioner on their respective test classes., The latest test patch failed.  Also the patch is rather old, and will need to be upmerged to apply to trunk., so glad this problem is updated.
and Robert, I think the given patch might not point out how the problem is gonna happen. the tested object should be an object from a class implements these two interfaces: Configurable, JobConfigurable, only in this case will the object be configured twice(1. setConf and 2. configure)., this file contains patch to ReflectionUtils.java and TestReflectionUtils.java.
setConf is a method called many times, this patch may have profound influence if it introduces any bug., Steven,

The build system on trunk has changed to use maven instead of ant so the location of the files are different.  In addition to this common, mapreduce, and HDFS were split up and common no longer has any direct dependencies on mapreduce, but the new test does, so it will not compile.  ReflectionUtils loads the mapreduce dependencies using reflection now, which is ugly, but works.  If you want this fix to go into the 1.0 line, 0.21 or even 0.22 we also need to have a version that is compatible with trunk so we do not get any regressions in newer versions.

In addition to this, looking at the code I am not sure of all of the ramifications of not having JobConfiguration.configure called for all classes that are both JobConfigurable and Configurable.  There are other classes that may rely on this behavior.  I think I would prefer to see KeyFieldBasedPartitioner updated to handle having both methods called., Filed MAPREDUCE-4771 to fix KeyFieldBasedPartitioner so it doesn't behave improperly if it is configured twice.]