[Forgot to mention that the cluster was run with the natvie compression lib., More details about the failed job:

1. It uses record-level compression
2. mapred.child.java.opts is set to be the default value: --Xmx512m
3. For the mapout, each key is a text and very small, but each value is a jute record, with an average size of approximate 25K. Some might be as big as mega bytes., Here is a patch while I continue further testing... Hairong could you try to see if it works for you? Thanks!

Basically I went ahead and implemented a 'codec pool' to reuse the direct-buffer based codecs so as to not create too many of them... 

Results while trying to sort 1Million records via TestSequenceFile with RECORD compression:

                                     trunk           H-1193
Compressors:          1382                  3
Decompressors:      1520                 12
-----------------------------------------------------
Total:                            2902                 15

Results are even more dramatic for BLOCK compression (we need 4 codecs per Reader with BLOCK compression for key, keyLen, val & valLen) ... in fact I have gone ahead and bumped up the default direct buffer size for zlib to 64K from 1K which should lead to improved performance too, on the back of this patch.

Appreciate any review/feedback., Here is an updated version of the patch with the changes I made to BigMapOutput to help test it (basically made it extend ToolBase and added an option to create the large map input too)...

I have tested this with large map inputs (>2G) and seems to hold up well i.e. the codec pool ensure we create only 1 compressor and very small no. of decompressors (less than 10) even for extremely large map inputs (>2G)., One comment - it would be nice to have the 'tmpReader' logic be triggered when such a flag is passed in the constructor of Reader. Apart from that there are some whitespace changes which can be removed.

, Thanks for the review Devaraj, patched-anew incorporating the comments..., -1, build or testing failed

2 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12359395/HADOOP-1193_3_20070611.patch against trunk revision r546310.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/269/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/269/console

Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., Updated patch to reflect changes to trunk..., +1

http://issues.apache.org/jira/secure/attachment/12359765/HADOOP-1193_4_20070614.patch applied and successfully tested against trunk revision r547159.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/284/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/284/console, I just committed this.  Thanks, Arun!, Integrated in Hadoop-Nightly #127 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/127/])]