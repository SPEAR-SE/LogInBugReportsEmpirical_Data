[Added a unit test that can reproduce the problem., I found a bit of time to look into this, so I'm dumping my notes here.  I'm not sure when I'll get some more time to work on it, so if someone feels brave enough to step in feel free.

Here's how I believe records get dropped with very small split sizes:
 # There's only one bz2 block in the file
 # The split size is smaller than 4 bytes
 # First split starts to read the data. It consumes the 'BZh9' magic header then updates the reported byte position of the stream to be 4
 # At this point the first split reader is beyond the end of the split before it ever read a single record, so it ends up returning with no records.
 # The second split starts in the middle of the 'BZh9' magic header and scans forward to find the start of a bz2 block and starts processing the split
 # Since this is not the first split, it throws away the first record with the assumption the previous split is responsible for it
 # Second split reader proceeds to consume all remaining data, since byte position is not updated until the next bz2 block and there's only one block
 # End result is first record is lost since first split never consumed it.

I think we can fix this scenario by not advertising a new byte position after reading the 'BZh9' header and only updating the byte position when we read the bz2 block header following the current bz2 block.

I didn't get as much time to look into the duplicated record scenario, but I suspect multiple splits end up discovering the beginning of the bz2 block and think it is their block to consume. Not sure yet how we can easily distinguish which split is the one, true split that is responsible for consuming the bz2 block given we're hiding the true byte offset from the upper layers most of the time., [~jlowe]

Thank you for your insights. I have created a patch based on your comment.

As far as I tested, all the unit tests passed and I confirmed that the issue I was seeing was solved.

 

I greatly appreciate any and someone take a look. Alternative proposals are also very welcome.

 

 

Regarding the duplicated record scenario, the record was read twice when BZip2Codec starts reading at position 0 (BZip2 header) and position 4 (first BZip2 marker).

test.bz2:0+1 -> read 100 records

test.bz2:3+4 -> read 99 records

 

2018-02-05 20:49:51,598 ERROR [Thread-3] mapred.TestTextInputFormat2 (TestTextInputFormat2.java:verifyPartitions(324)) - splits[0]=file:/Users/tanakah/work/count-mismatch2/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/TestTextInputFormat/test.bz2:0+1 count=100
2018-02-05 20:49:51,605 ERROR [Thread-3] mapred.TestTextInputFormat2 (TestTextInputFormat2.java:verifyPartitions(326)) - splits[1]=file:/Users/tanakah/work/count-mismatch2/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/TestTextInputFormat/test.bz2:1+1 count=0
2018-02-05 20:49:51,608 ERROR [Thread-3] mapred.TestTextInputFormat2 (TestTextInputFormat2.java:verifyPartitions(326)) - splits[2]=file:/Users/tanakah/work/count-mismatch2/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-dir/TestTextInputFormat/test.bz2:2+1 count=0

2018-02-05 20:49:51,614 ERROR [Thread-3] mapred.TestTextInputFormat2 (TestTextInputFormat2.java:verifyPartitions(313)) - read 1
2018-02-05 20:49:51,617 WARN  [Thread-3] mapred.TestTextInputFormat2 (TestTextInputFormat2.java:verifyPartitions(315)) - conflict with 1 in split 3 at position 7

 

 , Thanks for the patch!
{code:java}
      if (this.startingPos > 0 && this.startingPos <= 4) {
        this.startingPos = end + 1;
        this.compressedStreamPosition = end + 1;
      }
{code}
The code above is making the following assumptions that I believe could not be true in some cases:
 * The bzip2 file header is always present at starting pos 0. I think it should be checking isHeaderStripped/isSubHeaderStripped.
 * If the split starts after byte 0 but before byte 5 then it must also end on or before byte 5. (Splits are not required to be equally sized.)

If the bzip2 file header is four bytes, why is the condition {{<= 4}} instead of {{< 4}}?  Should this code leverage HEADER_LEN and SUB_HEADER_LEN here?

Nit: "emptry" s/b "empty", Thank you for the comment! I've updated the patch.

 
>The code above is making the following assumptions that I believe could not be true in some cases:
>The bzip2 file header is always present at starting pos 0. I think it should be checking isHeaderStripped/isSubHeaderStripped.
>If the split starts after byte 0 but before byte 5 then it must also end on or before byte 5. (Splits are not required to be equally sized.)
 
Thank you for pointing these out. I've modified the patch code and uploaded to the Jira.

 
> If the bzip2 file header is four bytes, why is the condition {{<= 4}} instead of {{< 4}}? Should this code leverage HEADER_LEN and SUB_HEADER_LEN here?

When we changed the condition to < 4, the first bz2 block will duplicate. Because 4 is a position of the first bz2 block marker, and an input stream will start reading the first bz2 block if the start position of the input stream is 4. 

The concept of my patch is changing the position of the first BZ2 block marker from 4 (when the bzip2 file header is present) to 0 forcibly. So, if the input stream tries to read from position 1-4, it will drop the first BZ2 block even though the block marker position is 4. I am not sure whether this is the best solution but I could not come up with another idea., Thanks for updating the patch!
{quote}Because 4 is a position of the first bz2 block marker, and an input stream will start reading the first bz2 block if the start position of the input stream is 4.
{quote}
Ah, right. Thanks for the explanation.
{quote}So, if the input stream tries to read from position 1-4, it will drop the first BZ2 block even though the block marker position is 4.
{quote}
This doesn't just drop the first bzip2 block, it drops the entire split. This goes back to my previous comment about the code assuming splits that start between bytes 1-4 are always tiny. Splits do not have to be equally sized, so theoretically there could be just two splits where the first split is a two-byte split starting at offset 0 and the other split is the rest of the file. I believe this change would cause all records to be dropped in that scenario. To fix that I think we only need to report a position that is one byte beyond the start of the first bzip2 block rather than at the end of the entire split (i.e.: header_len + 1 rather than end + 1).

The logic regarding the header seems backwards. If the header is stripped then that means there was a header present, yet the logic is only adding up bytes for a header length if it was *not* stripped which is the case when the header is not there.  I'm wondering how it's working since I think the header is always there in the unit tests., Thank you very much for the comments!
{quote}
This doesn't just drop the first bzip2 block, it drops the entire split. This goes back to my previous comment about the code assuming splits that start between bytes 1-4 are always tiny. Splits do not have to be equally sized, so theoretically there could be just two splits where the first split is a two-byte split starting at offset 0 and the other split is the rest of the file.
{quote}
Thank you for explaining the details. I understand the problem.

 
{quote}The logic regarding the header seems backwards. If the header is stripped then that means there was a header present, yet the logic is only adding up bytes for a header length if it was not stripped which is the case when the header is not there.
{quote}
That's right... Thank you for pointing this out.
After some tests, I noticed the following 2 points.

1. When reading from position 1-3 (on bzip2 header), isHeaderStripped/isSubHeaderStripped is always false. This is because the current readStreamHeader() works only when the start position is 0.

2. I set one byte beyond the start of the first bzip2 block (header_len + 1) to the InputStream's start position, but duplicated records issue still happened. When I set header_len + 5 (9), we can avoid the problem.

As far as I looked at the test bz2 file using binary editor, the first bz2 marker starts from position 4 (right after bz2 header). 
Still trying to understand why we need to set header_len + 5., Added the updated patch. Please let me know if I still misunderstand something.

I made following changes to the original code
 * Not advertising a new byte position when reading from BZip2 Header (position 0) 

 * Move reading position to right after the BZip2 header (position 5) when the position is between 1 and 4

This implementation moves the start position forcibly without checking whether the BZ2 file has a header or not. Because I could not determine whether the header exists when the start position is 4. However, I think it's safe to move the position even if the file does not have a bz2 header because we cannot put 2 bz2 blocks in the first 4 bytes of the file., Thanks for updating the patch!

It seems the basic problem is that split 0, the first split, is _always_ responsible for the first record even if that record is technically past the byte offset of the end of the split.  That's because all other splits will unconditionally throw away the first (potentially partial) record under the assumption the previous split is responsible for it.  Therefore we need to do two things to avoid drops and duplicates:
* If the first split ends before the start of the first bz2 block then we need to avoid advertising the updated byte position until we have started to consume the first bz2 block.  This avoids the dropped record.
* If subsequent splits start before the first bz2 block begins then we need to make sure any split that starts before the first block is artificially pushed past that first block.  This avoids the duplicates.

I'm wondering if it gets cleaner if we move this logic into readStreamHeader() and always call it.  That method can check the starting position and do one of the following:
* check for and read the full header if it is at starting position 0
* do nothing if start pos is past the full header + 1
* verify the bytes being skipped are the expected header bytes if start pos between 0 and full_header+1.  If they are not the expected bytes then we reset the buffered input (just like starting pos 0 logic does today if header is not there).

In the constructor we should be able to avoid updating the reported position if starting position is 0 (so we will always read into the first bz2 block), otherwise we advertise after reading any header so subsequent splits always start at least one byte after the start of the first bz2 block.
, Hi Jason,

I really appreciate your code review, I refactored readStreamHeader() , I changed the method's return value from buffered input to a length the buffered input read in the method. 
 * 
{quote}check for and read the full header if it is at starting position 0
{quote}

In the current implementation, read only "BZ" header when the read mode is CONTINUOUS. Do you think we should keep this?, Thanks for updating the patch!

bq. In the current implementation, read only "BZ" header when the read mode is CONTINUOUS. Do you think we should keep this?

Yes, because it's not important to read the header when the codec is in BLOCK mode.  IIUC the main difference between CONTINUOUS and BLOCK mode is that BLOCK mode will be used when processing splits and CONTINUOUS mode is used when we're simply trying to decompress the data in one big chunk (i.e.: no splits).  BLOCK mode always will scan for the start of the bz2 block, so it will automatically skip a bz2 file header while searching for the start of the first bz2 block from the specified start offset.

Given the splittable codec is always scanning for the block and doesn't really care what bytes are being skipped, I'm now thinking we can go back to a much simpler implementation.  I think the code can check if we're in BLOCK mode to know whether we are processing splits or not.  If we are in BLOCK mode we avoid advertising the byte position if start offset is zero just as the previous patches.  In BLOCK mode we should also skip to file offset HEADER_LEN + SUB_HEADER_LEN + 1 if the start position is >=0 and < HEADER_LEN + SUB_HEADER_LEN.  That will put us one byte past the start of the first bz2 block, and BLOCK mode will automatically scan forward to the next block.  This proposal is very similar to what was implemented in patch 003.  I think we just need to make it only do the position adjustment if we're in BLOCK mode., Thank you, I updated the patch.

 

Implemented the following behavior:
 * If we are in BLOCK mode we avoid advertising the byte position if start offset is zero 
 * In BLOCK mode we should also skip to file offset HEADER_LEN + SUB_HEADER_LEN + 1 if the start position is >=0 and < HEADER_LEN + SUB_HEADER_LEN. That will put us one byte past the start of the first bz2 block, and BLOCK mode will automatically scan forward to the next block

One thing  I am still not sure is when resetState() is called.  We need to skip file reading position to 5 to avoid duplicated records f the start position is >=0 and < HEADER_LEN + SUB_HEADER_LEN.  I implemented this in readStreamHeader(). On the other hand, when InputStream is reset, readStreamHeader() is called. I changed the readStreamHeader() is executed only when this.compressedStreamPosition is 0 , but we might be okay to remove calling readStreamHeader() when InputStream is reset.

 

 , Thanks for updating the patch!

I believe the latest patch will break CONTINUOUS mode since it will no longer strip the bzip2 file header in that case.

I don't think it will be OK to remove calling readStreamHeader when reset() is called.  We're resetting the codec state to start afresh, and that means potentially reading a new file header (e.g.: concatenated bzip2 files).  My thinking is that we need to read the header, but we should not report the byte position being updated when doing so while we're in BLOCK mode (i.e.: split processing).

I think we need to revert the stream header reading logic to the original behavior.  Instead we can put a small change in the BZip2InputStream constructor to handle the special case of small splits that can start at or before the first bz2 block.  If the read mode is BLOCK and 0 < startingPos <= HEADER_LEN + SUB_HEADER_LEN then we skip bytes until we get to the HEADER_LEN + SUB_HEADER_LEN + 1 offset in the stream.  The bufferedIn.skip method will be useful here, but it needs to be called in a loop in case the skip fails to skip everything in one call (per the javadoc).
, Thank you very much for the review!

I've added the updated patch to the ticket. Please let me know if you have any questions on this., Thanks for updating the patch! Looks good overall, just a few nits. I think we're close, so moving this to Patch Available so the QA bot can comment on this as well.

Why are we only skipping one byte at a time instead of trying to skip the rest of the way in one call? The code can track the remaining bytes in skipBytes, decrement that by the number of bytes skipped in the loop, then loop while skipBytes > 0.

There is trailing whitespace on a couple of lines which would be nice to cleanup. I expect the QA bot to flag this in its whitespace check.

I'm not sure it's necessary to call out the JIRA in the comments. That's what {{git blame}} is for. ;) Otherwise the code would be littered with JIRA numbers in every bugfix change.

"steam is on BZip2 header" should be "a split is before the first BZip2 block", | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 32s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 36s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red} 17m  4s{color} | {color:red} root in trunk failed. {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 29s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 18s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  3s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 33s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 59s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red} 11m 59s{color} | {color:red} root generated 178 new + 1053 unchanged - 0 fixed = 1231 total (was 1053) {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 48s{color} | {color:orange} root: The patch generated 1 new + 50 unchanged - 0 fixed = 51 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 29s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 2 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m  9s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 11s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 24s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}120m 10s{color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 32s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}211m 18s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.mapreduce.v2.TestMRJobs |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15206 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12910617/HADOOP-15206.006.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 8769a4c10c6a 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 8f66aff |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| compile | https://builds.apache.org/job/PreCommit-HADOOP-Build/14126/artifact/out/branch-compile-root.txt |
| findbugs | v3.1.0-RC1 |
| javac | https://builds.apache.org/job/PreCommit-HADOOP-Build/14126/artifact/out/diff-compile-javac-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/14126/artifact/out/diff-checkstyle-root.txt |
| whitespace | https://builds.apache.org/job/PreCommit-HADOOP-Build/14126/artifact/out/whitespace-eol.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/14126/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14126/testReport/ |
| Max. process+thread count | 1406 (vs. ulimit of 5500) |
| modules | C: hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14126/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thank you!! I updated the patch.
 * Changed to try to skip the rest of the bytes in one call
 * Deleted tailing whitespaces
 * Deleted comments in the code
 * Changed comment "steam is on BZip2 header" to "a split is before the first BZip2 block"

 

The unit test for 006 failed but it looks that the failed test was not related to this patch. , | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 22s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 18m 36s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 15s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 46s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  4s{color} | {color:orange} root: The patch generated 1 new + 50 unchanged - 0 fixed = 51 total (was 50) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  9s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 19s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m  6s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}140m 26s{color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 41s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}237m 35s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.fs.shell.TestCopyPreserveFlag |
|   | hadoop.mapred.TestMRTimelineEventHandling |
|   | hadoop.mapred.TestMiniMRChildTask |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15206 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12910679/HADOOP-15206.007.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 79b2b1960b41 3.13.0-133-generic #182-Ubuntu SMP Tue Sep 19 15:49:21 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 8f66aff |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/14130/artifact/out/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/14130/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/14130/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14130/testReport/ |
| Max. process+thread count | 1459 (vs. ulimit of 5500) |
| modules | C: hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14130/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, bq. Deleted comments in the code

Sorry, I didn't mean the entire comment needs to be deleted.  I think the comments were very helpful to explain why the logic is there, but I just didn't see the need to call out the specific JIRA number.  That is something trivially obtained from git.

Speaking of comments, when they are reinstated I noticed that this comment is slightly incorrect:
{code}
        // HADOOP-15206: When we're in BYBLOCK mode and the start position
        // is >=0 and < HEADER_LEN + SUB_HEADER_LEN, we should also skip
        // to right after the BZip2 header to avoid duplicated records
        skipPos = HEADER_LEN + SUB_HEADER_LEN + 1 - this.startingPos;
{code}
"Skip to right after the BZip2 header" may lead someone to think there's an off-by-one bug in the code.  We need to skip to right after the start of the first bz2 block (which occurs right after the bz2 header).

Nit: skipPos is not really a position but rather the number of bytes being skipped, so it looks incorrect when the code calls updateReportedByteCount on what appears to be a position rather than a byte delta.  Something like numSkipped or numBytesSkipped would be a less confusing name.

It would be nice to fix the checkstyle warning about line length on the comment.

The unit test failures appear to be unrelated, and they pass for me locally with the patch applied., Thank you. Since this is the first time to apply a patch to the community, I apologize for having bothered you.

Update the patch, I think I fixed the points you pointed out, please let me know if there is anything I should do., bq. Since this is the first time to apply a patch to the community, I apologize for having bothered you.

No worries whatsoever.  It is very common to go back and forth on a number of patches before anything is committed, so this is simply development as usual from my perspective.  I deeply appreciate your contribution on this subtle and tricky issue!

+1 for the latest patch, pending Jenkins., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 23s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 15m 23s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 17s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 44s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 29s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m 11s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}126m 38s{color} | {color:red} hadoop-mapreduce-client-jobclient in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 43s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}226m 57s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.mapreduce.v2.TestMRJobs |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15206 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12910782/HADOOP-15206.008.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 62fa7bbdb8e3 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / da59acd |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/14136/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14136/testReport/ |
| Max. process+thread count | 1363 (vs. ulimit of 5500) |
| modules | C: hadoop-common-project/hadoop-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14136/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, The TestMRJobs failure is unrelated and tracked by MAPREDUCE-7053.

Committing this., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13673 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13673/])
HADOOP-15206. BZip2 drops and duplicates records when input split size (jlowe: rev 0898ff42e9e5c53f2fce7ccdeb4e1cd7d0f123b3)
* (edit) hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestTextInputFormat.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java
, Thanks, [~tanakahda]!  I committed this to trunk, branch-3.1, branch-3.0, branch-2, branch-2.9, branch-2.8, and branch-2.7., Just reviewing this as part of a backport. Quick question [~jlowe] and [~tanakahda]:

 
{noformat}
+        long skipBytes = numSkipped;
+        while (skipBytes > 0) {
+          long s = bufferedIn.skip(skipBytes);
+          if (s > 0) {
+            skipBytes -= s;
+          } else {
+            if (bufferedIn.read() == -1) {
+              break; // end of the split
+            } else {
+              skipBytes--;
{noformat}

Why is {{skipBytes}} decremented here?  skip() returned <= 0, doesn't that mean that no bytes were skipped?  I know we want this loop to terminate eventually but I did not understand this part.

{noformat}
+            }
+          }{noformat}, skipBytes is decremented because of the read() call.  The skip() call is not guaranteed to be able to skip, and the workaround in that case is to try to read().  If the read() is successful then we were able to skip one more byte and need to account for that in the total number of bytes trying to be skipped.
, In my understanding, skipBytes-- will not be executed basically.

bufferedIn.read() is executed only when skipBytes is 0, this usually means that the file position is at the end of the split. However, InputStream.skip says "Skips over and discards {{n}} bytes of data from the input stream. The {{skip}} method may, for a variety of reasons, end up skipping over some smaller number of bytes, possibly {{0}}. The actual number of bytes skipped is returned." ([https://docs.oracle.com/javase/7/docs/api/java/io/FilterInputStream.html).] So I thought InputStream.skip() might return 0 even if the position is not at the end of the split. 

 

Please let me know if my understanding is wrong. Thank you., Ah.. yes, I didn't notice the read() call.  Thank you, makes sense now.]