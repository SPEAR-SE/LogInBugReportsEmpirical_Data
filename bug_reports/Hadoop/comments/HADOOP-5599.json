[Consider the following simple scenario:
* Generate data
 * Datanodes in a given rack goes down(2 replicas of many block are lost)
* Run sort on the generated data
* Sort job fails
* The filesystem is declared CORRUPT

However the expected behavior would be to successfully sort the data available using the third replica of blocks., Does your randomwriter.cfg lower the default replication from 3 to 2 or 1?  Or does the randomwriter code?, bq. Does your randomwriter.cfg lower the default replication from 3 to 2 or 1? Or does the randomwriter code?
None of the above two scenarios occur., I ran the above job on a 480 node cluster with many racks and the JT was brought up using fairshare scheduler.

A similar kind of behavior is observed in the following case as well:
* Generate data and sort it
* Datanodes in a given rack go down
* Run testmapredsort. The job fails
* The filesystem is declared CORRUPT
, Only open case I know for block getting corrupt due to lost rack is HADOOP-4477., Logs of randomwriter console output,  fsck console output and namenode logs., I was able to reproduce the above said issue.

1) In the cluster, generate data using randomwriter
2) Get one rack and kill all the datanodes in that rack only.
3) Run sort job. It fails.
4) Run Fsck from root. It says data corrupt.

I have attached the logs of these ., Did you have the topology defined? If not, there is nothing DFS can do. If so, the question becomes why blocks ended up with all the replicas on one rack., No, the topology was not defined. Thats the reason why all the replicas were placed in the "default" rack. After defining the network topology, the jobs successfully completed using the remaining replica. Thanks Owen. 
]