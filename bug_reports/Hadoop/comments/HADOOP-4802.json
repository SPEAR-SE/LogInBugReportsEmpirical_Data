[Suggested fix: reset BAOS if larger than N * initial buffer size., I'd rather not see new configuration options for this.  We should rather try hard to pick values that work well in all cases, if possible.

I wonder if the simpler change of not re-using the buffer might work just as well.  If we simply allocate a  new buffer per request whose initial size is 10k.  I would be surprised if that allocation would be a significant RPC bottleneck, but we should of course test that with a benchmark.
, I was thinking on the same lines. Allocation per request would not hurt (mostly because various other extra costs). Coule of improvements by subclassing BAOS:

   # Keep a fixed buffer 10kb. inside reset() replace the previous buffer with this. This avoids allocation for most RPCs.
   # extra benefit : this would allow us to avoid a copy if the response could be written to socket in-line (common case). Right now server always copies (through asByteArray()).

Note that this jira will increase the buffer copies for large (multi-MB) responses because of copy each time buffer expands. (Partly negated by copies reduced by HADOOP-4797, since it takes multiple write() invocations to write all the data. in jdk, each write copies all the data!). A future improvement could be to write our own stream that uses a list of buffers.
, > extra benefit : this would allow us to avoid a copy if the response could be written to socket in-line (common case). 

Once we have this (here or in a future jira), the fixed buffer could be a direct buffer and that will remove yet another copy., Thanks for the feedback lads.

I like your suggestions Raghu.  This patch removes configuration and makes an attempt at #1.   #2 and the third suggestion of direct buffer to socket would take more work (though we should do them, yeah)., Patch applies to 0.19 and to TRUNK., I'm still not convinced we should do more than replace the buf.reset() with buf = new ByteArrayOutputStream() and remove the initialization of buf altogether., > I'm still not convinced we should do more than replace the buf.reset() with buf = new ByteArrayOutputStream() and remove the initialization of buf altogether.

Above is predicated on our running a 'benchmark'.  What would you suggest I run?

Why create new objects when it can be avoided (when response < 10k)?

, yes, other buffering improvements are more work and better suited in a new jira. Btw, we don't need the check '{{buf != initialBuffer}}'.

Couple of advantages to current patch :

  # we can clearly say this won't change performance for common case. Though I agree that creating new stream (i.e. two more allocations) each time won't be noticeable (except ay be more frequent partial G/C on benchmarks). 
  # Subclassing BAOS (or a new output stream) will be required for other buffering improvements any way., v3 removes the unnecessary equality test (Thanks Raghu), Also, the new class needs to be static. 

Subjective : I don't think we should have to use jira numbers in comments. I think this is a specific enough problem that comment or javadoc could explain it in enough detail. Readers don't need to go to jira., Made HBAOS static (Had to move it out of Server#Handler to do so).  Removed pointer to this issue from comments (Can go to svn log if really needed)., +1 for the patch.
, > Why create new objects when it can be avoided (when response < 10k)?

It's probably a premature optimization.  It adds code.  Less is more., > It's probably a premature optimization. It adds code. Less is more.

Do the above add up to a -1 on v4 of the patch?, > Do the above add up to a -1 on v4 of the patch?

No, more like a +0.  Without benchmarking its safest to not change things much.  Do we have a good pure RPC benchmark?, > stack - 09/Dec/08 12:32 PM
> Patch applies to 0.19 and to TRUNK.

I hope it'll be fixed at the 0.18.3 release., TestRpcCpu micro-benchmark attached to HADOOP-4797 can be used to asses the CPU penalty of this patch for larger buffers. May not matter much since we do expect extra copies with this patch for larger responses., Removed my patches.  Write me offline if want to know why., This is duplicate of HADOOP-6460, which has been fixed. Marking this as resolved as well.]