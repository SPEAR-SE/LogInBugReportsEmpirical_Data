[In theory, input streams aren't thread safe (that's at the java.io.InputStream spec level), so nothing should be using them in parallel.

But some apps are known to treat them as thread safe, because HDFS DFSInputStream did offer stronger guarantees and apps (HBase) coded against it. {{SequenceFile$Reader}} doesn't make that assumption, not AFAIK. The input stream it is reading is one created a few lines earlier in the {{initialize()}} clause...it's now lining up to talk to the start of the data

{code}
    if (fileSplit.getStart() > in.getPosition()) {
      in.sync(fileSplit.getStart());                  // sync to start
    }
{code}

Where it skips 4 bytes and tries to read a 16 byte header
{code}
        seek(position+4);
        in.readFully(syncCheck);   // HERE
{code}

If it's failing in multi threaded IO, I wouldn't put the blame on readFully. It's throwing an EOFException if there aren't enough bytes to fill up the buffer. Which implies either the offset the stream has synced to  is > the end of the blob, or the start of the readFully was within the stream len, but the total length wasn't.  Possible causes for these: partitioning is wrong, or the measured blob len coming back from getFileStatus is < that of the actual len.

I wouldn't directly point the blame at the page blob code here, unless there's something up with how it is doing its seeks. But if its not that code: why isn't it surfacing elsewhere?, Looking at this some more. InputStream.readFully() will raise an EOFException if InputStream.read() returned -1.  I'd look at the codepath there, in case some transient failure is causing -1 to be returned, which is then raising the new EOFException, Moving out to 2.10.0 as 2.9.0 release is ongoing.]