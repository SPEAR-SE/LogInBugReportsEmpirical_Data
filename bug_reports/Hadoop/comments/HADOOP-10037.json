[FYI, I recently upgraded our clusters (from CDH 4.3.0 / Hadoop to ) and it looks like this issue might now be solved.  I'm seeing some of the tasks of our Hadoop jobs (failing) as they should with the following wrong-#-of-byes-read exception, which then forces a re-try of the task.

{code}
org.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 346403598; received: 15815108
	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184)
	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:204)
	at org.apache.http.impl.io.ContentLengthInputStream.close(ContentLengthInputStream.java:108)
	at org.apache.http.conn.BasicManagedEntity.streamClosed(BasicManagedEntity.java:164)
	at org.apache.http.conn.EofSensorInputStream.checkClose(EofSensorInputStream.java:237)
	at org.apache.http.conn.EofSensorInputStream.close(EofSensorInputStream.java:186)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:87)
	at org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.releaseConnection(HttpMethodReleaseInputStream.java:102)
	at org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.close(HttpMethodReleaseInputStream.java:194)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.seek(NativeS3FileSystem.java:152)
	at org.apache.hadoop.fs.BufferedFSInputStream.seek(BufferedFSInputStream.java:89)
	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:63)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:102)
	at com.macrosense.mapreduce.io.PingRecordReader.initialize(PingRecordReader.java:80)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:478)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:671)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:330)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
{code}

Looks like this fix (in ContentLengthInputStream and/or EofSensorInputStream) was added to Apache HTTP Compoents and/or jets3t some time in the past few months, FYI, I recently upgraded our clusters (from CDH 4.3.0 to CDH5.0.0) and it looks like this issue might now be solved. I'm seeing some of the tasks of our Hadoop jobs (failing) as they should with the following wrong-#-of-byes-read exception, which then forces a re-try of the task.

{code}
org.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 346403598; received: 15815108
	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184)
	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:204)
	at org.apache.http.impl.io.ContentLengthInputStream.close(ContentLengthInputStream.java:108)
	at org.apache.http.conn.BasicManagedEntity.streamClosed(BasicManagedEntity.java:164)
	at org.apache.http.conn.EofSensorInputStream.checkClose(EofSensorInputStream.java:237)
	at org.apache.http.conn.EofSensorInputStream.close(EofSensorInputStream.java:186)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:87)
	at org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.releaseConnection(HttpMethodReleaseInputStream.java:102)
	at org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.close(HttpMethodReleaseInputStream.java:194)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.seek(NativeS3FileSystem.java:152)
	at org.apache.hadoop.fs.BufferedFSInputStream.seek(BufferedFSInputStream.java:89)
	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:63)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:102)
	at com.macrosense.mapreduce.io.PingRecordReader.initialize(PingRecordReader.java:80)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:478)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:671)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:330)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
{code}

Looks like this fix (in ContentLengthInputStream and/or EofSensorInputStream) was added to Apache HTTP Compoents and/or jets3t some time in the past few months, closing as Cannot Reproduce, as it appears to have gone away for you.

# Hadoop 2.6 is using a much later version of jets3t
# Hadoop 2.6 also offers a (compatible) s3a fiesystem which uses the AWS SDK instead. 

If you do see this problem, try using s3a to see if it occurs there, I confirmed this happens on Hadoop 2.6.0, and found the reason.

Here's the stacktrace.

{quote}

2015-03-13 20:17:24,866 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream threw an exception
org.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 296587138; received: 155648
	at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184)
	at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138)
	at org.jets3t.service.io.InterruptableInputStream.read(InterruptableInputStream.java:78)
	at org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.read(HttpMethodReleaseInputStream.java:146)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.read(NativeS3FileSystem.java:145)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:273)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)
	at org.apache.pig.builtin.PigStorage.getNext(PigStorage.java:259)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:204)
	at org.apache.tez.mapreduce.lib.MRReaderMapReduce.next(MRReaderMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POSimpleTezLoad.getNextTuple(POSimpleTezLoad.java:106)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:246)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNextTuple(POFilter.java:91)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.getNextTuple(POStoreTez.java:117)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:313)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:192)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-13 20:17:24,867 [TezChild] INFO  org.apache.hadoop.fs.s3native.NativeS3FileSystem - Received IOException while reading 'user/hadoop/tsato/readlarge/input/cloudian-s3.log.20141119', attempting to reopen.
2015-03-13 20:17:24,867 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream is fully consumed
2015-03-13 20:17:24,868 [TezChild] INFO  org.apache.tez.dag.app.TaskAttemptListenerImpTezDag - Commit go/no-go request from attempt_1426245338920_0001_1_00_000004_0
2015-03-13 20:17:24,868 [TezChild] INFO  org.apache.tez.dag.app.dag.impl.TaskImpl - attempt_1426245338920_0001_1_00_000004_0 given a go for committing the task output.

{quote}

The problem is that a job successfully finishes after the exception. Thus, its output gets truncated.

The cause is in the retry logic:

{code:title=NativeS3FileSystem#read(byte[] b, int off, int len)|borderStyle=solid}
      int result = -1;
      try {
        result = in.read(b, off, len);
      } catch (EOFException eof) {
        throw eof;
      } catch (IOException e) {
        LOG.info( "Received IOException while reading '{}'," +
                  " attempting to reopen.", key);
        seek(pos);
        result = in.read(b, off, len);
      }
{code}

After catching the IOException, it attempts to read again from the same inputstream. _NativeS3FileSystem#seek()_ does nothing because the position is not changed. Then, the successive _NativeS3FileSystem#read()_ returns -1 instead of throwing another exception to abort the job. It is done by *EofSensorInputStream* because _EofSensorInputStream#checkAbort()_ makes EofSensorInputStream return false from _EofSensorInputStream#isReadAllowed()_.. 

{code:title=EofSensorInputStream#read(byte[] b, int off, int len)|borderStyle=solid}
    public int read() throws IOException {
        int l = -1;

        if (isReadAllowed()) {
            try {
                l = wrappedStream.read();
                checkEOF(l);
            } catch (IOException ex) {
                checkAbort();
                throw ex;
            }
        }

        return l;
    }
{code}

I didn't reach to the history why such a retry is attempted. Even If it does so, I think the inner inputstream should be carefully updated.

If that's not obvious today, I think making it simply fail without retry is reasonable., Searching for the string {{Premature end of Content-Length delimited message body}} brings up [a stack overflow post|
http://stackoverflow.com/questions/9952815/s3-java-client-fails-a-lot-with-premature-end-of-content-length-delimited-mess] blaming the exception message on a GC of the s3 connection client.

Looking at the handler code, it was meant to fix the operation by-reopening the connection. But an optimisation in Hadoop 2.4 (also needed to fix another problem), turned seek(getPos()) to a no-op. Some other way of explicitly re-opening the connection is going to be needed.

For now, try using s3a:// as the URL to the data. It has different issues in Hadoop 2.6, but by Hadoop 2.7 should be ready to replace s3n completely, I think what Takenori Sato is describing is a separate issue than the one I originally reported.  The issue I reported was solved long ago by the "ConnectionClosedException: Premature end of Content-Length ..." exception.  Prior to that fix no exception was thrown if the socket didn't successfully read all the data - it would just return the incomplete data., David, thanks for your clarification.

I heard from Steve that my issue was introduced by some optimizations done for 2.4.

So let me close this as FIXED. I will create a new issue for mine., The issue that had reopened this turned out being a separate issue., Closing old tickets that are already part of a release.]