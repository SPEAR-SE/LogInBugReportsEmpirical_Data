[These 3 files show the different behavior between:
 - 20.AIX.Errors                              : AIX (IBM JVM)
 - 20.Ubuntu-i386.IBMJVM.Errors  : Ubuntu / i386 / IBM JVM
 - 22.OpenJDK.Errors                     : Ubuntu / i386 / OpenJDK, Class ./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoOutputStream.java :

public void write(byte[] b, int off, int len) throws IOException {
..........
        encrypt();
      }
    }
  }

private void encrypt() throws IOException {
 .......

    if (encryptor.isContextReset()) {
      /*
       * This code is generally not executed since the encryptor usually
       * maintains encryption context (e.g. the counter) internally. However,
       * some implementations can't maintain context so a re-init is necessary
       * after each encryption call.
       */
      updateEncryptor();
    }
  }

After adding many traces in the code, I've found that, on Ubuntu / i386 / OpenJDK, the updateEncryptor() method is never called from encrypt.
On AIX, I can see that encryptor.isContextReset() returns true and that updateEncryptor() is called.

When I comment this      updateEncryptor();  line and rerun tests on AIX, the number of error fall from :
     Failures: 5, Errors: 1,
to:
    Failures: 3, Errors: 1,

So, it looks to me that IBM JVM is a special case that is not correctly handled by Hadoop code., What does the test code and how the issue appears:

The code creates a OutputStream (a file) where it writes bytes.
Then, the test reads the file and must read an expected number of bytes.

File is:
  hadoop-common-project/hadoop-common/target/test/data/work-dir/localfs/test-file

The issue appears as:
@ readAll: 1 n= 8192 total= 3145728
@ readAll: N off+total= 3153920 len-total= 23586
@ readAll: 1 n= 8192 total= 3153920
@ readAll: N off+total= 3162112 len-total= 15394
@ readAll: 1 n= 8192 total= 3162112
@ readAll: N off+total= 3170304 len-total= 7202
@ readAll: 1 n= 7200 total= 3170304
@ readAll: N off+total= 3177504 len-total= 2
@ readAll: 1 n= -1 total= 3177504
@ readAll: 2 n= -1 total= 3177504
@ readAll: 0 b[len-100] 57
@ readAll: 0 b[len-20] -64
@ readAll: 0 b[len-10] 63
@ readAll: 0 b[len-3] 35
@ readAll: 0 b[len-2] 0
@ readAll: 0 b[len-1] 0
@ readCheck: n= 3177504 dataLen= 3177506

In that case, TWO bytes are missing in the file. Sometimes, it is 10, or 11, or anything else.


Traces of CryptoOutputStream show:
@ setUp: 2 dataLen = 3177506
@ testCryptoIV: iv1.length= 16 iv.length= 16
@ testCryptoIV: iv1= [B@3ec722c2 iv= [B@c12308c4 Long.MAX_VALUE= 9223372036854775807
@ writeData: 0 out= org.apache.hadoop.crypto.CryptoOutputStream@fc8d9acd data.len= 5341184 dataLen= 3177506
@ writeData: 0 data[len-100]= 57
@ writeData: 0 data[len-20]= -64
@ writeData: 0 data[len-10]= 63
@ writeData: 0 data[len-5]= -69
@ writeData: 0 data[len-4]= -118
@ writeData: 0 data[len-3]= 35
@ writeData: 0 data[len-2]= 123
@ writeData: 0 data[len-1]= -46
@ CryptoOutputStream.write: 0 off= 0 len= 3177506
@ CryptoOutputStream.write: 1 len= 3177506 remaining= 8192
@ CryptoOutputStream.write: N off= 8192 len= 3169314
@ CryptoOutputStream.encrypt: 0 padding= 0 inBuffer.position()= 8192
@ CryptoOutputStream.encrypt: 1 len=outBuffer.remaining()= 8192
@ CryptoOutputStream.encrypt: 2 len= 8192 streamOffset= 8192
...
@ CryptoOutputStream.write: 1 len= 23586 remaining= 8192
@ CryptoOutputStream.write: N off= 3162112 len= 15394
@ CryptoOutputStream.encrypt: 0 padding= 0 inBuffer.position()= 8192
@ CryptoOutputStream.encrypt: 1 len=outBuffer.remaining()= 8192
@ CryptoOutputStream.encrypt: 2 len= 8192 streamOffset= 3162112
@ CryptoOutputStream.write: 1 len= 15394 remaining= 8192
@ CryptoOutputStream.write: N off= 3170304 len= 7202
@ CryptoOutputStream.encrypt: 0 padding= 0 inBuffer.position()= 8192
@ CryptoOutputStream.encrypt: 1 len=outBuffer.remaining()= 8192
@ CryptoOutputStream.encrypt: 2 len= 8192 streamOffset= 3170304

@ CryptoOutputStream.write: 1 len= 7202 remaining= 8192

@ CryptoOutputStream.write: END len= 0 remaining= 8192

@ CryptoOutputStream.encrypt: 3 before updateEncryptor() padding= 0
@ CryptoOutputStream.encrypt: 3  after updateEncryptor() padding= 0

@ getInputStream: 0 bufferSize= 8192 codec= org.apache.hadoop.crypto.JceAesCtrCryptoCodec@ed2e6f3a bufferSize= 8192


Tracing the size of the file from the Java code shows:
-rw-r--r--    1 reixt    system      3166208 Jun 19 13:05 /home/reixt/HADOOP-2.6.0/hadoop-IBMSOE-branch-2.6.0-power/hadoop-common-project/hadoop-common/target/test/data/
work-dir/localfs/test-file
-rw-r--r--    1 reixt    system      3166208 Jun 19 13:05 /home/reixt/HADOOP-2.6.0/hadoop-IBMSOE-branch-2.6.0-power/hadoop-common-project/hadoop-common/target/test/data/
work-dir/localfs/test-file
-rw-r--r--    1 reixt    system      3166208 Jun 19 13:05 /home/reixt/HADOOP-2.6.0/hadoop-IBMSOE-branch-2.6.0-power/hadoop-common-project/hadoop-common/target/test/data/
work-dir/localfs/test-file
-rw-r--r--    1 reixt    system      3166208 Jun 19 13:05 /home/reixt/HADOOP-2.6.0/hadoop-IBMSOE-branch-2.6.0-power/hadoop-common-project/hadoop-common/target/test/data/
work-dir/localfs/test-file
-rw-r--r--    1 reixt    system      3174400 Jun 19 13:05 /home/reixt/HADOOP-2.6.0/hadoop-IBMSOE-branch-2.6.0-power/hadoop-common-project/hadoop-common/target/test/data/
work-dir/localfs/test-file
-rw-r--r--    1 reixt    system      3177504 Jun 19 13:05 /home/reixt/HADOOP-2.6.0/hadoop-IBMSOE-branch-2.6.0-power/hadoop-common-project/hadoop-common/target/test/data/
work-dir/localfs/test-file

Final value is not the expected one: 3177506. Rather 3177504 with 2 missing bytes., I would recommend you to run these tests on Linux/x86_64 with IBM JVM 1.7 .
Take care that these tests are failing randomly on Linux/x86_64 with IBM JVM, though they fail constantly on AIX.

mvn test -Dtest=org.apache.hadoop.crypto.TestCryptoStreamsForLocalFS -l mvn.Test.TestCryptoStreamsForLocalFS.res -fn -X

You can get the IBM JVM from here:
           http://www.ibm.com/developerworks/java/jdk/aix/service.html, Since the issue deals more with IBM JVM than with AIX, I've changed the title of the defect.
It would be useful to test Hadoop with IBM JVM too, at least for these "crypto" tests., I think that the issue is there, when data is built:

./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java


  private void process(ByteBuffer inBuffer, ByteBuffer outBuffer)
        throws IOException {
...
 >> HERE >>>       int n = cipher.update(inBuffer, outBuffer);
System.out.println("@ JceAesCtrCryptoCodec.process: 0" + " n= " + n + " inputSize= " + inputSize );
        if (n < inputSize) {
          /**
           * Typically code will not get here. Cipher#update will consume all
           * input data and put result in outBuffer.
           * Cipher#doFinal will reset the crypto context.
           */
          contextReset = true;
System.out.println("@ JceAesCtrCryptoCodec.process: 0" + " Typically code will not get here" + " contextReset= " + contextReset);
          cipher.doFinal(inBuffer, outBuffer);
        }

On AIX, I get traces like:
@ CryptoOutputStream.write: 1 len= 5283 remaining= 8192
@ CryptoOutputStream.write: END len= 0 remaining= 8192
@ CryptoOutputStream.encrypt: 0 padding= 0 inBuffer.position()= 5283
@ CryptoOutputStream.encrypt: 01 encryptor.encrypt
@ JceAesCtrCryptoCodec.encrypt: 0
@ JceAesCtrCryptoCodec.process: 0 n= 5280 inputSize= 5283
@ JceAesCtrCryptoCodec.process: 0 Typically code will not get here contextReset= true

Though on Ubuntu/x86_64/OpenJDK, I get:
@ CryptoOutputStream.write: END len= 0 remaining= 8192
@ CryptoOutputStream.encrypt: 0 padding= 0 inBuffer.position()= 4149
@ CryptoOutputStream.encrypt: 01 encryptor.encrypt
@ JceAesCtrCryptoCodec.encrypt: 0
@ JceAesCtrCryptoCodec.process: 0 n= 4149 inputSize= 4149


At the end of  CryptoOutputStream.write() , the line:
              int n = cipher.update(inBuffer, outBuffer);
returns: n < inputSize on AIX/IBMJVM though with Linux/OpenJDK we have : n == inputSize .

It looks that cipher belongs to: javax.crypto.Cipher .



More traces :

System.out.println("@ JceAesCtrCryptoCodec.process: BEFORE cipher.update" + " inBuffer= " + inBuffer.toString() + " outBuffer= " + outBuffer.toString());
        int n = cipher.update(inBuffer, outBuffer);
System.out.println("@ JceAesCtrCryptoCodec.process: 0" + " n= " + n + " inputSize= " + inputSize );
System.out.println("@ JceAesCtrCryptoCodec.process: AFTER cipher.update" + " inBuffer= " + inBuffer.toString() + " outBuffer= " + outBuffer.toString());


AIX/IBM JVM:
@ JceAesCtrCryptoCodec.process: BEFORE cipher.update
        inBuffer  = java.nio.DirectByteBuffer[pos=0 lim=2634 cap=8192]
        outBuffer= java.nio.DirectByteBuffer[pos=0 lim=8192 cap =8192]
@ JceAesCtrCryptoCodec.process: 0 n= 2624 inputSize= 2634
@ JceAesCtrCryptoCodec.process: AFTER  cipher.update
        inBuffer  = java.nio.DirectByteBuffer[pos=2634 lim=2634 cap=8192]
        outBuffer= java.nio.DirectByteBuffer[pos=2624 lim=8192 cap=8192]

2624 instead of 2634 !!!


OpenJDK:
@ JceAesCtrCryptoCodec.process: BEFORE cipher.update
        inBuffer  = java.nio.DirectByteBuffer[pos=0 lim=7623 cap=8192]
        outBuffer= java.nio.DirectByteBuffer[pos=0 lim=8192 cap=8192]
@ JceAesCtrCryptoCodec.process: 0 n= 7623 inputSize= 7623
@ JceAesCtrCryptoCodec.process: AFTER  cipher.update
        inBuffer  = java.nio.DirectByteBuffer[pos=7623 lim=7623 cap=8192]
        outBuffer= java.nio.DirectByteBuffer[pos=7623 lim=8192 cap=8192]


Yes, cipher.update(inBuffer, outBuffer) does not work fine with IBM JVM.

However, I do not know the expected behavior of Cypher.update() .
Maybe not returning  n==inputSize  is a correct - but rare - behavior, and Hadoop code is not handling correctly this rare case since it has never been tested with IBM JVM ?, Adding some more traces arount doFinal() :

System.out.println("@ JceAesCtrCryptoCodec.process: 1" + " Typically code will not get here" + " contextReset= " + contextReset);
System.out.println("@ JceAesCtrCryptoCodec.process: BEFORE cipher.doFinal" + " inBuffer= " + inBuffer.toString() + " outBuffer= " +  outBuffer.toString());
          int m = cipher.doFinal(inBuffer, outBuffer);
System.out.println("@ JceAesCtrCryptoCodec.process: 0" + " m= " + m );
System.out.println("@ JceAesCtrCryptoCodec.process: AFTER cipher.doFinal" + " inBuffer= " + inBuffer.toString() + " outBuffer= " +  outBuffer.toString());


 I've got:

@ CryptoOutputStream.encrypt:
  inBuffer  = java.nio.DirectByteBuffer[pos=0 lim=3235 cap=8192]
  outBuffer= java.nio.DirectByteBuffer[pos=0 lim=8192 cap=8192]
@ JceAesCtrCryptoCodec.encrypt: 0
@ JceAesCtrCryptoCodec.process: BEFORE cipher.update
  inBuffer  = java.nio.DirectByteBuffer[pos=0 lim=3235 cap=8192]
  outBuffer= java.nio.DirectByteBuffer[pos=0 lim=8192 cap=8192]

@ JceAesCtrCryptoCodec.process: 0 n= 3232 inputSize= 3235

@ JceAesCtrCryptoCodec.process: AFTER  cipher.update
  inBuffer  = java.nio.DirectByteBuffer[pos=3235 lim=3235 cap=8192]
  outBuffer= java.nio.DirectByteBuffer[pos=3232 lim=8192 cap=8192]

@ JceAesCtrCryptoCodec.process: 1 Typically code will not get here
               contextReset= true
@ JceAesCtrCryptoCodec.process: BEFORE cipher.doFinal
  inBuffer  = java.nio.DirectByteBuffer[pos=3235 lim=3235 cap=8192]
  outBuffer= java.nio.DirectByteBuffer[pos=3232 lim=8192 cap=8192]

@ JceAesCtrCryptoCodec.process: 0 m= 0

@ JceAesCtrCryptoCodec.process: AFTER  cipher.doFinal
  inBuffer  = java.nio.DirectByteBuffer[pos=3235 lim=3235 cap=8192]
  outBuffer= java.nio.DirectByteBuffer[pos=3232 lim=8192 cap=8192]


doFinal() did NOTHING

Maybe because inBuffer : [pos=3235 == lim=3235 ?!?!!
  Nothing to do ?

Cipher.doFinal()) documentation says:
"All input.remaining() bytes starting at input.position() are processed.
 ... Upon return, the input buffer's position will be equal to its
limit; its limit will not have changed. The output buffer's position
will have advanced by n, where n is the value returned by this method;
the output buffer's limit will not have changed. ", Adding : inBuffer.position(n);  where n comes from last cipher.update()
did fix something:

System.out.println("@ JceAesCtrCryptoCodec.process: BEFORE cipher.doFinal" + " inBuffer= " + inBuffer.toString() + " outBuffer= " +  outBuffer.toString());
        inBuffer.position(n);
          int m = cipher.doFinal(inBuffer, outBuffer);
System.out.println("@ JceAesCtrCryptoCodec.process: 0" + " m= " + m );
System.out.println("@ JceAesCtrCryptoCodec.process: AFTER cipher.doFinal" + " inBuffer= " + inBuffer.toString() + " outBuffer= " +  outBuffer.toString());

Traces from Maven are:
Error: java.lang.AssertionError: expected:<3206307> but was:<3206304>
was moved in: java.lang.AssertionError: expected:<18> but was:<-1>

Traces from Java code are:
@ JceAesCtrCryptoCodec.process: 1 Typically code will not get here          contextReset= true
@ JceAesCtrCryptoCodec.process: BEFORE cipher.doFinal
  inBuffer  = java.nio.DirectByteBuffer[pos=6956 lim=6956 cap=8192]
  outBuffer= java.nio.DirectByteBuffer[pos=6944 lim=8192 cap=8192]
@ JceAesCtrCryptoCodec.process: 0 m= 24
@ JceAesCtrCryptoCodec.process: AFTER  cipher.doFinal
  inBuffer  = java.nio.DirectByteBuffer[pos=6956 lim=6956 cap=8192]
  outBuffer= java.nio.DirectByteBuffer[pos=6968 lim=8192 cap=8192]

and check is now OK :
@ readCheck: n= 3177254 dataLen= 3177254, I have opened a defect against IBM JVM. Their answer, for now, is that they see NO issue there within their code.
I need someone from Hadoop to help me about this complex issue., Component: I've said: "secutity". However, "encryption" would be better I think, but it is not proposed., Issue was in the IBM JVM. Some rare and subtle case that was fixed in OpenJDK and not in IBM JVM.]