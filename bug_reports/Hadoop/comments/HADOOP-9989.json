[Based on how JobSubmitter process the value of the mapreduce.job.credentials.json key, the tokenCacheFile passed in with the command line should be a JSON file, which makes sense as the GenericOptionsParser reads the value of this file and set it to the mapreduce.job.credentials.json in the configuration originally. Attached patch is based on the assumption that the tokenCacheFile is a JSON file., +1 I have the same issue, mrv1 in CDH also needs "mapreduce.job.credentials.json" be a json file, A polite -1.  We have use cases where fetchdt is used to create a token cache file that will be used in conjunction with -tokenCacheFile.  We might consider something like using the file suffix to determine if it's json.

Out of curiosity, what generates a json token cache file?, [~daryn] I think there is another way to fix this problem using "mapreduce.job.credentials.binary" instead of "mapreduce.job.credentials.json" for 
 -tokenCacheFile in the following code:
{code}
     UserGroupInformation.getCurrentUser().addCredentials(
          Credentials.readTokenStorageFile(p, conf));
      conf.set("mapreduce.job.credentials.json", p.toString(),
               "from -tokenCacheFile command line option");
{code}
Currently the -tokenCacheFile option is broken for both MapReduce MR2 and MR1:
The following code in JobSubmitter.java will parse the file from  -tokenCacheFile as JSON file not as binary file.
But Credentials.readTokenStorageFile(p, conf) will expect it as binary file not  JSON file.
{code}
    String tokensFileName = conf.get("mapreduce.job.credentials.json");
    if(tokensFileName != null) {
      LOG.info("loading user's secret keys from " + tokensFileName);
      String localFileName = new Path(tokensFileName).toUri().getPath();

      boolean json_error = false;
      try {
        // read JSON
        ObjectMapper mapper = new ObjectMapper();
        Map<String, String> nm = 
          mapper.readValue(new File(localFileName), Map.class);

        for(Map.Entry<String, String> ent: nm.entrySet()) {
          credentials.addSecretKey(new Text(ent.getKey()), ent.getValue()
              .getBytes(Charsets.UTF_8));
        }
      } catch (JsonMappingException e) {
        json_error = true;
      } catch (JsonParseException e) {
        json_error = true;
      }
      if(json_error)
        LOG.warn("couldn't parse Token Cache JSON file with user secret keys");
    }
  }
{code}
, [~daryn] Are you OK with the second solution, I uploaded a patch "HADOOP-9989.001.patch" for this

{code}
 UserGroupInformation.getCurrentUser().addCredentials(
          Credentials.readTokenStorageFile(p, conf));
      conf.set("mapreduce.job.credentials.binary", p.toString(),
               "from -tokenCacheFile command line option");
{code}

I think use binary file for -tokenCacheFile option is good, because most other hadoop token related command use binary file.
For example: hadoop fetchdt (hdfs fetchdt --renew).
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664793/HADOOP-9989.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.security.token.delegation.web.TestWebDelegationToken

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4560//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4560//console

This message is automatically generated., The test failure is not related to this change:

Running org.apache.hadoop.security.token.delegation.web.TestWebDelegationToken
Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.193 sec <<< FAILURE! - in org.apache.hadoop.security.token.delegation.web.TestWebDelegationToken
testDelegationTokenAuthenticationURLWithNoDTFilter(org.apache.hadoop.security.token.delegation.web.TestWebDelegationToken)  Time elapsed: 0.152 sec  <<< ERROR!
java.net.BindException: Address already in use

The test is passed in my local build.
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.security.token.delegation.web.TestWebDelegationToken
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.528 sec - in org.apache.hadoop.security.token.delegation.web.TestWebDelegationToken

Results :

Tests run: 9, Failures: 0, Errors: 0, Skipped: 0
, I'm ok with the current patch but I'm not sure if setting the conf is even necessary.  This is common code that contains a reference to a mapreduce conf key which I believe is a relic.  MR passes tokens in the UGI context during job submission which is why the prior line is reading the credentials from the cmdline option into the UGI.  If you've found other references outside of job submission, or feel it's too risky, the current patch is ok.

, Hi [~daryn], thanks for agreeing to my patch(HADOOP-9989.001.patch).
It will be safe to keep the mapreduce configuration which will be save in JobContextImpl.credentials by JobSubmitter.java.
tnanks
zhihai, After poking Zhiahi offline for a bit on this I think I understand the whole story here. My concern was that this would be an incompatible change, but it is safe to change from json to binary the property because MR submitter code handles both.

+1, Thanks Zhihai and Daryn for reviewing it. 

Committed to trunk and branch-2., FAILURE: Integrated in Hadoop-Yarn-trunk #676 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/676/])
HADOOP-9989. Bug introduced in HADOOP-9374, which parses the -tokenCacheFile as binary file but set it to the configuration as JSON file. (zxu via tucu) (tucu: rev b100949404843ed245ef4e118291f55b3fdc81b8)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1867 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1867/])
HADOOP-9989. Bug introduced in HADOOP-9374, which parses the -tokenCacheFile as binary file but set it to the configuration as JSON file. (zxu via tucu) (tucu: rev b100949404843ed245ef4e118291f55b3fdc81b8)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1892 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1892/])
HADOOP-9989. Bug introduced in HADOOP-9374, which parses the -tokenCacheFile as binary file but set it to the configuration as JSON file. (zxu via tucu) (tucu: rev b100949404843ed245ef4e118291f55b3fdc81b8)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java
, Thanks a lot [~tucu00]
Many thanks to [~daryn] for the review and comments., Hi [~tucu00], I just found out the "mapreduce.job.credentials.binary" configuration parameter need a path name without URI Scheme. 
{code}
    String binaryTokenFilename =
      conf.get("mapreduce.job.credentials.binary");
    if (binaryTokenFilename != null) {
      Credentials binary = Credentials.readTokenStorageFile(
          new Path("file:///" + binaryTokenFilename), conf);
      credentials.addAll(binary);
    }
{code}
the MR will add scheme "file:///" to the parameter,
I update a new patch HADOOP-9989.addendum0.patch to fix this issue:
use p.toUri().getPath() to remove the URI Scheme.
Please review it.
thanks
zhihai, Zhihai,

Please open a new JIRA to handle all URIs properly. Also, it would be great if we can test all possible URIs work: absolute and relative URIs, if the code is prepending "file://', we should convert the URIs to absolute (and the prepending should use 2 '/', not 3)., also, is possible to use non-local FS URIs?  if so, then the code does not handle that currently, right?, Hi [~Alejandro Abdelnur], That is a good question. It is not possible to use non-local FS URIs in "-tokenCacheFile" option parameter
The current code in GenericOptionsParser.java  will always suppose the "-tokenCacheFile" option parameter is local file, 
otherwise It will throw an exception at 
{code}
      FileSystem localFs = FileSystem.getLocal(conf);
      Path p = localFs.makeQualified(new Path(fileName));
      if (!localFs.exists(p)) {
          throw new FileNotFoundException("File "+fileName+" does not exist.");
      }
{code}.

You gave a good suggestion to change  "mapreduce.job.credentials.binary" configuration to support all path format absolute and relative URIs.
I will create a separate JIRAs for this and I will also test all possible path name.

And also if we want to support non-local FS URIs in "-tokenCacheFile" option parameter, we can change  
{code}
      if (!localFs.exists(p)) {
          throw new FileNotFoundException("File "+fileName+" does not exist.");
      }
{code}
to
{code}
      if (!p.getFileSystem(conf).exists(p)) {
          throw new FileNotFoundException("File "+fileName+" does not exist.");
      }
{code}
, I created a JIRA MAPREDUCE-6086 to handle all URIs properly in "mapreduce.job.credentials.binary" configuration, I uploaded a patch for [MAPREDUCE-6086 | https://issues.apache.org/jira/browse/MAPREDUCE-6086]. With [that patch | https://issues.apache.org/jira/secure/attachment/12668484/MAPREDUCE-6086.000.patch], we don't need this addendum patch (HADOOP-9989.addendum0.patch).]