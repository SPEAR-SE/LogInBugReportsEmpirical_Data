[Patch based on branch-1-win is attached. I believe this bug exists in branch-1 and trunk as well, but I haven't tested that., It is correct behavior for moving /foo to /bar to produce /foo/bar.  Mv mimics the unix shell., Daryn: please read the repro and result more carefully. mv /foo /bar doesn't produce /foo/bar, it produces /bar with wrong contents in there (/X/X instead of /X); at least on Windows in branch-1-win (again, haven't tried in other branches)., I'm about to post some patches that resolve this and also some other cases.  I'm pasting a comment I originally entered on HADOOP-9767 that explains the root cause:

POSIX rename semantics state that renaming a source directory to an existing destination directory that is empty will act as a full replacement of that existing destination. RawLocalFileSystem#rename calls Java's File#renameTo, which provides these semantics on most platforms. On some platforms (notably Windows), File#renameTo does not provide this behavior. This causes problems for MapReduce and other downstream components like Hive, HCatalog, and Sqoop that depend on the replacement behavior., I've attached patches for trunk, branch-1, and branch-1-win.  The branch-1-win patch is slightly different, because there had been a prior attempt to fix this issue on that branch, but it wasn't quite correct.

I've successfully tested these patches on Mac, Linux, and Windows., +1

Thanks for the detailed analysis, Chris!
I don't remember the subtle differences of File#renameTo() on Windows and Linux.
I verified the differences in renaming behavior, and agree with your fix., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594027/HADOOP-9507-trunk.1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1152 javac compiler warnings (more than the trunk's current 1151 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2838//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2838//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2838//console

This message is automatically generated., Thanks Chris - +1., I'm uploading version 2 of the trunk patch to fix the javac deprecation warning.  Here is the incremental diff:

{code}
diff --git hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java hadoop-common-projec
index 2ee6c70..6b45fe3 100644
--- hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java
+++ hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java
@@ -332,7 +332,7 @@ public boolean rename(Path src, Path dst) throws IOException {
     // fails.  Copy source content to destination and delete source.
     if (this.exists(dst)) {
       FileStatus sdst = this.getFileStatus(dst);
-      if (sdst.isDir() && dstFile.list().length == 0) {
+      if (sdst.isDirectory() && dstFile.list().length == 0) {
         if (LOG.isDebugEnabled()) {
           LOG.debug("Copying contents of " + src + " to " + dst);
         }
{code}

On branch-1, we don't have {{FileStatus#isDirectory}}, and {{FileStatus#isDir}} is not deprecated.  Therefore, we don't need to update the branch-1 and branch-1-win patches., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594069/HADOOP-9507-trunk.2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2841//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2841//console

This message is automatically generated., If we use copy, it may unnecessarily copy a large amount of data.  Why not first delete the empty dst directory and then rename the src directory?  Or moving the children from src to dst.  Not sure if these two methods would work in windows., Thanks, Nicholas.  I'm uploading version 3 of the patch for all branches to delete destination and then reattempt renaming source to destination.  This implementation still passes all of my tests, including on Windows.  [~chuanliu] and [~mostafae], do you think this technique looks OK for Windows?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594210/HADOOP-9507-trunk.3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.util.TestLightWeightCache

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2846//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2846//console

This message is automatically generated., {quote}
-1 core tests. The patch failed these unit tests in hadoop-common-project/hadoop-common:
org.apache.hadoop.util.TestLightWeightCache
{quote}

This test doesn't perform any file access, so the failure must be unrelated to this patch., +1

The new changes look good to me!, I don't know if this new approach would work if the destination is on a separate drive - can you check?, {quote}
I don't know if this new approach would work if the destination is on a separate drive - can you check?
{quote}

Thanks for catching this, Mostafa.  That's a good point.  No, {{java.io.File#renameTo}} fails when attempting to rename from one drive to another.  Likewise on Linux, you can't cross different file systems.

[~szetszwo], I think we need to go with the prior version of the patch that does copying.  Does that sound OK to commit?

After we fully migrate to Java 7, we can come back and do an optimization to check if source and destination are on the same file system, and then use delete/rename.  This can only be done with the new Java 7 file APIs, so we can't do it right now while we're still supporting Java 6.
, Also, just so this is clear, the patch will not cause copying when run on Linux, because on Linux, the first {{java.io.File#renameTo}} succeeds, and the method exits early.  This patch just provides fallback behavior for platforms where {{java.io.File#renameTo}} doesn't exactly match POSIX semantics., Chris and I discussed this offline. v3 of the patch is the better alternative. It is closest to the existing behavior on non-Windows platforms while avoiding the extra copy on Windows except for cross-volume renames.

Regarding the v3 trunk patch (9507-trunk.3.patch):

{code}
+        if (this.delete(dst, true) && srcFile.renameTo(dstFile)) {
{code}
The recurse flag to delete should be false since we only want to remove an empty directory.

I verified that the new tests pass with this patch. +1 with the above change., > ... I think we need to go with the prior version of the patch that does copying. ...

Does HADOOP-9507-trunk.3.patch work of separated drives?  The approach in it seems the best.  It first tries (1) rename, then (2) delete & rename for copying src dir to empty dst dir, and finally (3) copy.  If yes, +1 on the patch.  If it does not work for separated drives, I am fine to commit the prior version, +1 on that patch too.  Thanks for checking it.

I will take a look at TestLightWeightCache.  It definitely is not related to this., {quote}
It is closest to the existing behavior on non-Windows platforms
{quote}

Just to elaborate on this a bit, talking with Arpit made me realize that version 2 of the patch (copy-delete) would have accidentally introduced new behavior on Linux.  Currently on Linux, a rename crossing file systems would fail in {{File#renameTo}} and then fall through to {{FileUtil#copy}}.  In version 2, we would instead trigger the copy-delete logic, which would have a different result.

The goal of this issue was to provide a match to this method's existing behavior on platforms where {{File#renameTo}} behaves differently, like Windows.  Therefore, I'm now in favor of version 3 of the patch (delete-rename), not only for performance but also for correctness.

I'll provide new patches shortly to address Arpit's final comment.
, I'm uploading version 4 of the patch for all branches.  [~szetszwo], are you still +1 after this small change suggested by Arpit?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594289/HADOOP-9507-trunk.4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1152 javac compiler warnings (more than the trunk's current 1151 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2849//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2849//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2849//console

This message is automatically generated., Actually, version 5 is the correct patch for all branches.  I corrected a mistake in my call to non-recursive delete., Thanks for fixing the recursive delete!

+1 for v5 versions of trunk, branch-1 and branch-1-win patches. I verified all three patches on Windows and OSX, as applicable.

{quote}
version 2 of the patch (copy-delete) would have accidentally introduced new behavior on Linux. Currently on Linux, a rename crossing file systems would fail in File#renameTo and then fall through to FileUtil#copy. In version 2, we would instead trigger the copy-delete logic, which would have a different result.
{quote}
Good point, another reason for going with the delete+rename approach for the empty target directory., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594299/HADOOP-9507-trunk.5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2850//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2850//console

This message is automatically generated., Thank you, everyone, for the great code reviews on this patch.

[~szetszwo], are you still +1 for the patch with the change suggested by Arpit to use non-recursive delete?  If so, then I'd like to commit today.  It's a one-line change since last time (see incremental diff below).  Thanks!

{code}
diff --git hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java hadoop-common-projec
index 2553880..dbe29a2 100644
--- hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java
+++ hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java
@@ -338,7 +338,7 @@ public boolean rename(Path src, Path dst) throws IOException {
           LOG.debug("Deleting empty destination and renaming " + src + " to " +
             dst);
         }
-        if (this.delete(dst, true) && srcFile.renameTo(dstFile)) {
+        if (this.delete(dst, false) && srcFile.renameTo(dstFile)) {
           return true;
         }
       }
{code}
, Thanks [~cnauroth], +1 from my side., +1 the latest patch looks good.  Thanks., I've committed this to trunk, branch-2, branch-2.1-beta, branch-1, and branch-1-win.  Thank you to Chuan, Mostafa, Arpit, and Nicholas for the excellent code review feedback!]