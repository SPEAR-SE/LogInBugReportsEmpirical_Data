[Simple fix in 4979.1.patch., Vivek, the fix looks good.

Some minor comments on the test case:
- Can you please rename the test to something that reflects the test more descriptively ? One suggestion is testBlockingAcrossTaskTypes() or something similar.
- Also the number of reduce tasks should be 0 in the first job. Otherwise, the buggy code would still continue to return null in assignTasks, as the first job's reduces (or maps) would be looked at first and it would cause the cluster to block anyway., Attached patch (4979.2.patch) incorporates Hemanth's suggestions. , Patch looks good. Results of test-patch:

     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 Eclipse classpath. The patch retains Eclipse classpath integrity.
, This patch only removes CR-LF characters from the last patch, (ran dos2unix on it), I just committed this to trunk and branch 0.20. Thanks, Vivek !, Integrated in Hadoop-trunk #708 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/708/])
    . Fix capacity scheduler to block cluster for failed high RAM requirements across task types. Contributed by Vivek Ratan.
]