[I've seen the exact same behaviour during a sort500 run (1000 reduces) which held up atleast 30-30 reduces..., Mukund, do you have the corresponding datanode logs?, I saw the same behavior. The reduce tasks times out and gets killed when this happens., Ca it be related to HADOOP-1374?, Does anyone konw how could I reproduce this?

Or, if you have the datanode logs showing this problem, please upload them., Lohit and I are finally able to reproduce the problem.  We ran randomwriter on a 500-node cluster and got a lot of map tasks failure.  The problem is due to out of disk space in some datanodes.  When a dfs client writes a block to datanode pipeline, if there is a datanode in the pipeline ran out of disk space.  It throws
{noformat}
2008-06-13 01:01:58,909 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(xx.xx.xx.xx:52961, storageID=DS-yyyyyyy, infoPort=58576, ipcPort=50020):DataXceiver: 
org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: No space left on device
        at org.apache.hadoop.dfs.DataNode.checkDiskError(DataNode.java:590)
        at org.apache.hadoop.dfs.DataNode.access$1400(DataNode.java:82)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2611)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2664)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1267)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1027)
        at java.lang.Thread.run(Thread.java:619)
{noformat}
However, the datanode does not report the exception back to client and the client will time-out.  That's why we see SocketTimeoutException from the client., In the case of DiskOutOfSpaceException, datanode should send the upstream an error code.  Then, the client can possibly re-route the pipeline.

An out-of-space datanode probably should refuse pipeline connection in the beginning., Nicholas, Isn't the excepton trace different from the one reported?, Hi Devaraj, the trace in the description is TaskTracker log while the trace I posted is Datanode log., Oops, missed that!, I tried running randomwriter in a 500-node cluster with one low-disk-space node.  The low-disk-space node got filled up soon and 11 map tasks failed in the job.

Then, I killed the low-disk-space node and ran another randomwriter job.  There were no failing tasks at all., Since this is a Datanode configuration problem, I am closing this issue.

I will create a new issue for improving Datanode DiskOutOfSpaceException handling.]