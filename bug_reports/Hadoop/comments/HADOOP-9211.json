[Compared to past, the HADOOP_HEAPSIZE is only for services, going forward. The HADOOP_CLIENT_OPTS therefore applies to clients alone and is the config point for client-side heap.

If an easier, numeric setting is needed for clients, we can perhaps have HADOOP_CLIENT_HEAPSIZE, but HADOOP_HEAPSIZE should not generally apply to clients given the newer HADOOP_CLIENT_OPTS., Maybe this can be better documented inside hadoop-env.sh? Out of the box changing HADOOP_HEAPSIZE adds a -Xmx argument to the beginning of the bin/hadoop java command, but the hard-coded 128M heap size in HADOOP_CLIENT_OPTS overrides that option with a second -Xmx. As a new user this was very unintuitive, especially since most advice given around the internet for earlier versions will refer to adjusting HADOOP_HEAPSIZE. 

Also, at least in my case, 128M was not enough to run various examples from the hadoop examples jar, so the first thing many new users might encounter is trying to figure out how to increase heap size. Maybe 128M is a bad value given the default settings of other variables?, In distributed mode, with no local tasks, the 128M should suffice for simply launching the apps/shell/etc. I think. We can surely improve docs around this, and if you wish, we can make default 256m?, I do not know enough about hadoop to feel confident in making a recommendation on what the default max heap size should be. Also, I was running the examples in non-distributed mode. 256m does appear to be enough so that the pi and wordcount examples from share/hadoop/mapreduce/hadoop-mapreduce-examples-2.0.2-alpha.jar do not run out of memory in non-distributed mode., I've been using the following patch locally to work around a similar problem.

Java takes the last -xmx opt it can find as the actual value, so moving the arg to the end of the opts seemed to help., Using the default 128m for services will actually cause crashes in DataNodes. I linked the closed JIRA I raised as a reference.

Indeed the cause for my earlier was this default 128m hardcode. In order to workaround it I needed to add the heap size change into HADOOP_NAMENODE_OPTS manually.

Is it possible to disable the 128m default for now, or maybe bump up the default heap to 256m or 512m? 512m seems to not cause any DataNode crashes during my runs of Slive on a small 3 node cluster.

I will attach a simple patch for a bump request., +1. If there are no objections I'll commit this to avoid confusing OOMs.
Following formal Jenkins verification., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12577296/HADOOP-9211.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2445//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2445//console

This message is automatically generated., Integrated in Hadoop-trunk-Commit #3609 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3609/])
    HADOOP-9211. Set default max heap size in HADOOP_CLIENT_OPTS to 512m in order to avoid OOME. Contributed by Plamen Jeliazkov. (Revision 1467380)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1467380
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/conf/hadoop-env.sh
, I just committed this to trunk and branch-2. Thank you Plamen., Integrated in Hadoop-Yarn-trunk #182 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/182/])
    HADOOP-9211. Set default max heap size in HADOOP_CLIENT_OPTS to 512m in order to avoid OOME. Contributed by Plamen Jeliazkov. (Revision 1467380)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1467380
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/conf/hadoop-env.sh
, Integrated in Hadoop-Hdfs-trunk #1371 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1371/])
    HADOOP-9211. Set default max heap size in HADOOP_CLIENT_OPTS to 512m in order to avoid OOME. Contributed by Plamen Jeliazkov. (Revision 1467380)

     Result = FAILURE
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1467380
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/conf/hadoop-env.sh
, Integrated in Hadoop-Mapreduce-trunk #1398 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1398/])
    HADOOP-9211. Set default max heap size in HADOOP_CLIENT_OPTS to 512m in order to avoid OOME. Contributed by Plamen Jeliazkov. (Revision 1467380)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1467380
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/conf/hadoop-env.sh
, I think the problem comes on line 17 of hadoop-env.sh

# Extra Java runtime options. Empty by default.
export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true $HADOOP_CLIENT_OPTS"

this is assigning teh 128m or any other hadoop heap size to early to HADOOP_OPTS and places the -Xmx after the HADOOP_HEAPSIZE var is set so overriding that.  the HADOOP_CLIENT_OPTS should not be added to HADOOP_OPTS until it is determined to not be one of the nn/snn/jt/dn/tt servers and actually be a client.]