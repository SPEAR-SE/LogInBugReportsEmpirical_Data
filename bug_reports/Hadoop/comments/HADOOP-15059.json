[CC [~andrew.wang], [~vinodkv], [~jlowe]., Move to Hadoop as the fix could be hadoop/YARN/MR., HADOOP-13123 looks related., Atsv2 officially claims HBase-1.2.6 as backend. It works _absolutely fine_ in non-secure mode i.e installing *Hadoop-3.0 + HBase-1.2.6*. 
But the same deployment in secured cluster does not work because HBase-1.2.6 does not communicate to Hadoop-3.x because of token proto mismatch. Basically HMaster daemon start fails with exception while it is connecting into Hadoop-3.x in secure cluster!

To simplify the problem, Hadoop-2.x clients(HBase-1.2.6 compiled against Hadoop-2.x) doesn't communicate with Hadoop-3.x cluster. Are we going to keep binary compatibility across hadoop-2.x and hadoop-3.x? Similar scenario can happen while rolling upgrade as well which reported in this JIRA. 

Btw, from ATSv2 we are planning to add this in documentation as known issue until hbase release 2.x. cc:/[~vrushalic] [~varun_saxena], bq. Are we going to keep binary compatibility across hadoop-2.x and hadoop-3.x?

Wire compatibility between 2.x clients and 3.x servers is a prerequisite to supporting a rolling upgrade from 2.x to 3.x, but I do not think everyone realizes wire compatibility between a 3.x client and a 2.x server is also very important to many of our users.  There are many cases where more than one cluster is involved in a workflow.  Requiring that all clusters upgrade from 2.x to 3.x simultaneously is a huge hurdle for adoption, and most users will upgrade them one at a time.  As individual clusters upgrade there will be clients/jobs on a newly upgraded 3.x cluster trying to interact with an older 2.x cluster.

Back to the issue of launching jobs using an incompatible token format -- here's a couple of options we could consider:

1) YARN nodemanager writes out *two* token credential files, the original 2.x file for backwards compatibility and a new 3.x file.  The 3.x UGI code looks for the new file and falls back to the old one if it cannot find it.  The 2.x code will simply load the old format from the original filename as it does today.

2) Application submission context contains information on which version of credentials to use for an application.  This gets transferred to the container launch context for each container, and the nodemanager writes out the appropriate credentials version based on what was specified in the container launch context.  In other words, the nodemanager knows which version of the credentials format the container is expecting to find and writes the token file in that format.
, Update : Fortunately I couldn't reproduce the issue which I reported in earlier comment. I am able to install Hadoop-3.0-RC0 + HBase-1.2.6 in secure mode and run successfully today. I am not sure that any issues post Hadoop-alpha-2 has fixed this issue. IIRC, the build which I used to test this combination is Hadoop-3.0-alpha2/3 + HBase-1.2.4/5!  Anyway its good news for ATSv2 folks which we were worried about this. 

I will be keep trying to reproduce this weekend as well. If there any issues found, I will be updating here. Till that time, please ignore that issue. I would appreciate if someone else can also validate the behavior. This gives additional confidence that wire compatibility across Hadoop-2 and Hadoop-3 is achieved! , Yeah, I'm pretty sure with HDFS-11096, we were launching jobs entirely from a 2.8.0 machine and not the reverse (i.e. not testing the 3.0 client with the 2.9 MR jars).  That's my error for not catching that.

I had to read up a bit on delegation tokens before I could understand how things worked.  I'm +1 on [~jlowe]'s first suggestion (writing out two credential files).  That seems like the simplest workaround.  I don't think it will be that much of a burden and the file should be removed during container cleanup., Thanks for the feedback, Ray.  I'll take a crack at implementing the two token file approach., Attaching a patch that has the container launch process write two token files, the legacy token format in the existing container_tokens file and the new version 1 format in a new container_tokens-v1 file.  I left the container localizer path alone since localizers are running the same code as the nodemanager and therefore can directly support the new v1 format as-is.

The basic idea is to tack on the "-v1" suffix to the legacy token pathname to form the new v1 token pathname.  The container launcher and container executors both do this, so the interface between them did not have to change.  The legacy path is passed between them to indicate where both token files can be located (once the suffix is applied to form the new token path).  It's definitely not the cleanest, but it was relatively simple to implement.  I refactored some names in the container start context to make it more clear which path is being used.

This needs a lot more testing, but I was able to run a sleep job on a simple security pseudo-distributed cluster and manually verified both container token files were being written and each was the proper format.  I also manually forced the launcher to omit the new environment variable for the version 1 file, forcing the UGI to load the legacy token file, and that worked as well.  I have not had a chance yet to test the rolling-upgrade-with-tarball scenario nor the native container-executor changes, but I thought it was far enough along to at least get some feedback.

If others could take a look at the patch and/or take it for a test drive that would be great.
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 10s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m  3s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 47s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 14s{color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 43s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 11s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} cc {color} | {color:red} 12m 11s{color} | {color:red} root generated 7 new + 7 unchanged - 0 fixed = 14 total (was 7) {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 11s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  1s{color} | {color:orange} root: The patch generated 4 new + 364 unchanged - 7 fixed = 368 total (was 371) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  8m 54s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 42s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 59s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 38s{color} | {color:green} hadoop-yarn-api in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 16m 22s{color} | {color:red} hadoop-yarn-server-nodemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 29s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}111m 41s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15059 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12899517/HADOOP-15059.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  cc  |
| uname | Linux 460244f5d4bb 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d8923cd |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/13754/artifact/out/branch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-api-warnings.html |
| cc | https://builds.apache.org/job/PreCommit-HADOOP-Build/13754/artifact/out/diff-compile-cc-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/13754/artifact/out/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/13754/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/13754/testReport/ |
| Max. process+thread count | 1356 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13754/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Heh, clearly the container-executor changes were broken.  Attaching a new patch that fixes those along with changes to address the checkstyle issues.  The findbug issue is pre-existing and unrelated, and the unit test failure also appears to be unrelated.  It passes for me locally with the patch applied.

I manually tested the container-executor changes on a single-node cluster configured to run with LinuxContainerExecutor., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  9m 38s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 30s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 52s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 16s{color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 50s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 11m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 56s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  5s{color} | {color:orange} root: The patch generated 1 new + 364 unchanged - 7 fixed = 365 total (was 371) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 56s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 49s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 57s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 42s{color} | {color:green} hadoop-yarn-api in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 16m 37s{color} | {color:green} hadoop-yarn-server-nodemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 34s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}123m 41s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15059 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12899641/HADOOP-15059.002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  cc  |
| uname | Linux d2dc47a813ed 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 641ba5c |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/13755/artifact/out/branch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-api-warnings.html |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/13755/artifact/out/diff-checkstyle-root.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/13755/testReport/ |
| Max. process+thread count | 1466 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13755/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, One last tweak to get rid of the last checkstyle warning, although it makes the new line stick out like a sore thumb compared to the others.  I'd be happy to put the warnings back in just for the sake of code consistency if someone cares.

Filed YARN-7576 for the existing findbugs warning in trunk.
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  9m 35s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 41s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 26s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 51s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 17s{color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 48s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} cc {color} | {color:green} 12m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  7s{color} | {color:green} root: The patch generated 0 new + 363 unchanged - 7 fixed = 363 total (was 370) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 48s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m  6s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 50s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  7m 59s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 44s{color} | {color:green} hadoop-yarn-api in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 17m  5s{color} | {color:green} hadoop-yarn-server-nodemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 35s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}124m 13s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestRaceWhenRelogin |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15059 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12899666/HADOOP-15059.003.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  cc  |
| uname | Linux 3c060d38c499 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 30941d9 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/13756/artifact/out/branch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-api-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/13756/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/13756/testReport/ |
| Max. process+thread count | 1487 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13756/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, *Long pondering:* The credentials format is not something that should have been trifled without careful compatibility considerations.  A major compatibility break like this should have been proceeded with a bridge release – ie. proactively add v0/v1 support to popular 2.x releases but continue to use the v0 format.  Flip the 3.x default format to v1 and everything works.  But it's too late.

I'm concerned the compatibility issues may not be confined to just yarn.  Perhaps a bit contrived but I've seen enough absurd UGI/token handling that little ceases to amaze me anymore.
# 2.x/3.x code rewrites the creds, only updates the v0-file creds because that's all that existed since the dawn of security, leading to:
# If 2.x rewrites creds, 2.x subprocess works.
# If 2.x or 3.x rewrites creds, 3.x subprocess is oblivious to intended changes – it found the unchanged v1-file.
# If 3.x rewrites creds, 2.x subprocess blows up because the v0-file was incidentally changed to the v1 format.

For the sake of discussion: the ugi methods to read/write the creds would be the better place to transparently handle the dual-version files instead of all the yarn changes.  It won't avoid the aforementioned pitfalls, and it doesn't fix the stream methods.  Not to mention we are now stuck supporting two files.  If the format changes again, do we have to support 3 files?  The whole point of writing the version into the file is to support multiple versions.
––
*Short answer:*  I think 3.0 has to be the bridge release and continue writing the v0 format but supporting reading v1 format.  3.1 or 3.2 can flip the default format to v1.  Changing the file format appears to be cosmetic so I don't see any harm in waiting for v1 to be the default., Thanks for taking a look, Daryn!

bq. Not to mention we are now stuck supporting two files. If the format changes again, do we have to support 3 files?

We would only support each file location as long as that token format is supported.  Just like the alternative "bridge releases" approach, we would eventually remove support for older token versions (and thus their locations).  We would not be forced to support umpteen files unless we wanted to support umpteen versions of the token format.  The advantage of what I'm calling the "bridge releases" approach is that there's only one place where tokens are written, no ugly fallback code, etc., but a significant disadvantage is that it forces two things:
# a strict upgrade path between releases (i.e.: users must fully update across all applications to the bridge release before upgrading further).
# the new token format must be introduced long before it can actually be used

We should not underestimate the burden of a bridge release especially if there's only one magical release.  It's not enough to qualify the software works on the bridge release, one must verify via other means that every part of the application is using the new bridge release jars.  If any part of the application bundles its own, older Hadoop jars the app will still work fine on the bridge release but will fail when the cluster upgrades.  Therefore it's difficult for admins and users to know for sure they're ready to move beyond the bridge release safely because knowing the app runs on the bridge release isn't sufficient.

The multi-location approach is not pretty at all, but it is much more flexible in upgrade paths and ability for newer software to leverage the new format once it's introduced.

bq. The whole point of writing the version into the file is to support multiple versions.

Yes, and writing that version does enable supporting multiple versions, _but only for software that understands what those version numbers mean_.  Adding a version number enables newer software to read an older file format, but old software will not be able to read a newer file format.  I only see two options for supporting older software consuming tokens created by newer software:
# write the new version to a different location and have new software prefer the other location with fallback
# or force everyone to use the old format until nobody until the old software is no longer supported and then we switch to the new format in the existing files.

I see the theoretical problem with a 3.x create->2.x rewrite->3.x read scenario, although I suspect it would not happen in practice.  If it does the fix is to force the 2.x code to upgrade to 3.x, and that's essentially what we're requiring with a bridge release.

I'm completely OK with going with the "bridge release" approach for this if we decide it is the right thing to do here.  It could make more sense if indeed the new token format isn't providing any additional features being leveraged today that cannot be expressed in the old token format, i.e.: no use-cases are broken by forcing the old format even in 3.0.  There would be no pressing need to move to the new format, so hopefully this would continue for many 3.x releases.  That could increase the likelihood that everyone has naturally updated to some version that has v1 support before we start forcing v1 support on everyone.
, bq. Yes, and writing that version does enable supporting multiple versions, but only for software that understands what those version numbers mean.

... and can actually read the file. This makes it more than a cosmetic change.  It's a pretty big deal if one is processing token files with anything other than Java or both endians. Theoretically, switching from Java serialization to protobuf should greatly lessen the impact if additions to the token file are needed. The file format is now something easily portable across a wide variety of scenarios.

It's probably worth pointing out how well this change goes hand-in-hand with things such as slider/yarn-native-services.  It should enable jobs to directly interact with the system without needing a 'helper' running next to it.

Let's be clear: this is only a problem if one has a bundled hadoop-common.jar.  Is the end result of this JIRA going to be that all file formats are locked forever, regardless of where they come from? This seems like a very dark precedent for the project to take if it ever wants to move forward.  [It's looking more and more like Hadoop will be required to bite the bullet on log4j2, likely sooner rather than later.]

Also, Hadoop releases have broken rolling upgrade (and non-rolling upgrades, for that matter) in the middle of the 2.x stream before by removing things such as container execution types. Where is the actual line? It's clear the compatibility guidelines aren't really followed/enforced., Thanks for joining the conversation, Allen, and for pointing out the motivations behind the protobuf change.  Do you know of existing use cases that are relying on the new format?

I completely agree the new format is a great path forward for extensibility and portability, but unfortunately it breaks a number of existing use cases.

bq. Let's be clear: this is only a problem if one has a bundled hadoop-common.jar.

It's also important to point out that this is a rather common occurrence.  Besides the typical habit of users running their *-with-dependencies.jar on the cluster, anyone leveraging the framework-on-HDFS approach will be bitten by this as soon as the nodemanager upgrades.  

Having frameworks deploy via HDFS rather than picking them up from the nodemanager's jars has proven to be a very useful way to better isolate apps during cluster rolling upgrades and support multiple versions of the framework on the cluster simultaneously.

bq. Is the end result of this JIRA going to be that all file formats are locked forever, regardless of where they come from?

I don't think so.  As discussed above, we should be able to remove support for the Writable format when Hadoop no longer supports 2.x apps.  Yes, that's likely quite a long time, but it does not have to be forever.

bq. Hadoop releases have broken rolling upgrade (and non-rolling upgrades, for that matter) in the middle of the 2.x stream before by removing things such as container execution types.

We've completed rolling upgrades across all of our clusters for every minor release of 2.x since rolling upgrades were first supported in 2.6, so we must not have hit this landmine.  Was this the removal of the dedicated Docker container executor in favor of the unified Linux executor that does everything?

I'm attaching a patch that implements the "bridge release(s)" approach where the code supports reading the new format but will write the old format by default.  Code can still request the new format explicitly if necessary.  The main drawback is that we don't get to easily leverage the benefits of the new format since it's not the default format.  However I'm hoping native services and other things that need the new protobuf format can leverage dtutil to translate the credentials format for easier consumption., [~daryn]'s suggestion for 3.0->3.1 bridging isn't a perfect solution, but given the current status (HADOOP-12563 already checked in, this current issue blocking 3.0.0 GA, not wanting to hold up 3.0.0 for any sort of major redesign), I don't see a better compromise than to deviate from the compatibility guidelines for this issue.

Had I caught the issue much earlier during all the protobuf/JACC issue reviews, there could have been several possibilities for more rolling upgrade friendly changes to the DT file., bq. Do you know of existing use cases that are relying on the new format?

Yes, but I can't talk about it without violating NDA.  The NM writing both files would probably be an acceptable middle ground, but I'd need to confer. 

bq. However I'm hoping native services and other things that need the new protobuf format can leverage dtutil to translate the credentials format for easier consumption.

We should verify dtutil can actually do a translation without talking to a service:

* old format -> new format
* no krb5 creds required

I suspect append will do it though.  A specific 'translate' feature might be easier on the user.
, bq. We should verify dtutil can actually do a translation without talking to a service:

I verified that I could fetch an HDFS delegation token from one of our secure clusters via {{hdfs fetchdt}}, copy the old format token file to a machine that's not part of the Kerberos realm (and not kinit'd at all), and get {{dtutil append -format protobuf}} to translate the file.

bq.  A specific 'translate' feature might be easier on the user.

Is that something I should do as part of this JIRA, or is the latest patch approach not going to work for other reasons?
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 10m 39s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 42s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 18m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 27m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  4m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  6m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 20m 21s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 51s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 37s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  2s{color} | {color:orange} root: The patch generated 1 new + 20 unchanged - 4 fixed = 21 total (was 24) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 58s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 50s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m  8s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 91m 10s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 36s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}218m 47s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |
|   | hadoop.hdfs.server.namenode.TestReencryptionWithKMS |
|   | hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |
|   | hadoop.fs.TestUnbuffer |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15059 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12899852/HADOOP-15059.004.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux b102d346e302 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 3016418 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/13760/artifact/out/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/13760/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/13760/testReport/ |
| Max. process+thread count | 3756 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13760/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, I'm +1 on Jason's patch – with the minor suggestion of the private statics for version can be replaced with the enums.  Anyone object?

I think my statements earlier may have been misconstrued.  I'm not proposing a "dark precedent" unless it's going to be marvel vs a skynet powered by hadoop movie.  All kidding aside...

I'm perfectly fine with the format -being- becoming a PB.  Allow existing large scale deployments a rolling upgrade path or 3.x is DOA.  I can't tell my customers "I'm sorry, we're taking a hard downtime because <insert-charlie-brown-sound> changed.  There's no value add for you".  Let's gracefully phase it in.

If some limited use cases absolutely positively need it, beyond what the cmdline can do, then either the impetus is on them, or let's find a way to force the new output format before it becomes the default.  Those that don't care about incompatibility can accept the risk, while allowing the rest of the community a stable transition.
, Tx for taking this up [~jlowe]!

bq. I'm attaching a patch that implements the "bridge release(s)" approach where the code supports reading the new format but will write the old format by default.
+1 for the bridging approach.

bq. The main drawback is that we don't get to easily leverage the benefits of the new format since it's not the default format.
I realized it's just not about changing the default format. ContainerLaunchContext.tokens in YARN is unfortunately a byte-buffer. Taking a protobuf, wrapping it into a byte-buffer and sending it to the RM is backwards to me. The right way to use this in YARN is to assume that the existing tokens field is old-style credentials and then add a new first-class protobuf based Credentials field.

The patch looks mostly good to me.

bq. minor suggestion of the private statics for version can be replaced with the enums
If we are no longer going to bump up this version in the protobuf world, +1 - there will be only two of these ever. IAC, these are private.

One other minor suggestion if you are doing the above. Credentials.SerializedFormat can be static., bq.  I can't tell my customers "I'm sorry, we're taking a hard downtime because <insert-charlie-brown-sound> changed. There's no value add for you". 

If only YARN had a system whereby one could dynamically label a node based upon the current software stack.  Then one could schedule around that and/or set up distributed cache content to match.  Very similar to how PBS-based such as torque have had for 20+ years ..., bq. ContainerLaunchContext.tokens in YARN is unfortunately a byte-buffer.  Taking a protobuf, wrapping it into a byte-buffer and sending it to the RM is backwards to me.

I'm not sure I understand.  All the code parsing that buffer wraps a stream about the byte buffer and invokes {{Credentials#readTokenStorageStream}}.  The credentials is still more than a simple PB.  It has a header of magic bytes and format version so encoding this into a byte buffer doesn't seem wrong.

bq. The patch looks mostly good to me.

What would remove the word mostly from this sentence? :)
, Thanks for the reviews, Daryn and Vinod!  I'm attaching a patch that moves the byte value to enum mapping into the enum itself.  I'm not convinced the refactor was worth it, but here it is for you to decide.

bq. If only YARN had a system whereby one could dynamically label a node based upon the current software stack. Then one could schedule around that and/or set up distributed cache content to match.

The RM tracks which version each NM registered with, so it knows the software stack being provided by that node.  Unfortunately YARN doesn't know what software stack the user's application expects, so it cannot fixup the distributed cache to match the expectation.  In addition the framework could be embedded in the user's app which cannot be fixed in the general case by tweaking the distributed cache., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 13s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 34s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 50s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 31s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  0s{color} | {color:orange} root: The patch generated 2 new + 20 unchanged - 4 fixed = 22 total (was 24) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m  6s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 59s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 47s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 19s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 89m 27s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 34s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}182m 33s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.security.TestRaceWhenRelogin |
|   | hadoop.net.TestDNS |
|   | hadoop.fs.TestUnbuffer |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15059 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12900034/HADOOP-15059.005.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux b7d7efdfc04f 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 75a3ab8 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/13764/artifact/out/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/13764/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/13764/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/13764/testReport/ |
| Max. process+thread count | 3956 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13764/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Vinod, Allen, do you have any further comments on this patch? Our other blockers look like they'll close this week, and I'd like to cut RC1., I regret requesting the removal of the private statics...  All I got in return was more code and catching and rethrowing of runtime exceptions. :/  I'm begrudgingly fine with the current or prior patch since this really needs to go in., The patch looks good to me.

[~daryn], don't get scared by exception handling!

Uploading a new patch that is same as that of Jason's but with SerializedFormat made static.

The test case failures reported here are unrelated. Though it is surprising how badly the unit tests are broken - I'll debug them independently and file bugs.

If Jenkins says okay, I'll commit this unless [~daryn] / [~jlowe] say no., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 29s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 44s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 51s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 43s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 12s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  1s{color} | {color:orange} root: The patch generated 3 new + 20 unchanged - 4 fixed = 23 total (was 24) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  3s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 49s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 37s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 83m 58s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 35s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}180m 14s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |
|   | hadoop.fs.TestUnbuffer |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15059 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12900897/HADOOP-15059.006.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 23335f39401a 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 40b0045e |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/13793/artifact/out/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/13793/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/13793/testReport/ |
| Max. process+thread count | 4080 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13793/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Sorry for the delay in replying, been busy/traveling.

Inner enums are already static, so declaring them static is redundant.  I see checkstyle reported the same.

I also haven't manually tested the rolling upgrade case Junping originall reported with this latest patch.  It _should_ solve the problem, but in the interest of full disclosure I have not had time to test that scenario.
, That makes sense. Then I can commit the previous patch. [~djp], can you give 005 patch a spin? , I pushed the 005 patch through the build system I have access to.  It's the same as the one that Vijaya Krishna Kalluru Subbarao did for the 3.0.0-beta1 RC0 vote.  It's run through the build and tests cleanly (minus some flaky tests).

I'll see about running the tests tonight., bq. Daryn Sharp, don't get scared by exception handling!

I'm just grumbly about the chain of catching  ArrayIndexOutOfBoundsException, rethrowing as IllegalArgumentException, catching that, rethrowing as IOException.  In the end, it doesn't really matter though.

I'm still +1.  Full speed ahead., Hmmm...I can get the MR test to pass on my patched cluster, but I can't get it to fail on a cluster without this fix.  I'm obviously doing something incorrectly.  [~djp], can you let me know what you did?, Never mind.  I found the cut-and-paste error in the config file name.

Confirmed that I can duplicate the error that [~djp] saw on a cluster without patch 005 and the error is no longer there on the 005 patched cluster.

+1 (binding) from me., I just committed 005 patch to trunk, branch-3.0 and branch-3.0.0. Thanks [~jlowe] for the patch and the quick turn-around!

Thanks [~djp] for finding the issue, [~rchiang] for verifying the fix and [~daryn] for the reviews., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13348 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13348/])
HADOOP-15059. Undoing the switch of Credentials to PB format as default (vinodkv: rev f19638333b11da6dcab9a964e73a49947b8390fd)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java
* (edit) hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestDtUtilShell.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/DtFileOperations.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Credentials.java
, Thanks everyone for the great work on this issue!]