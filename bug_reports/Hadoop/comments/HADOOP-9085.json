[1.when stop namenode is success,we should be delete the namenode pid file.
2.when we start namenode,we should be ps,not use kill -0.

code in hadoop-daemo.sh

if [ -f $pid ]; then
      if kill -0 `cat $pid` > /dev/null 2>&1; then
        echo $command running as process `cat $pid`.  Stop it first.
        exit 1
      fi
    fi

we will change it like this:
    if [ -f $pid ]; then
      tmppid=`cat $pid`
      curpid=`ps -ww -eo pid,user,euid,cmd | grep "org.apache.hadoop.hdfs." | grep "$command" | grep $tmppid | grep -v "grep" | awk '{print $1}'`
      if [ -n "$curpid" ]; then
        echo $command running as process `cat $pid`.  Stop it first.
        exit 1
      fi
    fi
, Pid recycling is a permanent problem with Unix systems -you are correct that something needs to be done. We can't rely on deleting the pid file on a successful shutdown either, as all forms of killing are "successful" -even server reboot.

I don't think the proposed patch would work as it's still looking for a file {{$pid}}, even though it's no longer needed, and that file is also used in the error text. Better to skip the -f check and use {{$curpid}} in the error. Even after tha, it's pretty brittle against unintentional command matches.

What we need to do is move away from pid-file-liveness tests altogether.

There is a far more robust alternative, the service started up should create an exclusive write lock on a well-known file. When the process dies, the OS automatically releases this lock. I'll open a JIRA on it.
, Link to [HADOOP-9086], which proposes a more rigorous process check mechanism]