[Here's a patch for trunk which just swaps out the {{HashMap}} for a {{ConcurrentHashMap}}. I couldn't repro this, but I can see the potential for this race condition: {{addDeprecation}} is {{synchronized}}, but various read operations aren't, e.g. {{loadResource}} via {{getProps}}.

{{Configuration}} will still require external synchronization if your app requires strong consistency while concurrently loading resources and reading config keys, but at least this will hopefully prevent exceptions like the above., thinking about this, I think it would be better to use compare-and-swap to swap in a new immutable object containing both forward and backward maps, when updating the Configuration.  Using {{AtomicReference}} or something.

If you want to use ConcurrentHashMap, that's fine, I guess.  However, you should at least fix the places where we do a bunch of loads in a row, like this:

{code}
    DeprecatedKeyInfo keyInfo = deprecatedKeyMap.get(name);
    if (keyInfo == null) {
      altNames = (reverseDeprecatedKeyMap.get(name) != null ) ?
        new String [] {reverseDeprecatedKeyMap.get(name)} : null;
{code}

we don't want to have the inconsistency here.  Also, we're taking and releasing the lock an awful lot in this scenario., Good points. I was hoping to escape slapping locks everywhere since the deprecated list is append-only, but we do need consistent multi-key and cross-map operations.

Unfortunately, I don't think {{AtomicReference}} or {{ConcurrentHashMap}} really save us when we need to do multiple operations atomically. I think the only *really* safe solution is slapping down class monitor locks everywhere. This is heavyweight, but applications really shouldn't be accessing Configuration so much that it becomes a problem.

I did leave a few one-off ops unsynchronized since they should be handled by the {{CHM}}., I figured out a way to do this without any global locking.  Instead, we just have an {{AtomicReference}} to an immutable {{DeprecationContext}} object.  Every time we mutate the {{DeprecationContext}}, we create a new copy of the object.  I also added a batch mutate API for efficiency's sake.

This should eliminate the concurrent access exceptions, while not requiring a global lock for all {{Configuration}} objects.

Interestingly enough, I found a lot of places where access to a single {{Configuration}} object was single-threaded... like the use of the internally-synchronized {{Properties}} class.  It would be nice to get rid of those in the future and use some kind of concurrent map instead.  However, a lot of things would have to change, like the {{REGISTRY}} object and so forth, so that would be a bigger change., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12610430/HADOOP-9478.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

      {color:red}-1 javac{color}.  The applied patch generated 1550 javac compiler warnings (more than the trunk's current 1549 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3251//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/3251//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3251//console

This message is automatically generated., Use the {{Configuration#addDeprecations}} API in {{HdfsConfiguration}} to avoid a warning about using a deprecated function, fix some other uses of the deprecated methods, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12611455/HADOOP-9478.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3257//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3257//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12611473/HADOOP-9478.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-tools/hadoop-extras hadoop-tools/hadoop-gridmix.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3259//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3259//console

This message is automatically generated., Hey Colin, thanks for taking this on. I like the overall idea; it's a pity we can't use an built-in java class for this, but needs must when synchronizing across two maps. Some review comments:

* testAndSetAccessed: should this be instead named getAndSetAccessed?
* DeprecationContext#containsKey is never used
* I prefer using {{Preconditions}} checks over throwing a raw IllegalArgumentException, it'll have a nicer message
* I'd rather not expose that new {{addDeprecations(DeprecationDelta[] delta}} method publicly, users prefer manipulating strings. It seems like some past coder was also trying to move in the direction of simplifying this API to just the {{String}} variants by deprecating the {{String[]}} versions.
* The above would also help quash the diff
* Can we just do {{deprecationContext.get()}} in {{handleDeprecation(String)}} rather than passing it down in {{handleDeprecation}} etc?
* loadResource, could you move the global deprecation get down to where it's used for the first time?, bq. testAndSetAccessed: should this be instead named getAndSetAccessed?

yeah

bq.DeprecationContext#containsKey is never used

removed

bq. I prefer using Preconditions checks over throwing a raw IllegalArgumentException, it'll have a nicer message

I changed the checks to {{Preconditions}} and moved them to the {{DeprecationDelta}} constructor

bq. I'd rather not expose that new addDeprecations(DeprecationDelta[] delta method publicly, users prefer manipulating strings. It seems like some past coder was also trying to move in the direction of simplifying this API to just the String variants by deprecating the String[] versions.

This API is far more efficient and expressive.  In many cases we're adding hundreds of deprecated keys, so this does matter.

bq. Can we just do deprecationContext.get() in handleDeprecation(String) rather than passing it down in handleDeprecation etc?

No, because then we'd be doing an atomic get O(num_properties) times.

bq. loadResource, could you move the global deprecation get down to where it's used for the first time?

OK

I will also add a test, Just nitty things this time, thanks Colin. +1 once addressed:

* typo: "globals set of" -> "global set of"
* Can we slap {{@Deprecation}} on the {{DeprecationDelta}} constructors that take an array of new keys? We still want to discourage this usage.
* I don't think we need to deprecate {{addDeprecation(String, String)}}
* Similarly, don't we need a {{DeprecationDelta(String, String, String)}} constructor for parity with the existing non-deprecated methods?
* I sort of expected the new test to be using the new {{DeprecationDelta}} API, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12611680/HADOOP-9478.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-tools/hadoop-extras hadoop-tools/hadoop-gridmix.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3262//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3262//console

This message is automatically generated., bq. typo: "globals set of" -> "global set of"

OK

bq. Can we slap @Deprecation on the DeprecationDelta constructors that take an array of new keys? We still want to discourage this usage.

I made it package-private so it won't be part of the public API at all.  It's just used to implement some of the deprecated methods elsewhere in {{Configuration}}.

bq. I don't think we need to deprecate addDeprecation(String, String)

OK

bq. Similarly, don't we need a DeprecationDelta(String, String, String) constructor for parity with the existing non-deprecated methods?

added

bq. I sort of expected the new test to be using the new DeprecationDelta API

OK

resubmitting, will commit after jenkins., Thanks. :), {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12611748/HADOOP-9478.005.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-tools/hadoop-extras hadoop-tools/hadoop-gridmix:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3264//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3264//console

This message is automatically generated., TestBalancerWithNodeGroup failure is unrelated.  Thanks for the +1, Andrew-- will commit shortly., SUCCESS: Integrated in Hadoop-trunk-Commit #4691 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4691/])
HADOOP-9478. Fix race conditions during the initialization of Configuration related to deprecatedKeyMap (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1538248)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfigurationDeprecation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HdfsConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ConfigUtil.java
* /hadoop/common/trunk/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/Logalyzer.java
* /hadoop/common/trunk/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #381 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/381/])
HADOOP-9478. Fix race conditions during the initialization of Configuration related to deprecatedKeyMap (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1538248)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfigurationDeprecation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HdfsConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ConfigUtil.java
* /hadoop/common/trunk/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/Logalyzer.java
* /hadoop/common/trunk/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1598 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1598/])
HADOOP-9478. Fix race conditions during the initialization of Configuration related to deprecatedKeyMap (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1538248)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfigurationDeprecation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HdfsConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ConfigUtil.java
* /hadoop/common/trunk/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/Logalyzer.java
* /hadoop/common/trunk/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1572 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1572/])
HADOOP-9478. Fix race conditions during the initialization of Configuration related to deprecatedKeyMap (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1538248)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfigurationDeprecation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HdfsConfiguration.java
* /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ConfigUtil.java
* /hadoop/common/trunk/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/Logalyzer.java
* /hadoop/common/trunk/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
, I need to debug more, but it looks like this may caused an incompatible change in handling deprecations... in future, MR folks (I volunteer) appreciate a heads up on major configuration changes. Thanks., Can you be more specific about what the problem you are encountering is and why you think this is the cause?, Deprecation isn't working for MR when client and cluster are on 2.2 v/s 2.3-SNAPSHOT, still debugging., Why don't you file an MR JIRA with a description of the problems you are having and how to reproduce them so that the community can help with this process?  If this JIRA turns out to be the issue we can always link the JIRAs, like we did with so many other Configuration issues., After this change, I somehow get "NoClassDefFoundError: org/apache/commons/collections/map/UnmodifiableMap" when I run any test under trunk/hadoop-hdfs-project/hadoop-hdfs.  Running tests under project root (i.e. trunk/) is fine.  I wonder if it is a problem in my local environment.  Do you get the same thing?
{noformat}
Running org.apache.hadoop.hdfs.TestFileCreation
Tests run: 22, Failures: 0, Errors: 20, Skipped: 2, Time elapsed: 0.161 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestFileCreation
testServerDefaults(org.apache.hadoop.hdfs.TestFileCreation)  Time elapsed: 0.016 sec  <<< ERROR!
java.lang.NoClassDefFoundError: org/apache/commons/collections/map/UnmodifiableMap
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at org.apache.hadoop.conf.Configuration$DeprecationContext.<init>(Configuration.java:394)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:432)
	at org.apache.hadoop.hdfs.TestFileCreation.testServerDefaults(TestFileCreation.java:149)
{noformat}
, Hey Nicholas, I've been running trunk tests the last few weeks without seeing this. It might be your local environment like you suspect., I have not seen that.  I think it's your local environment.

The class you are referring to is part of {{org.apache.commons.collections}} and should be provided by {{commons-collections-3.2.1.jar}}.  If that jar is not in your {{CLASSPATH}}, you need to figure out why.  Note that we also used  {{org.apache.commons.collections}} in hadoop-common prior to this change, in {{FileUtil}}., We noticed that the changes in jira caused client side deployment of Tez to have errors. 
Tez is designed to have a client side install. So we package Tez and its dependencies and upload that onto HDFS and those jars are used to run Tez job. Tez brings in mapreduce-client-core.jar as a dependency for InputFormats etc.
When we build Tez against trunk then the mapreduce-client-core.jar that we bring in uses DeprecatedDelta added in that jar. However, the Configuration in the cluster comes from the cluster deployed jars for hadoop common and that does not have DeprecationDelta. So the execution fails.
This basically means that if someone compiles MR from trunk and runs MR against a cluster deployed with 2.2 then MR will not work., We have never supported mixing and matching jars from trunk with jars from other branches.  For example, you can't compile the trunk version of HDFS and run it against the branch-2.1 version of common.  It may happen to work sometimes, but it will never be a supported configuration.  I don't see why Tez would be any different here.

If you do want to mix and match in the Tez project, I suggest using Maven-shade to include the hadoop-common jar inside the client-side Tez jar.]