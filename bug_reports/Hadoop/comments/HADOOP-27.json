[Just to add specifics: since a task processor might've already completed previous tasks - when it runs out of space, it shouldn't be allocated new jobs, but it should stay available for serving map output to reduce jobs that need the completed output. Likewise, once such tasks have been cleared, and more space is available, the task runner should return to available status automatically., I just ran into this problem, but the machine did actually have space available.
However, it was on another disk set in dfs.data.dir. So the first disk is full and the second one
almost empty. Would it not make sense to just continue putting the map output to the second disk?, To begin with, disregard my last comment, I have no idea what I was thinking :)

Created this patch to stop a job from losing the map tasks already completed when a task tracker runs out of space.
Instead, the task tracker will stop accepting new tasks if the space runs below a certain threshold.
However, if for some reason space clears up it will start accepting tasks again.

If for example a reduce operation previously have been assigned to the task tracker there's a chance it will run out of space anyway. So if the tracker goes below a second threshold it will completely stop accepting new tasks until the job is done and also kill the reduce operation running, or if none is found a map task. It will try to take the one with the least progress.

The solution might not be ideal, but it's at least better then having the job fail all the time because the task trackers drop off one by one.

Suggestions are of course welcome.
I've tested this on our tiny cluster and it seems to work fine, just saved me a couple of hours of redundant computation on a big job

/Johan, Does anyone have any comments on the patch?
I guess this issue is beneath the radar since it doesn't have a "affects version" added to it.
It is very important to me though, since we run a cluster with limited disk space on a few of the nodes., I still think this issue is important. The point of MapReduce is to squeeze as much performance out of what you've got as possible. I regularly have machines with hundreds of gigs of space fail because one of their 4 drives is full. And, yes, the cascading failure problem is the real kicker. It sounds like your patch attacks both of those things - as soon as I have a chance, I'll test it against my current cluster and see how it performs., I think this is a good change to make, but the patch still has a few issues.

I don't think it is sufficient to check if any device has enough room, but rather, one should check that all devices have enough room.  Local file names are hashed to determine which device to store a file on.  Unless that algorithm changes (Configuration.getLocalPath()), we should make sure that all local devices have a minimum amount of space.

Also, the indentation and formatting of the patch is non-standard, using tabs instead of spaces, and one chunk of the patch fails to apply cleanly.

Thanks!, Now checks all the local dirs as requested
Also, changed from tab -> space, I just committed this.  Thanks, Johan., 
One impact of this fix is that an invalid directory name in the temp files list will now prevent a node from running, whereas before that was tolerated.

We have a couple of nodes in our cluster with 3 drives instead of 4 (they each have a dead drive), and previously we were able to run just fine with one config file on all nodes. 



]