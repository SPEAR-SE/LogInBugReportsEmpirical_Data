[The steps to reproduce the issue/bug, Here are the detailed steps to recreate this bug. FYI we are using LDAP for storing the Unix ID and group and the LDAP is integrated with Hadoop cluster.   
 
Preliminary steps
1.Created Group say group1 
2.Created Another Group say group2 
3.Now created a User say “pocuser” and associated it with group1 and group2 ( now pocuser is associated with group1 and group2)
uid=13500(pocuser) gid=3500(group1) groups=3500(group1),3600(group2)  
4 Created HDFS directory named data1 and assigned the ownership to hdfs:group1 with permission 770
5.Create Another HDFS directory say data2 and assigned the ownership to hdfs:group2 with permission 770
drwxrwx---   - hdfs group1          0 2014-06-14 04:14 /data1
drwxrwx---   - hdfs group2          0 2014-06-14 04:14 /data2
 
Steps to create bug
 1) Now login as “pocuser" in to the machine where HDFS is mounted and just type command “id" then you will see that  user “pocuser” is associated with two groups (group1 and group2)
uid=13500(pocuser) gid=3500(group1) groups=3500(group1),3600(group2)
 
   2)Execute "cd /hdfsmount; ls – ltra" then you will see that permissions are reflecting properly in the mounted file system
drwxrwx---.  2 hdfs group1       64 Jun 14 04:14 data1
drwxrwx---.  2 hdfs group2       64 Jun 14 04:14 data2
 
  3) Reconfirmed that user “pocuser” belongs to the two group  by executing command "groups pocuser"
 pocuser : group1 group2
 
4) Now the user “pocuser” can access only data1 and NOT data2 as for some reasons it ignores the secondary group while accessing the mounted file system and throws Permission denied.
-sh-4.1$ cd data2
-sh: cd: data2: Permission denied
-sh-4.1$ cd data1
-sh-4.1$
 
5) Now if execute the command  “newgrp group2” then the user can access data2 and not data1 as the the primary group of the user “pocuser” is group2
newgrp group2
cd data1
cd: data1: Permission denied
cd data2
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650778/HADOOP-10701.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs-nfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4083//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/4083//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-nfs.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4083//console

This message is automatically generated., The findbugs warnings are related, copied below:

{code}
EI	org.apache.hadoop.oncrpc.security.CredentialsSys.getAuxGIDs() may expose internal representation by returning CredentialsSys.mAuxGIDs
EI2	org.apache.hadoop.oncrpc.security.CredentialsSys.setAuxGIDs(int[]) may expose internal representation by storing an externally mutable object into CredentialsSys.mAuxGIDs
{code}

But I think returning a clone of the array each request would end up being expensive. We could make it into a list (unnecessary overhead again), or just ignore these medium level warnings. I've gone with the latter., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650820/HADOOP-10701.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs-nfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4084//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4084//console

This message is automatically generated., @Harsh, Thanks for comming up with Fix . But here are my questions

1) As per the fix you are using the List of GID's sent by NFS Client to NFS Proxy.  But in this approach are disadvantages like 1) max of 16 GID/ UNIX ID 2) Requires integration of Client machine with LDAP and its a big pain. 

2) Why the List of GID for each UID is not retrieved from OS/LDAP instead of list of List of GID's sent by NFS Client to NFS Proxy. Doing so it will eliminate the need for Client machines to integrate with LDAP., Thanks for giving it a look - I just reused what was already being stored by the AUTH_SYS. The other way could be done as well., If i am given an option, Yes i would vote for other option (List of GID received from LDAP/OS) for just making the deployment at Cloud very easy. Also i dont want limitation of 15 GID., Thank you, [~qwertymaniac], for the fix. The patch looks nice. What [~PKUKILLA] said makes sense too. To support the easy deployment in the Cloud, we could provide a config property to let NFS gateway ignore the GIDs in the request and do a lookup similar as those in IdUserGroup, which has a mapping cache already. , +1, the patch looks good to me. I agree that it'd be a nice feature to optionally support doing group resolution on the server side on the NFS Gateway, but seems to me like that should be done as a separate improvement.

I'm going to commit this momentarily., I've just committed this to trunk and branch-2.

Thanks a lot for the contribution, Harsh., SUCCESS: Integrated in Hadoop-trunk-Commit #5789 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5789/])
HADOOP-10701. NFS should not validate the access premission only based on the user's primary group. Contributed by Harsh J. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1606042)
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/dev-support
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/dev-support/findbugsExcludeFile.xml
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/pom.xml
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/CredentialsSys.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/SecurityHandler.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/SysSecurityHandler.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3Utils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestNfs3Utils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1814 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1814/])
HADOOP-10701. NFS should not validate the access premission only based on the user's primary group. Contributed by Harsh J. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1606042)
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/dev-support
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/dev-support/findbugsExcludeFile.xml
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/pom.xml
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/CredentialsSys.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/SecurityHandler.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/SysSecurityHandler.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3Utils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestNfs3Utils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, Thank you, guys. I've filed HADOOP-10761 to track the improvement., FAILURE: Integrated in Hadoop-Yarn-trunk #597 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/597/])
HADOOP-10701. NFS should not validate the access premission only based on the user's primary group. Contributed by Harsh J. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1606042)
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/dev-support
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/dev-support/findbugsExcludeFile.xml
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/pom.xml
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/CredentialsSys.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/SecurityHandler.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/SysSecurityHandler.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3Utils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestNfs3Utils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1788 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1788/])
HADOOP-10701. NFS should not validate the access premission only based on the user's primary group. Contributed by Harsh J. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1606042)
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/dev-support
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/dev-support/findbugsExcludeFile.xml
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/pom.xml
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/CredentialsSys.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/SecurityHandler.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/SysSecurityHandler.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3Utils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestNfs3Utils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
]