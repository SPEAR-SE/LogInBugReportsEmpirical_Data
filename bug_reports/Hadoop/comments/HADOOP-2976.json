[Dhruba mentioned, 

"On lease expiry, the namenode should check each block for the file to determine if any of them are below their intended replication. If not, those blocks should be inserted into the neededReplication queue.", Is that should as in, "I think it should already do this".

Or should as in, "What a good idea, we should implement that"

??, I think he meant latter.
 "it should, but we currently don't."

> Is that should as in, "I think it should already do this".
> Or should as in, "What a good idea, we should implement that", Does replication happen only on a trigger (like block deleted or file closed)? Doesn't Namenode recognize under replicated blocks periodically? 
Edit : I meant "should" it?, When the namenode processes a lease expiry event, it checks to see if all blocks of this file have achieved their intended replication target. Blocks that have fewer than their target replicas are inserted into neededReplication., This a good change. These are one time events, so might need or already have a catch all. I think NameNode is supposed to catch these during block reports, i.e. when the datanode that has one of the replicas send block report, NameNode should have noticed under replication., A block report calculates under-replicated and over-replicated values only if the block is not already in the blocksMap. 

In our case, the first datanode confirmed the block and the block is inserted into the blocksMap. When the next block report arrives from this datanode, the namenode notices that the blocksMap already contains this information. So, the namenode does not compute if this block is over-replicated or under-replicated. I guess it is expensive to compute under-replication and over-replication for each block in a block report. , +1 The patch looks good. Hopefully it has fixed all our mysterious under-replicated block problems. :-), -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12377475/leaseExpiryReplication.patch
against trunk revision 619744.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1965/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1965/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1965/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1965/console

This message is automatically generated., I just committed this., Integrated in Hadoop-trunk #431 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/431/])]