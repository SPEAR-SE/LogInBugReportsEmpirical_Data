[Linked to HADOOP-2066, which is another path related issue., Judging by the discussion in HADOOP-2066, there is no easy fix here. As a workaround you can set the fs.s3.awsAccessKeyId and fs.s3.awsSecretAccessKey properties then the URI would be S3:/mybucket/dest.
, I've looked into this and found a simple fix (see attached patch). It is definitely not the ideal way to do it, because schema-specific stuff should be kept out of Path.java. But Path.java will always have to do some url-decoding for this to work, and I wanted to avoid breaking other schemas by decoding the authority element for all schemas. I hope this is at least a step in the right direction., patch for HADOOP-3733 on 2.0.2-alpha
, Added newly-required timeouts to the patch unit tests.  (See HADOOP-9112.)
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12570593/HADOOP-3733-20130223T011025Z.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 tests included appear to have a timeout.{color}

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2222//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2222//console

This message is automatically generated., I would like to comment that this bug will bite anyone using AWS IAM credentials more often that one may think.  Considering that there are 40 characters in the IAM private key and 64 characters in the total choices, there is a 62.5% chance that a / is going to appear in the private key.  So basically there is a 62% chance that hadoop will fail on AWS for any person using this method of access.  Seems a bit more than a low priority bug., I can confirm, that this bug treats us very often. We do a lot in AWS, have many account, and the Admins must always remember, that credentials with special characters can not be used. I see, that there is yet a patch available. Is there some problem with it or can it not be committed and solved in the next minor release?
I/we would we very happy, if this would be the case. Thanks in advance!, I can confirm that I have been hitting this issue too, and other people at my company also hit it.
It would be great to see this patch in an upcoming release.
Thanks !, Aside from setting the credentials in /etc/hadoop/core-site.xml or regenerating your secret access keys, passing in the credentials through the command line also works using: -Dfs.s3n.awsAccessKeyId=<your-key> -Dfs.s3n.awsSecretAccessKey=<your-secret-key>. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12570593/HADOOP-3733-20130223T011025Z.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3754//console

This message is automatically generated., It would probably be better if secrets could be read from an environment variable.  Putting things on the command line is not very secure., Cancelling patch since it no longer applies., Hey, any follow up story on this issue? Its been a huge hurdle on using hadoop on S3.
The plus sign seems to cause a similar problem, should I submit another ticket for it?, The focus on s3 client stuff has shifted to S3a, which is a switchable replacement for s3n, once we're happy with it. Are you using s3:// direct?

I think nobody has been that concerned due to the workaround "regen the keys", but that isn't perfect. It does need fixing, but I don't see it happening this week, Howdy! Given the lack of activity/comments for almost a year, can I assume this issue is dead and regenerating the key is the acceptable solution? One of my buckets has '/' in the secrete key and I just hit this issue using s3a. Tried setting fs.s3a.access.key and fs.s3a.secret.key config in the CLI and no luck. Anyone got a workaround other than key regeneration using s3a?, Just to clarify my comment above, I'm using Hive to create a table overlay on an existing S3 folder and when I specify the location s3a:<access key>:<secret key>@<bucket>/<folder> where <secret key> has a '/', I get:

FAILED: IllegalArgumentException The bucketName parameter must be specified

which I know the / in the <secret key> is messing up Hive., We're using {{s3n}} and we set {{fs.s3n.awsAccessKeyId}} and {{fs.s3n.awsSecretAccessKey}} programmatically via {{hadoop.conf.Configuration.set()}} when we want to access a file. I think the same should work with {{s3a}}., Thanks, Daniel, but I had tried setting fs.s3a.access.key and fs.s3a.secret.key in both the core-site.xml as well as in the Hive CLI, but none of it "stuck". I ended up regenerating the key until I got one that only had alphanumeric characters and everything worked like a charm afterwards., We should see if we can get this whenever 3.0.0 is released, Can I observer that one of my secret keys does contain a / in it, and I've not seen this recently, Hi Steve! I seem to still be observing the issue. How are you running your command?

e.g. for me on trunk
{code} hadoop fs -ls s3a://<aws_access_key:<aws_secret_key>:@mybucket/mydirectory {code}

shows me the contents of {{mydirectory}} when {{aws_secret_key}} doesn't contain a {{/}} or {{%2F}} . When the secret key contains the {{/}}, even encoding it to {{%2F}} I get this:
{code}
ls: s3a://<aws_access_key:<aws_secret_key_with_encoded_slash>@mybucket/mydirectory: getFileStatus on s3a://<aws_access_key:<aws_secret_key_with_encoded_slash>@mybucket/mydirectory: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: <someAmazonID>), S3 Extended Request ID: <anotherAmazonID>
{code}

Here's my hadoop version
{code}
Hadoop 3.0.0-alpha1-SNAPSHOT
Source code repository https://git-wip-us.apache.org/repos/asf/hadoop.git -r 28b66ae919e348123f4c05a4787c9ec56c087c25
Compiled by raviprak on 2016-06-13T16:36Z
Compiled with protoc 2.5.0
From source with checksum 7dcbe718e8724ad01f916f2eaf705b14
This command was run using /home/raviprak/Code/hadoop/trunk/hadoop-dist/target/hadoop-3.0.0-alpha1-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-alpha1-SNAPSHOT.jar
{code}
, OK, I think I see the issue: you are putting the secret in the URL, not behind the scenes (config, more recently env var).

I suspect what is happening is some parsing of the URI is getting confused about where to split up the auth info and the URL itself.

It could be in the {{initalize()}} method, where the URI is built
{code}
uri = URI.create(name.getScheme() + "://" + name.getAuthority());
{code}

Maybe it should use {{name.getRawAuthority()}}, to skip expansion of encoded characters. Alternatively, that authority info should be broken up and used to set up the auth credentials. I'd prefer that as otherwise there's a risk of the URI details being printed. Actually, that's something we should be looking for anyway; making sure that the full URI never gets printed.

Ravi, do you want to look at this? See if using the raw auth works? If not, try parsing that directly and using it as the credenials
, ...ok root cause is that (a) the S3 filesystems are using {{URI.getAuthority()}} to build the URL s3 wants, not {{URI.getHost()}}. This is compounded by teh fact that {{URI.getUserInfo()}} doesn't seem up to handling a "/" in the string, so the password isn't extracted right.

The fix is to implement our own user:pass extraction code from the authority, write tests for the parsing; add a functional test to dynamically build a URI with the auth credentials, clear any in the configuration file and try to log on. This test MUST NOT log the URI, meaning the assertions will fail to meet my usual criteria of "provide meaningful diagnostics on a failure"., Fix for this

# pull out all URL user/pass extraction into a new class {{org.apache.hadoop.fs.s3native.S3xLoginHelper}}. It's in s3n as it is needed there too, I want it to outlive s3: removal, and reserve the option to backport.
# this performs its own parsing of the user:pass from the authority info; handles / in passwords.
# FS URI construction strips out the authority info, includes only the host in its URIs.
# S3, S3N, S3A all use this codepath
# S3A has had its code related to credentials modified to work with this too; it shares the same Login structure and is in a static S3AUtils method for easier testing.
# Lots of unit tests to verify parsing works
# There's an S3A functional test which verifies that passwords stuck in the FS URL are picked up.
# I have tested that suite with a password with / in it

If you do want to use a / in a password in a URL, do encode it with %2F; this will now be handled., testing: S3 ireland. Intermittent failure of S3A root test (see HADOOP-13271), which goes away when rerun.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 35s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 14s{color} | {color:green} branch-2 passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 14s{color} | {color:green} branch-2 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 16s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 19s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 30s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 12s{color} | {color:green} branch-2 passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} branch-2 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m  9s{color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 11s{color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 14s{color} | {color:red} hadoop-tools/hadoop-aws: The patch generated 10 new + 100 unchanged - 0 fixed = 110 total (was 100) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 40s{color} | {color:red} hadoop-tools/hadoop-aws generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m  9s{color} | {color:red} hadoop-tools_hadoop-aws-jdk1.8.0_91 with JDK v1.8.0_91 generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 12s{color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 11s{color} | {color:green} hadoop-aws in the patch passed with JDK v1.7.0_101. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 18s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 14m  3s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-tools/hadoop-aws |
|  |  org.apache.hadoop.fs.s3native.S3xLoginHelper$Login.EMPTY isn't final but should be  At S3xLoginHelper.java:be  At S3xLoginHelper.java:[line 80] |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:babe025 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12810415/HADOOP-3733-branch-2-001.patch |
| JIRA Issue | HADOOP-3733 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 0a50564508ae 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2 / 9c66fff |
| Default Java | 1.7.0_101 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_91 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/9771/artifact/patchprocess/diff-checkstyle-hadoop-tools_hadoop-aws.txt |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/9771/artifact/patchprocess/new-findbugs-hadoop-tools_hadoop-aws.html |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/9771/artifact/patchprocess/diff-javadoc-javadoc-hadoop-tools_hadoop-aws-jdk1.8.0_91.txt |
| JDK v1.7.0_101  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/9771/testReport/ |
| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/9771/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Patch 002

* fixes findbug bug.
* fixes checkstyle warnings, except for the package javadoc.
* logs at WARN that people shouldn't be sticking secrets inside the filesystem URIs and that it may get pulled at some time.
, Testing: s3 ireland. Again, all well except that intermittent root test which is now failing consistently in parallel mode..., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 10m 34s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m 34s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 10s{color} | {color:green} branch-2 passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 12s{color} | {color:green} branch-2 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 18s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 20s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 51s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 35s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 13s{color} | {color:green} branch-2 passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 15s{color} | {color:green} branch-2 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m  8s{color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 10s{color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 13s{color} | {color:red} hadoop-tools/hadoop-aws: The patch generated 3 new + 99 unchanged - 0 fixed = 102 total (was 99) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m  9s{color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 13s{color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 11s{color} | {color:green} hadoop-aws in the patch passed with JDK v1.7.0_101. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 25m 49s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:babe025 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12810582/HADOOP-3733-branch-2-002.patch |
| JIRA Issue | HADOOP-3733 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 38153d6b368b 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2 / 9c66fff |
| Default Java | 1.7.0_101 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_91 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/9774/artifact/patchprocess/diff-checkstyle-hadoop-tools_hadoop-aws.txt |
| JDK v1.7.0_101  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/9774/testReport/ |
| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/9774/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, +1 for the patch.  I did a successful parallel test run against US-west-2.  The Checkstyle warnings look trivial enough to fix on check-in if you'd like.  Thank you for the patch., Patch 003

* fixes canonicalization so that there shouldn't be errors if path checking now that auth details are being stripped out of fsUri
* tests this 
* I've not been able to replicate the checkpath/canonicalization problem which was reported to me by Ravi; he'll have to test himself.
* special message for the case where getHost()==null, getAuthority!=null; this situation arises if there is an unencoded / in the path:

{code}
-ls: Fatal internal error
java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
	at java.util.Objects.requireNonNull(Objects.java:228)
	at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:53)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:199)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:101)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2812)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:294)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
	at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:235)
	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:218)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:165)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:315)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:373)
  {code}
  , patch branch-2-004; fixes checkstyle.

Chris: thanks for the +1; I'm waiting for Ravi to do another attempt at trying to get this to work. 

FWIW, I don' think people should be trying to use credentials on the CLI. This patch tries to strip it from the URL and path, but they do creep out in error messages and stack traces., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 33s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 14s{color} | {color:green} branch-2 passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 15s{color} | {color:green} branch-2 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 16s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 20s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 33s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 12s{color} | {color:green} branch-2 passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 15s{color} | {color:green} branch-2 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 11s{color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 13s{color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 12s{color} | {color:red} hadoop-tools/hadoop-aws: The patch generated 3 new + 99 unchanged - 0 fixed = 102 total (was 99) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 1 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 44s{color} | {color:red} hadoop-tools/hadoop-aws generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m  9s{color} | {color:red} hadoop-tools_hadoop-aws-jdk1.8.0_91 with JDK v1.8.0_91 generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 12s{color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 13s{color} | {color:green} hadoop-aws in the patch passed with JDK v1.7.0_101. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 13m 13s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-tools/hadoop-aws |
|  |  Comparison of String objects using == or != in org.apache.hadoop.fs.s3native.S3xLoginHelper.checkPath(Configuration, URI, Path, int)   At S3xLoginHelper.java:== or != in org.apache.hadoop.fs.s3native.S3xLoginHelper.checkPath(Configuration, URI, Path, int)   At S3xLoginHelper.java:[line 162] |
|  |  Null passed for non-null parameter of toString(URI) in org.apache.hadoop.fs.s3native.S3xLoginHelper.checkPath(Configuration, URI, Path, int)  Method invoked at S3xLoginHelper.java:of toString(URI) in org.apache.hadoop.fs.s3native.S3xLoginHelper.checkPath(Configuration, URI, Path, int)  Method invoked at S3xLoginHelper.java:[line 170] |
|  |  Null passed for non-null parameter of toString(URI) in org.apache.hadoop.fs.s3native.S3xLoginHelper.checkPath(Configuration, URI, Path, int)  Method invoked at S3xLoginHelper.java:of toString(URI) in org.apache.hadoop.fs.s3native.S3xLoginHelper.checkPath(Configuration, URI, Path, int)  Method invoked at S3xLoginHelper.java:[line 170] |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:babe025 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12810848/HADOOP-3733-branch-2-004.patch |
| JIRA Issue | HADOOP-3733 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 5fc019659668 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2 / 7523514 |
| Default Java | 1.7.0_101 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_91 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/9785/artifact/patchprocess/diff-checkstyle-hadoop-tools_hadoop-aws.txt |
| whitespace | https://builds.apache.org/job/PreCommit-HADOOP-Build/9785/artifact/patchprocess/whitespace-eol.txt |
| findbugs | https://builds.apache.org/job/PreCommit-HADOOP-Build/9785/artifact/patchprocess/new-findbugs-hadoop-tools_hadoop-aws.html |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/9785/artifact/patchprocess/diff-javadoc-javadoc-hadoop-tools_hadoop-aws-jdk1.8.0_91.txt |
| JDK v1.7.0_101  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/9785/testReport/ |
| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/9785/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Patch 0005
* all s3 filesystems warn that you shouldn't be putting secrets in your URLs
* and s3n/s3 don't mention the technique in their error messages
* javadocs
* fix findbugs warnings, one through a fix, one through commenting it out. (it's essentially the same code copied and pasted from Filesystem; it's disabled there too)., patch branch-2-006. Add a couple of lines in the documentation telling people not to put secrets in their URLs, Patch 006; tested against S3 ireland. I haven't tested inline secrets in s3, s3n, or on the command line; just in a unit test that is set up to do this (and goes to effort to not log the details on a failure: the first time I've written a unit test to be deliberately useless when reporting failures), Thanks Steve! With patch v6, I am able to use aws secrets without slashes. With un-encoded slashes I see this
{code}
java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
	at java.util.Objects.requireNonNull(Objects.java:228)
	at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:60)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:199)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:101)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2812)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:294)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
	at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:235)
	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:218)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:165)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:315)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:373)
{code}

But with encoded slashes, I still can't do an {{ls}} successfully, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 34s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 36s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 22s{color} | {color:green} branch-2 passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 18s{color} | {color:green} branch-2 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 17s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 20s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 36s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} branch-2 passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 16s{color} | {color:green} branch-2 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 11s{color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 14s{color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 1 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  0s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 10s{color} | {color:red} hadoop-tools_hadoop-aws-jdk1.8.0_91 with JDK v1.8.0_91 generated 2 new + 0 unchanged - 0 fixed = 2 total (was 0) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 14s{color} | {color:green} hadoop-aws in the patch passed with JDK v1.7.0_101. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 15m  3s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:babe025 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12810891/HADOOP-3733-branch-2-006.patch |
| JIRA Issue | HADOOP-3733 |
| Optional Tests |  asflicense  findbugs  xml  compile  javac  javadoc  mvninstall  mvnsite  unit  checkstyle  |
| uname | Linux 73bbaabdd6b0 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2 / 7523514 |
| Default Java | 1.7.0_101 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_91 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_101 |
| findbugs | v3.0.0 |
| whitespace | https://builds.apache.org/job/PreCommit-HADOOP-Build/9786/artifact/patchprocess/whitespace-eol.txt |
| javadoc | https://builds.apache.org/job/PreCommit-HADOOP-Build/9786/artifact/patchprocess/diff-javadoc-javadoc-hadoop-tools_hadoop-aws-jdk1.8.0_91.txt |
| JDK v1.7.0_101  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/9786/testReport/ |
| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/9786/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Found it. In {{S3xLoginHelper.extractLoginDetails}} we should just do this: {code}password = java.net.URLDecoder.decode(login.substring(loginSplit + 1), "UTF-8");{code}
, I reviewed the patch. Thanks a lot Steve. It looks great! Nits:
1. {code}authority and scheme are not case sensitive{code} authority is case sensitive, isn't it?
2. In general checkPath is a little hard for me to understand. Could you please explain what you are checking in the javadoc?

After these 3 issues (decoding and these 2)  are addressed, I'm +1., Thanks for finding the final quirk..I'll address it.

I think we should also do a password length check...require the secret to be exactly 40 chars long, and fail fast if not. At least the error will be meaningful, not a generic "checksum failure", which is on a par with the kerberos one.

2. In the javadocs I'll point at the FileSystem one, which is where I lifted it, moving from getAuth to getHost. That's the only difference, Patch 007
Ravi's patch + javadocs.

Tested against s3 ireland, and on the command line. It's notable that the secrets end up everywhere, such as in the output of the {{hadoop fs -ls}} command. Putting AWS login details in URLs is just wrong., Thanks a lot Steve! +1 . Will commit to trunk and branch-2 shortly!, Thanks a lot everyone for your contributions on this long-standing issue. I'm glad we could close it out thanks to Steve!, [~raviprak], thank you very much for the thorough code review and testing., SUCCESS: Integrated in Hadoop-trunk-Commit #9971 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/9971/])
HADOOP-3733. "s3x:" URLs break when Secret Key contains a slash, even if (raviprak: rev 4aefe119a0203c03cdc893dcb3330fd37f26f0ee)
* hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3ACredentialsInURL.java
* hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3native/S3xLoginHelper.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3/S3FileSystem.java
* hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3/TestS3FileSystem.java
* hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AConfiguration.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3/S3Credentials.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
* hadoop-tools/hadoop-aws/dev-support/findbugs-exclude.xml
* hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3native/TestS3xLoginHelper.java
, pulled the fix into Hadoop 2.8 as well; changed the "fixed for" version marker appropriately., The new test fails if your AWS secret key contains a '+'.  I have posted a patch on HADOOP-13287., Stripping down the userinfo part of provided uri in the filesystem's {{uri}} makes the Distcp fail in following case.
1. AccessKey and Secret Key are provided in the URI itself (ideally it should not, but for ease of use, old application uses old style uris)
2. While doing distcp of directory, {{fs.listStatus()}} results in paths WITHOUT these AccessKeys and Secrets ( because fs.makeQualified() uses {{uri}} without AK:SK)
3. Which eventually fails in Maps as there is no other way of credentials are given.

As Release note says, supporting userinfo on URI itself would be useful for distcp, but currently its breaking the existing functionality.

[~stevel@apache.org]/[~cnauroth], whether old functionality should be added back?, Given that distcp clearly never worked if there was a "/" in the secret key, it's always been an intermittently available feature. But it is causing problems, which HADOOP-14439 looks at.

I don't want to revert this, as it does a lot. But I think we should have Path.toString() return the secrets,.,.after telling the user not to do it whenever the FS instance is created. We could even add a wiki link to tell users why it is wrong and what to do about it., bq. I don't want to revert this, as it does a lot. But I think we should have Path.toString() return the secrets,.,.after telling the user not to do it whenever the FS instance is created
Yes, I too didn't mean to revert everything. Just avoiding of striping down userinfo part from {{uri}}. Already alternative ways of configuring key and secret are present in the docs I think.
 We tried the following, it works fine.
{code}
--- a/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3native/S3xLoginHelper.java
+++ b/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3native/S3xLoginHelper.java
@@ -70,7 +70,7 @@ public static URI buildFSURI(URI uri) {
           " This can be caused by unencoded / in the password string");
     }
     Objects.requireNonNull(uri.getHost(), "null uri host.");
-    return URI.create(uri.getScheme() + "://" + uri.getHost());
+    return URI.create(uri.getScheme() + "://" + uri.getAuthority());
   }
{code}, I think that seems a reasonable strategy. Can you supply a patch on the HADOOP-14439 JIRA. We should also look and see if that fixes spark's issues]