[patch uploaded for branch-2, hi [~raviprak]
could you please give suggestions and review it ?, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 13m 41s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m 52s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  0s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} shellcheck {color} | {color:green}  0m  7s{color} | {color:green} There were no new shellcheck issues. {color} |
| {color:green}+1{color} | {color:green} shelldocs {color} | {color:green}  0m  8s{color} | {color:green} There were no new shelldocs issues. {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 43s{color} | {color:green} hadoop-common in the patch passed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 26m 53s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:b59b8b7 |
| JIRA Issue | HADOOP-13865 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12841667/HADOOP-13865-branch-2.001.patch |
| Optional Tests |  asflicense  mvnsite  unit  shellcheck  shelldocs  |
| uname | Linux 95f6fe22ca4f 3.13.0-93-generic #140-Ubuntu SMP Mon Jul 18 21:21:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2 / d58fca0 |
| shellcheck | v0.4.5 |
| JDK v1.7.0_121  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11195/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11195/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, -1

There is way too much stuff hanging out in tools to do this., hi [~aw]
Will it cause any problmes for adding tool jars to classpath?
Maybe many users encouner the problems when they run hadoop apps, if they use class in tools.And They will spend much time to resolve the problems.
In my opinion, it's meaningful for hadoop users, I have mixed feelings here. I like bits on the CLI, and indeed think we (hortonworks) stick some more of the hadoop-aws and hadoop-azure stuff on our CP, albeit by copying them to somewhere on that path

At the same time, the fact that we bleed so much of our CP into downstream programs makes it a dangerous minefield about updating anything; the size of that CP means that it's inevitable that we break things whenever we do â€”so are trapped into shipping out of date stuff (Guava, Jackson) to minimise this pain (see HADOOP-9991).

Looking at your specific problem: distcp runs on the `hadoop distcp` command. Why exactly were you trying to use it from hive?, hi [~stevel@apache.org]
In hive source code Hadoop23Shims.java, it calls DistCp like this

  public boolean runDistCp(Path src, Path dst, Configuration conf) throws IOException {

    DistCpOptions options = new DistCpOptions(Collections.singletonList(src), dst);
    options.setSyncFolder(true);
    options.setSkipCRC(true);
    options.preserve(FileAttribute.BLOCKSIZE);
    try {
      conf.setBoolean("mapred.mapper.new-api", true);
      DistCp distcp = new DistCp(conf, options);
      distcp.execute();
      return true;
    } catch (Exception e) {
      throw new IOException("Cannot execute DistCp process: " + e, e);
    } finally {
      conf.setBoolean("mapred.mapper.new-api", false);
    }
  }

So i encounter the error 'java.lang.NoClassDefFoundError: org/apache/hadoop/tools/DistCpOptions'
And i can solve the problem by setting HADOOP_CLASS
Because maybe many users encounter the problems, and maybe they spend much time to solve it, so i open the issue and submit patch 
i 

, hi [~stevel@apache.org]

Not only on the CLI, but also user applications using hadoop tools in their source code. Hadoop-aws ,hadoop-azure and other tools have been involved in hadoop, its meaningful for users using hadoop easily

If add tools to classpath, then CLASSPATH=${HADOOP_HOME}/share/hadoop/tools/*:${HADOOP_HOME}/share/hadoop/tools/lib/*:$CLASSPATH. Maybe CP is not so long. , Closing this as a duplicate of HADOOP-12721. , bq.  Will it cause any problmes for adding tool jars to classpath?

Yes, it does.  There are reasons why this isn't being done already.  This is why in hadoop 3.x how the shell scripts handle hadoop tools is completely revamped (see HADOOP_OPTIONAL_TOOLS and associated code). There are no more "add the entire directory to the classpath" bits anymore.

In branch-2, users can add whatever jars they want in the default classpath by modifying various environment variables in hadoop-env.sh.  Rather than having us force this upon, they can inflict whatever level of pain they can tolerate., Really? That's very unexpected. Which version of Hive are you using?

Because I'm running spark code with Hive 1.2.1 on the CP, and I can confirm, there is no DistCP in there. Which means either it was in a much older version, or someone has gone and added it as a dependency., I find the code with Hive 2.0.0, Having checked with colleagues, "Hive uses distcp when the copy has to happen across different filesystems (hdfs to s3 or across encryption zones) and when the data to be copied is large (hive.exec.copyfile.maxsize; defaults to 32M)."

I think an issue with Hive in 2.x is that even if you don't do that, it's been added to the shims so that unless distcp is on that CP, the shims don't load. I'd consider that a Hive problem, as it should only worry about distcp when the criteria are met. And, given it has a fallback, should be able to recover, thanks [~stevel@apache.org]

I will open an issue related to this problem on hive jira, and try to fix it., This is missing in trunk right now.
I'm trying to list adl:// from Windows and it cannot find tools/lib.
[~stevel@apache.org] should we add the Windows side to trunk?, My bad, it looks like based on the discussion in HADOOP-12721, this won't be done.
The solution was what is supported in HADOOP-12857.
None of this works for Windows though., [~elgoiri]: I don't see that adl: or abfs: integration happing on trunk with the FS shell, even though hadoop-aws has it (see HADOOP-15544 ). I don't know exactly what's meant to happen here

w.r.t windows, no, it won't work. I think the solution there is something which will have to be different, won't it. If its just the cloud stores you want, we could have hadoop-cloud-storage's dependency set pulled into hadoop-common/lib, somehow, bq. w.r.t windows, no, it won't work. I think the solution there is something which will have to be different, won't it. If its just the cloud stores you want, we could have hadoop-cloud-storage's dependency set pulled into hadoop-common/lib, somehow

I'm guessing that the current trunk approach is working nicely on Linux, right?
Pulling the cloud libs into commons might be a little heavy.
Should we just have a separate path for cloud storage?

Internally, I can just append the tools/lib into my classpath but this is not working out of the box., update: I know see whaI I do to get adl on the CP, but still unsure why hadoop-aws comes in automatically
{code}
export HADOOP_OPTIONAL_TOOLS="hadoop-kafka,hadoop-aws,hadoop-aliyun,hadoop-openstack,hadoop-ozone,hadoop-azure,hadoop-azure-datalake"
{code}]