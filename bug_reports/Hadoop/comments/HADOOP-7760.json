[Almost forgot, here is the output of a run of the test program:

$ java SequenceFileTest
== testing entry with one newline char ==
-> writing sequencefile with 1 record, which is a value with 1 newlines
11/10/20 11:13:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11/10/20 11:13:07 INFO compress.CodecPool: Got brand-new compressor
-> reading all sequencefile entries..
11/10/20 11:13:07 INFO compress.CodecPool: Got brand-new decompressor
--> reading a record
--> key: 1
--> value read line: 
== testing entry with two newline chars ==
-> writing sequencefile with 1 record, which is a value with 2 newlines
-> reading all sequencefile entries..
--> reading a record
--> key: 1
--> value read line: 
--> value read line: 
--> value read line: 
, You've fallen into a classic mistake with BytesWritable. Note the comment on the API:

[http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/BytesWritable.html#getBytes()]

You needed to write:

{code}
new ByteArrayInputStream(value.getBytes(), 0, value.getLength())
{code}, Whoops.  You're right. My mistake.  Not a bug.  This can be closed.
That said, I find it counter intuitive that ByteArrayInputStream needs to be told explicitly to stop at the last element in the array, and by default goes one element too far?  Any thoughts on that?
Thanks,
Dieter
, The "problem" is that BytesWritable and Text reuse the same byte array, for shorter data. That prevents reallocation at the cost of needing to check the length of the data.]