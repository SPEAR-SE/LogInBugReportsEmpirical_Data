[A namenode that drops 75% of its requests for 10 minutes is a problem.  I think the first thing to do is to control the replication rate, so that fewer than 50 replications are attempted per second.  This is fairly simple to do, since the namenode controls the issuance of replication requests.  For example, it can limit the number of outstanding replications, which will effectively control the rate.

Think of it this way, the namenode's observed current capacity is 200 heartbeats per second and 50 block replications per second.  We're attempting in excess of 50 replications and still attempting 200 heartbeats, and the many of the heartbeats are failing to arrive in a timely manner (as are probably many of the replication reports, but those are less critical).  Retrying heartbeats sooner will just increase the load on the namenode, aggravating the problem.

The other thing to do is limit the heartbeat traffic.  Currently, heartbeat traffic is proportional to cluster size, which is not scalable.  As a simple measure, we can make the heartbeat interval configurable.  Longer term we can make it adaptive.  Longer-yet, we could even consider inverting the control, so that the namenode pings datanodes to check if they're alive and hand them work.

Another long-term fix would of course be to improve the namenode's performance and lessen its bottlenecks, so that it can handle more requests per second.  But no matter how much we do this, we still need to make sure that all request rates are limited, and do not increase linearly with cluster size., I think the main problem with the heartbeats right now is the 1 minute timeout before they fail.
Reducing the timeout to say 3 seconds (or may be 0 seconds we will need to experimant with that)
could be an easy short term solution.
First of all, this will randomize data-nodes' access to the name-node, and give them equal chances to
acknowledge their existence within the 10 minute interval.
Secondly, we will let other requests, the most often of which are leases extensions, to go through
and succeed, which eventually will reduce the failure rate of map-reduce tasks.

IMO, this will not increase the load on the name-node, because the name-node does nothing to reject
timed out requests. I expect the name-node replication rate to drop automatically since it will be
processing other requests between individual block replications, which is desirable. The data-nodes
will have to leave with higher rate of TimeoutExceptions, which is acceptable.

I like the idea of self-adjusting heartbeats. Why not, if a data-node observes a consistent 30%
rate of timed out heartbeats it should increase the heartbeat interval by 30%. And making the adjusted
parameters persistent is a good alternative to configuring.

I agree the request rates should not increase linearly with the cluster size. This makes self-adjustments
even more important, since optimizing configurations for different cluster sizes becomes a non-trivial task.
, > IMO, this will not increase the load on the name-node ...

It depends on where the bottlenecks are in the namenode.  For example, if heartbeats are already using 75% of its capacity, and we want replications to use the last 25%, then vastly increasing the heartbeat rate will starve the replications.  To my thinking, we should design things so that we don't see timeouts in normal operation (except when trying to contact nodes that are malfunctioning).  In particular, we shouldn't use timeouts as a primary control mechanism.

Also note that there are different kinds of timeouts.  There are connect timeouts, which mean that the server never saw the request.  Response timeouts, however, usually mean that the server has recieved the request and will eventually respond to it, but just not in the time you're willing to wait.  In the latter case, the server won't notice that the client has timed out until *after* the response has been computed.  So, if you're going to retry sooner, you should only do so after a connect timeout, and even then I'd argue that this is a poor solution.
, How about this approach:
* The DataNodes register once with the NameNode after the latter comes up (and registers just once - no heartbeats).
* Whenever the NameNode requires the services of a DataNode (to store/delete blocks), it pings the chosen DataNode to see whether it is indeed alive. As a response to the "ping", the DataNode sends its latest status (block report, free disk space, etc.). The response can also be controlled - if the DataNode sent its status once in the last 30 secs, it doesn't send it again.
* For each DataNode, the NameNode maintains a list of the blocks it hosts.
* A separate thread in the NameNode pings the set of known DataNodes and whenever it is not able to ping a particular DataNode, it issues the replication requests of the blocks that were there in that datanode (for this it takes the help of the inverse mapping from a "block" to the set of DataNodes containing that block).

A directory walk may also be a thing to look at - the NameNode walks the file system hierarchy and for each file it pings the set of all DataNodes containing the blocks of that file but in this case, we will end up pinging the same DataNode multiple times. To avoid this, the (DataNode->{blocks}) mapping can be used., Devaraj: I think your proposal is part of the long-term solution.  A short-term fix will simply be to control the rate of replication and increase the heartbeat interval.  Longer-term we should consider the sort of inverted control you propose., This issue has been addressed by the following fixes:

HADOOP-725
HADOOP-641
HADOOP-642
HADOOP-255]