[Could change the scope of slf4j-log4j12 to "provided" for the hdfs project so that jar is not included in the hdfs project's lib directory., Joe,

That makes senes. Would you be filing a patch changing that in the maven conf.?, Will do!, Oops, s/senes/sense :), Moved to HADOOP as it targets all daemons. Edited summary to match that., Would be good to fix this for 23.1, makes the start-* scripts much noisier. , Should we change the slf4j-log4j12 dependency to "provided" in hadoop-common, then, rather than trying to sprinkle it throughout the other projects? Or is that too dangerous/invasive?

Sorry for taking so long to get a patch -- I'll try to get to it tonight, but I won't be offended if someone else has the time and does it., So I tried changing slf4j-log4j12 to "provided" in hadoop-common, but this didn't fix the problem. It turns out that the startup scripts add all of the libs in both the hdfs and mapred dirs to the classpath for all daemons, so it appeared on the classpath, twice.

It seems like the best solution, for the startup scripts, at least, is to exclude the jars from hdfs/mapred and then just keep the jar in hadoop-common. This is contrary to the best-practices for slf4j when your jar is a "library" jar though -- in which case an adapter shouldn't be included.

Thoughts?, We can also address the problem by removing the slf4j jars from the hdfs and mapreduce assemblies.  HDFS no longer seems to need slf4j, and we can explicitly filter the slf4j jars from the mapreduce project.  Patch attached for comment.

If desired, I can file the HDFS and MAPREDUCE JIRAs to get the corresponding changes into those projects., Resolving since this is fixed by the combination of HDFS-3136 and MAPREDUCE-4060, both of which were committed to branch-2 and trunk., Updating fixed versions, as the two fixes have now been merged to 0.23.]