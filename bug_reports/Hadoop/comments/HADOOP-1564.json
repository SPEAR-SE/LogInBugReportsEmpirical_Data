[This test creates a bunch of files/directories with replication factor of 2. Then verifies that a client can automatically accesses the remaining valid replica inspite of the following types of simulated errors:

1. Delete meta file on one replica
2. Truncates meta file on one replica
3. Corrupts the meta file header on one replica
4. Corrupts any random offset and portion of the meta file
5. Swaps two meta files, i.e the format of the meta files are valid but their CRCs do not match with their corresponding data blocks

Another portion of the test is commented out till HADOOP-1557 is addressed:
1. Create file with 2 replica, corrupt the meta file of replica, decrease replication factor from 2 to 1. Validate that the remaining replica is the good one.
2. Create file with 2 replica, corrupt the meta file of one replica, increase replication factor of file to 3. verify that the new replica was created from the non-corrupted replica.
, The above tests are run for varied values of io.bytes.per.checksum and dfs.block.size. It tests for the case when the meta file is multiple blocks., Added javadoc comments to the Unit test., Submitting patch so that it triggers findbugs and javadocs warnings (if any). This can be applied only after HADOOP-1134 is committed.
, -1, build or testing failed

2 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12361213/crctest.4 against trunk revision r553080.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/363/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/363/console

Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., Of course the patch process failed. Because HADOOP-1134 is a pre-requisite for this one., Dhruba,

Yesterday we started enforcing the condition that block size should be a multiple of io.bytes.per.checksum for new files. This test fails since one of the configs does not match. Could you update the patch with the correction?
, One more change in DFSTestUtil.java: read() used to try to read as much of data as possible. Now FSInputChecker reads at most bytesPerChecksum in each read. It also provides readFully() which reads requested number of bytes in a loop. DFSTestUtil should replace read() with readFully.

With the above two changes, these tests are passing.
, This should be marked "Patch Available" again after HADOOP-1134 is committed., Unit tests for block CRC., -1, build or testing failed

2 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12361998/crctest.5 against trunk revision r556754.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/426/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/426/console

Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., I just committed this.  Thanks, Dhruba!]