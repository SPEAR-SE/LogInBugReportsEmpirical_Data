[We really need to have tools to detect whether the sort output is correct to detect large data path problems., This warrants a point release.  I've scheduled one for this Friday, hoping we find a fix by then., Same problem here. I have a bunch of M/R jobs that process some 100Gb of text data and collect different types of stats. Since Hadoop 0.10.0 I started noticing that some of the reduce outputs were missing data (some of them were empty). I modified the TestMapRed() JUnit to run the following test. I create two separate sequence files by generating N random integers and storing them in one file as IntWritable() values and as Text() writables in the other one. Matching integers have the same key. Then I ran the M/R job with an identity mapper (I use GenericWritable() to wrap the different value types) and in the reducer I check if both the integer and the string representation are received for a given key. The job runs just fine until up about 50M keys, then it starts failing for 100M keys or more. The assertion exception is caused by the reducer receiving only one value not both. One problem is that the job does not fail consistently and it always throws the assertion exceptions for different keys and within different reduce tasks. Our M/R reduce cluster is composed of 10 high-end dual-core linux boxes with about 8Tb of aggregate HD capacity. I cannot get this JUnit (or any of my jobs) to fail  with Hadoop 0.9.2. They actually run on it like a breeze., Riccardo, could you please upload your test as a patch on this issue? That will be helpful. Thanks., Do you see any other error/exception for the failing job (other than the assertion exception)? , No exceptions besides the assertion exceptions. I am uploading the patch to TestMapRed(). As I mentioned before it doesn't fail consistently, but when it does fail it is simply because it loses (key,value) pairs, no other exception is reported. At least on our cluster, the failure rate is 0% for less than 10M keys, and close to 40-50% for 100M+ keys. Hope this helps, but if I can do anything else to help out please let me know., Here it is. It was created against the Hadoop 0.11.1 release., Here is a much more compelling case. Same M/R job but instead of using GenericWritable() I use my own WritableWrapper() code (extremely simple). This JUnit fails for any number of keys (try just 50,000) with Hadoop 0.10.0 or newer, while it runs with no problems whatsoever on 0.9.2., I didn't debug this, it's just an idea ... Could this be related to the fix to HADOOP-917?, Riccardo, the problem with your testcase was that in the "readFields" method of the WritableWrapper class you were not setting the "type" field so that "write" would always write the value '0' for 'type' in the map output file. Hence the reduces won't get the intended map outputs. I have attached the fixed TestMapRed.java.
In your map method, you instantiate a WritableWrapper object which has the type field correctly set. After that you do output.collect which would have different behaviors in the versions 0.9 and 0.10+. In the version 0.9, the output.collect will write the data directly to the final map output file. However, with the 0.10+ versions, the output gets buffered and written later and there is deserialization/serialization happening when the output is finally written to disk from the buffer. In your code, the deserialization code (readFields) was not setting the type field and hence the serialization (write) would always write 0 (type - INT_WRITABLE) for the type field. The reducer, thus, would never see UTF8.

Also attached a patch that would disable inmem merge (basically sets the buffer size for the ramfs to 0, and does some checks for that). This should remove the blocker.

Mike, Albert and Riccardo - please comment whether this solves the issues you reported for the time being. I will continue to debug the inmem merge. Must be some race condition somewhere since the failures are not consistent. Thanks., Devaraj, 
As I email you yesterday, that patch solved my problem. But, I will do some more tests and will let you know the results. 
Mike, Disabling the in-mem merge has fixed this for me.  I went back and checked my outputs from 0.9 against the patched 0.11 and they match perfectly.  Thanks Devaraj!, Submitting patch for review on behalf of Devaraj (whose ISP is wonky right now) since we need to fix this 'blocker'...

Thanks to Albert and Mike for helping us test this., This looks like a reasonable workaround for right now. Although it seems like a better answer for getPercentUsed would be 1.0f instead of 0.1f.

+1, +1, because http://issues.apache.org/jira/secure/attachment/12351227/zero-size-inmem-fs.patch applied and successfully tested against trunk revision r507276., We got a ConcurrentModificationException in the ramfs, which is likely relevant: 

2007-02-15 02:58:08,997 INFO org.apache.hadoop.mapred.TaskRunner: task_0002_r_002131_0 Copying task_0002_m_001581_0 output from X.
2007-02-15 02:58:09,000 WARN org.apache.hadoop.mapred.TaskRunner: Merge of the inmemory files threw an exception: java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:787)
        at java.util.HashMap$KeyIterator.next(HashMap.java:823)
        at org.apache.hadoop.fs.InMemoryFileSystem.getFiles(InMemoryFileSystem.java:370)
        at org.apache.hadoop.mapred.ReduceTaskRunner$InMemFSMergeThread.run(ReduceTaskRunner.java:731)
, Regarding the comment on 0.1f vs 1.0f, I think it is better to leave it as is. I would like to avoid modifying this patch (which seems to remove the instability) & instead work towards making the ramfs stable. Makes sense?, I just committed this.  Thanks, Devaraj., I see the cause of the ConcurrentModificationException and likely the corruption problem. From the JavaDoc on the Collections.synchronizedMap:

| It is imperative that the user manually synchronize on the returned map when iterating over any of its 
| collection views:
|
|  Map m = Collections.synchronizedMap(new HashMap());
|      ...
|  Set s = m.keySet();  // Needn't be in synchronized block
|      ...
|  synchronized(m) {  // Synchronizing on m, not s!
|      Iterator i = s.iterator(); // Must be in synchronized block
|      while (i.hasNext())
|          foo(i.next());
|  }
| 
|
| Failure to follow this advice may result in non-deterministic behavior.

The InMemoryFileSystem locks the InMemoryFileSystem rather than the synchronizedMap. This leads to the non-determinism since the other operations (add, remove, etc.) are locking the synchronizedMap.]