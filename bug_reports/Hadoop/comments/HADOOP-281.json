[I have attached a fairly simple patch which essentially checks to ensure that all existing components of the path are directories by checking while 'collecting' the parents in dfs.FSDirectory.mkdirs.

This patch also necessiated (straightforward) changes to a few places where the return-value of 'mkdirs' wasn't being checked...

thanks,
Arun, Hi Arun, 
I was simultaneously working on this bug and had a different fix.  Hairong and I think it would be simpler to check the path/parent is a directory in addNode (which is called by unprotectedMkdir which in turn is called by mkdirs).  I think the check in addNode is required regardless of mkdirs. 

I have attached my patch file along with test code for the change.  

Please look at it and let me know what you think.  

Thanks,
Wendy
, Hi Wendy,

  Your approach looks fine too... I'll just let Doug/Konstantin or someone else with more experience with dfs codebase decide.

  However, a subtle issue: there are some places in the code base (e.g. FileUtil.java) which makes a call to 'mkdirs' and then fail to check the return-value to ensure that it completed successfully, which lead to other bugs. At the very least we will need to patch FileUtil.java (it's a part of my mkdirs.patch). Could you further test your patch using the dfs shell? 

  E.g. 
  $ hadoop dfs -mkdir /tmp/test-mkdir
  $ hadoop dfs -copyFromLocal ./foo.txt /tmp/test-mkdir/foo.txt
  $ hadoop dfs -mkdir /tmp/test-mkdir/foo.txt/d1/d2
  $ hadoop dfs -mkdir /tmp/test-mkdir/foo.txt/d1
  $ hadoop dfs -put ./some_directory /tmp/test-mkdir/foo.txt/some_directory

  Your patch fails only in the last test-case above (due to non-existent return-value check in FileUtil.java). 
  Could you please take a look and maybe include my patch for FileUtil.java too?

thanks,
Arun
 , I think addNode() is a good place to check whether the parent is a directory.
mkdirs() does not need to check all parents in the path, just the last one, since
all other parents have already been checked before.
I agree with Arun, the return value of mkdirs() should be checked every time
it is called to make sure the directories were actually created, including FileUtil
copy methods.
For mkdirs() I would let the FileNotFoundException exception thrown by addNode()
go through, rather than catching it and returning boolean, so that whoever calls
mkdirs() could decide what to do.
But there might be different opinions on that, and this will require more code changes.
, Yes, the callers of mkdirs will need to check the return value.  I looked over Arun's changes to FileUtil and they seem fine.  I can make a separate patch which includes those changes once we decide if mkdirs should pass along the FilenotFoundException or only return a boolean.

, Why not throw an exception on failure?  That seems like the more correct thing to do.  
Checking return codes for failure is so 20th century., throwing an exception was suggested, but rejected in order not to change a public interface that may be (is) used by applications., We can still return the correct boolean value to preserve public interface,
but throw an exception at the same time. The prototype is in place.
Or we can use a usual "trick" of deprecating the old mkdirs() , and define a
new one that is void.
Which mkdirs() is called by applications?
I believe mkdirs() is public since it is a part of the ClientProtocol interface.
There are other mkdirs() further in the NameNode data structure implementation.
Those should not be and are not called by anything outside the dfs project.
These mkdirs() don't need to be public at all, imo.
, Have we considered this to be a feature. Some newer general purpose file system allow a file to also be a directory (and visa versa). It is an abstraction that nicely folds in the functionality of streams and resource forks. For example, rather than sticking crcs and other attributes for a file in the same directory as a file (which necessitates a bunch of ugly skip logic in the code), we could simply put them in a subdirectory of the file., I just committed this.  Thanks, Wendy!

Ben: yes, it would be a feature, but a feature with many bugs.  There are lots of places in the current API that assume files are distinct from directories.  So, perhaps, in a separate issue, we could discuss adding this as a feature (and how we'd implement it on linux and windows), but, for now, the most consistent thing to do is treat it as a bug.]