[Correction to previous comment:

We never ran that particular job with Hadoop #835, only with #810., Christian was going to try setting 

<property>
 <name> dfs.socket.timeout</name>
 <value>180000</value>
 <description>timeout in milliseconds</description>
</property>

to get the old behaviour., The job ran much better with the higher timeout value of  180,000.
Unless we are the only ones having issues with 'all datanodes are bad' I would suggest this as the default value., This patch fixes three problems:

1. The datanode used to ack a packet before its content was flushed from the buffered output stream for the block file. This means that if the flush fails, then  data could get corrupted. This patch flushes the block file and the metadata file before sending a positive ack to the client. I have verified that this does degrade performance of dfsiotest and randonwriter.
2. The original timeout value of 1 minute * length-of-pipeline has been restored. This change reduces the number of socket timeouts when a datanode is heavily loaded.
3. The Datanode verifies that a packet replay does not create holes in the block file (sparse files). The offset-in-block of every packet should be less than or equal to the size of the current block file., Would like hadoop-QA to get a shot at this one., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12377050/packetResponse.patch
against trunk revision 619744.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1889/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1889/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1889/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1889/console

This message is automatically generated., This patch packetResponse_0.16.patch is for the 0.16 branch., I ran the Sort and DFSIO benchmarks on 500 nodes are here are the results comparing it with 0.16.0


Sort on 500 nodes: 
| |Hadoop-0.16.0|Hadoop 0.16 branch + patch|% change from Hadoop-0.16.0|
|Random Writer (hrs)|0.462|0.268|-41.99 %|
|Sort (hrs)|1.514|1.257|-16.97 %|
|Sort Validation (hrs)|0.452|0.246|-45.58 %|

Aggregate cluster throughput using DFSIO on 500 nodes:
| |Hadoop-0.16.0|Hadoop 0.16 branch + patch|% change from Hadoop-0.16.0|
|write 309 GB total with 990 clients concurrently (MB/sec)|4088.52|7897.05|93.15 %|
|read 309 GB total with 990 clients concurrently (MB/sec)|10539.45|18133.25|72.05 %|, I just committed this., Integrated in Hadoop-trunk #420 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/420/]), My bad!. We were switching hardware and the Hadoop-0.16.0 numbers are from the older hardware and the patch numbers are from the newer hardware. I will do a re-run of the baseline and post the results today. Sorry about that., Here is the correct data. Please ignore the previously posted data:

Sort on 500 nodes:
| |Hadoop-0.16 branch|Hadoop 0.16 branch + patch|% change from Hadoop-0.16 branch|
|Random Writer (hrs)|0.273|0.268|-1.83 %|
|Sort (hrs)|1.023|1.257|22.87 %|
|Sort Validation (hrs)|0.249|0.246|-1.2 %|

Aggregate cluster throughput using DFSIO on 500 nodes:
| |Hadoop-0.16 branch|Hadoop 0.16 branch + patch|% change from Hadoop-0.16 branch|
|write 309 GB total with 990 clients concurrently (MB/sec)|7418.88|7897.05|6.45 %|
|read 309 GB total with 990 clients concurrently (MB/sec)|16492.32|18133.25|9.95 %|, It is interesting that the "sort" takes longer whereas the random-writer runs faster with this patch. I am suspecting that the sort numbers are an abberation. Does it make sense to run the sort one more time?]