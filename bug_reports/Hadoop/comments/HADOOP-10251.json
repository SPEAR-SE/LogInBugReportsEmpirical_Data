[ZKFC health check, checks the state of the NameNode, but it doesnot validate it with expected state.
, Attaching a patch for the above case. Please review, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12624335/HADOOP-10251.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ha.TestZKFailoverController

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common:

org.apache.hadoop.ha.TestZKFailoverControllerStress

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3457//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3457//console

This message is automatically generated., Updated the patch to fix test failures.
These are mainly time issues., Updated the patch again.
Also enabled tests in Windows., Attaching the updated patch, fixed some synchronization issue., Hi, Can someone please review the patch.. Thanks, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12624583/HADOOP-10251.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 2 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3528//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3528//console

This message is automatically generated., Hi.. 
Can please someone review this patch? 
Thanks, Hi.. 
Can please someone review this patch? 
Thanks in advance, I think your idea will work and patch almost looks good to me.

One question:
{code}
 /**
+   * Callback interface for service state change events.
+   * 
+   * This interface is called whenever there is a change in the service state.
+   */
{code}
Seems like this interface will be called on every health monitor status check. But doc says its a service state changed event. It is exposing impl details as you do that state comparisions in implementation and do necessary actions on state change. So, at this interface level, we are not sure whether service state changed from last state or not right. Can you update this something like Service state notification?, Updated the java doc for the interface. 
Removed the implementation level details in interface., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12640785/HADOOP-10251.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3809//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3809//console

This message is automatically generated., +1 on the latest patch. I will commit it shortly., I have just committed this to trunk and branch-2 . Thanks a lot, Vinay for the patch!, SUCCESS: Integrated in Hadoop-trunk-Commit #5554 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5554/])
HADOOP-10251. Both NameNodes could be in STANDBY State if SNN network is unstable. Contributed by Vinayakumar B. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589494)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverController.java
, ABORTED: Integrated in Hadoop-Yarn-trunk #550 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/550/])
HADOOP-10251. Both NameNodes could be in STANDBY State if SNN network is unstable. Contributed by Vinayakumar B. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589494)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverController.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1767 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1767/])
HADOOP-10251. Both NameNodes could be in STANDBY State if SNN network is unstable. Contributed by Vinayakumar B. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589494)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverController.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1742 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1742/])
HADOOP-10251. Both NameNodes could be in STANDBY State if SNN network is unstable. Contributed by Vinayakumar B. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589494)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverController.java
, hi，nn1 and nn2  are alternately transformed into active state, as long as running hdfs haadmin -failover nn1 nn2.
After removing the code // healthMonitor.addServiceStateCallback(new ServiceStateCallBacks());   failover command  recovery normal


thank you ., bq. After removing the code // healthMonitor.addServiceStateCallback(new ServiceStateCallBacks()); failover command recovery normal
i did not understand the problem, Can you elaborate more please?, Attaching some logs：

2015-06-10 02:57:53,727 DEBUG org.apache.hadoop.ipc.Server: Successfully authorized userInfo {
  effectiveUser: "hdfs"
}
protocol: "org.apache.hadoop.ha.ZKFCProtocol"

2015-06-10 02:57:53,728 DEBUG org.apache.hadoop.ipc.Server:  got #0
2015-06-10 02:57:53,728 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8019: org.apache.hadoop.ha.ZKFCProtocol.gracefulFailover from 10.43.156.196:49132 Call#0 Retry#0 for RpcKind RPC_PROTOCOL_BUFFER
2015-06-10 02:57:53,730 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs (auth:SIMPLE) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
2015-06-10 02:57:53,755 DEBUG org.apache.hadoop.security.Groups: Returning fetched groups for 'hdfs'
2015-06-10 02:57:53,755 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Allowed RPC access from hdfs (auth:SIMPLE) at 10.43.156.196
2015-06-10 02:57:53,756 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs (auth:SIMPLE) from:org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:603)
2015-06-10 02:57:53,760 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:null serverPath:null finished:false header:: 4,4  replyHeader:: 4,38654797504,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,F  response:: #aa636c75737465723139351236e6e321a67a646831393620ffffffa84628ffffffd33e,s{38654797500,38654797500,1433876199859,1433876199859,0,0,0,93891338261633392,31,0,38654797500} 
2015-06-10 02:57:53,768 DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to hdfs://zdh196:9000
2015-06-10 02:57:53,770 INFO org.apache.hadoop.ha.ZKFailoverController: Asking NameNode at zdh196/10.43.156.196:9000 to cede its active state for 10000ms
2015-06-10 02:57:53,772 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:57:53,779 DEBUG org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2015-06-10 02:57:53,779 DEBUG org.apache.hadoop.ipc.Client: Connecting to zdh196/10.43.156.196:8019
2015-06-10 02:57:53,781 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:8019 from hdfs: starting, having connections 2
2015-06-10 02:57:53,781 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:8019 from hdfs sending #147
2015-06-10 02:57:53,969 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x34dcf74b50a05d7
2015-06-10 02:57:53,970 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x34dcf74b50a05d7
2015-06-10 02:57:53,974 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: NodeDeleted with state:SyncConnected for path:/hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:53,979 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:8019 from hdfs got value #147
2015-06-10 02:57:53,979 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 5,1  replyHeader:: 5,38654797507,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock 
2015-06-10 02:57:53,979 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: cedeActive took 200ms
2015-06-10 02:57:53,982 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:53,982 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-10 02:57:53,985 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:null serverPath:null finished:false header:: 6,4  replyHeader:: 6,38654797507,-101  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,F  response::  
2015-06-10 02:57:53,996 INFO org.apache.hadoop.ha.ActiveStandbyElector: No old node to fence
2015-06-10 02:57:53,996 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...
2015-06-10 02:57:54,002 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:null serverPath:null finished:false header:: 7,1  replyHeader:: 7,38654797508,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},0  response:: '/hadoop-ha/cluster195/ActiveBreadCrumb 
2015-06-10 02:57:54,006 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming active for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000

2015-06-10 02:57:54,008 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...
2015-06-10 02:57:54,010 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:57:54,012 DEBUG org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2015-06-10 02:57:54,012 DEBUG org.apache.hadoop.ipc.Client: Connecting to zdh195/10.43.156.195:9000
2015-06-10 02:57:54,014 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs: starting, having connections 3
2015-06-10 02:57:54,014 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #148
2015-06-10 02:57:54,084 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #149

2015-06-10 02:57:56,071 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #148
2015-06-10 02:57:56,071 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #149
2015-06-10 02:57:56,071 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToActive took 2059ms
2015-06-10 02:57:56,071 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 1988ms
2015-06-10 02:57:56,072 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #150
2015-06-10 02:57:56,073 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #150
2015-06-10 02:57:56,073 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 2ms
2015-06-10 02:57:56,073 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state

2015-06-10 02:57:56,079 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 8,3  replyHeader:: 8,38654797508,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,T  response:: s{38654797507,38654797507,1433876273975,1433876273975,0,0,0,238074455480993239,31,0,38654797507} 
2015-06-10 02:57:56,081 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: StatNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000

2015-06-10 02:57:56,092 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:8019 from hdfs got value #151
2015-06-10 02:57:56,092 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: cedeActive took 18ms
2015-06-10 02:57:56,092 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully became active. Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state
2015-06-10 02:57:56,093 DEBUG org.apache.hadoop.ipc.Server: Served: gracefulFailover queueTime= 20 procesingTime= 2345
2015-06-10 02:57:56,094 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8019: responding to org.apache.hadoop.ha.ZKFCProtocol.gracefulFailover from 10.43.156.196:49132 Call#0 Retry#0
2015-06-10 02:57:56,095 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8019: responding to org.apache.hadoop.ha.ZKFCProtocol.gracefulFailover from 10.43.156.196:49132 Call#0 Retry#0 Wrote 32 bytes.
2015-06-10 02:57:56,120 DEBUG org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8019: disconnecting client 10.43.156.196:49132. Number of active connections: 0
2015-06-10 02:57:57,077 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #152
2015-06-10 02:57:57,079 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #152
2015-06-10 02:57:57,079 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 3ms
2015-06-10 02:57:57,080 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #153
2015-06-10 02:57:57,081 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #153
2015-06-10 02:57:57,082 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 3ms
2015-06-10 02:57:57,082 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.
2015-06-10 02:57:57,082 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-06-10 02:57:57,085 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Terminating ZK connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:57,085 DEBUG org.apache.zookeeper.ZooKeeper: Closing session: 0x34dcf74b50a05d7
2015-06-10 02:57:57,085 DEBUG org.apache.zookeeper.ClientCnxn: Closing client for session: 0x34dcf74b50a05d7
2015-06-10 02:57:57,088 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x34dcf74b50a05d7
2015-06-10 02:57:57,089 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x34dcf74b50a05d7
2015-06-10 02:57:57,090 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:null serverPath:null finished:false header:: 9,-11  replyHeader:: 9,38654797511,0  request:: null response:: null
2015-06-10 02:57:57,090 DEBUG org.apache.zookeeper.ClientCnxn: Disconnecting client for session: 0x34dcf74b50a05d7
2015-06-10 02:57:57,090 INFO org.apache.zookeeper.ZooKeeper: Session: 0x34dcf74b50a05d7 closed
2015-06-10 02:57:57,090 DEBUG org.apache.zookeeper.ClientCnxn: An exception was thrown while closing send thread for session 0x34dcf74b50a05d7 : Unable to read additional data from server sessionid 0x34dcf74b50a05d7, likely server has closed socket
2015-06-10 02:57:57,091 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x34dcf74b50a05d7
2015-06-10 02:57:57,091 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-06-10 02:57:58,091 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #154
2015-06-10 02:57:58,093 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #154
2015-06-10 02:57:58,093 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 3ms
2015-06-10 02:57:58,094 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #155
2015-06-10 02:57:58,095 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #155
2015-06-10 02:57:58,096 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 3ms
2015-06-10 02:57:58,096 DEBUG org.apache.hadoop.ha.ZKFailoverController: rechecking for electability from bad state
2015-06-10 02:57:58,099 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Attempting active election for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,102 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Establishing zookeeper connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,102 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@31b40327
2015-06-10 02:57:58,105 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-10 02:57:58,106 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session
2015-06-10 02:57:58,106 DEBUG org.apache.zookeeper.ClientCnxn: Session establishment request sent on zdh195/10.43.156.195:2181
2015-06-10 02:57:58,109 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1967, negotiated timeout = 10000
2015-06-10 02:57:58,112 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Created new connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,116 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: None with state:SyncConnected for path:null connectionState: TERMINATED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,116 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-10 02:57:58,117 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 1,1  replyHeader:: 1,38654797515,-110  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response::  
2015-06-10 02:57:58,120 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: -110 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,123 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming standby for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,123 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at zdh195/10.43.156.195:9000 should become standby
2015-06-10 02:57:58,125 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:57:58,125 DEBUG org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2015-06-10 02:57:58,126 DEBUG org.apache.hadoop.ipc.Client: Connecting to zdh195/10.43.156.195:9000
2015-06-10 02:57:58,128 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #156
2015-06-10 02:57:58,128 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs: starting, having connections 4
2015-06-10 02:57:58,132 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #156
2015-06-10 02:57:58,132 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToStandby took 7ms
2015-06-10 02:57:58,132 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state
2015-06-10 02:57:58,135 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Monitoring active leader for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,138 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 2,3  replyHeader:: 2,38654797515,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,T  response:: s{38654797512,38654797512,1433876277092,1433876277092,0,0,0,165948932252965222,31,0,38654797512} 
2015-06-10 02:57:58,141 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: StatNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,850 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x24d91acb5cb1967
2015-06-10 02:57:58,850 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x24d91acb5cb1967
2015-06-10 02:57:58,854 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: NodeDeleted with state:SyncConnected for path:/hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,858 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 3,1  replyHeader:: 3,38654797517,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock 
2015-06-10 02:57:58,861 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:57:58,861 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-10 02:57:58,863 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:null serverPath:null finished:false header:: 4,4  replyHeader:: 4,38654797517,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,F  response:: #aa636c75737465723139351236e6e321a67a646831393620ffffffa84628ffffffd33e,s{38654797508,38654797513,1433876273998,1433876277194,1,0,0,0,31,0,38654797508} 
2015-06-10 02:57:58,864 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e
2015-06-10 02:57:58,867 DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to hdfs://zdh196:9000
2015-06-10 02:57:58,868 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh196/10.43.156.196:9000
2015-06-10 02:57:58,870 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:57:58,871 DEBUG org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.
2015-06-10 02:57:58,871 DEBUG org.apache.hadoop.ipc.Client: Connecting to zdh196/10.43.156.196:9000
2015-06-10 02:57:58,872 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs sending #157
2015-06-10 02:57:58,873 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs: starting, having connections 5
2015-06-10 02:57:58,949 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs got value #157
2015-06-10 02:57:58,949 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToStandby took 79ms
2015-06-10 02:57:58,949 DEBUG org.apache.hadoop.ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:57:58,949 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to standby state without fencing
2015-06-10 02:57:58,949 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...
2015-06-10 02:57:58,954 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:null serverPath:null finished:false header:: 5,5  replyHeader:: 5,38654797518,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,1  response:: s{38654797508,38654797518,1433876273998,1433876278951,2,0,0,0,31,0,38654797508} 
2015-06-10 02:57:58,955 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming active for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000



2015-06-10 02:57:58,955 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...
2015-06-10 02:57:58,956 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:57:58,957 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #158

2015-06-10 02:57:59,113 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #159
2015-06-10 02:58:00,545 DEBUG org.apache.hadoop.ipc.Server: IPC Server idle connection scanner for port 8019: task running
2015-06-10 02:58:00,573 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #159
2015-06-10 02:58:00,573 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #158
2015-06-10 02:58:00,574 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 1461ms
2015-06-10 02:58:00,574 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToActive took 1617ms
2015-06-10 02:58:00,574 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state
2015-06-10 02:58:00,574 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #160

2015-06-10 02:58:00,576 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #160
2015-06-10 02:58:00,576 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 2ms
2015-06-10 02:58:00,577 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Monitoring active leader for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:00,580 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 6,3  replyHeader:: 6,38654797520,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,T  response:: s{38654797517,38654797517,1433876278854,1433876278854,0,0,0,165948932252965223,31,0,38654797517} 

2015-06-10 02:58:01,578 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #161
2015-06-10 02:58:01,580 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #161
2015-06-10 02:58:01,580 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 2ms
2015-06-10 02:58:01,581 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #162
2015-06-10 02:58:01,582 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #162
2015-06-10 02:58:01,582 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 2ms
2015-06-10 02:58:01,583 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.
2015-06-10 02:58:01,583 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-06-10 02:58:01,585 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Terminating ZK connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:01,586 DEBUG org.apache.zookeeper.ZooKeeper: Closing session: 0x24d91acb5cb1967
2015-06-10 02:58:01,586 DEBUG org.apache.zookeeper.ClientCnxn: Closing client for session: 0x24d91acb5cb1967
2015-06-10 02:58:01,588 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x24d91acb5cb1967
2015-06-10 02:58:01,588 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x24d91acb5cb1967
2015-06-10 02:58:01,589 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:null serverPath:null finished:false header:: 7,-11  replyHeader:: 7,38654797521,0  request:: null response:: null
2015-06-10 02:58:01,589 DEBUG org.apache.zookeeper.ClientCnxn: Disconnecting client for session: 0x24d91acb5cb1967
2015-06-10 02:58:01,589 DEBUG org.apache.zookeeper.ClientCnxn: An exception was thrown while closing send thread for session 0x24d91acb5cb1967 : Unable to read additional data from server sessionid 0x24d91acb5cb1967, likely server has closed socket
2015-06-10 02:58:01,589 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1967 closed
2015-06-10 02:58:01,590 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1967
2015-06-10 02:58:01,590 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-06-10 02:58:02,590 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #163
2015-06-10 02:58:02,592 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #163
2015-06-10 02:58:02,593 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 3ms
2015-06-10 02:58:02,593 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #164
2015-06-10 02:58:02,595 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #164
2015-06-10 02:58:02,595 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 2ms
2015-06-10 02:58:02,595 DEBUG org.apache.hadoop.ha.ZKFailoverController: rechecking for electability from bad state
2015-06-10 02:58:02,598 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Attempting active election for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:02,600 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Establishing zookeeper connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:02,600 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@6d4f0202
2015-06-10 02:58:02,603 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-10 02:58:02,603 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session
2015-06-10 02:58:02,603 DEBUG org.apache.zookeeper.ClientCnxn: Session establishment request sent on zdh195/10.43.156.195:2181
2015-06-10 02:58:02,606 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1969, negotiated timeout = 10000
2015-06-10 02:58:02,610 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Created new connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:02,613 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: None with state:SyncConnected for path:null connectionState: TERMINATED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:02,614 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-10 02:58:02,614 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 1,1  replyHeader:: 1,38654797525,-110  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response::  
2015-06-10 02:58:02,618 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: -110 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:02,621 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming standby for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:02,621 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at zdh195/10.43.156.195:9000 should become standby
2015-06-10 02:58:02,622 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:58:02,622 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #165
2015-06-10 02:58:02,624 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #165
2015-06-10 02:58:02,624 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToStandby took 2ms
2015-06-10 02:58:02,624 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state
2015-06-10 02:58:02,625 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Monitoring active leader for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:02,627 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 2,3  replyHeader:: 2,38654797525,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,T  response:: s{38654797522,38654797522,1433876281592,1433876281592,0,0,0,165948932252965224,31,0,38654797522} 
2015-06-10 02:58:02,628 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: StatNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:03,185 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x24d91acb5cb1969
2015-06-10 02:58:03,185 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x24d91acb5cb1969
2015-06-10 02:58:03,188 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: NodeDeleted with state:SyncConnected for path:/hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:03,191 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 3,1  replyHeader:: 3,38654797527,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock 
2015-06-10 02:58:03,194 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000
2015-06-10 02:58:03,194 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-10 02:58:03,196 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:null serverPath:null finished:false header:: 4,4  replyHeader:: 4,38654797527,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,F  response:: #aa636c75737465723139351236e6e321a67a646831393620ffffffa84628ffffffd33e,s{38654797508,38654797523,1433876273998,1433876281689,3,0,0,0,31,0,38654797508} 
2015-06-10 02:58:03,198 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e
2015-06-10 02:58:03,201 DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to hdfs://zdh196:9000
2015-06-10 02:58:03,202 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh196/10.43.156.196:9000
2015-06-10 02:58:03,204 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:58:03,205 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs sending #166
2015-06-10 02:58:03,269 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs got value #166
2015-06-10 02:58:03,270 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToStandby took 65ms
2015-06-10 02:58:03,270 DEBUG org.apache.hadoop.ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:58:03,270 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to standby state without fencing
2015-06-10 02:58:03,270 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...
2015-06-10 02:58:03,274 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:null serverPath:null finished:false header:: 5,5  replyHeader:: 5,38654797528,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,3  response:: s{38654797508,38654797528,1433876273998,1433876283271,4,0,0,0,31,0,38654797508} 
2015-06-10 02:58:03,276 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming active for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000

2015-06-10 02:58:03,277 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...
2015-06-10 02:58:03,278 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85
2015-06-10 02:58:03,278 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #167
, You can try the command hdfs haadmin -failover nn1 nn2 , and then see if the active node nn1 is normal.
nn1 will always change state .active -> standby -> active -> standby .......
sorry for my poor english ,hope you can understand.thanks., Which version of Hadoop You are using?
Because I can see below logs (excluded DEBUG),
{noformat}2015-06-10 02:57:56,073 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state
2015-06-10 02:57:56,092 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully became active. Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state
2015-06-10 02:57:57,082 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.{noformat}

Immediately after {{becomeActive()}}, ERROR log is showing state expected is {{standby}}. {{serviceState}} is changed to {{active}} in {{becomeActive()}} immediately after above log.

IMO, this is possible only if {{volatile}} is not present while declaring {{serviceState}}
{code}private volatile HAServiceState serviceState = HAServiceState.INITIALIZING;{code}

do you have this  in your code?
, version 2.5.0. 
volatile is present , There is no difference in the code for the normal auto failover and failover using haadmin.

Normal auto failover is working for you?, auto failover is normal . when i kill the process of active namenode nn1 ,nn2 can transition to active .
I find if the key ha.health-monitor.check-interval.ms setting is two different values. hdfs haadmin is no problem.
nn1 setting
        <property>
        <name>ha.health-monitor.check-interval.ms</name>
        <value>2000</value>
        </property>
nn2 setting
        <property>
        <name>ha.health-monitor.check-interval.ms</name>
        <value>1000</value>
        </property>
, bq. when i kill the process of active namenode nn1 ,nn2 can transition to active .
When you kill active nn2, whether nn1 is transitioning to active?, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active, I am not getting whats the problem in your cluster. That too only with haadmin failover where as auto failover works fine.

Can you share the autofailover logs for zkfc., 顶！, Use Cases：
1.NN1 was Active and NN2 was Standby ,kill NN1 . NN2 transition to active.
2.hadoop-daemon.sh start namenode NN2. NOW.NN1 was Standby and NN2 was Active .
3.kill NN2 ,NN1 transition to active.
Attaching hdfs-nn1-zkfc-host195.log file and hdfs-nn2-zkfc-host196.log file
, ############################hdfs-nn1-zkfc-host195.log################################

2015-06-12 02:25:38,799 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Failed on local exception: java.io.EOFException; Host Details : local host is: "zdh195/10.43.156.195"; destination host is: "zdh195":9000; 
2015-06-12 02:25:38,799 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING
2015-06-12 02:25:38,800 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_NOT_RESPONDING
2015-06-12 02:25:38,800 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at zdh195/10.43.156.195:9000 and marking that fencing is necessary
2015-06-12 02:25:38,800 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-06-12 02:25:38,803 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1c33 closed
2015-06-12 02:25:38,803 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1c33
2015-06-12 02:25:38,803 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-06-12 02:25:40,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 02:25:40,807 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Call From zdh195/10.43.156.195 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
...
...
...
2015-06-12 02:27:22,976 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Call From zdh195/10.43.156.195 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2015-06-12 02:27:24,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 02:27:24,979 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Call From zdh195/10.43.156.195 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2015-06-12 02:27:26,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 02:27:27,810 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY
2015-06-12 02:27:27,810 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_HEALTHY
2015-06-12 02:27:27,811 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@651a6959
2015-06-12 02:27:27,812 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-12 02:27:27,812 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session
2015-06-12 02:27:27,814 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1c51, negotiated timeout = 10000
2015-06-12 02:27:27,815 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-12 02:27:27,816 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at zdh195/10.43.156.195:9000 should become standby
2015-06-12 02:27:27,824 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state
2015-06-12 02:27:48,662 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-12 02:27:48,663 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e
2015-06-12 02:27:48,665 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh196/10.43.156.196:9000
2015-06-12 02:27:49,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 02:27:49,670 WARN org.apache.hadoop.ha.FailoverController: Unable to gracefully make NameNode at zdh196/10.43.156.196:9000 standby (unable to connect)
java.net.ConnectException: Call From zdh195/10.43.156.195 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor24.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.transitionToStandby(Unknown Source)
	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToStandby(HAServiceProtocolClientSideTranslatorPB.java:112)
	at org.apache.hadoop.ha.FailoverController.tryGracefulFence(FailoverController.java:172)
	at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:516)
	at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:507)
	at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)
	at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:894)
	at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:901)
	at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:800)
	at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:415)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:605)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499)
Caused by: java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-06-12 02:27:49,671 INFO org.apache.hadoop.ha.NodeFencer: ====== Beginning Service Fencing Process... ======
2015-06-12 02:27:49,671 INFO org.apache.hadoop.ha.NodeFencer: Trying method 1/2: org.apache.hadoop.ha.SshFenceByTcpPort(null)
2015-06-12 02:27:49,674 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connecting to zdh196...
2015-06-12 02:27:49,674 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connecting to zdh196 port 22
2015-06-12 02:27:49,676 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connection established
2015-06-12 02:27:49,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Remote version string: SSH-2.0-OpenSSH_5.3
2015-06-12 02:27:49,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Local version string: SSH-2.0-JSCH-0.1.42
2015-06-12 02:27:49,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: CheckCiphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-ctr,arcfour,arcfour128,arcfour256
2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-ctr is not available.
2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-ctr is not available.
2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-cbc is not available.
2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-cbc is not available.
2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: arcfour256 is not available.
2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT sent
2015-06-12 02:27:49,708 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT received
2015-06-12 02:27:49,708 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: server->client aes128-ctr hmac-md5 none
2015-06-12 02:27:49,708 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: client->server aes128-ctr hmac-md5 none
2015-06-12 02:27:49,712 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXDH_INIT sent
2015-06-12 02:27:49,712 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: expecting SSH_MSG_KEXDH_REPLY
2015-06-12 02:27:49,719 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: ssh_rsa_verify: signature true
2015-06-12 02:27:49,720 WARN org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Permanently added 'zdh196' (RSA) to the list of known hosts.
2015-06-12 02:27:49,720 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS sent
2015-06-12 02:27:49,720 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS received
2015-06-12 02:27:49,721 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_REQUEST sent
2015-06-12 02:27:49,721 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_ACCEPT received
2015-06-12 02:27:49,723 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password
2015-06-12 02:27:49,723 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: gssapi-with-mic
2015-06-12 02:27:49,727 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: publickey,keyboard-interactive,password
2015-06-12 02:27:49,727 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: publickey
2015-06-12 02:27:49,771 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentication succeeded (publickey).
2015-06-12 02:27:49,772 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connected to zdh196
2015-06-12 02:27:49,772 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Looking for process running on port 9000
2015-06-12 02:27:51,758 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Indeterminate response from trying to kill service. Verifying whether it is running using nc...
2015-06-12 02:27:53,454 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Verified that the service is down.
2015-06-12 02:27:53,455 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Disconnecting from zdh196 port 22
2015-06-12 02:27:53,455 INFO org.apache.hadoop.ha.NodeFencer: ====== Fencing successful by method org.apache.hadoop.ha.SshFenceByTcpPort(null) ======
2015-06-12 02:27:53,455 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...
2015-06-12 02:27:53,455 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Caught an exception, leaving main loop due to Socket closed

2015-06-12 02:27:53,458 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...

2015-06-12 02:27:54,871 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state

2015-06-12 02:27:56,878 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.
2015-06-12 02:27:56,878 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-06-12 02:27:56,882 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1c51 closed
2015-06-12 02:27:56,882 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1c51
2015-06-12 02:27:56,882 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-06-12 02:27:57,885 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@70f0acec
2015-06-12 02:27:57,886 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-12 02:27:57,887 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session
2015-06-12 02:27:57,889 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1c52, negotiated timeout = 10000
2015-06-12 02:27:57,891 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-12 02:27:57,895 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-12 02:27:57,896 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e
2015-06-12 02:27:57,896 INFO org.apache.hadoop.ha.ActiveStandbyElector: But old node has our own data, so don't need to fence it.
2015-06-12 02:27:57,896 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...

2015-06-12 02:27:57,899 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...
2015-06-12 02:27:57,901 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state

, ######################################hdfs-nn2-zkfc-host196.log file#######################################

2015-06-12 02:25:54,146 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-12 02:25:54,147 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e
2015-06-12 02:25:54,149 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh195/10.43.156.195:9000
2015-06-12 02:25:55,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 02:25:55,152 WARN org.apache.hadoop.ha.FailoverController: Unable to gracefully make NameNode at zdh195/10.43.156.195:9000 standby (unable to connect)
java.net.ConnectException: Call From zdh196/10.43.156.196 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor24.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.transitionToStandby(Unknown Source)
	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToStandby(HAServiceProtocolClientSideTranslatorPB.java:112)
	at org.apache.hadoop.ha.FailoverController.tryGracefulFence(FailoverController.java:172)
	at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:516)
	at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:507)
	at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)
	at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:894)
	at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:901)
	at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:800)
	at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:415)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:605)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499)
Caused by: java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-06-12 02:25:55,153 INFO org.apache.hadoop.ha.NodeFencer: ====== Beginning Service Fencing Process... ======
2015-06-12 02:25:55,154 INFO org.apache.hadoop.ha.NodeFencer: Trying method 1/2: org.apache.hadoop.ha.SshFenceByTcpPort(null)
2015-06-12 02:25:55,157 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connecting to zdh195...
2015-06-12 02:25:55,157 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connecting to zdh195 port 22
2015-06-12 02:25:55,159 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connection established
2015-06-12 02:25:55,188 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Remote version string: SSH-2.0-OpenSSH_5.3
2015-06-12 02:25:55,188 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Local version string: SSH-2.0-JSCH-0.1.42
2015-06-12 02:25:55,189 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: CheckCiphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-ctr,arcfour,arcfour128,arcfour256
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-ctr is not available.
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-ctr is not available.
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-cbc is not available.
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-cbc is not available.
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: arcfour256 is not available.
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT sent
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT received
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: server->client aes128-ctr hmac-md5 none
2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: client->server aes128-ctr hmac-md5 none
2015-06-12 02:25:55,197 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXDH_INIT sent
2015-06-12 02:25:55,197 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: expecting SSH_MSG_KEXDH_REPLY
2015-06-12 02:25:55,205 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: ssh_rsa_verify: signature true
2015-06-12 02:25:55,205 WARN org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Permanently added 'zdh195' (RSA) to the list of known hosts.
2015-06-12 02:25:55,205 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS sent
2015-06-12 02:25:55,205 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS received
2015-06-12 02:25:55,206 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_REQUEST sent
2015-06-12 02:25:55,207 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_ACCEPT received
2015-06-12 02:25:55,209 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password
2015-06-12 02:25:55,209 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: gssapi-with-mic
2015-06-12 02:25:55,215 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: publickey,keyboard-interactive,password
2015-06-12 02:25:55,215 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: publickey
2015-06-12 02:25:55,263 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentication succeeded (publickey).
2015-06-12 02:25:55,263 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connected to zdh195
2015-06-12 02:25:55,263 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Looking for process running on port 9000
2015-06-12 02:25:57,418 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Indeterminate response from trying to kill service. Verifying whether it is running using nc...
2015-06-12 02:25:59,266 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Verified that the service is down.
2015-06-12 02:25:59,266 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Disconnecting from zdh195 port 22
2015-06-12 02:25:59,267 INFO org.apache.hadoop.ha.NodeFencer: ====== Fencing successful by method org.apache.hadoop.ha.SshFenceByTcpPort(null) ======
2015-06-12 02:25:59,267 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...
2015-06-12 02:25:59,267 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Caught an exception, leaving main loop due to Socket closed

2015-06-12 02:25:59,271 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh196/10.43.156.196:9000 active...

2015-06-12 02:26:00,596 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to active state

2015-06-12 02:26:02,608 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh196/10.43.156.196:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.
2015-06-12 02:26:02,608 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-06-12 02:26:02,614 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1c4f closed
2015-06-12 02:26:02,615 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1c4f
2015-06-12 02:26:02,615 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-06-12 02:26:03,619 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@33f89c4e
2015-06-12 02:26:03,622 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-12 02:26:03,623 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session
2015-06-12 02:26:03,626 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1c50, negotiated timeout = 10000
2015-06-12 02:26:03,627 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-12 02:26:03,629 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-12 02:26:03,631 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e
2015-06-12 02:26:03,631 INFO org.apache.hadoop.ha.ActiveStandbyElector: But old node has our own data, so don't need to fence it.
2015-06-12 02:26:03,631 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...

2015-06-12 02:26:03,635 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh196/10.43.156.196:9000 active...
2015-06-12 02:26:03,638 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to active state

2015-06-12 02:28:03,996 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh196/10.43.156.196:9000: Failed on local exception: java.io.EOFException; Host Details : local host is: "zdh196/10.43.156.196"; destination host is: "zdh196":9000; 
2015-06-12 02:28:03,997 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING
2015-06-12 02:28:03,997 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh196/10.43.156.196:9000 entered state: SERVICE_NOT_RESPONDING
2015-06-12 02:28:03,997 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at zdh196/10.43.156.196:9000 and marking that fencing is necessary
2015-06-12 02:28:03,997 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-06-12 02:28:04,000 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1c50 closed
2015-06-12 02:28:04,001 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1c50
2015-06-12 02:28:04,001 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-06-12 02:28:06,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 02:28:06,004 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh196/10.43.156.196:9000: Call From zdh196/10.43.156.196 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
...
...
...
2015-06-12 02:30:26,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 02:30:26,198 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh196/10.43.156.196:9000: Call From zdh196/10.43.156.196 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2015-06-12 02:30:28,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 02:30:28,200 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh196/10.43.156.196:9000: Call From zdh196/10.43.156.196 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
, bq. 2015-06-12 02:26:02,608 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh196/10.43.156.196:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.
bq. 2015-06-12 02:27:56,878 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.

The use case mentioned is normal auto failover. Not the manual failover using haadmin commands.
And I am seeing both NN1 and NN2 are not staying in Active mode if the transition happens from standby->active. This is strange.

Can you check this ?

1. Stop both ZKFCs. and NNs.
2. Start only one ZKFC and NN. It should successfully convert to active and check whether it is staying in ACTIVE for long time.
3. Attach the logs for ZKFC after restart, (logs from the restarted point, all lines)
4. Also attach the ZKFailoverController.java source code you are using., Answer 2.when start only one ZKFC and NN ，the NN  can be staying in ACTIVE for long time.

############################hdfs-nn1-zkfc-host195.log################################

2015-06-12 07:18:11,471 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at zdh195/10.43.156.195:9000
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 05/15/2015 03:44 GMT
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=zdh195
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.7.0_55
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk/jre
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/home/hdfs/hdfs/etc/hadoop:/home/hdfs/hdfs/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hdfs/hdfs/share/hadoop/common/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-net-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cdh-commons-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/gson-2.2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zdh-restclient-1.04.01.jar:/home/hdfs/hdfs/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cooma-0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/common/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/security-checkclients-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-aws-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-databind-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-lzo-0.4.20.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-annotations-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/aws-java-sdk-1.7.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=/home/hdfs/hdfs/lib/native
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux
2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=amd64
2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=2.6.32-431.el6.x86_64
2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=hdfs
2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/home/hdfs
2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/home/hdfs/hdfs
2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@3c1e1fd3
2015-06-12 07:18:11,624 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-12 07:18:11,626 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session
2015-06-12 07:18:11,632 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1c57, negotiated timeout = 10000
2015-06-12 07:18:11,636 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-12 07:18:11,662 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-06-12 07:18:11,684 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8019
2015-06-12 07:18:11,712 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-06-12 07:18:11,713 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8019: starting
2015-06-12 07:18:11,831 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY
2015-06-12 07:18:11,831 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_HEALTHY
2015-06-12 07:18:11,854 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at zdh195/10.43.156.195:9000 should become standby
2015-06-12 07:18:11,863 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state
2015-06-12 07:20:02,104 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at zdh195/10.43.156.195:9000
2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 05/15/2015 03:44 GMT
2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=zdh195
2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.7.0_55
2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk/jre
2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/home/hdfs/hdfs/etc/hadoop:/home/hdfs/hdfs/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hdfs/hdfs/share/hadoop/common/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-net-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cdh-commons-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/gson-2.2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zdh-restclient-1.04.01.jar:/home/hdfs/hdfs/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cooma-0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/common/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/security-checkclients-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-aws-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-databind-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-lzo-0.4.20.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-annotations-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/aws-java-sdk-1.7.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar
2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=/home/hdfs/hdfs/lib/native
2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux
2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=amd64
2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=2.6.32-431.el6.x86_64
2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=hdfs
2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/home/hdfs
2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/home/hdfs/hdfs
2015-06-12 07:20:02,252 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@3ddc25a9
2015-06-12 07:20:02,269 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh196/10.43.156.196:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-12 07:20:02,273 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh196/10.43.156.196:2181, initiating session
2015-06-12 07:20:02,285 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh196/10.43.156.196:2181, sessionid = 0x14d91acb8921ca0, negotiated timeout = 10000
2015-06-12 07:20:02,290 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-12 07:20:02,319 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-06-12 07:20:02,342 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8019
2015-06-12 07:20:02,373 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-06-12 07:20:02,373 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8019: starting
2015-06-12 07:20:03,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 07:20:03,446 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Call From zdh195/10.43.156.195 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2015-06-12 07:20:03,447 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING
2015-06-12 07:20:03,447 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_NOT_RESPONDING
2015-06-12 07:20:03,448 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at zdh195/10.43.156.195:9000 and marking that fencing is necessary
2015-06-12 07:20:03,448 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-06-12 07:20:03,451 INFO org.apache.zookeeper.ZooKeeper: Session: 0x14d91acb8921ca0 closed
2015-06-12 07:20:03,451 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-06-12 07:20:05,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 07:20:05,834 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY
2015-06-12 07:20:05,834 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_HEALTHY
2015-06-12 07:20:05,844 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@14b867db
2015-06-12 07:20:05,845 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh197/10.43.156.197:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-12 07:20:05,846 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh197/10.43.156.197:2181, initiating session
2015-06-12 07:20:05,849 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh197/10.43.156.197:2181, sessionid = 0x34dcf74b50a0896, negotiated timeout = 10000
2015-06-12 07:20:05,854 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-12 07:20:05,860 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-12 07:20:05,865 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e
2015-06-12 07:20:05,868 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh196/10.43.156.196:9000
2015-06-12 07:20:06,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
2015-06-12 07:20:06,877 WARN org.apache.hadoop.ha.FailoverController: Unable to gracefully make NameNode at zdh196/10.43.156.196:9000 standby (unable to connect)
java.net.ConnectException: Call From zdh195/10.43.156.195 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.transitionToStandby(Unknown Source)
	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToStandby(HAServiceProtocolClientSideTranslatorPB.java:112)
	at org.apache.hadoop.ha.FailoverController.tryGracefulFence(FailoverController.java:172)
	at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:516)
	at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:507)
	at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)
	at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:894)
	at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:901)
	at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:800)
	at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:415)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:605)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499)
Caused by: java.net.ConnectException: 拒绝连接
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-06-12 07:20:06,881 INFO org.apache.hadoop.ha.NodeFencer: ====== Beginning Service Fencing Process... ======
2015-06-12 07:20:06,881 INFO org.apache.hadoop.ha.NodeFencer: Trying method 1/2: org.apache.hadoop.ha.SshFenceByTcpPort(null)
2015-06-12 07:20:06,899 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connecting to zdh196...
2015-06-12 07:20:06,900 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connecting to zdh196 port 22
2015-06-12 07:20:06,907 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connection established
2015-06-12 07:20:06,931 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Remote version string: SSH-2.0-OpenSSH_5.3
2015-06-12 07:20:06,931 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Local version string: SSH-2.0-JSCH-0.1.42
2015-06-12 07:20:06,931 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: CheckCiphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-ctr,arcfour,arcfour128,arcfour256
2015-06-12 07:20:07,105 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-ctr is not available.
2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-ctr is not available.
2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-cbc is not available.
2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-cbc is not available.
2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: arcfour256 is not available.
2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT sent
2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT received
2015-06-12 07:20:07,107 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: server->client aes128-ctr hmac-md5 none
2015-06-12 07:20:07,107 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: client->server aes128-ctr hmac-md5 none
2015-06-12 07:20:07,115 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXDH_INIT sent
2015-06-12 07:20:07,115 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: expecting SSH_MSG_KEXDH_REPLY
2015-06-12 07:20:07,125 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: ssh_rsa_verify: signature true
2015-06-12 07:20:07,128 WARN org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Permanently added 'zdh196' (RSA) to the list of known hosts.
2015-06-12 07:20:07,128 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS sent
2015-06-12 07:20:07,128 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS received
2015-06-12 07:20:07,132 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_REQUEST sent
2015-06-12 07:20:07,133 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_ACCEPT received
2015-06-12 07:20:07,135 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password
2015-06-12 07:20:07,135 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: gssapi-with-mic
2015-06-12 07:20:07,142 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: publickey,keyboard-interactive,password
2015-06-12 07:20:07,142 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: publickey
2015-06-12 07:20:07,188 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentication succeeded (publickey).
2015-06-12 07:20:07,189 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connected to zdh196
2015-06-12 07:20:07,189 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Looking for process running on port 9000
2015-06-12 07:20:09,289 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Indeterminate response from trying to kill service. Verifying whether it is running using nc...
2015-06-12 07:20:11,049 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Verified that the service is down.
2015-06-12 07:20:11,050 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Disconnecting from zdh196 port 22
2015-06-12 07:20:11,052 INFO org.apache.hadoop.ha.NodeFencer: ====== Fencing successful by method org.apache.hadoop.ha.SshFenceByTcpPort(null) ======
2015-06-12 07:20:11,052 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...
2015-06-12 07:20:11,053 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Caught an exception, leaving main loop due to Socket closed
2015-06-12 07:20:11,061 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...
2015-06-12 07:20:12,333 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state

############################hdfs-nn2-zkfc-host196.log################################

2015-06-12 07:18:26,905 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at zdh196/10.43.156.196:9000
2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 05/15/2015 03:44 GMT
2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=zdh196
2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.7.0_55
2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk/jre
2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/home/hdfs/hdfs/etc/hadoop:/home/hdfs/hdfs/share/hadoop/common/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cooma-0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cdh-commons-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-net-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/common/lib/gson-2.2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zdh-restclient-1.04.01.jar:/home/hdfs/hdfs/share/hadoop/common/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/common/lib/security-checkclients-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs:/home/hdfs/hdfs/share/hadoop/hdfs/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/aws-java-sdk-1.7.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-lzo-0.4.20.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-annotations-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-databind-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-aws-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar
2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=/home/hdfs/hdfs/lib/native
2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux
2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=amd64
2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=2.6.32-431.el6.x86_64
2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=hdfs
2015-06-12 07:18:27,044 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/home/hdfs
2015-06-12 07:18:27,044 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/home/hdfs/hdfs
2015-06-12 07:18:27,044 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@1a722605
2015-06-12 07:18:27,060 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh196/10.43.156.196:2181. Will not attempt to authenticate using SASL (unknown error)
2015-06-12 07:18:27,065 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh196/10.43.156.196:2181, initiating session
2015-06-12 07:18:27,073 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh196/10.43.156.196:2181, sessionid = 0x14d91acb8921c9f, negotiated timeout = 10000
2015-06-12 07:18:27,077 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-06-12 07:18:27,104 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-06-12 07:18:27,125 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8019
2015-06-12 07:18:27,155 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-06-12 07:18:27,155 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8019: starting
2015-06-12 07:18:27,277 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY
2015-06-12 07:18:27,277 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh196/10.43.156.196:9000 entered state: SERVICE_HEALTHY
2015-06-12 07:18:27,301 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...
2015-06-12 07:18:27,309 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e
2015-06-12 07:18:27,312 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh195/10.43.156.195:9000
2015-06-12 07:18:27,338 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state without fencing
2015-06-12 07:18:27,338 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...
2015-06-12 07:18:27,348 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh196/10.43.156.196:9000 active...
2015-06-12 07:18:39,494 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to active state

, /**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hadoop.ha;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.security.PrivilegedAction;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.HadoopIllegalArgumentException;
import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.ha.ActiveStandbyElector.ActiveNotFoundException;
import org.apache.hadoop.ha.ActiveStandbyElector.ActiveStandbyElectorCallback;
import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;
import org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo;
import org.apache.hadoop.ha.HAServiceProtocol.RequestSource;
import org.apache.hadoop.util.ZKUtil;
import org.apache.hadoop.util.ZKUtil.ZKAuthInfo;
import org.apache.hadoop.ha.HealthMonitor.State;
import org.apache.hadoop.ipc.Server;
import org.apache.hadoop.security.AccessControlException;
import org.apache.hadoop.security.SecurityUtil;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hadoop.security.authorize.PolicyProvider;
import org.apache.hadoop.util.StringUtils;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.hadoop.util.ToolRunner;
import org.apache.zookeeper.data.ACL;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import com.google.common.base.Throwables;
import com.google.common.util.concurrent.ThreadFactoryBuilder;


@InterfaceAudience.LimitedPrivate("HDFS")
public abstract class ZKFailoverController {

  static final Log LOG = LogFactory.getLog(ZKFailoverController.class);
  
  public static final String ZK_QUORUM_KEY = "ha.zookeeper.quorum";
  private static final String ZK_SESSION_TIMEOUT_KEY = "ha.zookeeper.session-timeout.ms";
  private static final int ZK_SESSION_TIMEOUT_DEFAULT = 5*1000;
  private static final String ZK_PARENT_ZNODE_KEY = "ha.zookeeper.parent-znode";
  public static final String ZK_ACL_KEY = "ha.zookeeper.acl";
  private static final String ZK_ACL_DEFAULT = "world:anyone:rwcda";
  public static final String ZK_AUTH_KEY = "ha.zookeeper.auth";
  static final String ZK_PARENT_ZNODE_DEFAULT = "/hadoop-ha";

  /**
   * All of the conf keys used by the ZKFC. This is used in order to allow
   * them to be overridden on a per-nameservice or per-namenode basis.
   */
  protected static final String[] ZKFC_CONF_KEYS = new String[] {
    ZK_QUORUM_KEY,
    ZK_SESSION_TIMEOUT_KEY,
    ZK_PARENT_ZNODE_KEY,
    ZK_ACL_KEY,
    ZK_AUTH_KEY
  };
  
  protected static final String USAGE = 
      "Usage: java zkfc [ -formatZK [-force] [-nonInteractive] ]";

  /** Unable to format the parent znode in ZK */
  static final int ERR_CODE_FORMAT_DENIED = 2;
  /** The parent znode doesn't exist in ZK */
  static final int ERR_CODE_NO_PARENT_ZNODE = 3;
  /** Fencing is not properly configured */
  static final int ERR_CODE_NO_FENCER = 4;
  /** Automatic failover is not enabled */
  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED = 5;
  /** Cannot connect to ZooKeeper */
  static final int ERR_CODE_NO_ZK = 6;
  
  protected Configuration conf;
  private String zkQuorum;
  protected final HAServiceTarget localTarget;

  private HealthMonitor healthMonitor;
  private ActiveStandbyElector elector;
  protected ZKFCRpcServer rpcServer;

  private State lastHealthState = State.INITIALIZING;

  private volatile HAServiceState serviceState = HAServiceState.INITIALIZING;

  /** Set if a fatal error occurs */
  private String fatalError = null;

  /**
   * A future nanotime before which the ZKFC will not join the election.
   * This is used during graceful failover.
   */
  private long delayJoiningUntilNanotime = 0;

  /** Executor on which {@link #scheduleRecheck(long)} schedules events */
  private ScheduledExecutorService delayExecutor =
    Executors.newScheduledThreadPool(1,
        new ThreadFactoryBuilder().setDaemon(true)
            .setNameFormat("ZKFC Delay timer #%d")
            .build());

  private ActiveAttemptRecord lastActiveAttemptRecord;
  private Object activeAttemptRecordLock = new Object();


  protected ZKFailoverController(Configuration conf, HAServiceTarget localTarget) {
    this.localTarget = localTarget;
    this.conf = conf;
  }
  

  protected abstract byte[] targetToData(HAServiceTarget target);
  protected abstract HAServiceTarget dataToTarget(byte[] data);
  protected abstract void loginAsFCUser() throws IOException;
  protected abstract void checkRpcAdminAccess()
      throws AccessControlException, IOException;
  protected abstract InetSocketAddress getRpcAddressToBindTo();
  protected abstract PolicyProvider getPolicyProvider();

  /**
   * Return the name of a znode inside the configured parent znode in which
   * the ZKFC will do all of its work. This is so that multiple federated
   * nameservices can run on the same ZK quorum without having to manually
   * configure them to separate subdirectories.
   */
  protected abstract String getScopeInsideParentNode();

  public HAServiceTarget getLocalTarget() {
    return localTarget;
  }
  
  public int run(final String[] args) throws Exception {
    if (!localTarget.isAutoFailoverEnabled()) {
      LOG.fatal("Automatic failover is not enabled for " + localTarget + "." +
          " Please ensure that automatic failover is enabled in the " +
          "configuration before running the ZK failover controller.");
      return ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;
    }
    loginAsFCUser();
    try {
      return SecurityUtil.doAsLoginUserOrFatal(new PrivilegedAction<Integer>() {
        @Override
        public Integer run() {
          try {
            return doRun(args);
          } catch (Exception t) {
            throw new RuntimeException(t);
          } finally {
            if (elector != null) {
              elector.terminateConnection();
            }
          }
        }
      });
    } catch (RuntimeException rte) {
      throw (Exception)rte.getCause();
    }
  }
  

  private int doRun(String[] args)
      throws HadoopIllegalArgumentException, IOException, InterruptedException {
    try {
      initZK();
    } catch (KeeperException ke) {
      LOG.fatal("Unable to start failover controller. Unable to connect "
          + "to ZooKeeper quorum at " + zkQuorum + ". Please check the "
          + "configured value for " + ZK_QUORUM_KEY + " and ensure that "
          + "ZooKeeper is running.");
      return ERR_CODE_NO_ZK;
    }
    if (args.length > 0) {
      if ("-formatZK".equals(args[0])) {
        boolean force = false;
        boolean interactive = true;
        for (int i = 1; i < args.length; i++) {
          if ("-force".equals(args[i])) {
            force = true;
          } else if ("-nonInteractive".equals(args[i])) {
            interactive = false;
          } else {
            badArg(args[i]);
          }
        }
        return formatZK(force, interactive);
      } else {
        badArg(args[0]);
      }
    }

    if (!elector.parentZNodeExists()) {
      LOG.fatal("Unable to start failover controller. "
          + "Parent znode does not exist.\n"
          + "Run with -formatZK flag to initialize ZooKeeper.");
      return ERR_CODE_NO_PARENT_ZNODE;
    }

    try {
      localTarget.checkFencingConfigured();
    } catch (BadFencingConfigurationException e) {
      LOG.fatal("Fencing is not configured for " + localTarget + ".\n" +
          "You must configure a fencing method before using automatic " +
          "failover.", e);
      return ERR_CODE_NO_FENCER;
    }

    initRPC();
    initHM();
    startRPC();
    try {
      mainLoop();
    } finally {
      rpcServer.stopAndJoin();
      
      elector.quitElection(true);
      healthMonitor.shutdown();
      healthMonitor.join();
    }
    return 0;
  }

  private void badArg(String arg) {
    printUsage();
    throw new HadoopIllegalArgumentException(
        "Bad argument: " + arg);
  }

  private void printUsage() {
    System.err.println(USAGE + "\n");
  }

  private int formatZK(boolean force, boolean interactive)
      throws IOException, InterruptedException {
    if (elector.parentZNodeExists()) {
      if (!force && (!interactive || !confirmFormat())) {
        return ERR_CODE_FORMAT_DENIED;
      }
      
      try {
        elector.clearParentZNode();
      } catch (IOException e) {
        LOG.error("Unable to clear zk parent znode", e);
        return 1;
      }
    }
    
    elector.ensureParentZNode();
    return 0;
  }

  private boolean confirmFormat() {
    String parentZnode = getParentZnode();
    System.err.println(
        "===============================================\n" +
        "The configured parent znode " + parentZnode + " already exists.\n" +
        "Are you sure you want to clear all failover information from\n" +
        "ZooKeeper?\n" +
        "WARNING: Before proceeding, ensure that all HDFS services and\n" +
        "failover controllers are stopped!\n" +
        "===============================================");
    try {
      return ToolRunner.confirmPrompt("Proceed formatting " + parentZnode + "?");
    } catch (IOException e) {
      LOG.debug("Failed to confirm", e);
      return false;
    }
  }

  // ------------------------------------------
  // Begin actual guts of failover controller
  // ------------------------------------------
  
  private void initHM() {
    healthMonitor = new HealthMonitor(conf, localTarget);
    healthMonitor.addCallback(new HealthCallbacks());
    healthMonitor.addServiceStateCallback(new ServiceStateCallBacks());
    healthMonitor.start();
  }
  
  protected void initRPC() throws IOException {
    InetSocketAddress bindAddr = getRpcAddressToBindTo();
    rpcServer = new ZKFCRpcServer(conf, bindAddr, this, getPolicyProvider());
  }

  protected void startRPC() throws IOException {
    rpcServer.start();
  }


  private void initZK() throws HadoopIllegalArgumentException, IOException,
      KeeperException {
    zkQuorum = conf.get(ZK_QUORUM_KEY);
    int zkTimeout = conf.getInt(ZK_SESSION_TIMEOUT_KEY,
        ZK_SESSION_TIMEOUT_DEFAULT);
    // Parse ACLs from configuration.
    String zkAclConf = conf.get(ZK_ACL_KEY, ZK_ACL_DEFAULT);
    zkAclConf = ZKUtil.resolveConfIndirection(zkAclConf);
    List<ACL> zkAcls = ZKUtil.parseACLs(zkAclConf);
    if (zkAcls.isEmpty()) {
      zkAcls = Ids.CREATOR_ALL_ACL;
    }
    
    // Parse authentication from configuration.
    String zkAuthConf = conf.get(ZK_AUTH_KEY);
    zkAuthConf = ZKUtil.resolveConfIndirection(zkAuthConf);
    List<ZKAuthInfo> zkAuths;
    if (zkAuthConf != null) {
      zkAuths = ZKUtil.parseAuth(zkAuthConf);
    } else {
      zkAuths = Collections.emptyList();
    }

    // Sanity check configuration.
    Preconditions.checkArgument(zkQuorum != null,
        "Missing required configuration '%s' for ZooKeeper quorum",
        ZK_QUORUM_KEY);
    Preconditions.checkArgument(zkTimeout > 0,
        "Invalid ZK session timeout %s", zkTimeout);
    

    elector = new ActiveStandbyElector(zkQuorum,
        zkTimeout, getParentZnode(), zkAcls, zkAuths,
        new ElectorCallbacks());
  }
  
  private String getParentZnode() {
    String znode = conf.get(ZK_PARENT_ZNODE_KEY,
        ZK_PARENT_ZNODE_DEFAULT);
    if (!znode.endsWith("/")) {
      znode += "/";
    }
    return znode + getScopeInsideParentNode();
  }

  private synchronized void mainLoop() throws InterruptedException {
    while (fatalError == null) {
      wait();
    }
    assert fatalError != null; // only get here on fatal
    throw new RuntimeException(
        "ZK Failover Controller failed: " + fatalError);
  }
  
  private synchronized void fatalError(String err) {
    LOG.fatal("Fatal error occurred:" + err);
    fatalError = err;
    notifyAll();
  }
  
  private synchronized void becomeActive() throws ServiceFailedException {
    LOG.info("Trying to make " + localTarget + " active...");
    try {
      HAServiceProtocolHelper.transitionToActive(localTarget.getProxy(
          conf, FailoverController.getRpcTimeoutToNewActive(conf)),
          createReqInfo());
      String msg = "Successfully transitioned " + localTarget +
          " to active state";
      LOG.info(msg);
      recordActiveAttempt(new ActiveAttemptRecord(true, msg));

    } catch (Throwable t) {
      String msg = "Couldn't make " + localTarget + " active";
      LOG.fatal(msg, t);
      
      recordActiveAttempt(new ActiveAttemptRecord(false, msg + "\n" +
          StringUtils.stringifyException(t)));

      if (t instanceof ServiceFailedException) {
        throw (ServiceFailedException)t;
      } else {
        throw new ServiceFailedException("Couldn't transition to active",
            t);
      }
/*
* TODO:
* we need to make sure that if we get fenced and then quickly restarted,
* none of these calls will retry across the restart boundary
* perhaps the solution is that, whenever the nn starts, it gets a unique
* ID, and when we start becoming active, we record it, and then any future
* calls use the same ID
*/
      
    }
  }

  /**
   * Store the results of the last attempt to become active.
   * This is used so that, during manually initiated failover,
   * we can report back the results of the attempt to become active
   * to the initiator of the failover.
   */
  private void recordActiveAttempt(
      ActiveAttemptRecord record) {
    synchronized (activeAttemptRecordLock) {
      lastActiveAttemptRecord = record;
      activeAttemptRecordLock.notifyAll();
    }
  }

  /**
   * Wait until one of the following events:
   * <ul>
   * <li>Another thread publishes the results of an attempt to become active
   * using {@link #recordActiveAttempt(ActiveAttemptRecord)}</li>
   * <li>The node enters bad health status</li>
   * <li>The specified timeout elapses</li>
   * </ul>
   * 
   * @param timeoutMillis number of millis to wait
   * @return the published record, or null if the timeout elapses or the
   * service becomes unhealthy 
   * @throws InterruptedException if the thread is interrupted.
   */
  private ActiveAttemptRecord waitForActiveAttempt(int timeoutMillis)
      throws InterruptedException {
    long st = System.nanoTime();
    long waitUntil = st + TimeUnit.NANOSECONDS.convert(
        timeoutMillis, TimeUnit.MILLISECONDS);
    
    do {
      // periodically check health state, because entering an
      // unhealthy state could prevent us from ever attempting to
      // become active. We can detect this and respond to the user
      // immediately.
      synchronized (this) {
        if (lastHealthState != State.SERVICE_HEALTHY) {
          // early out if service became unhealthy
          return null;
        }
      }

      synchronized (activeAttemptRecordLock) {
        if ((lastActiveAttemptRecord != null &&
            lastActiveAttemptRecord.nanoTime >= st)) {
          return lastActiveAttemptRecord;
        }
        // Only wait 1sec so that we periodically recheck the health state
        // above.
        activeAttemptRecordLock.wait(1000);
      }
    } while (System.nanoTime() < waitUntil);
    
    // Timeout elapsed.
    LOG.warn(timeoutMillis + "ms timeout elapsed waiting for an attempt " +
        "to become active");
    return null;
  }

  private StateChangeRequestInfo createReqInfo() {
    return new StateChangeRequestInfo(RequestSource.REQUEST_BY_ZKFC);
  }

  private synchronized void becomeStandby() {
    LOG.info("ZK Election indicated that " + localTarget +
        " should become standby");
    try {
      int timeout = FailoverController.getGracefulFenceTimeout(conf);
      localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());
      LOG.info("Successfully transitioned " + localTarget +
          " to standby state");
    } catch (Exception e) {
      LOG.error("Couldn't transition " + localTarget + " to standby state",
          e);
      // TODO handle this. It's a likely case since we probably got fenced
      // at the same time.
    }
    serviceState = HAServiceState.STANDBY;
  }
  

  private synchronized void fenceOldActive(byte[] data) {
    HAServiceTarget target = dataToTarget(data);
    
    try {
      doFence(target);
    } catch (Throwable t) {
      recordActiveAttempt(new ActiveAttemptRecord(false, "Unable to fence old active: " + StringUtils.stringifyException(t)));
      Throwables.propagate(t);
    }
  }
  
  private void doFence(HAServiceTarget target) {
    LOG.info("Should fence: " + target);
    boolean gracefulWorked = new FailoverController(conf,
        RequestSource.REQUEST_BY_ZKFC).tryGracefulFence(target);
    if (gracefulWorked) {
      // It's possible that it's in standby but just about to go into active,
      // no? Is there some race here?
      LOG.info("Successfully transitioned " + target + " to standby " +
          "state without fencing");
      return;
    }
    
    try {
      target.checkFencingConfigured();
    } catch (BadFencingConfigurationException e) {
      LOG.error("Couldn't fence old active " + target, e);
      recordActiveAttempt(new ActiveAttemptRecord(false, "Unable to fence old active"));
      throw new RuntimeException(e);
    }
    
    if (!target.getFencer().fence(target)) {
      throw new RuntimeException("Unable to fence " + target);
    }
  }


  /**
   * Request from graceful failover to cede active role. Causes
   * this ZKFC to transition its local node to standby, then quit
   * the election for the specified period of time, after which it
   * will rejoin iff it is healthy.
   */
  void cedeActive(final int millisToCede)
      throws AccessControlException, ServiceFailedException, IOException {
    try {
      UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>() {
        @Override
        public Void run() throws Exception {
          doCedeActive(millisToCede);
          return null;
        }
      });
    } catch (InterruptedException e) {
      throw new IOException(e);
    }
  }
  
  private void doCedeActive(int millisToCede) 
      throws AccessControlException, ServiceFailedException, IOException {
    int timeout = FailoverController.getGracefulFenceTimeout(conf);

    // Lock elector to maintain lock ordering of elector -> ZKFC
    synchronized (elector) {
      synchronized (this) {
        if (millisToCede <= 0) {
          delayJoiningUntilNanotime = 0;
          recheckElectability();
          return;
        }
  
        LOG.info("Requested by " + UserGroupInformation.getCurrentUser() +
            " at " + Server.getRemoteAddress() + " to cede active role.");
        boolean needFence = false;
        try {
          localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());
          LOG.info("Successfully ensured local node is in standby mode");
        } catch (IOException ioe) {
          LOG.warn("Unable to transition local node to standby: " +
              ioe.getLocalizedMessage());
          LOG.warn("Quitting election but indicating that fencing is " +
              "necessary");
          needFence = true;
        }
        delayJoiningUntilNanotime = System.nanoTime() +
            TimeUnit.MILLISECONDS.toNanos(millisToCede);
        elector.quitElection(needFence);
        serviceState = HAServiceState.INITIALIZING;
      }
    }
    recheckElectability();
  }
  
  /**
   * Coordinate a graceful failover to this node.
   * @throws ServiceFailedException if the node fails to become active
   * @throws IOException some other error occurs
   */
  void gracefulFailoverToYou() throws ServiceFailedException, IOException {
    try {
      UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>() {
        @Override
        public Void run() throws Exception {
          doGracefulFailover();
          return null;
        }
        
      });
    } catch (InterruptedException e) {
      throw new IOException(e);
    }
  }

  /**
   * Coordinate a graceful failover. This proceeds in several phases:
   * 1) Pre-flight checks: ensure that the local node is healthy, and
   * thus a candidate for failover.
   * 2) Determine the current active node. If it is the local node, no
   * need to failover - return success.
   * 3) Ask that node to yield from the election for a number of seconds.
   * 4) Allow the normal election path to run in other threads. Wait until
   * we either become unhealthy or we see an election attempt recorded by
   * the normal code path.
   * 5) Allow the old active to rejoin the election, so a future
   * failback is possible.
   */
  private void doGracefulFailover()
      throws ServiceFailedException, IOException, InterruptedException {
    int timeout = FailoverController.getGracefulFenceTimeout(conf) * 2;
    
    // Phase 1: pre-flight checks
    checkEligibleForFailover();
    
    // Phase 2: determine old/current active node. Check that we're not
    // ourselves active, etc.
    HAServiceTarget oldActive = getCurrentActive();
    if (oldActive == null) {
      // No node is currently active. So, if we aren't already
      // active ourselves by means of a normal election, then there's
      // probably something preventing us from becoming active.
      throw new ServiceFailedException(
          "No other node is currently active.");
    }
    
    if (oldActive.getAddress().equals(localTarget.getAddress())) {
      LOG.info("Local node " + localTarget + " is already active. " +
          "No need to failover. Returning success.");
      return;
    }
    
    // Phase 3: ask the old active to yield from the election.
    LOG.info("Asking " + oldActive + " to cede its active state for " +
        timeout + "ms");
    ZKFCProtocol oldZkfc = oldActive.getZKFCProxy(conf, timeout);
    oldZkfc.cedeActive(timeout);

    // Phase 4: wait for the normal election to make the local node
    // active.
    ActiveAttemptRecord attempt = waitForActiveAttempt(timeout + 60000);
    
    if (attempt == null) {
      // We didn't even make an attempt to become active.
      synchronized(this) {
        if (lastHealthState != State.SERVICE_HEALTHY) {
          throw new ServiceFailedException("Unable to become active. " +
            "Service became unhealthy while trying to failover.");          
        }
      }
      
      throw new ServiceFailedException("Unable to become active. " +
          "Local node did not get an opportunity to do so from ZooKeeper, " +
          "or the local node took too long to transition to active.");
    }

    // Phase 5. At this point, we made some attempt to become active. So we
    // can tell the old active to rejoin if it wants. This allows a quick
    // fail-back if we immediately crash.
    oldZkfc.cedeActive(-1);
    
    if (attempt.succeeded) {
      LOG.info("Successfully became active. " + attempt.status);
    } else {
      // Propagate failure
      String msg = "Failed to become active. " + attempt.status;
      throw new ServiceFailedException(msg);
    }
  }

  /**
   * Ensure that the local node is in a healthy state, and thus
   * eligible for graceful failover.
   * @throws ServiceFailedException if the node is unhealthy
   */
  private synchronized void checkEligibleForFailover()
      throws ServiceFailedException {
    // Check health
    if (this.getLastHealthState() != State.SERVICE_HEALTHY) {
      throw new ServiceFailedException(
          localTarget + " is not currently healthy. " +
          "Cannot be failover target");
    }
  }

  /**
   * @return an {@link HAServiceTarget} for the current active node
   * in the cluster, or null if no node is active.
   * @throws IOException if a ZK-related issue occurs
   * @throws InterruptedException if thread is interrupted 
   */
  private HAServiceTarget getCurrentActive()
      throws IOException, InterruptedException {
    synchronized (elector) {
      synchronized (this) {
        byte[] activeData;
        try {
          activeData = elector.getActiveData();
        } catch (ActiveNotFoundException e) {
          return null;
        } catch (KeeperException ke) {
          throw new IOException(
              "Unexpected ZooKeeper issue fetching active node info", ke);
        }
        
        HAServiceTarget oldActive = dataToTarget(activeData);
        return oldActive;
      }
    }
  }

  /**
   * Check the current state of the service, and join the election
   * if it should be in the election.
   */
  private void recheckElectability() {
    // Maintain lock ordering of elector -> ZKFC
    synchronized (elector) {
      synchronized (this) {
        boolean healthy = lastHealthState == State.SERVICE_HEALTHY;
    
        long remainingDelay = delayJoiningUntilNanotime - System.nanoTime(); 
        if (remainingDelay > 0) {
          if (healthy) {
            LOG.info("Would have joined master election, but this node is " +
                "prohibited from doing so for " +
                TimeUnit.NANOSECONDS.toMillis(remainingDelay) + " more ms");
          }
          scheduleRecheck(remainingDelay);
          return;
        }
    
        switch (lastHealthState) {
        case SERVICE_HEALTHY:
          elector.joinElection(targetToData(localTarget));
          if (quitElectionOnBadState) {
            quitElectionOnBadState = false;
          }
          break;
          
        case INITIALIZING:
          LOG.info("Ensuring that " + localTarget + " does not " +
              "participate in active master election");
          elector.quitElection(false);
          serviceState = HAServiceState.INITIALIZING;
          break;
    
        case SERVICE_UNHEALTHY:
        case SERVICE_NOT_RESPONDING:
          LOG.info("Quitting master election for " + localTarget +
              " and marking that fencing is necessary");
          elector.quitElection(true);
          serviceState = HAServiceState.INITIALIZING;
          break;
          
        case HEALTH_MONITOR_FAILED:
          fatalError("Health monitor failed!");
          break;
          
        default:
          throw new IllegalArgumentException("Unhandled state:" + lastHealthState);
        }
      }
    }
  }
  
  /**
   * Schedule a call to {@link #recheckElectability()} in the future.
   */
  private void scheduleRecheck(long whenNanos) {
    delayExecutor.schedule(
        new Runnable() {
          @Override
          public void run() {
            try {
              recheckElectability();
            } catch (Throwable t) {
              fatalError("Failed to recheck electability: " +
                  StringUtils.stringifyException(t));
            }
          }
        },
        whenNanos, TimeUnit.NANOSECONDS);
  }

  int serviceStateMismatchCount = 0;
  boolean quitElectionOnBadState = false;

  void verifyChangedServiceState(HAServiceState changedState) {
    synchronized (elector) {
      synchronized (this) {
        if (serviceState == HAServiceState.INITIALIZING) {
          if (quitElectionOnBadState) {
            LOG.debug("rechecking for electability from bad state");
            recheckElectability();
          }
          return;
        }
        if (changedState == serviceState) {
          serviceStateMismatchCount = 0;
          return;
        }
        if (serviceStateMismatchCount == 0) {
          // recheck one more time. As this might be due to parallel transition.
          serviceStateMismatchCount++;
          return;
        }
        // quit the election as the expected state and reported state
        // mismatches.
        LOG.error("Local service " + localTarget
            + " has changed the serviceState to " + changedState
            + ". Expected was " + serviceState
            + ". Quitting election marking fencing necessary.");
        delayJoiningUntilNanotime = System.nanoTime()
            + TimeUnit.MILLISECONDS.toNanos(1000);
        elector.quitElection(true);
        quitElectionOnBadState = true;
        serviceStateMismatchCount = 0;
        serviceState = HAServiceState.INITIALIZING;
      }
    }
  }

  /**
   * @return the last health state passed to the FC
   * by the HealthMonitor.
   */
  @VisibleForTesting
  synchronized State getLastHealthState() {
    return lastHealthState;
  }

  private synchronized void setLastHealthState(HealthMonitor.State newState) {
    LOG.info("Local service " + localTarget +
        " entered state: " + newState);
    lastHealthState = newState;
  }
  
  @VisibleForTesting
  ActiveStandbyElector getElectorForTests() {
    return elector;
  }
  
  @VisibleForTesting
  ZKFCRpcServer getRpcServerForTests() {
    return rpcServer;
  }

  /**
   * Callbacks from elector
   */
  class ElectorCallbacks implements ActiveStandbyElectorCallback {
    @Override
    public void becomeActive() throws ServiceFailedException {
      ZKFailoverController.this.becomeActive();
    }

    @Override
    public void becomeStandby() {
      ZKFailoverController.this.becomeStandby();
    }

    @Override
    public void enterNeutralMode() {
    }

    @Override
    public void notifyFatalError(String errorMessage) {
      fatalError(errorMessage);
    }

    @Override
    public void fenceOldActive(byte[] data) {
      ZKFailoverController.this.fenceOldActive(data);
    }
    
    @Override
    public String toString() {
      synchronized (ZKFailoverController.this) {
        return "Elector callbacks for " + localTarget;
      }
    }
  }
  
  /**
   * Callbacks from HealthMonitor
   */
  class HealthCallbacks implements HealthMonitor.Callback {
    @Override
    public void enteredState(HealthMonitor.State newState) {
      setLastHealthState(newState);
      recheckElectability();
    }
  }

  /**
   * Callbacks for HAServiceStatus
   */
  class ServiceStateCallBacks implements HealthMonitor.ServiceStateCallback {
    @Override
    public void reportServiceStatus(HAServiceStatus status) {
      verifyChangedServiceState(status.getState());
    }
  }

  private static class ActiveAttemptRecord {
    private final boolean succeeded;
    private final String status;
    private final long nanoTime;
    
    public ActiveAttemptRecord(boolean succeeded, String status) {
      this.succeeded = succeeded;
      this.status = status;
      this.nanoTime = System.nanoTime();
    }
  }

}
, bq. Answer 2.when start only one ZKFC and NN ，the NN can be staying in ACTIVE for long time.
Yes, fine.

Its because, patch from this issue, is not merged properly to your code.
In your source code, {{becomeActive()}} doesn't have line {{serviceState = HAServiceState.ACTIVE;}}.

{code}private synchronized void becomeActive() throws ServiceFailedException {
 LOG.info("Trying to make " + localTarget + " active...");
 try
 {
  HAServiceProtocolHelper.transitionToActive(localTarget.getProxy( conf, FailoverController.getRpcTimeoutToNewActive(conf)), createReqInfo());
  String msg = "Successfully transitioned " + localTarget + " to active state";
  LOG.info(msg);
  recordActiveAttempt(new ActiveAttemptRecord(true, msg));
 } catch (Throwable t) {
   String msg = "Couldn't make " + localTarget + " active";
   LOG.fatal(msg, t);
   recordActiveAttempt(new ActiveAttemptRecord(false, msg + "\n" +
     StringUtils.stringifyException(t)));
   if (t instanceof ServiceFailedException) {
      throw (ServiceFailedException)t; }
   else {
      throw new ServiceFailedException("Couldn't transition to active", t);
   }
/*
TODO:
we need to make sure that if we get fenced and then quickly restarted,
none of these calls will retry across the restart boundary
perhaps the solution is that, whenever the nn starts, it gets a unique
ID, and when we start becoming active, we record it, and then any future
calls use the same ID
*/
 }
}{code}


So if the previous state of NameNode is not STANDBY, then it will stay for long time. But if its trasitioned from STANDBY, it will continously switch.

Add {{serviceState = HAServiceState.ACTIVE;}} in {{becomeActive()}}  after {{LOG.info(msg);}}, everything will be fine., thank u very very much . i am so careless.]