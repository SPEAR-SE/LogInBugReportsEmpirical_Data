[bq. dfs.datanode.socket.write.timeout=0 in hadoop-site.xml help?
yes. that will bring the DataNode behavior back to 0.16., Before 0.17, DataNode did not close client socket on its own. So when DFSClient detects any socket error from a datanode, it marks it as dead and the list of 'deadnodes' is maintained for each open file. This persists through life of the the open file. So the deadnode will not be contacted even for other blocks. This policy will not work with 0.17 when there are slow clients. 
 
Apart from slow clients where DataNode closes the connection after an 8 minute write timeout, HADOOP-3633 in 0.17.2 introduced another (I think more likely to happen) case where client sees errors from a datanode : if it already has 256 transfers going on.

Couple of fairly simple fixes :

* 1. When client detects a connection failure after it read some bytes from a DataNode, it should just retry again with same datanode before moving on to next one. 
         *- This will fix the write-timeout problem as reported in this jira.
         *- Checksum errors will still be handled the same way as before.

* 2. Clear the 'deadnode' list when client moves to a new block.
         *- This will reduce the effect of HADOOP-3633 when there are a lot of clients.
         *- the larger issue  remains : When is a datanode really dead and when should it be retried after transient errors?

This jira mainly requires (1). We could postpone (2) until we get more experience with HADOOP-3633. Thoughts?
 
   
    , The attached patch implements (1) above.

When a read from a datanode fails, DFSClient first retries without marking the current datanode 'dead'. 

This patch also introduces a internal config variable "dfs.client.max.block.acuire.retries" so that the unit test does not need too many iterations. , A few comments:
1. Is it true that seekToBlockSource always return true? 
2. Is it better that seekToBlockSource returns the new node?
3. Should retryCurrentNode set to be true if seekToBlock seeks to a new node?
4. it seems that the code on lines 1485-1495 can be simplified as follows:
if (!retryCurrentNode) addToDeadNodes(currentNode);
seekToNewSource(pos);, Thanks for the review Hairong.

* 1. : yes. this is a shorter version of {{seekToNewSource()}}.. with a relaxation that it could seek to the same source.
* 2. : I don't see an advantage.. please let me know. I just kept the similarity to {{seekToNewSouce()}}.. since it is a related. 
* 3. : It is not set to true because, if client fails to set up a new blockReader after {{seekToNew/BlockSource()}} that implies the datanode is in a bad condition. So there is no need to retry it (same as  trunk). The main fix in this jira is that client retries if we already established a blockReader and connection fails after reading a few bytes. 
* 4 : First I thought the same! then noticed that we call {{seekTo_Block_Source(pos)}} when the condition is true.


, I probably missed something. I understand when it needs to retry. But In your patch, retryCurrentNode is set to be true only once in the very beginning. Should not it be set to be true whenever it seeks to a new node. In case IO fails when writing to this new node, the client can retry it again., Hairong, updated patch adds this comment. Does this help ?:


{code}[...]
     /* we retry current node only once. So this is set to true just here.
       * Intention is to handle one common case of an error that is not a
       * failure on datanode or client : when DataNode closes the connection
       * since client is idle. If there are other cases of "non-errors" then
       * then a datanode might be retried by setting this to true again.
       */
 {code}, I'm not sure whether this is the same issue or not, but on my 4 slave
cluster, setting the below parameter doesn't seem to fix the issue.

What I'm seeing is that occasionally data nodes stop responding for up to 10
minutes at a time. In this case, the TaskTrackers will mark the nodes as
dead, and occasionally the namenode will mark them as dead as well (you can
see the "Last Contact" time steadily increase for a random node at a time
every half hour or so.

This seems to be happening during times of high disk utilization.

Two more things I noticed that happen when the datanodes become unresponsive
(i.e. The "Last Contact" field on the namenode keeps increasing):

1. The datanode process seem to be completely hung for a while, including
its Jetty web interface, sometimes for over 10 minutes.

2. The task tracker on the same machine keeps humming along, sending regular
heartbeats

To me this looks like there is some sort of temporary deadlock in the
datanode that keeps it from responding to requests. Perhaps it's the block
report being generated ?, Please get a stack trace form the datanode when it is hung and see if any thread is hung on doing a "df" on the data disk partition., Right, Stefan is mostly hitting the "du" problem : HADOOP-3232 . Fixed in 0.18.0, +1 on the patch., +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12389872/HADOOP-3831.patch
  against trunk revision 694562.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3255/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3255/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3255/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3255/console

This message is automatically generated., I just committed this., Patch for 0.18. ]