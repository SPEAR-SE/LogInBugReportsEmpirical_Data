[[~xinkenny] could provide the whole code ? thanks . , [~Jack-Lee] I have upload more code above again, [~xinkenny] Thanks ,I will try to reproduce it ., inline stack of NPE
{code}
18/12/28 11:43:31 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 23)
java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1119)
	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)
	at com.xxx.algo.hadoop.Writer1.call(Writer1.java:68)
	at com.xxx.algo.hadoop.Writer1.call(Writer1.java:34)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:219)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

And
{code}

2018-11-01 20:55:20 ERROR Executor:91 - Exception in task 0.0 in stage 11.0 (TID 11)
java.io.IOException: Not supported
	at org.apache.hadoop.fs.ChecksumFileSystem.append(ChecksumFileSystem.java:357)
	at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1131)
	at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1511)
	at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:277)
	at com.xxx.algo.hadoop.Writer1.call(Writer1.java:68)
	at com.xxx.algo.hadoop.Writer1.call(Writer1.java:34)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:219)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}
, 
The block UnsupportedOperationException is being raised because {{appendIfExists}} needs to append, and the FS in use doesn't support it. You aren't writing to HDFS, are you? file:??

The NPE is just HADOOP-13138; been fixed for a long time. Upgrading your Hadoop dependencies will make it go away. Closing as a duplicate. , * my default fs maybe file。
 * I try to change my project maven dependency to hadoop2.8.0. but also cause NPE.
 * I will change my cluster environment to hadoop2.8.0 for test later.]