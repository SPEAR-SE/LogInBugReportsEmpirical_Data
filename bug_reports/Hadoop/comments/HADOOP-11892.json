[Thanks Arun for creating this. I'm trying to see if my understanding is correct.

Per my understanding, the main concern here is that, some secure random implementation opens file descriptor /dev/urandom (e.g. {{OsSecureRandom}}).

For each call to {{CryptoCodec#getInstance}}, a new {{CryptoCodec}} object is created. If that happens to be {{OpensslAesCtrCryptoCodec}}, then at its {{setConf}} method, {{Random}} object is created. If this happens to be {{OsSecureRandom}}, then we open a fd. With too many of those calls we would end up with too many open fd's.

HADOOP-11891 fixes this in some sense by lazily filling the Reservoir. This jira would also alleviate the problem by adding caches to the {{CryptoCodec}} object, which in turn caches the {{Random}} object.

If any of this happens again, we should check from jstack to see exactly how/where the fd's are opened.

Do I understand this correctly? Thanks!, [~xiaochen] You got it right. And you can also use lsof /dev/urandom to list open fds.
However, I have a different opinion on the fix. When we typically see problems with too many open fds to /dev/urandom, it's not the cost of instantiating a new instance of {{CryptoCodec}} that is expensive; it is generating random numbers that is expensive. The problem is when available entropy is depleted, reading from /dev/urandom could block, and what happens is you would end up seeing many open fds, because they are all waiting to read from /dev/urandom.

IMO, fixing this isn't really useful., As a correction, /dev/urandom doesn't block. Concerns about running out of entropy are often exaggerated, since you only need a few bits of entropy to generate a lot of randomness, and only at system startup. See: http://www.2uo.de/myths-about-urandom/, Thanks for the comments Wei-Chiu and Andrew.
bq. And you can also use lsof /dev/urandom to list open fds.
Yes, we can get the process opening /dev/urandom using {{lsof}}. But for details on how those fds are kept open, I think we'd need to check jstack as well to help diagnose. 

bq. IMO, fixing this isn't really useful.
Fair point. As Andrew pointed out, /dev/urandom doesn't block. But we did see the process has thousands of /dev/urandom fd open, which indicates something else. I feel this jira could improve the situation in some degree.
OTOH, so far the occurrence of this issue are all without HADOOP-11891, so it's hard to tell whether there are other issues left (e.g. some rare case leaking etc.). If we don't feel much value of this jira, maybe we could revisit this if we see any reproduction with the existence of HADOOP-11891, and analyze the jstack then.]