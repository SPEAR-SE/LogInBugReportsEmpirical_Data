[GitHub user apetresc opened a pull request:

    https://github.com/apache/hadoop/pull/100

    HADOOP-13278. S3AFileSystem mkdirs does not need to validate parent path components

    According to S3 semantics, there is no conflict if a bucket contains a key named `a/b` and also a directory named `a/b/c`. "Directories" in S3 are, after all, nothing but prefixes.
    
    However, the `mkdirs` call in `S3AFileSystem` does go out of its way to traverse every parent path component for the directory it's trying to create, making sure there's no file with that name. This is suboptimal for three main reasons:
    
     * Wasted API calls, since the client is getting metadata for each path component 
     * This can cause *major* problems with buckets whose permissions are being managed by IAM, where access may not be granted to the root bucket, but only to some prefix. When you call `mkdirs`, even on a prefix that you have access to, the traversal up the path will cause you to eventually hit the root bucket, which will fail with a 403 - even though the directory creation call would have succeeded.
     * Some people might actually have a file that matches some other file's prefix... I can't see why they would want to do that, but it's not against S3's rules.
    
    [I've opened a ticket](https://issues.apache.org/jira/browse/HADOOP-13278) on the Hadoop JIRA. This  pull request is a simple patch that just removes this portion of the check. I have tested it with my team's instance of Spark + Luigi, and can confirm it works, and resolves the aforementioned permissions issue for a bucket on which we only had prefix access.
    
    This is my first ticket/pull request against Hadoop, so let me know if I'm not following some convention properly :)

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rubikloud/hadoop s3a-root-path-components

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hadoop/pull/100.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #100
    
----
commit 8a28062d34e5f0c0b83a9577dc9d818bab58c269
Author: Adrian Petrescu <apetresc@gmail.com>
Date:   2016-06-15T14:15:21Z

    No need to check parent path components when creating a directory.
    
    Given S3 semantics, there's actually no problem with having a/b/c be a prefix even if
    a/b or a is already a file. So there's no need to check for it - it wastes API calls
    and can lead to problems with access control if the caller only has permissions
    starting at some prefix.

----
, bq. This is my first ticket/pull request against Hadoop, so let me know if I'm not following some convention properly 

[~apetresc], thank you very much for participating in Apache Hadoop!.  Please see our [HowToContribute|https://wiki.apache.org/hadoop/HowToContribute] wiki page for more details on how the contribution process works.  If you're interested in working on S3A, then also please pay particular attention to the section on [submitting patches against object store|https://wiki.apache.org/hadoop/HowToContribute#Submitting_patches_against_object_stores_such_as_Amazon_S3.2C_OpenStack_Swift_and_Microsoft_Azure].  That section discusses our requirements for integration testing of patches against the back-end services (S3, Azure Storage, etc.).

I understand the motivation for the proposed change, but I have to vote -1, because it would violate the semantics required of a Hadoop-compatible file system.  The patch would allow a directory to be created as a descendant of a file, which works against expectations of applications in the Hadoop ecosystem.  More concretely, the patch causes a test failure in {{TestS3AContractMkdir#testMkdirOverParentFile}}, which tests for exactly this condition.  (See below.)

However, you might be interested to know that there is a lot of other work in progress on hardening and optimizing S3A.  This is tracked in issues HADOOP-11694 and HADOOP-13204, and their sub-tasks.

{code}
Running org.apache.hadoop.fs.contract.s3a.TestS3AContractMkdir
Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 11.502 sec <<< FAILURE! - in org.apache.hadoop.fs.contract.s3a.TestS3AContractMkdir
testMkdirOverParentFile(org.apache.hadoop.fs.contract.s3a.TestS3AContractMkdir)  Time elapsed: 1.924 sec  <<< FAILURE!
java.lang.AssertionError: mkdirs did not fail over a file but returned true; ls s3a://cnauroth-test-aws-s3a/test/testMkdirOverParentFile[00] S3AFileStatus{path=s3a://cnauroth-test-aws-s3a/test/testMkdirOverParentFile; isDirectory=false; length=1024; replication=1; blocksize=33554432; modification_time=1466026655000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} isEmptyDirectory=false

	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.fs.contract.AbstractContractMkdirTest.testMkdirOverParentFile(AbstractContractMkdirTest.java:95)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}
, Adrian: I recognise your disappointment here, and the cost of those checks.

# it may be possible to do some aggregation of the checks, that is, if there was a way to do a single listObjects with a pattern of the entire path,then perhaps the parent path scan could be O(1), after which the rejection of operations where a parent is a file would be straightforward. 

Actually, all we need to do is check for the pure file paths "/a", "a/b"; the fake directories "/a/", "/a/b/" are allowed, and after the creation an async deleteObjects call could clean them up, HADOOP-13222. That is something I could see working. It would still be O(path-elements), but the maximum number of operations would be reduced by a third., BTW, can you make that root bucket problem a separate issue? That sounds an independent problem, Hey [~cnauroth] and [~stevel@apache.org]; thank you for the very detailed explanations. I can totally understand that Hadoop-compatible filesystems may have additional semantics beyond just those of the underlying object store, which this change would violate.

I do want to note, though, that the main motivation behind my patch was the permissions-related issue it was solving, not the minor performance improvement. That issue still exists, and I'll file a new issue for it as [~stevel@apache.org] recommended, but first I just want to understand whether it is even possible/desirable for S3A to support buckets which have those kinds of IAM roles applied.

Consider a bucket with a policy like the one below (which is a pretty common setup):
{code}
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "prodS3EnvironmentData",
            "Effect": "Allow",
            "Action": [
                "s3:*"
            ],
            "Resource": [
                "arn:aws:s3:::mybucket/a/b/c/*"
            ]
        }
    ]
}
{code}

Given what I now understand about the S3A contract, it seems like you simply cannot layer an S3A filesystem on this bucket, even if it was "rooted" in directory {{a/b/c}}, since any directory, say {{a/b/c/d}} could never validate whether a file named {{a/b}} exists - it would throw a 403.

Is this correct? If so, I'm not sure a separate issue is needed; the use case would simply be unsupported and I'll have to move my S3A filesystem to a bucket that grants Hadoop/Spark root access., [~apetresc], I admit I hadn't considered interaction with IAM policies before, but I definitely see how this could be useful, and it's interesting to think about it.  Unfortunately, I don't see a viable way to satisfy the full range of possible authorization requirements that users have come to expect from a file system.

For the specific case that we started talking about here (walking up the ancestry to verify that there are no pre-existing files), it might work if that policy was changed slightly, so that the user was granted full access to /a/b/c/\*, and also granted read-only access to /\*.  I expect read access would be sufficient for the ancestry-checking logic.  Of course, if you also want to block read access to /, then this policy wouldn't satisfy the requirement.  It would only block write access on /.

Another consideration is handling of what we call a "fake directory", which is a pure metadata object used to indicate the presence of an empty directory.  For example, consider an administrator allocating a bucket, bootstrapping the initial /a/b/c directory structure by running mkdir, and then applying the policy I described above.  At this point, S3A has persisted /a/b/c to the bucket as what we call a "fake directory", which is a pure metadata object that indicates the presence of an empty directory.  After the first file put, say /a/b/c/d, S3A no longer needs that pure metadata object to indicate the presence of the directory.  Instead, the directory exists implicitly via the existence of the file /a/b/c/d.  At that point, S3A would clean up the fake directory by deleting /a/b/c.  That implies the user would need to be granted delete access to /a/b/c itself, not just /a/b/c/*.  Now if we further consider the user deleting /a/b/c/d after that, then S3A needs to recreate the fake directory at /a/b/c, so the user is going to need put access on /a/b/c.

bq. Is this correct? If so, I'm not sure a separate issue is needed; the use case would simply be unsupported and I'll have to move my S3A filesystem to a bucket that grants Hadoop/Spark root access.

Definitely the typical usage is to dedicate the whole bucket to persistence of a single S3A file system, with the understanding of the authorization limitations that come with that.  Anyone who has credentials to access the bucket effectively has full access to that whole file system.  This is a known limitation, and it's common to other object store file systems like WASB too.  I'm not aware of anyone trying to use IAM policies to restrict access to a sub-tree.  Certainly it's not something we actively test within the project right now, so in that sense, it's unsupported and you'd be treading new ground., ..or 
-an FS could have some parent limit which it wouldn't try to recurse up beyond.
-the special case of permission denied is somehow recognised as an IAM issue and skipped. Problem: how to do that?, I'm thinking we need to be able to recognise the problem of IAM permissions blocking read/write of a parent path, and simply stopping work there. One issue, now that we are doing all parent dir deletes as a single DELETE call, we'll need to recognise a partial failure of the operation due to security checks as a successful operation., + when you start using SSE-C & things, you need to handle decryption problems, otherwise SSE-C created mock dirs can't coexist with any other SSE-* encryption, HADOOP-15176 handles the IAM write problem, where the caller doesn't have the permission to create/delete parent directory markers. That addresses the situation where a client has restricted write access.

I don't think we can handle restricted read access, as it it at odds with the normal world view of a filesystem and so much could break., Linking to HADOOP-15220.

FWIW, Hadoop 3.1 has more IAM role support and handling of failed writes up the tree. Failing reads isn't something that is coped with there, which is probably of interest to [~fabbri]. If we do want to handle that situation, it'll cover more than just mkdirs though; I can imagine delete() & the scan for mock directories after a PUT needing coverage, Moving to branch-3.3

As noted, we do want to check up the path, so the current PR isn't going to work. The one thing we can do is handle permissions 

# during that walk up the tree an {{AccessDeniedException}} is raised, that can be caught and used to indicate that  "you can't do anything up there", and the mkdirs simply assumes that all is good.
# will need a test in org.apache.hadoop.fs.s3a.auth.ITestAssumeRole which creates a role with the restricted permissions (skipped if s3guard is enabled, BTW), and then verifies that the mkdirs(/a/b/c) fails even as getFileStatus("a") fails because A is blocked. 

Not got time to work on this; postponing to 3.3+, contributions *with that test* welcome.]