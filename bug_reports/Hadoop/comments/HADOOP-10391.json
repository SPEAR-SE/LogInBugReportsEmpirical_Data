[Let's also add a unit test for "none" as part of fixing this., Here is the stack trace seen since incorporating HADOOP-10211.  Thank you to [~dbalykin] and [~rramya] for reporting the bug.

{code}
2014-03-06 14:03:08,930 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.IllegalArgumentException: No enum const class org.apache.hadoop.security.SaslRpcServer$QualityOfProtection.NONE
	at java.lang.Enum.valueOf(Enum.java:196)
	at org.apache.hadoop.security.SaslRpcServer$QualityOfProtection.valueOf(SaslRpcServer.java:74)
	at org.apache.hadoop.security.SaslRpcServer.init(SaslRpcServer.java:187)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2156)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:897)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:505)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:480)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:742)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:296)
{code}
, Thanks for reporting this Chris. The change in behavior was by design. However it should be fine to add a special case for "none".

+[~benoyantony]., Agreed - since I was the one who asked Benoy to enforce valid values. :)  With the former defaulting unknown values to auth, it meant a typo would silently downgrade security from what was intended.

Rather than bend hadoop to conform to an invalid setting that "worked", wouldn't it be easier to just fix the conf?  It's a bit of a rabbit hole to add "compatibility" for specific cases of bad behavior.  You should be able to just remove the config key on the insecure clusters., I'll submit a patch soon., Just reviewed [~daryn]'s comments.  Please let me know if I need to add the patch., Let me think this over some more before we patch anything.  The comments from Arpit and Daryn make a lot of sense.  Thanks, everyone!, I'm resolving this as won't fix.  I agree with the comments from Arpit and Daryn that the stricter validation introduced in HADOOP-10211 is a good thing.  I agree that the old behavior was a bug, so there is less onus to support it from a backwards-compatibility perspective.

I do think it's important to publish a release note, so I added that to HADOOP-10211.  Could someone who worked the original issue please review what I entered?  Feel free to make changes to the content as you see fit.

Thank you, everyone, and sorry for the distraction.
, Chris, I apologize for missing your request to review the release note. It looks great to me, thanks for reporting the potential app-compat issue and adding the release note.]