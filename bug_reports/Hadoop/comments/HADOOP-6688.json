[Could this be a consistency issue, like HADOOP-6208? 

I'm not sure a blanket ignore of FileNotFoundExceptions is quite right: if you call delete with a path that doesn't exist should it not throw FileNotFoundException? I can see that if a file in a subdirectory of the path being deleted doesn't exist then that should not result in a FileNotFoundException. You might want to check if the new FileContext API exhibits the same problem, so it can be considered for the contract there too.

I don't think this is a blocker. As a workaround you could create a wrapped FileOutputCommitter that catches and ignores FileNotFoundException., Tom, this is a classic issue of coding "for the programmer" or "against the programmer".

What is the expected effect of calling FileSystem.delete(someFile)? A reasonable answer would be "someFile no longer exists", and the method should throw an exception only if that expected effect somehow did not happen, meaning the file is there.

For those unlikely users who care whether the file was actually deleted or wasn't there to begin with, you can have FileSystem.delete() return a boolean to convey this information. This is common practice in many file system APIs.

I have no idea as to the root cause of the file not existing. It might be a similar consistency issue to the one you mentioned - I have not looked into it. Since fixing FileSystem.delete is rather easy (and the change is very contained), I think it's a better solution than working around the issue., I think this is related to the discussion in HADOOP-6631 about having a 'force' option for delete. I didn't mean to suggest that this issue shouldn't be fixed, I was merely pointing out a workaround that might solve your problem in the meantime, while the filesystem semantics are discussed in more detail., While going over the code to suggest a patch, I found out this was partially fixed as part of HADOOP-6201: S3FileSystem.delete now catches FileNotFoundException and simply returns false, as opposed to propagating the exception as described in the issue.

The reason I'm calling this a partial fix is that such recursive directory delete will stop (returning false) the moment the above issue happens. The proper behavior, in my opinion, is to 1) continue deleting files 2) return false only if the top-most directory could not be found., I agree with Danny -it should be a skip. If a file/subdir has gone away during a delete, that is not a failure, as the outcome of the delete operation "the subtree is deleted" is still the same.

The lack of atomicity on recursive delete on blobstores is more dangerous if a recursive delete commences while a file is being written to a path underneath it takes place. It may be that the newly created file is created, while the delete removes the entries that represent the directories above it., # the original s3:// impl is gone in trunk
# s3a does check for this, and we have tests. 

For that reason, I'm going to close as a cannot reproduce. But Yes, Danny is correct: delete() must never raise FNFE.]