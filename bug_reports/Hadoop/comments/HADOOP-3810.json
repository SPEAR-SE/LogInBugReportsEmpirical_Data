[I posted an alternate idea on HADOOP-1938: removing full nodes from the datanode network topology (i.e. rack map)., > Once I have more details about a NameNode that was in this state

Still not sure what is actually affecting NN badly. The solution in description of the jira assumes that it is the excessive CPU for allocating the blocks that is causing this problem. But on a recent incident, ti does not look like there are many blocks being written. In fact at one point everything pretty much comes to halt. Don't have gc.log to see if this was a GC. 

I will attach couple of simon graphs that show what was going on during this time.
, The attached simon graph shows a few stats between 19:00 till 00:00. The namenode was restarted around 23:07.  There is not a lot write traffic. The fields plotted are :

- block written per sec
- num of heartbeats
- num of threads blocked
- gc/min (what does this mean?)
- gctime%

, The recent suspected incidence of this problem showed that NN failed to find a node on some racks only around 2000 attempts. That would account for no more than a few seconds of CPU time. I suspect the fix I proposed in the description does not actually fix or reduce this problem. , 
another minor fix : every node considered invokes 'getTotalLoad' which obtains heartbeat lock. We should remove this lock (either with a volatile, or just accepting a slightly stale value).
, > That would account for no more than a few seconds of CPU time.
This may not be a problem for file creation, but it causes problem for replicating under-replicated blocks. ReplicationMonitor could replicate up to 2*cluster_size blocks in an iteration. During each iteration, it holds the global lock. No wonder NameNode is not responsive!, This patch aims to improve NN responsiveness when a cluster is near full. It identified one cause of the problem is that ReplicationMonitor holds the global lock while replicating blocks in one iteration. So this patch made the following change:
1. does not allow ReplicationMonitor to hold the fsnamesystem global lock while replicating block in one iteration. Previously the logic is like:
{code}
synchronized computeReplicationWork( int blocksToProcess) {
  for (int i=0; i<blocksToProcess; i++) {
    select one block from under-replicated queue;
    Compute replication work for the block;
  }
}
{code}
Now it is changed to be:
{code}
computeReplicationWork(int blocksToProcess) {
  (synchronized) select blocksToProcess under-replicated blocks;
  for each selected block {
    (synchronized)compute replication work for the block;
  }
}
{code}
2. While computing replication work for a block, releases the global lock while choosing targets because this is the most computation intensive part., I did an experiment on a full cluster with 3000 data nodes and 1000 under-replicated blocks. Without the patch, the cluster is not responsive to any dfs commands. With the patch, it responds to commands immediately., Patch looks good. 

# The second computeReplicationWork() could be renamed.. may be something like scheduleReplicationForBlock()..
# The synchronized() blocks in above function should also synchronize on neededReplications (just like before the patch). 
   ** this also makes indentation match better with earlier indentation.
, Thanks Raghu for taking time to review the patch.

I renamed the second method as computeReplicationWorkForBlock. I agree that scheduleReplication is a better name than computeReplicationWork. But to keep names consistent, I still use computeReplicationWork.

The patch incorporates the  second review comment. As a matter of fact, holding global lock is good enough to access neededReplications queue, but it is a good suggestion to keep the previous behavior., +1., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12404288/globalLock1.patch
  against trunk revision 762509.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    -1 Eclipse classpath. The patch causes the Eclipse classpath to differ from the contents of the lib directories.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/157/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/157/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/157/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/157/console

This message is automatically generated., -1 Eclipse classpath is not related to this patch. The patch is manually tested., I've committed this!, I tested the patch on a cluster of 61 nodes. One node ran NameNode, and each of the other 60 nodes for ran a datanode cluster, each of which ran 50 simulated datanodes.
1.	Apply the patch on HADOOP-5556 to the trunk.
2.	Install hadoop.
3.	Configure hadoop: set dfs.block.size to be 10 and dfs.datanode.simulateddatastorage.capacity to be 10. 
4.	Create an edit log in the pre-configured name directory. The edit log with 3000 files, each of which has one block with a replication factor of 3.
Bin/hadoop org.apache.hadoop.hdfs.server.namenode.CreateEditsLog -f 3000 0 1 -r 3 -d dfs_name_dir
5.	Start NameNode using the created edit log
6.	Start DataNodeCluster on each of the 60 nodes with the following parameter: -n 50 -simulated -r 1 -inject $startBlock 1. $startBlock should be 0, 50, 100, ... on each node.

Now the cluster is up. The cluster is full but all 3000 blocks are under-replicated. With the patch, you can type any dfs command and get the response back immediately. Without the patch, shell commands hang.]