[This patch eliminates the use of split.idx.  Instead, get the information directly from the file system., +1

http://issues.apache.org/jira/secure/attachment/12360767/eliminate-split-idx.patch applied and successfully tested against trunk revision r551725.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/344/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/344/console, This doesn't work, because it doesn't preserve the offsets of the task logs as the older parts are deleted. One of the requirements for the task logs is that you can read them as they are generated. With this patch, the offsets will constantly be changing as old splits are deleted and thus prevent users from tailing the log.

This bug clearly does need to be fixed however..., bq. Why is there a split.idx file?

The index file is maintained to aid in tailing the log files (4KB / 8KB tail window), and hence it cannot be eliminated.

bq. This bug clearly does need to be fixed however...

No question.
One option is to just dump the contents of part-* files when there is no index file to read or when there is no information present in the index file... thoughts?, > One option is to just dump the contents of part-* files when there is no index file to read or when there is no information present in the index file... thoughts?

I assume you mean that we should compare the index with the file system, and do the right thing if a file is missing from the index.

That seems like a reasonable solution.
, > With this patch, the offsets will constantly be changing as old splits are deleted and thus prevent users from tailing the log.

Actually, the patch doesn't affect tailing the log, as the offsets from the end of the log will be preserved (since the filesystem knows their lengths).
, I believe you are missing the point. The older splits are deleted to limit the size of the task logs. This means that you can't use their lengths to compute offsets because they aren't there any more. 

I'd propose an approach where the index file looks like:

file|offset

dropping the length, so that the index can be written when the new split is started. This will preserve the current functionality and fix the problem, I believe., {quote}
I'd propose an approach where the index file looks like:

file|offset
{quote}

+1, So, why do we need offsets from the beginning of the logs?  It seems to me that we only need offsets from the end of the log, which doesn't require an index file at all.  

The only additional information the index file gives is the length of the deleted logs.
, That is because the tasklog.jsp supports reading from an offset with the intention of tailing the logs. If you don't keep the length of the deleted logs you can't tail because the offsets would change with every deletion., Ah, I see.  In that case, your solution sounds good., this patch writes to the split.idx file when a new log file is created, and does not store file lengths in the split.idx file., -1, build or testing failed

2 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12361700/accelerate-task-log.patch against trunk revision r555697.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/399/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/399/console

Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., Hudson failure looks unrelated to this patch.  Toggling status to see if I can make it work..., -1, build or testing failed

2 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12361700/accelerate-task-log.patch against trunk revision r555813.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/405/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/405/console

Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., My patch shouldn't affect the fs code at all.  I have no idea what is failing or why.  I have personally tried my patch on a two-node cluster without issue., This looks good., I just committed this.  Thanks, Michael!]