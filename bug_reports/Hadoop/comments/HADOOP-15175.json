[*Log From the DN*

{noformat}
2017-09-18 01:54:15,762 | ERROR | DataNode: [[[DISK]file:/srv/BigData/hadoop/data5/dn/, [DISK]file:/srv/BigData/hadoop/data6/dn/, [DISK]file:/srv/BigData/hadoop/data7/dn/, [DISK]file:/srv/BigData/hadoop/data8/dn/, [DISK]file:/srv/BigData/hadoop/data11/dn/, [DISK]file:/srv/BigData/hadoop/data2/dn/, [DISK]file:/srv/BigData/hadoop/data3/dn/, [DISK]file:/srv/BigData/hadoop/data4/dn/, [DISK]file:/srv/BigData/hadoop/data1/dn/, [DISK]file:/srv/BigData/hadoop/data9/dn/, [DISK]file:/srv/BigData/hadoop/data10/dn/]] heartbeating to X-Y-Z-1/X.Y.Z.1:25000 | Initialization failed for Block pool BP-141158008-X.Y.Y.1-1504865702183 (Datanode Uuid 4d9717af-7d34-4cfb-9c24-8e7e0e12d8e8) service to X-Y-Z-1/X.Y.Z.1:25000 Datanode denied communication with namenode because hostname cannot be resolved (ip=X.Y.Y.100, hostname=X.Y.Y.100): DatanodeRegistration(X.Y.Y.1:25009, datanodeUuid=4d9717af-7d34-4cfb-9c24-8e7e0e12d8e8, infoPort=0, infoSecurePort=25011, ipcPort=25008, storageInfo=lv=-56;cid=myhacluster;nsid=1749012596;c=1504865702183)
	at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:945)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerDatanode(FSNamesystem.java:4503)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.registerDatanode(NameNodeRpcServer.java:1419)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.registerDatanode(DatanodeProtocolServerSideTranslatorPB.java:96)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:29308)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:973)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2163)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2159)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1737)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2157)
 | BPServiceActor.java:697
{noformat}

Thinking like below while setup the connection 

at org.apache.hadoop.ipc.Client.Connection#setupConnection

1) Instead of assigning the {{null}} to {{bindAddr}} [here|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java#L665] can we update with like below..?
{code}
          InetSocketAddress bindAddr= new InetSocketAddress(InetAddress.getLocalHost(),0);
{code}
2) Introduce one config for {{bindAddr}}..?

[~kihwal]/[~daryn] any thoughts on this as you worked on HADOOP-14578..? Please correct me if I am wrong.

, For starters, the exception has nothing to do with kerberos.  You will get the same exception regardless of the security setting.

bq. Configure principal without hostname (i.e hdfs/hadoop@HADOOP.com)
That principal does have a hostname.  Did you mean hdfs@HADOOP.com?

bq. Configure floatingIP
Is floating ip a dynamic dhcp address?  If yes, is this for testing? Anyway, I need to understand more about X.Y.Y.1 and X.Y.Y.100.  I'm assuming X.Y.Y.1 is localhost/127.0.0.1?  X.Y.Y.100 is the dhcp assigned address?

Here's the problem with your proposal:  "getLocalHost" will attempt to resolve the system assigned hostname, which you are assuming will always be in /etc/hosts, but falls back to localhost.  So let's say that the hostname doesn't resolve though – intentional or not – which forces a bind to localhost which isn't going to be able to connect to anything external.  Might work well for testing on a single node cluster when it doesn't matter if the host uses its public interface or localhost to connect to itself, but not for general use.

I'd rather see the NN squawking about unresolvable addresses than DNs silently failing to connect because they are binding to localhost.  It would also cripple other rpc clients whose servers don't care about dns at all.

I also not keen on having more confs w/o having a clearly stated use case., [~daryn] thanks a lot for taking a look into this issue.

bq.That principal does have a hostname. Did you mean hdfs@HADOOP.com?
Supposed to tell,{{hostname}} which is not resolvable i.e hdfs/hadoop.hadoop.com@HADOOP.com. Here hadoop.hadoop.com is n't configured as hostname
bq.Is floating ip a dynamic dhcp address? If yes, is this for testing? Anyway, I need to understand more about X.Y.Y.1 and X.Y.Y.100. I'm assuming X.Y.Y.1 is localhost/127.0.0.1? X.Y.Y.100 is the dhcp assigned address?
Floating IP configured for HA purpose.Such that Active and standby can use same IP to access webUI.*Here datanode registered with X.Y.Y.1 and X.Y.Y.100 is floating IP which is used for further communication which will fail.*
bq.Here's the problem with your proposal: "getLocalHost" will attempt to resolve the system assigned hostname, which you are assuming will always be in /etc/hosts,
I was in assumption of host mapping will be configured for hadoop cluster. 
Ok, then we can make it configurable..?, bq.  Floating IP configured for HA purpose.Such that Active and standby can use same IP to access webUI.
bq. Here datanode registered with X.Y.Y.1 and X.Y.Y.100 is floating IP which is used for further communication which will fail.

Ok.  You're using ip failover, like we also do.  The stack trace shows that:
# The NN is "X.Y.Z.1".
# The NN sees the DN connection from "X.Y.Y.100"
# The DN's registration is self-identifying as "X.Y.Y.1"
# The DN's BP id contains "X.Y.Y.1" which should be the NN's ip...

This tells me the DN is running on the NN.  "X.Y.Z.1" doesn't exist.

The floating ip "X.Y.Y.100" is the interface ip, "X.Y.Y.1" is ip aliased to it which is why non-explicitly bound connections from the DN originate from "X.Y.Y.100".  (In practice you'll probably want the opposite, ie. your floating ip to be aliased, which would incidentally "fix" your issue)

Binding to loopback per the proposal only works when the DN is running on the NN.  It won't work if the hosts are different.

{quote}
bq. I also not keen on having more confs w/o having a clearly stated use case.
Ok, then we can make it configurable..?
{quote}

No.  This appears to be a self-inflicted injury in perhaps a test environment.  I recommend either switching your interface and aliased ips, or set {{dfs.namenode.datanode.registration.ip-hostname-check=false}}., [~daryn] thanks a lot. you are correct. I tried both the ways,it's worked., bq.I recommend either switching your interface and aliased ips
Looks It's not possible in our environment. It used for another components also.
bq. set dfs.namenode.datanode.registration.ip-hostname-check=false.
This can aviod the ERROR, but it will overide the [folating IP|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java#L1025] hence clients will get this IP from Namenode but Datanode will listen on another IP?


I feel, when principal having hostname which can't resolvable (i.e bindAddr = null),we can have one config like below..? This can be configured DualIP machines.

{code}
          if (bindAddr == null) {
            String bindAddrIp = conf.get(
                CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_LOCAL_BIND_IP_KEY);
            if (bindAddrIp != null && !bindAddrIp.isEmpty()) {
              bindAddr = new InetSocketAddress(bindAddrIp, 0);
            }
          }
          
{code}, Attaching patch with above approach.Kindly review., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 28s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 52s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 35s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 38s{color} | {color:orange} hadoop-common-project/hadoop-common: The patch generated 3 new + 212 unchanged - 0 fixed = 215 total (was 212) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m  2s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 46s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 41s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 80m 21s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:ca8ddc6 |
| JIRA Issue | HDFS-12532 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12893880/HDFS-12532.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 98c4242c40f7 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d7f3737 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/21811/artifact/patchprocess/diff-checkstyle-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21811/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21811/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, This is very unorthodox use case that I'm not convinced is worth "fixing".  DN is running on the NN.  No dns.  DN principal contains a hostname the node itself cannot resolve.  This makes no sense because:
# I don't think kerberos will even work without dns and/or /etc/hosts entries.
# The NN performs principal hostname authorization for the DN protocol.  That should fail unless also turned off?
# Spnego doesn't work with ips which means the HA NNs shouldn't be able to communicate.
# The host file reader for the allow/deny ignores unresolvable addresses.

I feel the stated problem has been over generalized.  What is the specific issue you encountered?  A transient misconfigured dns entry or /etc/hosts entry?, Scenario:
* In this usecase, Kerberos principal doesnt contain FQDN for service principals, all service principals will be in generic forms. i.e. hdfs/hadoop@HADOOP.COM, 'hadoop' is not really mapping to any host. (Except for HTTP principals to work with SPNEGO, in which FQDN is used.)
* NN and DNs are in different machines.
* DN machine have floating IP along with real interface. Both interfaces uses IPs of same segment. (This floating IP of same segment is for some other usecase of customer)

* Example: 
*#  NN is in 192.168.132.5
*# DN1 is in 192.168.132.10 and all configurations have 192.168.132.10
*# DN1 machine also have floating ip with 192.168.132.188
*# There is no DNS entry for 192.168.132.188 in NN.
*# Since DN's principal cannot resolve to a hostname, client binding address will be null and selects random binding address. In this case it selects floating ip 192.168.132.188
*# Now, DN1 carries 192.168.132.10 in Dn ID, but RPC request itself will be sent using floating IP 192.168.132.188 to NN.
*# NN throws back exception saying name resolution fails.


Disabling the strict ip-hostname check {{dfs.namenode.datanode.registration.ip-hostname-check=false}} will just avoid exception, but it registers the Datanode with floating IP itself (registerDatanode() overwrites the DnID with floating IP). 
This makes clients' requests to fail while trying to connect to DN (because all listening ports are in actual IP (192.168.132.10) in DN).


So currently there are following ways to solve this case.
# Use FQDN in kerberos principal. This needs major change in deployment mechanism.
# Disable Floating IP. This affects customers' other usecase.
# Provide configuration to specify client side local-bind address for RPC requests from dual IP machines.

Since #1 and #2 are not feasible in our case, #3 looks feasible and doesn't affect any existing behavior., I see.  You want to sacrifice security for convenience: using a single principal and keytab for all nodes instead of unique principals for the nodes.  If yes, I hope your customer specifically requested this insecure setup, or has been informed this misuse of kerberos will seriously degrade security.

While I do not condone this setup, you should be able to append the junk service host "hadoop"  to the line in /etc/hosts corresponding to the interface you want to use.  Or make the DN listen on 0.0.0.0., bq.  see. You want to sacrifice security for convenience: using a single principal and keytab for all nodes instead of unique principals for the nodes. If yes, I hope your customer specifically requested this insecure setup, or has been informed this misuse of kerberos will seriously degrade security.
I agree that, setup may not be fully complying to kerberos standards.
But same issue is applicable for non-secure setup with dual IP, in which case client should be able to specify local-bind IP.

If you still feel that, adding an optional configuration to specify client bind address is 'worthless' then we can go ahead and close this ticket., FYI, Usecase for the floating IP is to provide HA for a Service. Active service will bind to floating IP.
So, using floating IP in Non-Secure setup also may still lead to same problem. which is a valid use-case IMO., [~daryn] making configurable is make sense for you..? As [~vinayrpet],mentioned issue can come with non-secure cluster for dualIP machines..?, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  5s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m 39s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 31s{color} | {color:orange} hadoop-common-project/hadoop-common: The patch generated 3 new + 212 unchanged - 0 fixed = 215 total (was 212) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  8m 25s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 12s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 76m 30s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem |
|   | hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-12532 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12893880/HDFS-12532.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux ab9d86a3b893 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 7fe6f83 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/22527/artifact/out/diff-checkstyle-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22527/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22527/testReport/ |
| Max. process+thread count | 1426 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22527/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 38s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 19s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m 14s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 39s{color} | {color:orange} hadoop-common-project/hadoop-common: The patch generated 3 new + 212 unchanged - 0 fixed = 215 total (was 212) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 10s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m 14s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 33s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 84m 58s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15175 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12893880/HDFS-12532.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux c946a1603488 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 41049ba |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/13980/artifact/out/diff-checkstyle-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/13980/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HADOOP-Build/13980/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 1347 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13980/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Moved to haoop-common and updated the patch.. [~daryn] and [~vinayrpet] would you please check once..?, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 10m 20s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 18m 49s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 12s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 14m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  2s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  9m 37s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}102m  1s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HADOOP-15175 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12907113/HADOOP-15175-002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux f4e4146c4dfa 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 6e27b20 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14016/testReport/ |
| Max. process+thread count | 1360 (vs. ulimit of 5000) |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14016/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, bq. Usecase for the floating IP is to provide HA for a Service. Active service will bind to floating IP.
Understood.  We use a floating IP for all HA.  The issue in this case is the floating address is apparently used for routing outbound client connections.  That's a system level config that I'm not sure hadoop should be responsible for solving.  The original use case was essentially "I want a universal package to deploy everywhere" which contradicts adding a conf for a system specific ip.

The local bind address is not random and should be determined by the routing table.  Can you change the system's default route to use the specific interface/alias that you need?  Or at least add a route for the NN using the interface you need?

I'm hesitant to add another conf for a niche case that is better configured by the system.  ]