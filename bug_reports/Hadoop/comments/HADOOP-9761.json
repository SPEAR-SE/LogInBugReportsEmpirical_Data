[I think the fix is first qualifying paths before they are passed to the underlying FileSystem. I'm a bit surprised VFS doesn't do this already, but I guess it's worked so far since DFS (except for rename) just assumes it's the default FS when given an unqualified path.

I think I'll qualify just for VFS#rename, but it could conceivably be done for all VFS calls., Sounds good.  It would be nice to have some kind of unit test for this as well., Had to think this one over a bit. FileSystem isn't like FileContext, in that given an unqualified path, it doesn't try to qualify it against the default FS first. It's supposed to just treat the path as one of its own.

So, simply move the cross-FS check in DFS#rename. Also tweak FileSystemLinkResolver to match.

The existing test cases weren't picking this up because the DFS returned from the minicluster use the minicluster conf, rather than the viewfs client's conf like in a real environment. So, I make sure to set the DFS defaultFS to the viewfs. I tested "fail before/works after" with the testcase and tested with the shell.

Also took the opportunity to correct some javadoc in DFS., {code}
    // Assumes path belongs to this FileSystem.
    // Callers are responsible for checking this
    FileSystem fs = filesys;
{code}

I don't see where you are actually checking that the path belongs to the FileSystem in most of the callers.

For example, back before symlinks support in FileSystem, code like this:
{code}
FileSystem fs = FileSystem.get(hdfs...);
fs.open("file:///boguspath");
{code}

Would generate a stacktrace like this:
{code}
Exception in thread "main" java.lang.IllegalArgumentException: Wrong FS: file:/boguspath, expected: hdfs://localhost:6000
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:590)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:170)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:246)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:79)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:711)
{code}

After the symlinks support, it generated this:
{code}
Exception in thread "main" java.lang.IllegalArgumentException: Wrong FS: file:/tmp/t, expected: hdfs://localhost:6000
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:636)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:181)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:91)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:290)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:286)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:78)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:286)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:757)
{code}

It seems like this change would simply pass the incorrect path through to the FileSystem, which seems wrong.

treating unqualified (i.e., "schemeless") Paths as owned by the current FileSystem should not require disabling the checks for Paths which *do* have a wrong scheme., They're checked in DistributedFileSystem#getPathName by calling FileSystem#checkPath (which does the validation you posted). All of our paths in DFS go through getPathName (or at least are supposed to)., I'm not sure why Jenkins didn't run on this, but I re-submitted it., resubmit because jenkins did not run the first time
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594798/hadoop-9761-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDFSShell

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2869//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2869//console

This message is automatically generated., I believe the test failure is related to the HADOOP-9652 build breakage that we had, not this patch.  Basically a stray println was causing the regular expressions in TestDFSShell to fail.
, I'm re-uploading this to get a clean Jenkins run.  I added a newline to CHANGES.txt in the mapreduce and yarn directories, so that we can get a full jenkins run on this before committing., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594836/HADOOP-9761_viewfilesystem_rename_fails.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2875//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2875//console

This message is automatically generated., still didn't get an MR unit test run last time.  Let's try again., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594990/0001-HADOOP-9761.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client:

                  org.apache.hadoop.yarn.client.api.impl.TestNMClient

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2885//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2885//console

This message is automatically generated., Thanks for the poking around, Colin. I ran the failed test successfully locally, so I'm guessing it's a flake in YARN.

I think we should commit the earlier patch without the whitespace changes as it also does have a jenkins +1, unless there are further review comments., {code}
+    // Assumes path belongs to this FileSystem.
+    // Callers are responsible for checking this
+    FileSystem fs = filesys;
{code}

Thanks for the explanation about this.  Can we add a comment like "this is validated in {{FileSystem#checkPath}}"?

{code}
+          if (fs.getUri() != getUri()) {
+            throw new IOException("Renames across FileSystems not supported");
+          }
{code}

We should be using {{equals}} here, not {{!=}}.

+1 once those are addressed., Rolled up your suggested changes, thanks, On revisiting this though, I went a bit further:

* Removed the same check from  {{rename(Path, Path, Options)}} that I missed before
* Removed throwing the rename across FileSystems IOException at all, since this is handled already in FileSystem#checkPath, and symlink tests in fact check for that {{IllegalArgumentException}}., OK, good.  So basically this boils down to removing unnecessary checks that were hampering {{ViewFileSystem}}-- that were already being done in {{getPathName}}, etc.

{code}
         public Void next(final FileSystem fs, final Path p)
             throws IOException {
-          // Since we know it's this DFS for both, can just call doCall again
+          // Should just throw an error in FileSystem#checkPath
           return doCall(p);
         }
{code}

I think the comment should be something more like "if the new path is in a different filesystem, doCall will throw an exception."  "Should just throw an error" implies that it always fails, which definitely isn't true... if the symlink is to something on the same FS., So the way {{FileSystemLinkResolver#resolve}} works, it calls {{#doCall}} if the symlink points to within the same FileSystem, and goes out to {{#next}} if it's a different FileSystem. So, I think this means the {{#doCall}} here should always fail the {{FileSystem#checkPath}} check. This is a bit confusing compared to {{FSLinkResolver}}, since {{#next}} is being used differently., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12595272/hadoop-9761-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2897//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2897//console

This message is automatically generated., Yeah, you're absolutely right.  There is a difference between {{FileSystemLinkResolver}}, where you have this code in {{FileSystemLinkResolver#resolve}}:

{code}
        // Have to call next if it's a new FS
        if (!fs.equals(filesys)) {
          return next(fs, p);
        }
{code}

and {{FSLinkResolver#resolve}}, where you have this code:
{code}
    // Loop until all symlinks are resolved or the limit is reached
    for (boolean isLink = true; isLink;) {
      try {
        in = next(fs, p);
        isLink = false;
      ...
{code}

In other words, FSLinkResolver doesn't have {{doCall}}, only {{next}}.

bq. So, I think this means the #doCall here should always fail the FileSystem#checkPath check.

I think it's better to just raise an exception directly, than to rely on another method to do it.  There's less coupling in the code and it's clearer.  Maybe something like this?
{code}
         public Void next(final FileSystem fs, final Path p)
             throws IOException {
           throw new IllegalArgumentException("Wrong FS: "+path+
                                       ", expected: "+ fileSys.getUri());
         }
{code}

I think this makes it a lot more obvious what's going on... what do you think, I had that thought, it's a tradeoff between clarity and duplicating the exception. I was hoping that having a comment would alleviate the clarity concern, but I can re-rev if you want.

It's also perhaps worth noting that duplication is across sub-projects, i.e. {{FileSystem#checkPath}} is in Common while DFS is in HDFS., OK.  That is a fair point.  Duplicating the exception could be a maintenance issue.

+1, commited in 2.3, SUCCESS: Integrated in Hadoop-trunk-Commit #4206 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4206/])
HADOOP-9761.  ViewFileSystem#rename fails when using DistributedFileSystem (Andrew Wang via Colin Patrick McCabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1509874)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemLinkResolver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemHdfs.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #290 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/290/])
HADOOP-9761.  ViewFileSystem#rename fails when using DistributedFileSystem (Andrew Wang via Colin Patrick McCabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1509874)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemLinkResolver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemHdfs.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1480 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1480/])
HADOOP-9761.  ViewFileSystem#rename fails when using DistributedFileSystem (Andrew Wang via Colin Patrick McCabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1509874)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemLinkResolver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemHdfs.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1507 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1507/])
HADOOP-9761.  ViewFileSystem#rename fails when using DistributedFileSystem (Andrew Wang via Colin Patrick McCabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1509874)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemLinkResolver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemHdfs.java
, Reopening to track backporting this to branch-2.1 or similar for GA., Is this required for 2.1.2-beta?, It breaks rename when using viewfs ontop of HDFS, which is a pretty big regression. We've tested this internally on our 2.1.0-based branch and it seems okay., Committed to branch-2.1 and updated the CHANGES.txt in other branch-2 and trunk., SUCCESS: Integrated in Hadoop-trunk-Commit #4466 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4466/])
Move CHANGES.txt entry for HADOOP-9761 (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1526293)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #344 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/344/])
Move CHANGES.txt entry for HADOOP-9761 (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1526293)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1560 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1560/])
Move CHANGES.txt entry for HADOOP-9761 (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1526293)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1534 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1534/])
Move CHANGES.txt entry for HADOOP-9761 (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1526293)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, Closing old tickets that are already shipped in a release.]