[The problem turned out to be in the test. This is how it works. 
It first calculates the file system blockSize, then creates a file of size 2 * blockSize,
and then verifies that du for this file returns 2 * blockSize.
The blockSize is calculated by writing a small file (128 bytes) and calling
du on it, which is supposed to return a multiple of the block size in kilobytes.
In ntfs small files occupy only 1K (per du report) even though the block size is 4K.
NTFS is probably not allocating any blocks for small files and keeps their data directly in MFT.

Anyway, the test is somewhat inconsistent in that it verifies du based on the blockSize,
which is calculated using the same du.
I changed the test so that it now writes a 32K file and checks that du returns the same value.
The problem reported in HADOOP-2845 would be caught by such test.
I think this is a more general approach for testing DU., +1 code review, I verified this patch fixes the test on Windows and still runs fine on Linux.  I also ran the test-patch target with these results:
 
    [exec] +1 overall.
     [exec]
     [exec]     @author +1.  The patch does not contain any @author tags.
     [exec]
     [exec]     tests included +1.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     javadoc +1.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     javac +1.  The applied patch does not generate any new javac compiler warnings.
     [exec]
     [exec]     findbugs +1.  The patch does not introduce any new Findbugs warnings.

I will commit the patch now., I just committed this.  Thanks Konstantin!, Integrated in Hadoop-trunk #451 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/451/])]