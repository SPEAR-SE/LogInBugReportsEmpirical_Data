[[~jnp] [~hagleitn] fyi, Irrespective of the fix, sounds like the hadoop decompressor code should do a followup check that the #of bytes returned is non-zero, [~stevel@apache.org] [~jnp] is it possible to get some traction on this actually? We now also have to work around this in ORC project, and this is becoming a pain, bq.  this is becoming a pain

This is a huge perf hit right now, the workaround is much slower than the original codepath., There was another JIRA on this wasn't there? Sergei, can you find it?, Tentative cause (still confirming) - calling end() on ZlibDirectDecompressor breaks other unrelated ZlibDirectDecompressor-s. So it may not be related to buffers as such.
, Hmm, nm, it might be a red herring, Ok, here's repro code. I get the bar exception if dd2 is added. Note that dd2 is not used for anything and is not related in any way to dd1.
If I instead end dd1 and then reuse it, I get a NPE in Java code. But if I end dd2, internals of Java object are not affected in dd1; looks like the native side has some issue.
{noformat}

    dest.position(startPos);
    dest.limit(startLim);
    src.position(startSrcPos);
    src.limit(startSrcLim);

    ZlibDecompressor.ZlibDirectDecompressor dd1 = new ZlibDecompressor.ZlibDirectDecompressor(CompressionHeader.NO_HEADER, 0);
    dd1.decompress(src, dest);
    dest.limit(dest.position()); // Set the new limit to where the decompressor stopped.
    dest.position(startPos);
    if (dest.remaining() == 0) {
      throw new RuntimeException("foo");
    }

    ZlibDecompressor.ZlibDirectDecompressor dd2 = new ZlibDecompressor.ZlibDirectDecompressor(CompressionHeader.NO_HEADER, 0);
    dest.position(startPos);
    dest.limit(startLim);
    src.position(startSrcPos);
    src.limit(startSrcLim);
    dd2.end();
    dd1.decompress(src, dest);
    dest.limit(dest.position()); // Set the new limit to where the decompressor stopped.
    dest.position(startPos);
    if (dest.remaining() == 0) {
      throw new RuntimeException("bar");
    }

{noformat}

As a side note, Z_BUF_ERROR error in the native code is not processed correctly. See the detailed example for this error here http://zlib.net/zlib_how.html ; given that neither the Java nor native code handles partial reads; and nothing propagates the state to the caller, this should throw an error just like Z_DATA_ERROR.
The buffer address null checks should probably also throw and not exit silently.
Z_NEED_DICT handling is also suspicious. Does anything actually handle this?
, Update: turns out, end() was a red herring after all - any reuse of the same object without calling reset causes the issue.
Given that the object does not support the zlib library model of repeatedly calling inflate with more data, it basically never makes sense to call decompress without calling reset.
Perhaps the call should be built in? I cannot find whether zlib itself actually requires one to reset (at least, for the continuous decompression case, it doesn't look like it's the case), so perhaps cleanup could be improved too.
At any rate, error handling should be fixed to not return 0.]