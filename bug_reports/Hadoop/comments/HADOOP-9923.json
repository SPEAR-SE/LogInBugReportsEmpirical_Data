[I've always thought that it is better to initialize HDFS before the first use just like we've come to expect to initialize a freshly formatted root Linux filesystem with something like 'base-files' package in Debian. In fact, Bigtop provides a specicial initialization script just for that purpose so that users of Bigtop-derived distributions of Hadoop can simply run init-hdfs and now worry about minute details of exact perms/ownerships.

You can take a look at our implementation over here: https://git-wip-us.apache.org/repos/asf?p=bigtop.git;a=blob;f=bigtop-packages/src/common/hadoop/init-hdfs.sh;h=bc96761cef604a6bb42fc09e7d439b8250993973;hb=HEAD

We also plan to improve it for Bigtop 0.7.0 release.
   , bigtop for sure can help, but shouldn't the hadoop distribution itself be deployable "as-is", even if it is not the most optimal way? 

The fact, that the history server creates a directory, that it is only supposed to read from, is IMO a bug. If it is not a bug, then it should at least be documented, how to initialize hdfs correctly, by just using the hadoop tarball. It is somewhat mentioned in the docs, but the fact that you have to do that in the middle of the daemon start-ups ins non-obvious. At least to me. 

It is also a regression from hadoop 1.x, where you could fire up a cluster with start-all.sh and it worked. (I know that start-all.sh is not the most ideal way, but it does work, for certain use cases.), try set users(hdfs, yarn, mapred) to the members of the same group(ex. hadoop) and change the group of staging dir.

I think this is right usage.]