[Moved this from MapReduce to Common since this is a fs issue., The problem is that org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath(Job job, Path path) uses the default FileSystem, instead of the FileSystem specified in the path.
{code}
//org.apache.hadoop.mapreduce.lib.input.FileInputFormat
  public static void addInputPath(Job job, 
                                  Path path) throws IOException {
    Configuration conf = job.getConfiguration();
    FileSystem fs = FileSystem.get(conf);
    path = path.makeQualified(fs); // In our case, path scheme is har:
                                   // but fs scheme is hdfs://
    ...
  }
{code}, c6564_20100222tmp_fix.patch: a tmp fix which slightly changed FileInputFormat and CompressionCodecFactory.  Then, WordCound works with hdfs://., nicholas, does this jira belong to mapreduce then?, > nicholas, does this jira belong to mapreduce then? 
Probably yes.  Let me do some more tests first.

I posted the changes on CompressionCodecFactory in HADOOP-6588., Tested har:// with ls, cp and distcp.  All worked fine.  I am closing this as invalid.

Will file a jira for FileInputFormat., Filed MAPREDUCE-1522 for the FileInputFormat problem.]