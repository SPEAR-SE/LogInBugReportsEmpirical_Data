[
  Say!  Here's a patch that implements the JobTracker rewrite.  Please take a look and let me know what you think...., As Mr Burns would say "eggcelent"  I'll give this a try.  BTW, is it possible to implement functionality that would start jobs that are lagging on nodes that have completed tasks like google does?  for example if your 90% done and the last 10 jobs are hung because of bad hardware, slow response or failure and have the ability to redo the long running jobs in parallel on alternate nodes and complete the first one that finishes?  this way if you have a huge crawl and certain nodes slow or fail those jobs can be alternated on completed nodes to try and wrap up and terminate any dead jobs when done?

hope that makes sense.., Byron, that's exactly what Mike means by "speculative execution"., Guys,

Greg Barish and the folks who worked on the Theseus planning system for information agents at USC did a lot of work on this subject. Theseus is implemented in java, and as I recall, includes the capability to perform speculative execution...

http://www.isi.edu/~barish/, so thats what they call it :) thanks, 
  One more thing:

  You'll see in this patch that ReduceTasks contain a 2D array of map tasks.  They used to
contain a 1D array, one map task for each map-split of the data.

  This 2D business is less than perfect, so let me explain...

  Each Reduce task needs to know when its map task predecessors have completed.
In the old days, this was easy.  There was one map id for each split of the data, so for
k splits, there were k map ids to know about.

  But in a world with speculative execution and early-start of reduce tasks, there could
be multiple possible map tasks that work on the same split of data.  So for each of the
k splits, there could be up to M tasks working on it.  Thus, the reduce task knows about
k * M map ids.  When any one of the M for each split has completed, the reduce task knows
it can move on.

   But this is a little silly.  The JobTracker has a "TaskInProgress" abstraction that represents
the idea of a "split's worth of work".  A single TIP contains M map task ids.  Instead of the
reduce task looking all over for map task ids, it should just deal with TIP ids.  That way,
we're back to a 1D array, and the reduce task code is easier to understand.

  Anyway, it will still work as is.  I'll improve the code in a future patch.  For the moment,
I'll let this patch stand.  I just wanted to let people know...
, I tested this patch and jobs seem to run into deadlocks when one node crashes while others are loading map output data from that node. Here some lines tasktracker on node B that tries to copy map data from node A which has crashed:

060124 181752 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040
060124 181753 task_r_7jjqag 0.17820947% reduce > copy >
060124 181753 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040
060124 181754 task_r_7jjqag 0.17820947% reduce > copy >
060124 181754 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040
060124 181755 task_r_7jjqag 0.17820947% reduce > copy >
060124 181755 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040
060124 181756 task_r_7jjqag 0.17820947% reduce > copy >
[...]
060124 223510 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040
060124 223511 task_r_7jjqag 0.17820947% reduce > copy >
060124 223511 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040
060124 223512 task_r_7jjqag 0.17820947% reduce > copy >
060124 223512 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040
060124 223513 task_r_7jjqag 0.17820947% reduce > copy >
060124 223513 task_r_27r56x 0.2212838% reduce > copy > task_m_1ujusx@nodeA:50040

Node A was removed from the jobtracker's node list but it seems like not all tasks depending on that node have been killed., Mike committed this last week., This is likely me being dumb, but I don't think this issue is fixed.

When I run any of the provided example programs wordcount/grep (also pi with specualtive excecution enabled) reduce tasks does not start before all map tasks have completed.

My cluster contains three nodes and I am running Hadoop 0.4.0.]