[Hi [~Eric88] thanks for the detailed report!
It seems what you discovered is similar to HDFS-10429. 

I have not yet reviewed the patch in depth, but it looks like your patch contains irrelevant stuff. Could you remove them and rebase against trunk? Thanks., Delete the unnecessary context in patch and re-patch based on each single file., Hi Wei-Chiu Chuang,
Thanks for your comments very much!
Please help to review the new patches.
, [~Eric88] Thanks for reporting the issue and working on patch! I added you as a Hadoop Contributor and assigned the JIRA to you. Please read https://wiki.apache.org/hadoop/HowToContribute and name the patch properly., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  4s{color} | {color:red} HADOOP-14691 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HADOOP-14691 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12880516/CommandWithDestination.patch |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12964/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, It's a very good catch but the solution seems makes the things more complicated and it is not backward-compatible since we change the signature of a method.
Instead, I suggest fixing HADOOP-5943. Using try-with-resources at the same place where the resource is created seems a better practice and used widely in Java world.
We can introduce the new methods without closing ability and keep the old ones as deprecated to keep the compatibility. I am happy to send a patch for HADOOP-5943.
Thoughts?

P.s.: I am removing the linked issue since it is not related to the exception in HDFS-10429., Can we just add {{out = null}} after {{out.close()}} to ensure no-op to close a closed stream?
{code:java|title=FSDataOutputStream.PositionCache#close}
    @Override
    public void close() throws IOException {
      // ensure close works even if a null reference was passed in
      if (out != null) {
        out.close();
        out = null;      <<<<<==
      }
    }
{code}

{code:java|title=FSDataOutputStream#close}
  public void close() throws IOException {
    if (out != null) {             <<<<=
      out.close(); // This invokes PositionCache.close()
      out = null;                 <<<<==
    }
  }
{code}
, [~jzhuge],

I think we should eliminate the multiple close calls instead of handling it.
{code:title=CommandWithDestination#writeStreamToFile}
try {
        out = create(target, lazyPersist, direct);
        IOUtils.copyBytes(in, out, getConf(), true); // 1st close call
      } finally {
        IOUtils.closeStream(out); // second close call
      }
{code}
I beleive the original author assumed that after calling {{IOUtils.copyBytes}} with true the out will be null so the second close call has no effect.
One possible solution is to call {{IOUtils.copyBytes}} with false and close the stream with try-catch-resource:
{code}
void writeStreamToFile(InputStream in, PathData target,
        boolean lazyPersist, boolean direct)
        throws IOException {
      try (FSDataOutputStream out = create(target, lazyPersist, direct)) {
        IOUtils.copyBytes(in, out, getConf(), false);
      }
    }
{code}
But I agree with the reporter about {{IOUtils.copyBytes}} is misleading and should be changed.
If we want to fix this specific double call I suggest go with the code few lines above.

But I still suggest fixing HADOOP-5943 since there are more double call in the code which shows the current API is misleading.
Please check {{FileContext.Util#copy}} for example.]