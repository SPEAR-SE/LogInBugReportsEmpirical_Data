[How many tasks were there in the job? Did you try increasing the heap size for the JT?, 
It's not my job, and since the web gui does not response, I don't know the exact number of mappers.
But through log, I know it has at least 60000 mappers.

The heapsize is 1000M (the default, I believe).

, Runping, are these mappers executing relatively fast?
As side-effect of HADOOP-2119, without any patch, our JobTracker needed at least 2GB (1000+ node cluster, 50,000+ mappers). , Even I have seen it. One can be reproduced as follows
{noformat}
1. Run random writer
2. for i=0; i<20; i++
   2.1 Run sort
   2.2 delete sort output
{noformat}
, Asked Runping.  The user was using 0.15.3.
So, HADOOP-2119(commited in 0.17) is not the cause.
, I guess its the history in the JT that causes these heap issues. As of now we keep 24hr history in the JT. I observed this when I ran lot of jobs within 24hrs back to back. Thoughts?, bq. As of now we keep 24hr history in the JT.

The JobTracker keeps only a 100 jobs per user in memory (which you can configure to be a lesser no.) ... so I'm not very convinced - these would have to be significantly large back-to-back jobs. Is that what you observed?, My jobs were back to back sort200. I kept it running for a day in a loop., Hi..

We are hitting the same issue. It has happened two times in two days for us., Amar,

What version of Hadoop are you hitting this issue on. We're seeing it pretty frequently on our 15.3 cluster.  I don't see any patch or JIRA pointing to a patch, so I assume this hasn't been fixed even in 0.16.1. Just wondering if reproducible in 16.

thanks, pete
, I saw these errors on trunk., I suspect the *queue* in the task commit queue. Pre HADOOP-2119 the queue items were processed one by one and hence the queue used to build up. Now with HADOOP-2119, commit is done in batches of size 5000. Chances of this error occurring with the trunk is less. We tried running 320,000 maps on 250 nodes where each map generated 8mb of data (similar to RandomWriter) and we never saw this error. Can someone confirm this? , This has gone stale. FWIW, I've never seen an OOM hang on the clusters in the last one and half year. Crashes most of the time there's a leak or an overload in JIP retention (tweakable).]