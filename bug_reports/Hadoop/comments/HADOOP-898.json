[This must be my changes that got checked in yesterday. I am looking into this.
, storedBlock is null.
This NPE implies node N is listed as one of the nodes that has a block B (in blocksMap) but N.blocks map does not contain B., 
Attached HADOOP-898.patch that reverses the patch in HADOOP-803 and one line change for patch in HADOOP-855 (changing List<> to Collection).

Owen could you try the patch in your build? Thanks.
, You add block to containingNodes inside addStoredBlock(), but add this same block to the
DatanodeDescriptor.blocks outside of addStoredBlock().
If something goes wrong in addStoredBlock() and it throws an exception AFTER the block
was added to containingNodes, will that be the case we are seeing now?

We should think how to add blocks to the DatanodeDescriptor inside addStoredBlock().


, That can explain this case.. I still need to get used thinking in exceptions. I will check if client got an IPC exception.
Both addStoredBlock() and addBlock() can be called outside the while() loop. So they could be just one function.
, 
Attached much smaller 2.patch that basically check if storedBlock is null. 

An IPC exception that Konstantin mentioned  would create same situation before HADOOP-803 as well (i.e. containingNodes has N but N does not have the block) . I am not sure yet whether that is ok or if it could lead to some other problems. Only difference now is that we are requiring that such a situation not exist.  The attached patch allows this apparent inconsistency just like before.

Both patches pass 'ant test'., 
Both patches worked in Owen's cluster.

Doug, could you apply HADOOP-898-2.patch to the current trunk while I look more into the problem? The first patch is much more conservative and reverts whole of HADOOP-803, we could do this if we don't find more about the problem soon. I am fairly confident of the second patch.

, -1 for  HADOOP-898-2.patch

+1 for  HADOOP-898.patch.  

Of the two patches, I find that only HADOOP-898.patch reduces the probability of TestSmallBlock failing from around 20% to 0% after 100+ runs.

For those interested, when TestSmallBlock fails, it's because one call to blocks[i].getNumBytes() in the FSNamesystem.getDatanodeHints() method returns 0 when it should return 1.  Since it is sporadic, it seems like a synchronization/timing issue., 
Thanks Nigel. Was the test failing 20% of the time before any of these patches? Was it occationally failing before HADOOP-803 was checked in?
, Not sure whether the issues I see are related to this bug or not. 

Along with sporadic failures of TestSmallBlock testcase, I also see TestPread failing once in a while with the exception:
Impossible situation: could not find target position 8192
java.io.IOException: Impossible situation: could not find target position 8192
        at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:873)
        at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:61)
        at org.apache.hadoop.fs.FSDataInputStream$Checker.readFully(FSDataInputStream.java:155)
        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.readFully(FSDataInputStream.java:212)
        at org.apache.hadoop.fs.FSDataInputStream$Buffer.readFully(FSDataInputStream.java:265)
        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:315)
        at org.apache.hadoop.dfs.TestPread.pReadFile(TestPread.java:92)
        at org.apache.hadoop.dfs.TestPread.testPreadDFS(TestPread.java:119)

Once I also saw the TestDFSShell failing (forgot to note the exception). 

The DFS has become somewhat flaky lately., During a run of the sort benchmark many reduces failed with these exceptions:

EXCEPTION TRACE #1
----------------------------------
java.lang.NullPointerException
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.closeBackupStream(DFSClient.java:972)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1219)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1181)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.write(DFSClient.java:1163)
	at org.apache.hadoop.fs.FSDataOutputStream$Summer.write(FSDataOutputStream.java:85)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:114)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
	at java.io.DataOutputStream.flush(DataOutputStream.java:106)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:140)
	at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:558)
	at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.close(SequenceFileOutputFormat.java:72)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:331)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1367)

EXCEPTION TRACE #2
---------------------------------
java.io.IOException: Trying to write to backupStream but it already closed or not open
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flushData(DFSClient.java:1195)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1183)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:1315)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at org.apache.hadoop.fs.FSDataOutputStream$Summer.close(FSDataOutputStream.java:99)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:558)
	at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.close(SequenceFileOutputFormat.java:72)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:331)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1367)

EXCEPTION TRACE #3 (this is Hadoop-758 I think)
----------------------------------------------------------------------
java.io.FileNotFoundException: /export/crawlspace/kryptonite/ddas/dfs/data/tmp/client-1257934407504380214 (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.(FileInputStream.java:106)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1229)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:1318)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at org.apache.hadoop.fs.FSDataOutputStream$Summer.close(FSDataOutputStream.java:98)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at java.io.FilterOutputStream.close(FilterOutputStream.java:143)
	at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:558)
	at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.close(SequenceFileOutputFormat.java:72)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:331)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1367)

EXCEPTION TRACE #4 (although this I think has something to do with RPC timing out at the namenode, I hadn't encountered this exception in a long long time)
--------------------------------------------------------
java.net.SocketTimeoutException: timed out waiting for rpc response
	at org.apache.hadoop.ipc.Client.call(Client.java:467)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:164)
	at org.apache.hadoop.dfs.$Proxy1.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:248)
	at org.apache.hadoop.dfs.DFSClient.(DFSClient.java:105)
	at org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:65)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:160)
	at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:119)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:91)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1366), > Impossible situation: could not find target position 8192
> java.io.IOException: Impossible situation: could not find target position 8192
I don't think this is related. We should file a different bug.

>java.lang.NullPointerException
>at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.closeBackupStream(DFSClient.java:972)
Not related to this bug but caused by one of my recent patches. will fix today.

#2 is not a bug. #2 and #3 need patch in HADOOP-758.

I don't know much about #4 but my understanding is that RPC timeouts are expected with a busy nameserver. 
, Regd 898-post-803-revert.patch Nigel Daley wrote:
> Rather than apply a patch that effectively reverts HADOOP-803, it would 
> be better to revert the patch using svn and then apply the neccessary 
> (one line) patch to re-introduce the change needed by the overlapping 
> HADOOP-855 patch.  The svn command that I've applied and tested is:
> 
> svn merge -r 496845:496844 .
> 
> (where . is my trunk directory)
> 
> I will also attach the one line diff that was necessary to apply after 
> the above merge command.
, I just committed this.  Thanks, Nigel & Raghu!]