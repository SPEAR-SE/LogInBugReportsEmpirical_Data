[reword that first sentance: 

Reduce progress grinds to a halt with lots of MapOutputProtocol timeouts and transferring the same file over and over again because of system thrashing.
, It would be good to find which of these changes actually made the difference for you.  Each TaskTracker's map output server only accepts up to mapred.tasktracker.tasks.maximum connections at once.  Since this is typically around 2, I would be surprised if a small buffer size results in lots of seeks, since the OS should perform readaheads in its buffer cache.

What OS are you using?  What do you have mapred.tasktracker.tasks.maximum set to?

If on linux, what does 'iostat -x 1 10' show when things are slow?  How about 'sar -n DEV 1 10'?
, 
As it turns out, my changes did not fix the problem, just changed the timing.

The thrashing was occucring because one reducer was in the merge phase, and the other reducer was in the file copy phase.  The particular file that was failing, was being copied from the local system.  I have the concurrent merges set to 24 and the task count set to 4.

I added logging statements, and the file was clearly being received in full by MapOutputFile, yet ReduceTaskRunner was getting a timeout on that file about 1 minute and 20 seconds later, request it again and again, and each time receive the file yet get a timeout just over a minute later.

I did find two interesting bug in RPC.java while trying to track this down (which im filing separately), but for now I am completely stumped.

At the moment the cluster is otherwise busy, so I cant do any more experiments until perhaps tomororw. Any suggestions would be very welcome. We are using Linux, and I'll try the commands you suggested when Im able to recreate it, but for now this does not look like a disk or TCP problem, it really looks like an RPC scheduling problem. , Some timeouts during the copy phase may not be bad.  If too many nodes are transferring from a given node, then it may time out additional requests.  And if a one node is already transferring from a another node for one task, then attempts by a second task to transfer may timeout (due to the shared connection pool).  These should not affect overall performance too much, especially if the timeout is relatively short., 
A few timeouts would be fine. The problem is when the same files timeout over and over again, and progress ceases completely. 

I was able to make the problem go away by increasing the number of mappers by 6X, making the map output files 1/6th as large, so I have given up on finding the problem.

So here is the summary:

- with 700MB map output files (18 mappers), original code: the job would never progress past reduce progress of  17% or 18%.
- with 700MB map output files (18 mappers), large buffers: the job completed in 27 hours
- with 120MB map output files (106 mappers), and large buffers: the job completed in 6 hours

Im happy to share logs that include the timeouts and extended logging information on MapOutputFile.java if anyone is interested, but i wont post them here because they are several hundred megabytes.

Otherwise I will continue to use the workaround of smaller map output files., Paul,
   Is this still happening with the http map output transfer or can I close this?, 
   [[ Old comment, sent by email on Wed, 2 Aug 2006 13:47:05 -0700 ]]

Close it out! The new shuffle path is really great.


]