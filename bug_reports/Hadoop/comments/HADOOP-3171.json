[I think a simpler solution would be that the Job Client does NOT load the hadoop-default.xml , instead it only loads the hadoop-site.xml if it's present.
This way, only the default values on Server side take effect., Not a blocker for Hadoop 0.17.0, The intention was to ensure that we only serialize non-default values (i.e. values in hadoop-site.xml) via Configuration.write. This would ensure that the right behaviour occured..., Hmm... untested patch., -1 on Arun's patch because the first resource is not distinguished as the default resource. Not saving it in the serialization will lead to confusion and delay.

If a cluster admin wants to force speculation off, they just need to set it off on the job tracker as a final parameter. This is not a bug, but a configuration problem.., If a cluster wants to force spec-exec (or any other prop) to a value can set it as final.

If a cluster wants to set spec-exec (or any other prop) to a default value for that cluster, this is not possible (it is only possible to do this for the hadoop-default.xml value).

IMO, I should be able to set the default value I want my cluster to use if the property is not set, and this is not supported now., I don't see the use case for having the cluster pushing a default for speculative execution. You can either:
  1. You can build a new client jar that has the default changed. 
  2. You can force the value from the server.
  3. The client can define a HADOOP_CONF_DIR with a hadoop-site.xml that changes the default.

None of this should reasonably change depending on what server you connect to. It is *not* a property of a given server., On #1 

Asking people to rejar the hadoop.jar is not reasonable as it may lead to nasty debug situations.

On #2

spec exec is an example only (in our case most of our jobs cannot run with spec-exec but a few, so we want to set it off by default, and only set it on in the few jobs that we want it on).

Take the case of number of task retries, replication factor, compression or compression-type properties. You may want to set the default behavior of your cluster, but not make them final.

On #3

It is not always possible to have a HADOOP_CONF_DIR for example from within a webApp. It gets complicated. 

---

And if I have a hadoop client that dispatches jobs to different clusters things may get even more complicated.


My concern is that a cluster should be able to control what a default value is without forcing with a final. This is not possible today.
, I left off:
  4. Use the tool interface that lets you define properties and config files on the command line.

Either the command line or the environment variable should work for you., Closing this issue as it is not reflect the real problem, I'll open another one.]