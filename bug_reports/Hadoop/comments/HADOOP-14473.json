[Ping [~vahemesw] and [~snayak]., I'm +1 on this. Thanks,, which endpoint did you test against? -1 until that's declared. , This is going to read forward no matter how big the file is, even if you are going to the last MB of a 20 GB file. Is this really the most optimal.

Rajesh, you are pulling over the s3a input stream work again, aren't you? Maybe its best here to group them into 1 patch. That s3a work also added stream instrumentation {{org.apache.hadoop.fs.s3a.S3AInstrumentation.InputStreamStatistics}} , so we could actually measure what is going on, *and use it in tests*. This seek work here & related is the opportunity to do the same for Azure, which will benefit production monitoring too. In particular, here I'd like to track the #of bytes skipped in forward seeks, and the #of close/open pairs, so we can detect when there's a lot of skipping going on, plus make better tests. Ideally I'd like something like {{ITestS3AInputStreamPerformance}}, so as to catch any performance regressions in various read sequences (whole file vs skip forwards vs full random), Since it was easier to combine this patch with HADOOP-14478, I have merged it and posted the revised patch there.

In the revised patch, I have fixed an issue in seek() and shared the test results as well there. Tests were run against "japan west region" end point.

{{BlobInputStream::skip()}} is more of a no-op call. Issue was related to closing the stream and opening it again via {{store.retrieve()}} as it would end up creating new {{BlobInputStream}}. And that would internally need additional http call as it needs to download blob attributes internally in {{BlobInputStream}}. This has been avoided in the patch. 

I completely agree that it would be good to get the instrumentation similar to s3a, and it was very useful. Please let me know if this could be done in incremental tickets., Closing this ticket, since HADOOP-14478 takes care of this., Closing as nothing to be done for releases.]