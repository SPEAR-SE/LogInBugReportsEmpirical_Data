[This is very similar to HDFS-8068, but that tries to work around the issue at the application layer when it tries to setup the proxy.  Ideally this should be handled as much as possible in the IPC layer itself so we can treat UnknownHostException like other retriable exceptions.

One possible approach is to have the Connection constructor try to call the updateAddress method if the ConnectionId socket address is unresolved.  Then we would actually try to re-resolve the address.  One downside to this appraoch is we could end up with multiple clients for the same server, since the ConnectionId socket address is used as part of the hashcode.  However this seems better than either retrying forever for no benefit or requiring app-level code to retry this code on their own when setting up the proxy., Moving out all non-critical / non-blocker issues that didn't make it out of 2.7.1 into 2.7.2., What happens when the address is really bad due to misconfiguration, dns update, etc.?  Instead of retrying forever, we want it to fail after some time. As long as there is a way to properly propagate the failure up, I am fine with this approach.  The retry proxy and the app should realize that the rpc proxy is unusable. It should not use the same proxy to retry., Moving out all non-critical / non-blocker issues that didn't make it out of 2.7.2 into 2.7.3. Please revert back if you disagree., 2.7.3 is under release process, changing target-version to 2.7.4., Moving target version to 2.7.5 due to 2.7.4 release., Moving target version to 2.7.6 due to 2.7.5 release., [~shahrs87] and [~jlowe], any progress? We hit the same issue when the non-HA NN went down andÂ AWS spun up another NN instance with a different IP address. Both Job History Server and Spark History Server were stuck because NameNodeProxy held on to the old IP address., [~jzhuge]: I don't have enough cycles to work on this jira.
Please go ahead and re-assign if you plan to work on this.]