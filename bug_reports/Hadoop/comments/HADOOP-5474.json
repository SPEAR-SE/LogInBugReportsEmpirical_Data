[bq. When a tasktracker with a completed map task failed, the map task will be re-exectuted, and all reduce tasks that haven't read the data from that tasktracker should be re-executed. 
Reduce tasks are not re-executed, they will fail in fetching the map output and retry the fetch, will succeed once the reexecuted-map succeeds., bq. In this situation, if the outputs of multi map tasks on the same dataset are different, for example outputting a random number, the outputs of maptask and the re-executed maptask will probably are different. Then the re-executed reduce tasks will read the new output of the re-executed maptask, but reduce tasks that have read the data from the failed tasktracker have read the old output. This probably will cause correctness of the result.

I think your application should be tolerant to this happening and be written assuming that maps/reduces could fail or get killed, etc. We really don't want to do what you suggest., I don't agree with you that the application should be tolerant to this situation. But the cost of re-execution of all reduce tasks is very high, do you have any suggestions to solve this issue?, Will seperating the job into two jobs resolve your problem? the first job only do the map, and if it is done, the outputs of map tasks are steady., The cost of this change would be huge. Basically, any node going down or a crc failure in shuffle would cause you to kill all currently running reduces. That is unacceptable. Your application needs to be tolerant of reexecution of tasks. That is a fundamental constraint of map/reduce programming. In order to make your example work, the map could use the hash of the input split as the seed to the random number generator. That way, re-executions will have consistent behavior.]