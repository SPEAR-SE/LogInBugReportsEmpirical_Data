[We are experiencing the same problem.

We have a cluster of HyperV VM's running Scientific Linux and Hadoop 0.20.2.

1 VM for Namenode & SecondaryNamenode
1 VM for Jobtracker
3 VMs for DN/TT each

Each machine shares the same configuration with:

mapred.local.dir=/mapred
mapred.system.dir=/hadoop/mapred/system
dfs.name.dir=/hdfs/namespace,/hdfs_dupe/namespace
dfs.data.dir=/hdfs/data
hadoop.tmp.dir=/tmp

When running the example provided in the Getting Started tutorials we are getting this error in the tasktracker (TT) running the single reduce task

2011-11-01 13:07:24,310 WARN org.apache.hadoop.mapred.TaskTracker: getMapOutput(attempt_201110312140_0003_m_000010_0,0) failed :
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_201110312140_0003/attempt_201110312140_0003_m_000010_0/output/f
ile.out.index in any of the configured local directories
        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:389)
        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)
        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:2887)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:324)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)

2011-11-01 13:07:24,310 WARN org.apache.hadoop.mapred.TaskTracker: Unknown child with bad map output: attempt_201110312140_0003_m_000010_0. Ignored.
2011-11-01 13:07:24,310 INFO org.apache.hadoop.mapred.TaskTracker.clienttrace: src: 127.0.0.1:50060, dest: 127.0.0.1:38338, bytes: 0, op: MAPRED_SHUFFLE, cliI
D: attempt_201110312140_0003_m_000010_0
2011-11-01 13:07:24,310 WARN org.mortbay.log: /mapOutput: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_20111
0312140_0003/attempt_201110312140_0003_m_000010_0/output/file.out.index in any of the configured local directories
2011-11-01 13:07:25,237 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) >
2011-11-01 13:07:28,238 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) >
2011-11-01 13:07:29,328 WARN org.apache.hadoop.mapred.TaskTracker: getMapOutput(attempt_201110312140_0003_m_000010_0,0) failed :
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_201110312140_0003/attempt_201110312140_0003_m_000010_0/output/f
ile.out.index in any of the configured local directories
        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:389)
        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)
        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:2887)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
:



Any and all help will be much appreciated.

Cheers,
Tom, Bit more log in case it helps:

2011-11-01 13:09:55,298 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:09:58,299 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:04,301 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:09,349 WARN org.apache.hadoop.mapred.TaskTracker: getMapOutput(attempt_201110312140_0003_m_000010_0,0) failed :
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_201110312140_0003/attempt_201110312140_0003_m_000010_0/output/file.out.index in any of the configured local directories
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:389)
	at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)
	at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:2887)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)

2011-11-01 13:10:09,350 WARN org.apache.hadoop.mapred.TaskTracker: Unknown child with bad map output: attempt_201110312140_0003_m_000010_0. Ignored.
2011-11-01 13:10:09,354 INFO org.apache.hadoop.mapred.TaskTracker.clienttrace: src: 127.0.0.1:50060, dest: 127.0.0.1:38341, bytes: 0, op: MAPRED_SHUFFLE, cliID: attempt_201110312140_0003_m_000010_0
2011-11-01 13:10:09,354 WARN org.mortbay.log: /mapOutput: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_201110312140_0003/attempt_201110312140_0003_m_000010_0/output/file.out.index in any of the configured local directories
2011-11-01 13:10:10,303 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:13,304 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:19,306 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:25,308 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:28,309 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:34,312 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:40,314 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:43,315 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:49,317 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:55,319 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:10:58,320 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:11:04,322 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:11:10,326 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:11:13,327 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:11:19,329 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:11:25,331 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) > 
2011-11-01 13:11:28,332 INFO org.apache.hadoop.mapred.TaskTracker: attempt_201110312140_0003_r_000000_1 0.051282056% reduce > copy (2 of 13 at 0.00 MB/s) >, The answer to this problem (for me at least) was that the contents of the /etc/hosts file..

After reading this:

http://www.mail-archive.com/core-user@hadoop.apache.org/msg03635.html

I did this:

Added lines to my /etc/hosts file so it looked like this:


127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

10.0.0.235      namenode secondarynamenode
10.0.0.236      jobtracker
10.0.0.237      slave0
10.0.0.238      slave1
10.0.0.239      slave2

Then on each machine I overwrote the /etc/hosts file with that above and then executed the following:

sudo hostname namenode (on 10.0.0.235)
OR 
sudo hostname jobtracker (on 10.0.0.236)
OR 
sudo hostname slave0 (on 10.0.0.237)
OR 
sudo hostname slave1 (on 10.0.0.238)
OR 
sudo hostname slave2 (on 10.0.0.239)

Then I rebooted all machines in the cluster, disabled the firewalls on all and reran the job successfully!, I followed the steps on naming the hosts etc, unfortunately I have only two servers. I named one as namenode and another as slave. but I have put all ip entries (jobtracker, tasktracker etc) in /etc/hosts. It is not solving the problem. Is there a patch or solution that we should wait for ? is it fixed in 1.0 beta ? , I solved it by editing the /etc/hosts.
All your nodes MUST have real net IP addresses instead of 127.0.0.1 or localhost., Hadoop requires working host name resolution.  Closing as won't fix.]