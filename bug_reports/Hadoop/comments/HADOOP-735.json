[Attached a patch that replaces ByteArrayOutputStream as a buffer-representation in Hadoop record I/O by BytesWritable. Waring, this would break any existing code that uses records containing buffer type in Hadoop Record I/O., You've also added bounds checking to all of the raw comparator i/o primitives.  That might be a good thing, but I'm not sure why its part of this patch.  It seems like a separate issue that should be benchmarked, to make sure that it doesn't affect performance, as these comparators are the heart of our innermost sort loops., Indeed. This was necessary because the variable sized integer and longs were throwing IOException in their read methods. The serialization of BytesWritable uses int. For a record that contains only a buffer, the generated code was catching IOException, and resulted in a compile-error for trying to catch an exception that was not thrown. That's why it is part of this patch., Another option was to have a separate serialization format for BytesWritable (one that uses VInt instead of Int to denote length of the buffer) when used inside a Record. That would not require the above change, but it would be ugly to have diffferent serialization formats for the same datatype in two diffferent contexts., > the generated code was catching IOException, and resulted in a compile-error for trying to catch an exception that was not thrown

So the fix is to cause an exception to be thrown?  I still don't follow.  Code which processes a buffer only is technically not doing i/o, so IOException is also a strange one to add to these methods.

I also still don't completely see the issue.  Can you provide some examples of the generated code, or point to where in the generated test cases I might look?  Thanks., The public static methods readInt and readVInt of WritableComparator had different contracts as far as exceptions are concerned. readVInt throws IOException if the supplied byte[] is smaller than expected, whereas readInt does not (it will throw ArrayIndexOutOfBounds exception, which being a RuntimeException, need not be declared.) The generated raw compare method contract will differ (or we will have to catch alll exceptions, log them and ignore them) if we are either using readVInt or readInt. That is the issue., Okay, now I get it.

Mightn't it be easier to change the exception thrown by readVInt to a RuntimeException like ArrayIndexOutOfBounds?  That's a back-compatible change.  Then, separately, we can consider whether to do more bounds checking in all of the other raw comparator methods.  Would that make sense?, I think throwing IOException in the rest of the read* methods (even if one is reading from byte[]) is the right thing to do. If there are performance implications of doing an explicit length check, I could remove the explicit checkLength call, and replace it by try-catch, and throw an IOException on catching ArrayIndexOutOfBounds exception. This will still have some performance implications, but it will be smaller than an explicit method call and bounds check., I have removed this patch, because it creates incompatibility between serialization formats for buffer in c++ and java. I will upload the correct patch soon. The new patch will also eliminate the need to modify the WritableComparator's read* static methods., I have uploaded a patch that uses BytesWritable to represent Hadoop Record I/O's buffer type instead of ByteArrayOutputStream., +1, because http://issues.apache.org/jira/secure/attachment/12349207/BytesWritable.patch applied and successfully tested against trunk revision r497583., I just committed this.  Thanks, Milind!]