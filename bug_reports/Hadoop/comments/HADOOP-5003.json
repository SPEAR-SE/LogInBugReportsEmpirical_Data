[I'll put a patch up once we commit other pending patches for the Scheduler, as the code changes the core scheduler file which is changed by the other patches too. , I think rounding down is more appropriate. If you have 10 slots and three queues that each get 33%, I believe the current algorithm is more stable if they each get 3 slots instead of 4., bq. ... I believe the current algorithm is more stable ...
How so? 

If we round down, we can end up with the situation where the sum of GCs is slightly less than the overall cluster capacity. If we 'round up' (note that my suggestion of using Math.round() doesn't always round up; it only rounds up if the decimal value is 0.50 or greater, and rounds down otherwise), the sum of GCs can be slightly more than the overall cluster capacity. Neither of these situations affects the stability of the algorithm. GCs are used to sort the queues when deciding the order in which queues are considered for a TT. GCs are also used to decide when, and how much, to reclaim capacity. The difference between rounding up and down a queue's GC is at at most 1 slot. That really shouldn't make a difference (hence I marked this issue as 'Minor').

My only argument for this Jira is that rounding sounds more fair than casting from a float to an int. (BTW, your example is not quite right. With 10 slots, and 33% capacity, each queue still gets 3 slots, whether you use Math.round(), as I am proposing, or whether you used an int cast. But I think i get your point. If the # of slots is 11, each queue's GC will be 4 with the former,and 3 with the latter.) Math.round() doesn't always round up - it rounds up anything over, and equal to, 0.50, and rounds down anything below 0.50. That seems fairer than always rounding down. If a queue's GC% is 45%,and the # of slots is 4, it seems fairer to give that queue a GC of 2, rather than a GC of 1. , With rounding instead of floor,  the issue is the same, just with 11 slots instead of 10.

Let's say that with the 11 slots and three queues at 33% and all 3 queues get jobs at the same moment. One of the queues gets 4 slots and the other two get 3. Since they all have guaranteed capacity of 4, the timers start. When the timers go off, one of the tasks from queue A is killed and given to B. A few seconds later, the task from queue B is killed and the slot is given to queue C. 

-1, Owen, this is not how the Capacity Scheduler works. A timer for reclaiming capacity is started ONLY if some queue is above capacity (i.e., if there is capacity to be reclaimed). In your example, A is running at capacity (which is 4). There will be NO timers for B and C unless A's number of running tasks goes above 4, which it won't. So no timers will go off, and no tasks will be killed. , If that is true, it is a bug in the capacity scheduler. The timer for SLA compliance should be from when the queue has load that isn't being satisfied., I don't see this as a bug. There will be times when the sum of GCs of all queues will not be equal to the actual cluster capacity. In particular, there will be times when this sum is greater than the cluster capacity. For example, TTs could be down, and it may take a while before that is detected and the cluster capacity (as defined by the ClusterStatus object obtained from TaskTrackerManager) updated. So there just may not be enough slots to fulfill requests for all queues. To handle such situations, the Capacity Scheduler has a simple check that starts a timer to reclaim capacity only if there is at least one queue that is using extra capacity, i.e., there is capacity to reclaim. I think this is a sensible check and avoids starting timers too quickly. I see the SLA as taking effect when there is legitimate capacity to reclaim. In a perfect world, we'd know instantly that TTs were down and recompute GCs, but in our distributed world, we have to rely on possibly stale data from the ClusterStatus object., Wanted to revive the discussion. Does my explanation above make sense? , *Sigh* No, I thought it was pretty clear this issue was resolved.

It should never be the case that the sum of the guaranteed capacities are higher than the cluster size. To do so is totally counter intuitive. If the cluster size changes, of course the guaranteed capacities change too. At any point in time, the JobTracker should have queues set so that there are enough slots to satisfy all of the guaranteed capacities. Otherwise, the name should be something other than *guaranteed*.

Please file a jira to remove the check. The SLA timers should take effect when the queue has demand that isn't being satisfied. Clearly, in most cases the check should be redundant and therefore irrelevant. (Since you shouldn't have an underserved queue for long without an overserved queue.) However, in the presence of bugs in the scheduler, it would be better to not prevent the timer from starting., Well, I thought I was clear why this isn't is a bug, but let me give you a detailed example.  My argument is simple: in theory, you do not want the sum of GCs to be larger than the cluster size (and we ensure that when we start up and read the config file), but in practice, given that this is a distributed system, there will be situations when the sum of GCs is greater than the 'actual' cluster size at a given moment. This happens when TTs fail. Consider the situation when you have two queues: Q1 and Q2. Assume their GCs are 5 slots (map or reduce, doesn't matter) each, i.e., there are 10 slots in the system. For simplification, assume there are 10 TTs and 1 slot per TT. Now suppose that Q1 is running at capacity, and Q2 is only using 4 out of 5 slots, because it doesn't have any more tasks to run. So, 1 TT  is free. Also assume that the tasks are long running, i.e. they take minutes to complete. This is time T0. Now, suppose a user submits a job with lots of tasks to Q2, at time T0+1second. Also suppose that at around the same time, i.e., at T0+1, the idle TT dies (it doesn't have to be the same time, but anytiem before it sends a heartbeat) . Further, suppose that the reclaim capacity thread runs at time T0+3 seconds (it runs every 5 seconds by default). What should it do? The actual cluster capacity is 9, but the JT and scheduler do not know that yet. Remember it takes over 10 minutes for the JT to detect that the TT is down, and to update the cluster status. So, what does the Scheduler do? 

Your suggestion is that a timer is started for Q2, since it's below capacity and has pending tasks. So, at time T0+3, a timer gets started. Assuming that the reclaim time for the queue is 3 minutes, this timer will go off at T0+183 (in seconds). When the timer goes off, what happens? We still haven't detected the lost TT (that will happen at T0+600 at the earliest, I believe). The timer has gone off, and we need to kill. Well, do we kill from Q1? If you say no (Q1 is, after all, running at capacity only), the timer is wasted. If you say yes, it's unfair to Q1. 

I am arguing that the timer shouldn't have been set in the first place. The SLA is valid, as I see it, only IF there is capacity to reclaim. If nobody has taken my capacity, there is no SLA. Had we known instantly about the TT going down, you would recompute capacities, and Q2's GC would be 4 instead of 5, and it would be at capacity. Q2's demand is not being satisfied because there is an incorrect view of what Q2's capacity is. The SLA should not apply here. 

If you look at the way we've worded the requirement for reclaiming of capacity in HADOOP-3421, it reads "...the system will guarantee that excess resources taken from an Org will be restored to it within N minutes of its need for them". The key phrase is 'resources taken from an Org'. If no queue is running over capacity, no resources have been taken from a queue, and hence the SLA is not in force. , It is still a bug. It just implies that it is unreasonable to have the reclaim timer be shorter than the TaskTracker time out. That makes sense. You can't enforce a SLA that is tighter than the accuracy of the information about the cluster.

Now back to your example. From Q2's point of view, he doesn't see that TT1 is down. He sees that he is allocated 50% and that he isn't getting the 5 slots he should. He gets mad that no timer is running to get him his slot back.

Take home messages:
  1. We should probably warn the user if a SLA time is less than the TaskTracker timeout.
  2. We need to continue to truncate when computing guaranteed capacity.
  3. We should remove the limitation that the timer can only start when another queue is over capacity., People are not going to want SLAs that are 10 mins or higher. I think they'll be OK waiting a few mins, maybe 5. It should be OK if we let them set whatever time they want for the queue, but print a suitable warning message indicating that if the capacity is not reclaimed, it's likely because of failed TTs. Small values for the SLA seem perfectly reasonable, especially when all TTs are running. 

bq. From Q2's point of view, he doesn't see that TT1 is down. He sees that he is allocated 50% and that he isn't getting the 5 slots he should. He gets mad that no timer is running to get him his slot back.
Well, even if you start a timer the moment the job is submitted, there is no task to kill because nobody is running over capacity. So the timer is wasted. This is, of course, with the assumption that the reclaim time is less than the TT failure detection time (which, as I wrote earlier, should be allowed). 

I still don't agree with #2 and #3 of your 'Take home messages'. As per existing requirements, the SLA is in force if there are resources to be claimed. We can change the requirements, but it's not clear to me that we should. I'd rather modify the documentation to let users know that if they don't see a timer being started, it's because some TTs are down. Versus asking them to set SLAs higher than 10mins. 

I understand that you want it to be clear to the user why they're not getting all their slots. But your only choices seem to be to force SLA times to be very high, or to provide an explanation somewhere (in documentation, UI, or whatever). We should do the latter, but realize that if we're accepting smaller SLA times, setting timers early will not help - users will still not get their slots back if TTs are down. , Let me summarize the various points I'm making: 

# If you accept that reclaim SLA times can be smaller than 10 mins (which is the time to detect if a TT is dead), starting timers when no queue is running over capacity is a waste. When the timer goes off, there is nothing to kill. A user will still see their queue is not getting enough slots, irrespective of whether you start a timer or not. What they can also see is that no other queue is running over capacity, and this should be a good enough indicator that some TT is down (of course, we can document this explanation to help them make sense). Starting a timer will not change what the user sees. 
# The SLA requirement currently states "...the system will guarantee that excess resources taken from an Org will be restored to it within N minutes of its need for them". I read that as starting a timer when a queue has enough pending tasks AND somebody else has stolen capacity from it. If you start a timer too early, you kill too early, and you lose the benefit of letting queues borrow capacities from others for N minutes. 

, I think it is reasonable to check that the reclaim time limit is something that we can actually meet. Since the lost tasktracker detection interval is a time period when the cluster state may be inaccurate, and we know there are cases when we cannot meet it, I think it is reasonable to have users set a limit that's larger than this value. Note that this time period is also configurable. So if users feel that the default 10 minutes is too high, the cluster could be potentially configured with a smaller value for that parameter as well (after doing due diligence on the side effects of this).

Whether or not 10 minutes is high time depends on the profile of the jobs - for small jobs this might be high, but for larger jobs (which take significantly more time to run), a difference of a few minutes doesn't seem like it would make a lot of difference.

, I really think we shouldn't tie the reclaim time limit with the time to detect TT failures. You can reclaim capacity when everything is running fine, but you can also reclaim capacity even when some TTs are failing. As long as some queue is running over capacity, you can reclaim it independent of TT failures. I can't see any fundamental connection between the two times. For large clusters, you probably do not want low timeouts for detecting TT failures as there can be a relatively large number of transient failures. For large clusters, with many queues, you also likely want smallish reclaim time limits as there will lots of spare capacity being moved around between the queues.  Like I said earlier, setting reclaim time limits to larger than 10 mins is not going to fly well with users, IMO. 

You reclaim capacity as best as you can, in the presence or absence fo TT failures, and I think that's in spirit of the SLA requirement. 

bq. Whether or not 10 minutes is high time depends on the profile of the jobs - for small jobs this might be high, but for larger jobs (which take significantly more time to run), a difference of a few minutes doesn't seem like it would make a lot of difference.
I'm not sure I follow. I assume you're referring to reclaim time limit here. Your queue will likely get a mixture of jobs. Plus, we're talking about reclaiming individual slots to run individual tasks. If I submit a job, large or small, long running or not, I expect to start getting slots for my tasks within N minutes. I'm not sure why an acceptable value of N depends on what kind of job I submit. , Vivek, the connection between the reclaim time limit and the time to detect TT failures lies in the corner cases that you so rightly identified. We know there are scenarios when we cannot honor the SLA. So, not committing to it during the time seems like a conservation approach to me, that's all.

bq. Like I said earlier, setting reclaim time limits to larger than 10 mins is not going to fly well with users, IMO.

I had a brief chat with Milind on this, and he actually did think timeouts in the order of 15-20 minutes is going to be OK, because users typically wait much longer than that currently.

An in-between approach could be as follows:
- Start a timer irrespective of whether there's spare capacity.
- When you need to reclaim, and find that there's no capacity to reclaim, log a warning that can be viewed by users (so maybe in some format in the Web UI for the queue) that the SLA may not be met becaue the system state is not synchronized.
- Then reset the timer to check again after a lost tracker timeout interval.

Would that work ? Thoughts ?
, Let's move the discussion of this over to more relevant HADOOP-5110. In no circumstances should the capacities be rounded.]