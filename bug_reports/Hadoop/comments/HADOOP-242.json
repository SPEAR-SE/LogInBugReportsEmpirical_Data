[perhaps we should not record failure until a job has been tried on two nodes?  Then we could collect stats on frequency that a job fails on a given node.

Also 4 failures seems odd.  Shouldn't we just have a configurable allowable % failure?  That will scale evenly., It is 4 failures of the _same_ fragment of work. So it takes map-0000000 failing 4 times before the job is killed. The job tracker does know which machines that each task has failed on and prefers to run other tasks on that node., Here is a patch that increases the logging associated with this failure. It also tries to switch the condition around based on the possibility of multiple processes or threads. So it does:

fs.mkdirs(dir) || fs.exists(dir)

instead of:

fs.exists(dir) || fs.mkdirs(dir), I just committed this.  Thanks, Owen., My patch didn't fix the problem, so I need to track this one down. , I think the fix for HADOOP-277 fixed this one. We've seen it since, but only when the local dir was running out of space.]