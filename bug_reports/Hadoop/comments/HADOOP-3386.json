[Do you have a proposal in mind to fix this behaviour?, I am thinking about taking step 2 and 3 from 3370 proposed solution. (Note: in the 3370 patch I only did 1 to make sure the critical fix can get out as soon as possible).

Proposed solution from 3370:
1. On failed task, remove the task from runningJobs, but do not delete runningJobs job entry even if it's the only task of the job; (which means we should NOT call TaskTracker.removeTaskFromJob)

2. JobTracker should keep another data structure: jobsToTracker, for recording all the TaskTrackers that a job has started a task on.

3. When the job finished, JobTracker will send "KILL" job command to the TaskTrackers, based on jobsToTracker data structure.

, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org
  against trunk revision 656122.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2461/console

This message is automatically generated., bq. When the job finished, JobTracker will send "KILL" job command to the TaskTrackers, based on jobsToTracker data structure.

+1, I'm not seeing failed task dirs flood my local directories during runtime. Possibly was fixed long ago.]