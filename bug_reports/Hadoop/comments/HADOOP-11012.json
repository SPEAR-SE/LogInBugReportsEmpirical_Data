[{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664697/HDFS-6915.201408271824.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7787//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7787//console

This message is automatically generated., Thanks for fixing this, [~eepayne]. LGTM overall. 

Now you read the "magic" twice however.  I woud change the original code just by enclosing the switch statement into try-catch-EOF.

{code}
[tw-mbp-gshegalov hadoop-common (trunk)]$ git diff -b 
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
index a72af7a..1f76177 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
@@ -18,6 +18,7 @@
 package org.apache.hadoop.fs.shell;
 
 import java.io.ByteArrayOutputStream;
+import java.io.EOFException;
 import java.io.IOException;
 import java.io.InputStream;
 import java.util.LinkedList;
@@ -126,8 +127,9 @@ protected InputStream getInputStream(PathData item) throws IOException {
     protected InputStream getInputStream(PathData item) throws IOException {
       FSDataInputStream i = (FSDataInputStream)super.getInputStream(item);
 
+      try {
         // Check type of stream first
-      switch(i.readShort()) {
+        switch (i.readShort()) {
           case 0x1f8b: { // RFC 1952
             // Must be gzip
             i.seek(0);
@@ -159,6 +161,8 @@ protected InputStream getInputStream(PathData item) throws IOException {
             break;
           }
         }
+      } catch (EOFException ignored) {
+      }
 
       // File is non-compressed, or not a file container we know.
       i.seek(0);
{code}, Thanks for reporting the issue and providing a patch, Eric!

It would be more efficient and a bit clearer if we saved off the result of the initial readShort call and switched on that rather than throwing it away as a "test read."

There's a lot of duplication setting up the input stream in the unit tests, and it's probably worth it to factor this out.

Given there appears to be no overlap between the three added test cases (0-byte, 1-byte, and 2-byte files) it would be nice to put these in separate unit tests.  Then the unit test that fails makes it obvious which test case is broken.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664697/HDFS-6915.201408271824.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ipc.TestFairCallQueue

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4556//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4556//console

This message is automatically generated., [~jira.shegalov], thank you for reviewing this patch.
bq. Now you read the "magic" twice however. I woud change the original code just by enclosing the switch statement into try-catch-EOF.
Would it be sufficient to do as [~jlowe] suggests and save the magic bytes and switch on that?, [~jlowe], thank you for taking the time to review this patch.
{quote}
It would be more efficient and a bit clearer if we saved off the result of the initial readShort call and switched on that rather than throwing it away as a "test read."
{quote}
This has been done in this new patch.
{quote}
There's a lot of duplication setting up the input stream in the unit tests, and it's probably worth it to factor this out.

Given there appears to be no overlap between the three added test cases (0-byte, 1-byte, and 2-byte files) it would be nice to put these in separate unit tests. Then the unit test that fails makes it obvious which test case is broken.
{quote}
In this latest patch, I created separate tests for each of the added use cases and combine the duplicate setup code into a separate method., The key difference between those two approaches is that trying to instantiate one of the specific codec streams may itself throw an EOFException as it tries to consume a codec header or what-not.  If we wrap the whole thing in EOFException then we will treat files that appear to be a specific codec but otherwise have a truncated header as an uncompressed file rather than a corrupted codec file.  Just wrapping the initial short read means any errors encountered during setup of a codec-specific stream will continue to be propagated.  I'm assuming we don't want to assume that a file that has the signature bytes of a codec file is an uncompressed file if we can't decompress the codec header., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664751/HDFS-6915.201408272144.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4558//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4558//console

This message is automatically generated., [~Eric Payne] Jason's suggestion is better for the reasons he explained. I see indeed some compressed streams trying to read headers in the constructor. , When I looked at the patch yesterday, my feedback was going to match Jason's.  Only suggestion is removing use of reflection and using {{new FSShell(conf).run(...)}}., [~daryn], thank you for reviewing this patch. using {{fsshell.run}} might not be simple since that would entail creating a new output stream, setting the {{out}} instance variable for {{Display.Text}}, and then reading from that stream. 

However, with this patch, I was able to anonymously extend the {{Display.Text}} class and override the {{getInputStream}} method to be public, and then call {{getInputStream}} directly. Please let me know what you think., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12665074/HDFS-6915.201408282053.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to cause Findbugs (version 2.0.3) to fail.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4573//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4573//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12665074/HDFS-6915.201408282053.txt
  against trunk revision b03653f.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-common-project/hadoop-common:

org.apache.hadoop.http.TestHttpServerLifecycle

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4598//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4598//console

This message is automatically generated., +1 lgtm.  Will commit this next week to give [~daryn] a chance to comment further., Thanks to Eric for the contribution and to Gera and Daryn for additional review!  I committed this to trunk and branch-2., SUCCESS: Integrated in Hadoop-Yarn-trunk #669 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/669/])
HADOOP-11012. hadoop fs -text of zero-length file causes EOFException. Contributed by Eric Payne (jlowe: rev 01e8f056d9b7245193e6050f9830ca058db02a6e)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java
* hadoop-common-project/hadoop-common/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1860 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1860/])
HADOOP-11012. hadoop fs -text of zero-length file causes EOFException. Contributed by Eric Payne (jlowe: rev 01e8f056d9b7245193e6050f9830ca058db02a6e)
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1885 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1885/])
HADOOP-11012. hadoop fs -text of zero-length file causes EOFException. Contributed by Eric Payne (jlowe: rev 01e8f056d9b7245193e6050f9830ca058db02a6e)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
]