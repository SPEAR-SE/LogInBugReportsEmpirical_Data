[Fair point.

We probably need to track _opened but not yet closed_ files along with ramManager.close and then get waitForDataToMerge to return a boolean indicating that the InMemMergeThread should gracefully exit. Thoughts?, {quote} We probably need to track opened but not yet closed files along with ramManager.close and then get waitForDataToMerge to return a boolean indicating that the InMemMergeThread should gracefully exit {quote}
would just calling doInMemMerge after the _while (!exitInMemMerge)_ loop in InMemFSMergeThread work?

few other observations:
1. The condition in _ShuffleRamManager.reserve_ is not related to condition in _ShuffleRamManager.waitForDataToMerge()_. So there could be situations (depends on the various thresholds set) when the merge thread is waiting for thresholds to be met; and at the same time shuffle is stalled waiting for merge to happen.
2. i think the _RamManger.reserve(int requestedSize, InputStream in)_ interface could be simplied to _RamManger.reserve(int requestedSize)_. There is no real need to close the connection in reserve method. The connection could be created after the call to reserve in getMapOutput method., bq. would just calling doInMemMerge after the while (!exitInMemMerge) loop in InMemFSMergeThread work?
It probably would work, but as with race-conditions it's hard to _prove_ it works. Either way, this is hacky at best. *smile*

bq.1. The condition in ShuffleRamManager.reserve is not related to condition in ShuffleRamManager.waitForDataToMerge(). So there could be situations (depends on the various thresholds set) when the merge thread is waiting for thresholds to be met; and at the same time shuffle is stalled waiting for merge to happen.
This definitely should not happen - the idea is that amount of data in-memory and no. of in-memory file are the only thresholds. 'dataAvailable' and 'close' are the variables tracking these.

bq. 2. i think the RamManger.reserve(int requestedSize, InputStream in) interface could be simplied to RamManger.reserve(int requestedSize). There is no real need to close the connection in reserve method. The connection could be created after the call to reserve in getMapOutput method.
No. We need to open to connection to 'figure' _requestedSize_ and we absolutely need to close the connection if we are waiting for requestedSize to be available..., {quote} It probably would work, but as with race-conditions it's hard to prove it works. Either way, this is hacky at best. {quote}
not sure I understand the possible race condition here. with which thread/threads do you see the race condition? exitInMemMerge is set to true only after all copier threads are done. 

{quote} This definitely should not happen - the idea is that amount of data in-memory and no. of in-memory file are the only thresholds. 'dataAvailable' and 'close' are the variables tracking these. {quote}
Thread waiting conditions are based on thresholds (MAX_INMEM_FILESYS_USE, MAX_SINGLE_SHUFFLE_SEGMENT_PERCENT etc) If you independently tweak these nos, there could be situation for both threads waiting.
consider this case:
MAX_INMEM_FILESYS_USE = 0.66f
MAX_SINGLE_SHUFFLE_SEGMENT_PERCENT = 0.4f
mergeThreshold = 0
maxSize = 100
fullSize = size = 65
requestedSize = 36

Wont the shuffle threads and the merge thread, both be waiting for the above case? OR am I missing anything? 

{quote}No. We need to open to connection to 'figure' requestedSize and we absolutely need to close the connection if we are waiting for requestedSize to be available...{quote}
Agreed..it seems i missed the requestedSize part. 

, bq. Wont the shuffle threads and the merge thread, both be waiting for the above case?

You are right and the same hit me last evening too! *smile*

So here is the proposal to get around this case:

In ShuffleRamManager.waitForDataToMerge we should check for 2 more conditions to break the deadlock:
1. The no. of shuffle threads blocked on the ShuffleRamManager is greater than 75% of the total no. of shuffle threads. This ensures that we don't block the merge if there is no possiblity of more data being available.
2. To get around the case where we never get to the point where 75% of shuffle-threads are blocked, we need to un-stall the merge when the no. of shuffle-threads blocked is greater than or equal to the number of required map-outputs.

Thoughts?, Proposed fix while I continue testing..., {quote} 2. To get around the case where we never get to the point where 75% of shuffle-threads are blocked, we need to un-stall the merge when the no. of shuffle-threads blocked is greater than or equal to the number of required map-outputs. {quote}
this condition is redundant. 'no of shuffle threads blocked (on RamManager.reserve)' will *always* be less than or equal to 'required map-outputs' . So in the current implementation, merge would never wait.

In the patch, noticed that additional variable numMapsInJob is not required. Already there is numMaps member variable which can be used., Probably you meant '<' instead of '>' sign. I think this should work:

{noformat}  
                 && 
                 (numPendingRequests < 
                      numCopiers*MAX_STALLED_SHUFFLE_THREADS_FRACTION &&
                   numPendingRequests < numRequiredMapOutputs)) 
{noformat} , Updated patch to fix those minor irritants..., Same patch, with fixes to forrest documentation..., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12383882/HADOOP-3517_2_20080611.patch
  against trunk revision 666620.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2642/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2642/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2642/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2642/console

This message is automatically generated., bq.  -1 findbugs. The patch appears to introduce 1 new Findbugs warnings.

Findbugs seems confused... http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2642/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html says:

{quote}
IS 	Inconsistent synchronization of org.apache.hadoop.mapred.ReduceTask$ReduceCopier$ShuffleRamManager.numPendingRequests; locked 66% of time
	

Bug type IS2_INCONSISTENT_SYNC (click for details)
In class org.apache.hadoop.mapred.ReduceTask$ReduceCopier$ShuffleRamManager
Field org.apache.hadoop.mapred.ReduceTask$ReduceCopier$ShuffleRamManager.numPendingRequests
Synchronized 66% of the time
Unsynchronized access at ReduceTask.java:[line 793]
Unsynchronized access at ReduceTask.java:[line 793]
Synchronized access at ReduceTask.java:[line 723]
Synchronized access at ReduceTask.java:[line 753]
Synchronized access at ReduceTask.java:[line 753]
Synchronized access at ReduceTask.java:[line 762]
Synchronized access at ReduceTask.java:[line 762]
{quote}

which isn't true! numPendingRequests is always guarded by 'dataAvailable'., Ah! I see that findbugs complains that numPendingRequests is also synchronized on ShuffleRamManager 66% of the time, that shouldn't matter since it's guarded by ShuffleRamManager.dataAvailable 100% of the time. *smile*, Same patch, removed changes to generated docs in trunk/docs, +1 patch looks good., I just committed this. Thanks, Arun!, Integrated in Hadoop-trunk #520 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/520/])]