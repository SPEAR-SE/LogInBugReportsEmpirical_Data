[Here is a sample pom.xml 

<?xml version="1.0" encoding="UTF-8"?>
<project>
<modelVersion>4.0.0</modelVersion>
<groupId>org.apache.hadoop</groupId>
<artifactId>hadoop-core</artifactId>
<version>0.20.0</version>
</project>

When you release hbase you could upload the jar and this pom to the public maven repository. For instance, the hadoop avro project seems to be uploaded on the public repo. You could contact the hadoop avro team to know how do they upload their last release (avro 1.2.0), How to upload a jar to the Apache directory ?

The public answer :
http://maven.apache.org/guides/mini/guide-central-repository-upload.html

The apache internal answer :
http://www.apache.org/dev/repository-faq.html
, How does Maven deal with C code?  Hadoop is not pure Java, at least, it isn't if you care about performance, the pipes API, etc, etc, ..., With hadoop-5107 we would be able to publish hadoop jars to apache maven repo., Duplicate of the HADOOP-3302 bugrep. Now that Hadoop doesn't depend on -snapshot artifacts, it is possible to push 0.21+ out through the repository, Allen, I don't think you could use the pure Java codebase for production servers, so we need a better way of packaging/distributing the native bits, which are tricky precisely because they aren't x-platform.

But if you are doing client side coding, or bringing up shortlived test clusters, there's no reason why the JARs you need can't go up in the repository for Ivy or Maven users to pick up. I do this in my own SCM-managed repository; I publish my builds, everyone else gets the JARs. 

We could also publish some of the test libraries to the repo, benchmarks and the like, so that its easier to run them against clusters you are just bringing up.

]