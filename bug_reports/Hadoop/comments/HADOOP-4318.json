[This most likely is related to HADOOP-4264, I would be surprised if this is the same issue.
Part of this issue seems to be that distcp tries to go around the AlreadyBeingCreatedException by deleting the file, but it uses the *wrong path*.

From what I can see a distcp task starts copying into a destination file, fails because of some issue, and the next retries cannot create the destination file because there is still a lease on it. And attempts to delete the file do not succeed because they use the *wrong path* /user/.../3164.

Here is the corresponding log of the namenode:

2008-09-30 22:54:49,429 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.startFile: failed to create file ..._distcp_tmp_dvml74/3169 for DFSClient_task_200809121811_0034_m_001085_1 on client xxx.yyy.zzz.uuu because current leaseholder is trying to recreate file
.
2008-09-30 22:54:49,429 INFO org.apache.hadoop.ipc.Server: IPC Server handler 58 on 8600, call .../_distcp_tmp_dvml74/3169, rw-r--r--, DFSClient_task_200809121811_0034_m_001085_1, true, 3, 134217728) from xxx.yyy.zzz.uuu:36614: error: org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file
..._distcp_tmp_dvml74/3169 for DFSClient_task_200809121811_0034_m_001085_1 on client xxx.yyy.zzz.uuu because current leaseholder is trying to recreate file.

org.apache.hadoop.dfs.AlreadyBeingCreatedException: failed to create file ..._distcp_tmp_dvml74/3169 for DFSClient_task_200809121811_0034_m_001085_1 on client xxx.yyy.zzz.uuu because current lease
holder is trying to recreate file.
        at org.apache.hadoop.dfs.FSNamesystem.startFileInternal(FSNamesystem.java:1010)
        at org.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:967)
        at org.apache.hadoop.dfs.NameNode.create(NameNode.java:269)
        at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

2008-09-30 22:54:49,431 WARN org.apache.hadoop.dfs.StateChange: DIR* FSDirectory.unprotectedDelete: failed to remove */user/.../3169* because it does not exist.
, > but it uses the wong path.
By wrong path, are you saying that .../_distcp_tmp_dvml74/3169 and /user/.../3169 are distinct?

DistCp first copies src to tmp and then rename tmp to dst. The file it deleting is dst but not tmp.

From the log above, I think the problem is that the map task dies when copying src to tmp and leaves the tmp file open (even there is a close in the codes). So there is an AlreadyBeingCreatedException in the retries.

If it is the case, we should try deleting tmp before copy., 4318_20081001_0.17.patch: delete tmp before copy.

Christian, how could I reproduce this?  Or could you try the patch?, bq. By wrong path, are you saying that .../_distcp_tmp_dvml74/3169 and /user/.../3169 are distinct?
Yes, they are distinct.

bq. DistCp first copies src to tmp and then rename tmp to dst. The file it deleting is dst but not tmp.
/user/.../3169 is not dst. It is a different file. The user is correct but the distcp command specified a different dst directory than /user., Nicholas, I will try your patch in a couple of hours. Thanks.

What about the wrong dst file name. Is this something which could be fixed easily?, > What about the wrong dst file name. Is this something which could be fixed easily?
I found another bug for the wrong dst.  I will upload a new patch soon., 4318_20081001_0.17b.patch: fixed a bug in cleanup, I am applying now both patches..., hi Christian, you only need 4318_20081001_0.17b.patch., I noticed that 4318_20081001_0.17b.patch
did not contain


-304,8 +304,11
     
     private FSDataOutputStream create(Path f, Reporter reporter,
         FileStatus srcstat) throws IOException {
+      if (destFileSys.exists(f)) {
+        destFileSys.delete(f, false);
+      }

Don't I need this? When looking at the source code of startFileInternal I see that the AlreadyBeingCreatedException is thrown before overwrite is checked for deleting the file.
, > Don't I need this?

I thought we should delete the tmp file right before copying.  However, there are already some cleanup codes (which has a bug).  So I reverted this change., I would argue that we need both, because a task can get killed in the middle of copying with having no time to cleanup., After applying the patches, distcp worked. Thanks, Nicholas.

For the record, I applied both patches., Thanks, Christian.  I will combine two patches unless it conflicts with the original design., 4318_20081002_0.17.patch: combined patch for 0.17, 4318_20081002.patch: for trunk

No new tests are added since the codes changes are simple and I don't have any idea to write a test.  The test should start a distcp job and then some how make some tasks failing., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12391377/4318_20081002.patch
  against trunk revision 700997.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3425/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3425/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3425/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3425/console

This message is automatically generated., +1 Looks good, I just committed this., Integrated in Hadoop-trunk #622 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/622/])
    . DistCp should use absolute paths for cleanup.  (szetszwo)
]