[All paths in Jets3tFileSystemStore are absolute - they begin with '/' since this is enforced in S3FileSystem by the makeAbsolute method. As you point out, they do not include scheme and authority parts. However, I believe this is in line with HDFS, where DistributedFileSystem makes sure paths are absolute but doesn't include scheme and authority parts.

What is the problem that you have encountered - could you give more details (e.g. set up, stacktrace) please?, Hi Tom,

I will try to produce some stack traces for you. But, ultimately, if you look at the DistributedFileSystem implementation of listPaths, it clearly creates fully qualified paths using the DfsPath(DFSFileInfo,FileSystem) constructor. In the case of the s3 implementation, the listPaths, as I mentioned, returns sub-paths without the scheme or the bucket name (authorization). If the default file system is not s3, then the hadoop library returns improper results by trying to resolve the returned sub-path against the default FileSystem ( since the scheme is missing from the path object).

I am working on enabling map-reduce functionality for scenarios where either both, or at least one file specification (map input, and reduce output)  in a map reduce spec points to the s3 file system. The above mentioned bug breaks the code in a couple of different places. When I implement keytoPath in Jet3FileSystemStore as follows, everything works. 

private Path keyToPath(String key) {
    return new Path("s3://"+bucket.getName()+key);
  }

Suffice it to say, there are other (performance related) issues that I am also looking at in order to enable satisfactory use of s3 as a potential input/output for a mapreduce job. But, by far, this bug is the most critically broken issue. 

Sorry about the lack of stack traces. I just need to recreate a proper test environment to get you these, and hopefully I will be able to submit something to you next week. 

Thanks,

Ahad., On a broader note, I believe it would be a good thing to make it a rule that all FileSystem implementations return fully qualified (scheme and host specified) paths in all cases. Since the system uses Path objects everywhere, and in most cases does a late, on demand, resolution from Path to FileSystem object and then Input / OutputStream, an improperly qualified path combined with the use of more than one FileSystem for an Input / Output source, is bound to cause unpredictable behavior elsewhere in the code as well.

Just a suggestion. 

Thanks,

Ahad., Hi Ahad,

bq. But, ultimately, if you look at the DistributedFileSystem implementation of listPaths, it clearly creates fully qualified paths using the DfsPath(DFSFileInfo,FileSystem) constructor.

Yes - I missed this.

bq. In the case of the s3 implementation, the listPaths, as I mentioned, returns sub-paths without the scheme or the bucket name (authorization).

I've created a patch that fixes this (in the same way as DistributedFileSystem). Does this solve your problem?, > I believe it would be a good thing to make it a rule that all FileSystem implementations return fully qualified (scheme and host specified) paths in all cases.

This sounds reasonable to me.  Could you submit a new issue for it?  I think the fix would be to (1) note this in FileSystem's javadoc; (1) fix all implementations; (3) add some unit tests.
, Hi Tom, 

The patch looks good to me. Are you going to add the JIRA issue related to updating the FileSystem's javadocs ? 

Thx.

Ahad., I've just added HADOOP-1909 for the fix for other implementations., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12364948/HADOOP-1783.patch
against trunk revision r576315.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests -1.  The patch failed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/781/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/781/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/781/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/781/console

This message is automatically generated., I've just committed this., Integrated in Hadoop-Nightly #248 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/248/])]