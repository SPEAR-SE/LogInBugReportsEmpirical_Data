[You mean instead of calling fatalError() like its doing now?, More background: We saw this when ZK became inaccessible for a few minutes. ZKFC went down and the corresponding master was transitioned to Standby. 

bq. You mean instead of calling fatalError() like its doing now?
Yes. Or, we should have two retry modes. The retries we have today followed by a call to becomeStandby, within an outer retry-forever loop that sleeps for a shorter time between inner-loops.

, Preliminary patch that articulates what I have in mind., Logs from when we saw this error:

{noformat}
zzzz-yy-xx 06:01:30,039 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 3335ms for sessionid 0x2459abcbfd0027f, closing socket connection and attempting reconnect
zzzz-yy-xx 06:01:30,144 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
zzzz-yy-xx 06:01:30,233 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server MASKED-1/10.1.128.51:2181. Will not attempt to authenticate using SASL (unknown error)
zzzz-yy-xx 06:01:30,233 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to MASKED-1/10.1.128.51:2181, initiating session
zzzz-yy-xx 06:01:31,901 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 1667ms for sessionid 0x2459abcbfd0027f, closing socket connection and attempting reconnect
zzzz-yy-xx 06:01:32,405 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server MASKED-2/10.1.128.48:2181. Will not attempt to authenticate using SASL (unknown error)
zzzz-yy-xx 06:01:32,406 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to MASKED-2/10.1.128.48:2181, initiating session
zzzz-yy-xx 06:01:32,409 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server MASKED-2/10.1.128.48:2181, sessionid = 0x2459abcbfd0027f, negotiated timeout = 5000
zzzz-yy-xx 06:01:32,412 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
zzzz-yy-xx 06:01:35,742 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 3334ms for sessionid 0x2459abcbfd0027f, closing socket connection and attempting reconnect
zzzz-yy-xx 06:01:35,850 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
zzzz-yy-xx 06:01:35,966 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server MASKED-3/10.1.128.49:2181. Will not attempt to authenticate using SASL (unknown error)
zzzz-yy-xx 06:01:35,967 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to MASKED-3/10.1.128.49:2181, initiating session
zzzz-yy-xx 06:01:35,968 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server MASKED-3/10.1.128.49:2181, sessionid = 0x2459abcbfd0027f, negotiated timeout = 5000
zzzz-yy-xx 06:01:35,972 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
zzzz-yy-xx 06:01:39,303 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 3335ms for sessionid 0x2459abcbfd0027f, closing socket connection and attempting reconnect
zzzz-yy-xx 06:01:39,411 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
zzzz-yy-xx 06:01:39,904 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server MASKED-1/10.1.128.51:2181. Will not attempt to authenticate using SASL (unknown error)
zzzz-yy-xx 06:01:39,904 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to MASKED-1/10.1.128.51:2181, initiating session
zzzz-yy-xx 06:01:41,572 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 1668ms for sessionid 0x2459abcbfd0027f, closing socket connection and attempting reconnect
zzzz-yy-xx 06:01:41,678 FATAL org.apache.hadoop.ha.ActiveStandbyElector: Received stat error from Zookeeper. code:CONNECTIONLOSS. Not retrying further znode monitoring connection errors.
zzzz-yy-xx 06:01:41,926 INFO org.apache.zookeeper.ZooKeeper: Session: 0x2459abcbfd0027f closed
zzzz-yy-xx 06:01:41,927 FATAL org.apache.hadoop.ha.ZKFailoverController: Fatal error occurred:Received stat error from Zookeeper. code:CONNECTIONLOSS. Not retrying further znode monitoring connection errors.
zzzz-yy-xx 06:01:41,927 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x2459abcbfd0027f
zzzz-yy-xx 06:01:41,927 INFO org.apache.hadoop.ipc.Server: Stopping server on 8018
zzzz-yy-xx 06:01:41,927 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
zzzz-yy-xx 06:01:41,928 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
zzzz-yy-xx 06:01:41,928 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8018
zzzz-yy-xx 06:01:41,928 INFO org.apache.hadoop.ha.HealthMonitor: Stopping HealthMonitor thread
zzzz-yy-xx 06:01:41,928 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
{noformat}, After posting the patch, I had tried to validate but couldn't reproduce it. Closing this as "Cannot Reproduce". Please reopen if you run into this again. , I met the similar error for YARN RM that enabled HA automatic-failover. 

{noformat}
2015-01-04,12:42:30,682 FATAL org.apache.hadoop.ha.ActiveStandbyElector: Received stat error from Zookeeper. code:CONNECTIONLOSS. Not retrying further znode monitoring connection errors.
2015-01-04,12:42:30,886 INFO org.apache.zookeeper.ZooKeeper: Session: 0x2498936f2a8c448 closed
2015-01-04,12:42:30,888 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-01-04,12:42:30,888 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type EMBEDDED_ELECTOR_FAILED. Cause:
Received stat error from Zookeeper. code:CONNECTIONLOSS. Not retrying further znode monitoring connection errors.
2015-01-04,12:42:30,891 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
{noformat}, Reopening to investigate it more. If anyone wants to pick this up, they are more than welcome. , Ran into this on one of our test clusters. Looks like this needs fixing. , Does seem critical to me, but haven't seen any activity in a while. [~kasha] / [~xgong], can one of you comment on a possible fix? If it's uncertain, I'd like to move this to 2.7.2. Tx., We run into this occasionally on our test clusters. From my previous investigation, the patch I posted should help. However, I couldn't test because I couldn't find a way to reproduce the problem. It should be okay to punt to 2.7.2. , [~vinodkv] [~kasha]

bq. From my previous investigation, the patch I posted should help.

It does help. But in the patch, reJoinElection(0) is called, which will further call  joinElectionInternal
{code}
  private void joinElectionInternal() {
    Preconditions.checkState(appData != null,
        "trying to join election without any app data");
    if (zkClient == null) {
      if (!reEstablishSession()) {
        fatalError("Failed to reEstablish connection with ZooKeeper");
        return;
      }
    }
    createRetryCount = 0;
    wantToBeInElection = true;
    createLockNodeAsync();
  }
{code}
Since the ZK quorum is unavailable, we still have the same issue. The difference is that with the patch we will retry 45s more(by using the default configuration).

So if we will want to use retry-then-exist pattern, I think that both current code and current code + the patch are fine. We also need to modify the configurations based on the cluster.

Or, if we do not expect RM exists because of this reason (ZK quorum is unavailable), instead of doing
{code}
    public void handle(RMFatalEvent event) {
      LOG.fatal("Received a " + RMFatalEvent.class.getName() + " of type " +
          event.getType().name() + ". Cause:\n" + event.getCause());

      ExitUtil.terminate(1, event.getCause());
    }
{code}

We could check the eventType, and transit the RM to standby ,then rejoin electorService., Moving this to 2.7.2 after ack from Karthik.

bq. It does help. But in the patch, reJoinElection(0) is called, which will further call joinElectionInternal ..
bq. Since the ZK quorum is unavailable, we still have the same issue. The difference is that with the patch we will retry 45s more(by using the default configuration).
Tx for pointing this out, [~xgong]. That still sounds bad though. IIRC, when we were originally discussing the design of state-stores for RM, we were assuming that RM should give up on Zookeeper only after trying for may be ~an hour. /cc [~jianhe]. Looking at the recent flow of tickets, that doesn't seem to be the case?
, Sorry for pitch in late. After looking at the logic, I also feel this case can occur in the production clusters. On ZooKeeper connection loss ActiveStandbyElector will do certain number of retries and finally notifies {{ActiveStandbyElectorCallback#notifyFatalError()}}. I could see the {{EmbeddedElectorService#notifyFatalError}} implementation is handling the case by immediately terminating the service. I think we have room to improve this logic instead of immediately terminating.

About the proposed patch, IIUC it is not required to do an additional handling of ZooKeeper exceptions and do re-election in ActiveStandbyElector class. Presently we have {{ActiveStandbyElector#processWatchEvent}} logic to handle the ZK connection state changes. On connection state changes, ZooKeeper client will notify this to the registered ZK watcher like, SyncConnected, Disconnected, Expired etc. Based on the STATE {{ActiveStandbyElector}} is notifying the registered {{ActiveStandbyElectorCallback}} and does the state transitions. Please see [ActiveStandbyElector.java#L550|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java#L550]

What I meant is ZooKeeper client will be alive which internally does the connection re-establishment infinitely. IMHO, we could think of implemeting {{EmbeddedElectorService#enterNeutralMode}} to handle the NEUTRAL transition of RM. Also, {{ActiveStandbyElectorCallback#notifyFatalError()}} has to be refined. Any thoughts?

{code}
  public void enterNeutralMode() {
    /**
     * Possibly due to transient connection issues. Do nothing.
     * TODO: Might want to keep track of how long in this state and transition
     * to standby.
     */
  }
{code}, Too late for 2.7.2, moving this out.

Overall, I am tending to think that ActiveStandbyElector is already doing the right thing and it's just the higher layers passing in insufficient retry configurations.

All the retries inside ActiveStandbyElector are for CONNECTIONLOSS and OPERATIONTIMEOUT events, so it sounds silly that we pass in a retry-count of 3 (together with a 10 seconds in YARN / 5 seconds in HDFS session time-out) in both HDFS and YARN.

If you agree, I think we should just bump up these defaults so that we can retry for 'enough' time as is acceptable. Thoughts?, Based on my recollection from a while ago and briefly looking at the attached prelim patch, there are a couple of issues here:
# When RM loses connection while executing an operation, the operation just fails without enough retries. The patch adds a retry-loop to handle this.
# When RM loses connection to ZK but doesn't give up being Active. This leads to the RM continuing to serve apps and nodes connected to it. The patch, in addition to rejoining election, has the client (ZKFC/RM) enter neutral mode. Today, the RM doesn't do anything on {{enterNeutralMode}} but of course this can be improved going forward. 

I won't be able to work on this for the next month or so. If anyone has cycles, please feel free to take it up. , Adding a 3.x fix version for tracking, good bug to get fixed?, I'm looking at this issue now, and it seems to me that the issue could be resolved by reseting the retry counts when the session is reconnected.  If we've lost the session, then whatever retry counts we had previously don't really apply anymore, so we should reset them on reconnect.  It looks like this issue is happening only in the case that the ZK connection is flaky., Resetting the counts isn't the answer.  I can now reproduce this issue reliably by setting a break point in {{processWatchEvent()}} and shutting down ZK before continuing.  The issue is a race condition between the events from the ZK client and creating/statting the ZK node.  If the disconnected update event comes first, all is well.  If not, it will retry a few times and then fail the RM.

To echo earlier comments, why does ZK connection loss necessitate stopping the RM in this case?  It doesn't in any other case.  My proposal would be to remove the fatal error completely.  We could instead either transition to standby explicitly or just ignore the error (and hence the retries) on connection loss and wait for the ZK event to trigger the transition.  I kinda like the latter.  Any opinion?, Here's a rough first patch that removes the fatal errors from the {{ActiveStandbyElector}}.  I haven't done much testing yet, but I wanted to get it posted for comments.  This patch also eliminates wrapping {{InterruptedException}} in {{IOException}} so that it's possible to tell the two apart.  Feedback welcome!, Hi [~templedf],  does this still target for 2.9.1?  If not, can we push this out to next 2.9.2 release? ]