[I frequently see the same failure. I don't normally spend time in the HDFS code so am embarrassed to say that I haven't really tried to troubleshoot this.

Assertion is same as mentioned above. Here's the test's "-output.txt" file, in case this rings a bell for somebody:

{noformat}
2012-05-23 11:49:50,858 INFO  mortbay.log (Slf4jLog.java:info(67)) - Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2012-05-23 11:49:50,863 INFO  mortbay.log (Slf4jLog.java:info(67)) - Home dir base /home
2012-05-23 11:49:50,912 WARN  util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2012-05-23 11:49:51,005 INFO  fs.TrashPolicyDefault (TrashPolicyDefault.java:deleteCheckpoint(198)) - Deleted trash checkpoint: /home/jfinger/.Trash/120209191848
2012-05-23 11:49:51,007 INFO  fs.TrashPolicyDefault (TrashPolicyDefault.java:createCheckpoint(162)) - Created trash checkpoint: /home/jfinger/.Trash/120523114951
Moved: 'viewfs:/home/jfinger/hadoop-2.0.0-alpha-src/hadoop-common-project/hadoop-common/target/test/data/test/test/mkdirs/myFile' to trash at: file:/home/jfinger/.Trash/Current
Moved: 'viewfs:/home/jfinger/hadoop-2.0.0-alpha-src/hadoop-common-project/hadoop-common/target/test/data/test/test/mkdirs/myFile' to trash at: file:/home/jfinger/.Trash/Current
rmr: DEPRECATED: Please use 'rm -r' instead.
Moved: 'viewfs:/home/jfinger/hadoop-2.0.0-alpha-src/hadoop-common-project/hadoop-common/target/test/data/test/test/mkdirs' to trash at: file:/home/jfinger/.Trash/Current
rmr: DEPRECATED: Please use 'rm -r' instead.
Moved: 'viewfs:/home/jfinger/hadoop-2.0.0-alpha-src/hadoop-common-project/hadoop-common/target/test/data/test/test/mkdirs' to trash at: file:/home/jfinger/.Trash/Current
Deleted file:///home/jfinger/.Trash/Current/toErase
2012-05-23 11:49:51,121 INFO  fs.TrashPolicyDefault (TrashPolicyDefault.java:createCheckpoint(162)) - Created trash checkpoint: /home/jfinger/.Trash/120523114951
Moved: 'viewfs:/home/jfinger/hadoop-2.0.0-alpha-src/hadoop-common-project/hadoop-common/target/test/data/test/test/mkdirs/myFile' to trash at: file:/home/jfinger/.Trash/Current
rmr: DEPRECATED: Please use 'rm -r' instead.
Moved: 'viewfs:/home/jfinger/hadoop-2.0.0-alpha-src/hadoop-common-project/hadoop-common/target/test/data/test/test/mkdirs' to trash at: file:/home/jfinger/.Trash/Current
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: Cannot move "file:/home/jfinger" to the trash, as it contains the trash. Consider using -skipTrash option
expunge: Target /home/jfinger/.Trash/120523114951/Current is a directory
{noformat}
, I finally became sufficiently annoyed by this test failure that I took a quick look into it...  Here's something interesting:
{code}rmr: Cannot move "file:/Users/daryn" to the trash, as it contains the trash. Consider using -skipTrash option{code}

The test *tries to delete your home directory* but is stopped by some of the logic it's trying to test.  I'm going to look at this a bit more, but anyone else who works with it better beware., Finally tracked this down.  The clue is in the the expunge error message, as Current should always be directly under .Trash, but in this case it's under one of the snapshot directories.  Here's the sequence of events that causes it to happen:

# File is deleted, placing it under .Trash/Current (created if necessary)
# Trash is expunged, creating a new checkpoint that renames Current to the checkpoint name
# Another file is deleted, which creates .Trash/Current
# Trash is expunged *in the same wall-clock second* as the previous expunge, inadvertently renaming Current to an existing directory and placing it *under* the checkpoint name
# Another file is deleted, which creates .Trash/Current
# Trash is expunged again *in the same wall-clock second* as the first expunge, trying to rename Current to an existing directory.  Current already exists under the target directory and the lower-level fs rename fails.  When it falls back to a copy, the checkDest() fails with the "Target is a directory" error.

So this explains why it's only failing sometimes.  The machine has to be fast enough so the test gets through three expunges in the same second., Patch to change the trash checkpoint name to include millisecond resolution.  Race is still there, but the window is 1000x smaller., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533890/HADOOP-8110.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1150//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1150//console

This message is automatically generated., On second thought, we should be able to eliminate the race and make this more robust by preventing the rename from succeeding if the destination name exists.  If we collide during the checkpoint rename, we can simply retry with an attempt suffix to search for a unique name., Patch that checks for renames to an existing path and retries with a -# suffix looking for a unique path., Jenkins crashed during build, so uploading same patch to kick it again.  No unit tests since this is fixing a test.

I manually tested it and verified that ~/.Trash had checkpoint directories with -# suffixes, since the test usually generates multiple checkpoints in the same second., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533976/HADOOP-8110.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1153//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1153//console

This message is automatically generated., +1 Looks good, 1000 might be steep but it shouldn't do more than a few iterations., I've committed to trunk, branch-2, and branch-23.  Big thanks to Jason for fixing the actual bug that has long caused the test failure!, Thank you Jason!, Integrated in Hadoop-Common-trunk-Commit #2422 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2422/])
    HADOOP-8110. Fix trash checkpoint collisions (Jason Lowe via daryn) (Revision 1356897)

     Result = SUCCESS
daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1356897
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java
, Integrated in Hadoop-Hdfs-trunk-Commit #2490 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2490/])
    HADOOP-8110. Fix trash checkpoint collisions (Jason Lowe via daryn) (Revision 1356897)

     Result = SUCCESS
daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1356897
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java
, Integrated in Hadoop-Mapreduce-trunk-Commit #2439 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2439/])
    HADOOP-8110. Fix trash checkpoint collisions (Jason Lowe via daryn) (Revision 1356897)

     Result = FAILURE
daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1356897
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java
, Integrated in Hadoop-Hdfs-trunk #1094 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1094/])
    HADOOP-8110. Fix trash checkpoint collisions (Jason Lowe via daryn) (Revision 1356897)

     Result = FAILURE
daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1356897
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java
, Integrated in Hadoop-Hdfs-0.23-Build #304 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/304/])
    HADOOP-8110. Fix trash checkpoint collisions (Jason Lowe via daryn) (Revision 1356913)

     Result = SUCCESS
daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1356913
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java
, Integrated in Hadoop-Mapreduce-trunk #1127 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1127/])
    HADOOP-8110. Fix trash checkpoint collisions (Jason Lowe via daryn) (Revision 1356897)

     Result = SUCCESS
daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1356897
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java
]