[Patch relative to trunk/, +1, because http://issues.apache.org/jira/secure/attachment/12352260/fsdataset.patch applied and successfully tested against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/512924. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch, Would it be worth checking the return value when deleting the temporary file? If the file couldn't be removed then throw the exception.

Apart from that, this looks good to me., Applied with the suggested change., This implies that there is also a failure to clean up blocks properly in the case of exceptions. That should be addressed instead of a time out. I'm just worried that the timeout could happen on a slow datanode and/or client and then we'll have more mysterious errors as one thread deletes block files out from under another thread., Right - the exception handling in DataNode.DataXceiver.writeBlock() is less than ideal - even though it catches the exception it still continues to FSDataset.finalizeBlock () as if nothing happened.

Regarding your concern: the timeout value here is 1 hour  - this is way way longer than the ipc.timeout, which determines how long the socket connections are kept around. Once the connection is broken we end up in this condition. It's true that we should avoid getting here - I'll see if I can figure out how to do it, and prepare a separate issue for this., > Owen: I'm just worried that the timeout could happen on a slow datanode and/or client and then we'll have more mysterious errors [...]

Then perhaps we should log a warning whenever this happens?

> Andrzej: the timeout value here is 1 hour - this is way way longer than the ipc.timeout

Perhaps this should then instead be expressed as a function of that or some other parameter rather than hard-coded?]