[One easy way to recreate this problem is as follows. Do

{code}
hadoop jar some_jar -D mapred.reduce.tasks=100 ...
{code}

where my class uses ToolRunner to do its task:

{code}
public static void main(String[] args) {
  ToolRunner.run(new Configuration(), new MyClass(), args);
}
{code}

ToolRunner will start using GenericOptionsParser to parse the -D overrides, and it will call Configuration.set() to set the overrides. However, in this case the JobConf class is still not initialized. Therefore, the "mapred.reduce.tasks" key is not registered as deprecated. As a result, the Configuration will have "mapred.reduce.tasks" set to 100, but "mapreduce.job.reduces" (the new key) still set to 1 (the default).

The configuration will be left in this inconsistent state until the JobConf class is initialized and a get() call for either of these keys is made.

In our particular scenario, a map was created out of this Configuration using Configuration.iterator(), and eventually caused it to lose the overridden value.

I think this inconsistent state has many ways to cause havoc, and I believe this is a major bug., Marking as blocker. W/o this Cascading was not able to properly replace deprecated keys with new ones in the config causing jobs to fail., [~sjlee0] Do you know what caused this issue with Cascading? Is it using new keys? Or, are you putting new keys (mapreduce.job.reduces) in your configs?, I'm wondering if the fix is simply to use the appropriate configuration object for the tool in question.  Since the tool requires supporting MapReduce configs, have it use JobConf which is MapReduce-aware instead of Configuration which is not.  For example:

{code}
public static void main(String[] args) {
  ToolRunner.run(new JobConf(), new MyClass(), args);
}
{code}

Otherwise I don't see how we can determine that the particular Configuration object is really going to later be a JobConf object and load the deprecated keys ahead of time. Even if we tried to fix it up after the fact we could encounter situations where the config has conflicting values for keys.  For example, two different chunks of code use different keys for the same concept and we end up with something like -Dmapred.reduce.tasks=20 and -Dmapreduce.job.reduces=100.  If these keys were set before the Configuration morphed into a JobConf, we wouldn't know which was the correct setting that should survive once JobConf arrives and tries to rectify the situation., [~acmurthy], the sequence of events is a little involved. The specific combination is scalding and cascading. The problem starts when the user sets an old key. Let's say the user set the old key to 100 (where the default is 1).

Scalding runs the task with the ToolRunner but uses Configuration. In its tool, scalding converts the configuration items from the Configuration into a Map<String,String> and passes it to cascading. In cascading, using a Map as opposed to a Configuration or JobConf is a standard mode of operation. When scalding creates the Map, it uses Configuration.iterator() to get the values out of the Configuration. So up until this point, the deprecated keys from JobConf have not been registered (because JobConf was not initialized), and thus the Map now contains inconsistent values for any map-reduce-related config params. At this point, the Map has 100 under the old key, and 1 under the new key.

Once cascading receives this Map, it eventually constructs a JobConf instance using the Map. This triggers all the deprecated keys for JobConf to be registered. And when values from Map are set onto JobConf via set(), the new value from the Map wins (which is still the default value, not the user-overridden value), thus wiping out the user override. The JobConf instance now has 1 for both the new and the old keys., [~jlowe] Actually that's precisely what we ended up doing with scalding to address this issue: https://github.com/twitter/scalding/commit/c8d963496cbe3e327359a893ee737cd42367a4a4

And I also cannot think of a clean solution that can prevent this type of problems, or I would have suggested a patch.

But I feel some discomfort in this in the sense that there is nothing that stops people from setting map-reduce specific properties using Configuration, and yet there is this window of opportunity between when Configuration is initialized and JobConf is initialized where such an action of setting a property will leave the internal state of Configuration in an inconsistent state.

Since we fixed this on the scalding side, we're proceeding OK now. However, it would be great if we could at least clarify this somewhere (javadoc or wiki or?) that if you're going to use map-reduce properties you must use JobConf instead of vanilla Configuration when setting the properties.

Thoughts?
]