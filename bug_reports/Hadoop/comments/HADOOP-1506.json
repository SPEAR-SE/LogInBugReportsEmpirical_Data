[We could avoid using FileSystem.getReplication and FileSystem.getBlockSize(). They will result in separate RPCs to namenode. Instead, we can get getFileSystus introduced by HADOOP-1377. Or use FileSystem.listPaths because it caches file attributes in the Path object., This should probably be optional. DFS in general wants to enforce the condition that dfs.block.size should be a multiple of io.bytes.per.checksum. This could be an issue if we are copying files written in earlier versions. Making this optional also has the advantage that we could change block size etc if we want to. , What is the reason for keeping the same block size in the target file?
The target file system may have a different default block size.
Why do we want to go against the default in this case?
Ex. If we copy a file from ext2 with 1K blocks to ext3 with 8K blocks we are not trying to preserve any block sizes.
, Blocksizes cannot be preserved until they are recorded., Fixed by HADOOP-1569.]