[
[A comment|https://issues.apache.org/jira/browse/HADOOP-1292#action_12506677] in HADOOP-1292 says :

{quote}
3. I am unable to understand the behaviour of "another" file in FsShell.copyToLocal. Will discuss this with you.

If renaming tmp to dst failed, tmp is renamed to another file since tmp will be deleted on exit. The error message would tell the user that src is copied to "another" successfully but cannot be renamed to dst.
{quote}

I am not sure about the above comment either. When rename() fails wouldn't that be just another case of failure to copy? Once we use same filesystem as the intended destination, rename() won't fail. And if it does fail, we could just treat it as any other failure.
, Fix is to pass {{dfs.getAbsolutePath()}} instead of {{dst}} for {{FileUtil.createLocalTempFile()}}.

Also this removes 'if conditional' around File.copy() since return is expected to be true and not handled when it is false anyway.

This patch also removes the special treatment of rename(). It is strictly not required for this fix. Should I remove the change?, There are some more inconsistencies in copyToLocal() :

For example : 
{noformat}
hadoop-14> bin/hadoop dfs -copyToLocal '/user/rangadi/x*' tmp
copyToLocal: src "/user/rangadi/x*" does not exist.
hadoop-14> mkdir tmp
hadoop-14> bin/hadoop dfs -copyToLocal '/user/rangadi/x*' tmp
hadoop-14> ls tmp
x
{noformat}

I will fix this as well. There is also an extra isDirectory() that is not required., There are at least three versions of copyToLocal : one in FsShell, one in ChecksumFileSystem, and FileUtil. All of these implement same logic and recursion.. but are slightly different. FsShell and ChecksumFS versions in turn invoke FileUtil version. We should remove FsShell and ChecksumFS, at least in 0.15 or 0.16. , Attached patch keeps copyToLocal() in FsShell.java closer to 0.13 structure. 

Fixes following regressions : 
- copying a file with simple file name for destination as described in the description.
- handling of globes : {{globePath()}} was invoked late and when it returns just one path, treated it as non-globed path, unless the destination was a directory (see example in earlier comment).

Retains the following change between 0.13 and 0.14 :
- {{bin/hadoop fs -get dir1 dir2 localdir}}   # two dirs can be specified with a glob
-- 0.13 copies _contents_ of dir1 and dir2 into localdir
-- 0.14.x copies dir1 and dir2 into localdir (matches with regular cp)

The following behaviour is new :
- when rename() fails during the copy does not copy temp file to another temp file.
- {{bin/hadoop fs -get dir1 localdir}} # and localdir exists
-- 0.13 and 0.14.1 copy contents of dir1 into localdir
-- 0.14.2 copies dir1 into localdir 
-- btw, when localdir does not exist, all versions copy contents of dir1 into localdir., Raghu, your patch cannot be applied to the current trunk.  Could you update it?, This patch applies to 0-14 branch. I thought I tested it with trunk as well. I will attach an updated patch for trunk., Updated patch for trunk.

If we want to retain the partially copied file, then fix I would suggest is to remove 'deleteOnExit' flag for the tmp file., +1
Patch looks good.  Below are some minor thoughts.

- I totally agree that there are redundant copyToLocal methods.  See also HADOOP-1544.

- In the case of rename failing, we know that the tmp file is good but cannot be renamed to dst.  User can easily rename the tmp file.  For the other exception cases, user don't know what to do to fix the problem.  If we remove "deleteOnExit" flag, then we cannot easily tell whether the tmp file is perfect or not.

- Should we not use deprecated API anymore?  e.g. srcFS.isDirectory(src), srcFS.listPaths(src), >     *  Should we not use deprecated API anymore? e.g. srcFS.isDirectory(src), srcFS.listPaths(src)

yeah, I saw that. I just used what ever was there before. I think it will be removed when all the references to it are removed.

> In the case of rename failing,

With this fix rename should rarely fail. I think users will be wary of any error. The main concern that Dhruba mentioned is while users are copying very large file and they might might be ok with partial files. Retaining the file only in case of rename failure does not fix that problem.
, -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12367039/HADOOP-1961.patch
against trunk revision r581745.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests -1.  The patch failed core unit tests.

    contrib tests -1.  The patch failed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/881/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/881/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/881/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/881/console

This message is automatically generated., I was dealing with multiple patches and totally missed running the unit tests for this. 

TestDFSShell faile because of  the following change : 

{quote}
    - bin/hadoop fs -get dir1 localdir # and localdir exists
          -- 0.13 and 0.14.1 copy contents of dir1 into localdir
          -- 0.14.2 copies dir1 into localdir
{quote}

There are no change the fix. Only TestDFSShell.java is changed. Also updated the patch to copy multiple directories.
, +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12367089/HADOOP-1961.patch
against trunk revision r581745.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/884/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/884/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/884/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/884/console

This message is automatically generated., I just committed this. Thanks Raghu!, Integrated in Hadoop-Nightly #261 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/261/])]