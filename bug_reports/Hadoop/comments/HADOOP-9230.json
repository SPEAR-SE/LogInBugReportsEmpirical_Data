[checkAgainstLegacy() compares the generated splits against a legacy split generation. I don't quite understand the purpose behind this check. Can anyone who knows this better throw some light on why we need the test.

I noticed a conflict in the math between UniformSizeInputFormat split generation and the legacy generation:

Current:
{code}
long nBytesPerSplit = (long) Math.ceil(totalSizeBytes * 1.0 / numSplits);
{code}

Legacy:
{code}
final long targetsize = totalFileSize / numSplits;
{code}

I would expect the math discrepancy to lead to more than 10% failure rate though., I put more thought into this, but couldn't figure out the need for check against legacy splits that is failing.

Could change the legacySplits implementation, but then the test doesn't really make any sense.

Not finding an alternative, removed the check against legacy splits - uploading the corresponding patch., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12567942/hadoop-9230.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-distcp.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2147//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2147//console

This message is automatically generated., I guess that the legacy split check was included as a sanity check. Since the split calculation is slightly different, the occasional difference should not be too surprising. Nor should it be a problem for correctness, since the aim of the calculation is to produce roughly equal-sized splits for distcp. Therefore I think we can remove the legacy check in order to ensure that the test always passes. +1 for the patch.

bq. I would expect the math discrepancy to lead to more than 10% failure rate though.

Although the difference between floor and ceiling will almost always produce different target split sizes differing by 1 (whenever the number of splits doesn't exactly divide the total size), only very rarely will this affect whether a file is or is not included in any particular split. This is why the failures are relatively rare. 

E.g. on one failure I got the file sizes were 1482, 2012, 1860, ... (making a total size of 31439) and for 9 maps the legacy target size works out at 3493, while the new target size is 3494. The first legacy split only included 1482 (since 1482 + 2012 > 3493) while the first split for the new code includes both 1482 and 2012.
, Thanks for the investigation, Tom. As tailoring the test just to make it pass doesn't make sense, I think we should get rid of the test. Pinged [~mithun] about 10 days ago to see if he has any additional insights on this.

I think it is safe to commit the patch. , Agree, committing patch., Thanks Karthik. Committed to trunk and branch-2., Integrated in Hadoop-trunk-Commit #3396 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3396/])
    HADOOP-9230. TestUniformSizeInputFormat fails intermittently. (kkambatl via tucu) (Revision 1451291)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1451291
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
, Integrated in Hadoop-Yarn-trunk #142 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/142/])
    HADOOP-9230. TestUniformSizeInputFormat fails intermittently. (kkambatl via tucu) (Revision 1451291)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1451291
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
, Integrated in Hadoop-Hdfs-trunk #1331 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1331/])
    HADOOP-9230. TestUniformSizeInputFormat fails intermittently. (kkambatl via tucu) (Revision 1451291)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1451291
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
, Integrated in Hadoop-Mapreduce-trunk #1359 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1359/])
    HADOOP-9230. TestUniformSizeInputFormat fails intermittently. (kkambatl via tucu) (Revision 1451291)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1451291
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestUniformSizeInputFormat.java
, Thanks, Karthik!  I committed this to branch-0.23 as well.]