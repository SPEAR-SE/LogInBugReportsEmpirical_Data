[The attached patch refactors the HttpServer2  code such that a single filter is created and is shared among different contexts., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650478/HADOOP-10703.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4071//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4071//console

This message is automatically generated., I think it looks ok, but someone with more jetty knowledge should confirm there's no unexpected side-effects., By servlet spec it is expected that servlets/filters instances are not shared among webapps.  If we want to respect that semantics the current behavior is correct, each context (webapp) should have its own instance., I quickly  scanned the servlet spec, but could not find the related text.
[~tucu00], could you please point me to the text ?

What's the benefit in creating duplicate filter instances in this use case ?
, Could I please get a review of this patch ?
We have been running with this change on a our internal clusters for last 6 months and no issue has been detected. 
I have not seen anything in the servlet spec which prohibits sharing filter s across web apps., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650478/HADOOP-10703.patch
  against trunk revision 7574df1.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5482//console

This message is automatically generated., Cancelling patch, as it no longer applies., Rebased the patch with trunk, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12703172/HADOOP-10703-002.patch
  against trunk revision de1101c.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ha.TestSshFenceByTcpPort
                  org.apache.hadoop.ipc.TestRPCWaitForProxy

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5892//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5892//console

This message is automatically generated., +1 for the patch. Thanks [~benoyantony]., Also I am holding off committing since [~tucu00] has not signed off yet. For my part, I didn't see anything in the documentation requiring the instances to be unique., I plan to commit this on monday.
[~tucu00], Please let me know if you found any specification which prevents sharing filter instances across webapps. Do let me know, if there are any other objections to this patch.
, Please hold off -- I'm not fully convinced myself this is the right thing to do. I'll take a closer look at the relevant jiras in the upcoming days and come back., Search around and checked the source code of jetty 6. It looks like that it is common that a single servlet / filter instance serves multiple requests concurrently. Some frameworks might hide the complexity by creating new servlet instance on the fly -- at the very least jetty will dispatch multiple requests to the same servlet based on a quick skim over the code.

The patch looks quite good for me. Some minor nits:

{code}
     final String[] USER_FACING_URLS = { "*.html", "*.jsp" };
{code}

Since we no longer have jsp, I think you can simple merge {{USER_FACING_URLS}} into {{ALL_URLS}}.

{code}
+  private static void defineFilter(Context ctx, FilterHolder holder,
+      FilterMapping fmap ) {
{code}

There is an additional space in the declaration. +1 once addressed., [~benoyantony], can you please update the patch?

[~aw], does committing this jira mean that HDFS-5796 is no longer a blocker for 2.7? What are there other things need to be done?, Thanks [~wheat9]. 
I have updated the patch to address space issue. 

I am not sure about the impact of merging USER_FACING_URLS with ALL_URLS.. Can we please address this in a separate jira ?, bq. Can we please address this in a separate jira?

Sure.

+1 pending jenkins., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12705455/HADOOP-10703-003.patch
  against trunk revision 20b4922.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5967//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5967//console

This message is automatically generated., I've committed the patch to trunk, branch-2 and branch-2.7. Thanks [~benoyantony] for the contribution., FAILURE: Integrated in Hadoop-trunk-Commit #7370 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7370/])
HADOOP-10703. HttpServer2 creates multiple authentication filters. Contributed by Benoy Antony. (wheat9: rev 19b298f6124f98770c0831dc9e13ddfccb525a3c)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
* hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #137 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/137/])
HADOOP-10703. HttpServer2 creates multiple authentication filters. Contributed by Benoy Antony. (wheat9: rev 19b298f6124f98770c0831dc9e13ddfccb525a3c)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #871 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/871/])
HADOOP-10703. HttpServer2 creates multiple authentication filters. Contributed by Benoy Antony. (wheat9: rev 19b298f6124f98770c0831dc9e13ddfccb525a3c)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
* hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2069 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2069/])
HADOOP-10703. HttpServer2 creates multiple authentication filters. Contributed by Benoy Antony. (wheat9: rev 19b298f6124f98770c0831dc9e13ddfccb525a3c)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
* hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #128 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/128/])
HADOOP-10703. HttpServer2 creates multiple authentication filters. Contributed by Benoy Antony. (wheat9: rev 19b298f6124f98770c0831dc9e13ddfccb525a3c)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
, I'll see if we can get a trunk build with this up and running today/tomorrow and see if this unblocks us for 2.7.  , FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #137 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/137/])
HADOOP-10703. HttpServer2 creates multiple authentication filters. Contributed by Benoy Antony. (wheat9: rev 19b298f6124f98770c0831dc9e13ddfccb525a3c)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2087 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2087/])
HADOOP-10703. HttpServer2 creates multiple authentication filters. Contributed by Benoy Antony. (wheat9: rev 19b298f6124f98770c0831dc9e13ddfccb525a3c)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java
, This patch does not fix the issue for authentication with a custom alt-kerberos class, when browsing to the dfs file browser the console log says "Failed to load resource: the server responded with a status of 401 (org.apache.hadoop.security.authentication.util.SignerException: Invalid signature)" indicating that WebHDFS is not accepting the signature from the hadoop.auth cookie issued when accessing the namenode UI.

The core of the problem is that the WebHDFS authentication filter is still being initialized differently compared to the rest of the filters (which are configured to be initialized with the AuthenticationFilterInitializer class). Unifying the initialization of the WebHDFS authentication filter with the other filters will fix this issue., Thanks for the explanation. Does it mean that unifying the signer for all authentication filter will resolve the issue?, Unifying the signers and allowing WebHDFS to be configured with an alt-kerberos auth handler sub class will resolve the issue.]