[This is seen in all places where queue scheduling info is used - jobtracker.jsp, jobqueue_details.jsp and hadoop CLI. Seems like a problem in our calculations in CapacityTaskScheduler itself., _CapacityTaskScheduler_ uses _JobInProgress.getPendingMaps()_ and _JobInProgress.getPendingReduces()_. The way the pending maps and reduces are calculated in JIP can return negative values.

i.e. in case of an always failing task:

Number of task  - number of running task - number of failed task - finished task + speculative task.

When number of failures become large the method returns negative value. But not sure if this is expected.

, This happens not just with the tasks that always fail, but with any TIPs that fail for a few times before finishing/failing. I guess we should also maintai mapRepeatAttempts and reduceRepeatAttempts to track repetitive task-execution because of failures. These are similar to speculative{map|Reduce}Tasks to track speculative task-execution., I think it would be simpler to deduce the number of pending task by subtracting values from _desiredMaps()_ and _finishedMaps()_. Similarly for reduces, I am making an assumption that value of the _finishedMapTasks_ and _finishedReduceTasks_ are bumped up by one whenever a speculative task has finished. This way, we don't need to keep track of the number of failed maps or reduces, since our scheduling algorithm does not assign weight to number of failures., Marking as a blocker, as it is giving incorrect user feedback for a usual scenario., This issue should also resolve the negative values appearing for running tasks. Linking HADOOP-4289, which is most likely related to HADOOP-4288, but it would be good if this issue nevertheless closes on this., Unlinked HADOOP-4289 which is related only to HADOOP-4288. Sorry, for any confusion I might have created.

But, I still see negative running reduces, although I've repetitively seen only a -1. This should be investigated as part of this JIRA., The formulae for determining the pending tasks have to be changed to the following in order to have the correct number of pending tasks to be determined.

The formulae would be:

number of desired task - number of running tasks - number of failed TIP for task - finished task + speculative task, After running through the logs, I've realized that the negative count on running tasks results because of TTs that report failed tasks *after* the job is killed due to maximum TIP failures being hit. A killed job resets its running tasks count to zero, any task failures reported, after the job is marked as killed, bring down the count to a negative value. Don't know for sure, but I guess we should not update task-status of jobs that are already marked succeeded/failed/killed., Attaching a patch for fixing the negative values in the running and pending cases. Running cases I have not been able to reproduce the issue. But I have made modifications according to Vinod's comments. waiting negative values have been fixed., I think this jira can remove the assignments for the counters from terminate method.
terminate() method, makes *runningMapTasks* and *runningReduceTasks* as zero, which is not necessary. They eventually become zero later., The counter assignments from terminate method should remain, because only thing which we are handling is prevention of updating the counters after the job has been terminated due to maximum failures reached, as mentioned by Vinod above.
 
If we remove the assignment then value of running reduces would not fall back to zero, in case a job which is terminated when it has 5 maps in total and has 2 running maps when it is terminated. Then if we remove the assignment from terminate then the JIP.running task would say 2 and waiting would always point to 3., After an offline discussion with Amareshwari Hemanth and Vinod, it is ok to remove the assignment in the terminate as the failedTask() is called after the TIP have been issued a kill so the numbers would be reset back to zero.

So removing the assignment in the terminate and leaving failedTask method as is., Attaching patch incorporating the comments, also attaching a test case which verifies that the pending and running task count does not go to negative in a case of always failing task., The fix is fine. I have a few test-case related comments.
 - You only have a test for failing maps, you should also have a test for failing reduces.
 - You should test for positive counts not just once, but through out the life time of the job; so you need to check it in a loop till job completion. Otherwise test-case success/failure would just be a matter of timing.
 - I think you can rename the test-case to TestJobInProgess, because part of which is what we are really testing here.
 - Minor : You shouldn't catch and ignore any exception thrown by RunningJob.runJob(). If something abnormal happens, let the test-case fail., Attaching the latest patch. The patch contains modified test case with following:
- New Test case to check for Failing reduce.
- Test case now waits till all the tasks of the job have reported failures.

One thing to be noted is test case takes nearly _125 seconds_ to complete.

bq.Minor : You shouldn't catch and ignore any exception thrown by RunningJob.runJob(). If something abnormal happens, let the test-case fail.

The reason why the exception is being caught is because when a Job fails exception is thrown back from JobClient.runJob. We catch the exception to continue working on the task counts.
, bq. Test case now waits till all the tasks of the job have reported failures.
It actually waits till all the tasks that are already launched get finished. Thus, it makes sure that tasks that report back after job is failed won't decrement the count below zero.

Reviewed the patch. The test-case now looks fine. About the test-case time, it could have been drastically reduced had we instead used fake objects for JT, TT etc., but I think it's fine for now as it is.

+1., Attaching local test patch output:

{noformat}
     [exec]
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 Eclipse classpath. The patch retains Eclipse classpath integrity.
     [exec]

{noformat}, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12391878/HADOOP-4287-3.patch
  against trunk revision 703923.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3450/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3450/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3450/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3450/console

This message is automatically generated., The failed test case org.apache.hadoop.hdfs.TestLeaseRecovery.testBlockSynchronization is not related to this particular patch which modifies only the mapred project in the core. Hemanth has already reported this issue on following JIRA [HADOOP-4403|https://issues.apache.org/jira/browse/HADOOP-4403], I just committed this. Thanks, Sreekanth!, Integrated in Hadoop-trunk #634 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/634/])
    ]