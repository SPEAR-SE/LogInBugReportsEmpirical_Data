[Since we have "Deleting ..." log message, it implies that Java "f.delete(file)" returned true. But looks s like the file is actually not deleted. , This problem occured again. This time the block in question does not appear in the dfs data directory. The namenode is requesting a block to be deleted. The datanode tries this operation and encounters an error because the block is not in the blockMap. The processCommand() method raises an exception. The code is such that the variable lastBlockReport is not set if processCommand() raises an exception. This means that the datanode immediately send another block report to the namenode.

In short, the above condition causes the datanode to send blockReports almost once every second!

I propose that we do the following:

1. in Datanode.offerService, replace the following piece of code 

          DatanodeCommand cmd = namenode.blockReport(dnRegistration,
                                                     data.getBlockReport());
          processCommand(cmd);
          lastBlockReport = now;


with

          DatanodeCommand cmd = namenode.blockReport(dnRegistration,
                                                     data.getBlockReport());
          lastBlockReport = now;
          processCommand(cmd);
          
2. In FSDataSet.invalidate:
    a) continue to process all blocks in invalidBlks[] even if one in the middle encounters a problem.
    b) if getFile() returns null, still invoke volumeMap.get() and print whether we found the block in 
       volumes or not. The volumeMap is used to generate the blockReport and this might help in debugging.





, bq. This time the block in question does not appear in the dfs data directory.

We've seen both cases.

1) block exist on local dfs dir. 
     Error messages repeats every second.

2) block doesn't exist on local dfs dir.
     Error message only shows up once.


It happened when a large number of files were deleted on the cluster.
, Implemented the changes described in the previous comment., 
The failure to delete the fail seems to happen when a lot of blocks are deleted at once. Should we trust f.delete() when it returns true or should we double check?

, > The failure to delete the fail seems [...]
should be: .. delete the FILE ...
, I am not too inclined to put in a check to f.exists() after every call to f.delete(). If you are doubting the correctness of f.delete() then what guarantees does f.exists() have? But I am printing the full path of the file when a file gets deleted, earlier it was only printing the blockid. This message might help in determining if we are deleting the correct file or not., 
It at least confirms to us if f.delete() is indeed the problem or not. f.exists() could be included temporarily until we find where the bug is.
, i think we should close this bug as "cannot reproduce". If it happens again, we can reopen it., I think this bug does not exist in the code anymore. , dhruba, should we remove the following code from {{FSDataset.java}}?

{noformat}
      if (f.exists()) {
        //
        // This is a temporary check especially for hadoop-1220. 
        // This will go away in the future.
        //
        DataNode.LOG.info("File " + f + " was deleted but still exists!");
      }
{noformat}, @Suresh: I am +1 for removing that section of code.

]