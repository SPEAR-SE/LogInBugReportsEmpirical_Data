[In the job, were there any map failures or lost trackers leading to killed maps? If so, could you please check the JobTracker logs and see whether the failed/lost tasks got reexecuted.
Were many reducers stuck? , 
I did not notice map failures
All maps completed successfully.
Four reducers got stuck.
A few reducers got re-executed because of too many failures in fetching map output.

, 

I think the problem was due to the slow network link of a machine.

This issue can be ignored for now.
, We have been  running into the same issue, very unlikely to be a network problem:

#nodes 1350
#maps: 100,000 (running about 5 minutes each)
#reduces: 2,500

The maps were finished after about 4 hours, the data shuffling took about 8 hours, the amount of data very small.

Typical pattern of reduces logs:

2008-05-05 19:32:43,215 INFO org.apache.hadoop.mapred.ReduceTask: task_200805050640_0001_r_001079_0 Need 6 map output(s)
2008-05-05 19:32:43,216 INFO org.apache.hadoop.mapred.ReduceTask: task_200805050640_0001_r_001079_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-05-05 19:32:43,216 INFO org.apache.hadoop.mapred.ReduceTask: task_200805050640_0001_r_001079_0 Got 5 known map output location(s); scheduling...
2008-05-05 19:32:43,216 INFO org.apache.hadoop.mapred.ReduceTask: task_200805050640_0001_r_001079_0 Scheduled 0 of 5 known outputs (0 slow hosts and 5 dup hosts)

Also saw log messages with '(xxx slow hosts and 0 dup hosts)
, I just encountered this using hadoop-0.17.0 on a 7-node cluster.  The performance of the affected reduce tasks has slowed dramatically - "24.43%
 reduce > copy (195 of 266 at 0.00 MB/s)".  However the progress percentage IS gradually increasing at the rate of approximately 0.10% every 20 minutes.

Is this related to this penalty box error that is showing up in my logs?

2008-06-02 18:37:43,437 INFO org.apache.hadoop.mapred.ReduceTask: Task task_200806021621_0002_r_000013_0: Failed fetch #6 from task_200806021621_0002_m_000244_0
2008-06-02 18:37:43,437 WARN org.apache.hadoop.mapred.ReduceTask: task_200806021621_0002_r_000013_0 adding host blah.corp.XXX.XXX to penalty box, next contact in 128 seconds

It seems that hadoop is autodetecting the wrong hostname for some of the cluster nodes, leading to issues when one node tries to contact another to retrieve information.  I have no idea if this is related to the issue.

Random observations:

2 of 28 reduce tasks are running extremely slowly.
All slow reduce tasks have precisely the same progress percentage., This most likely is because of the current backoff & retrial strategies in the shuffle. There are some jiras open to improve them - HADOOP-3478 & HADOOP-3327., I noticed something similar occuring to the original issue description (
2008-04-02 17:17:44,640 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Need 2 map output(s)
2008-04-02 17:17:44,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0: Got 0 new map-outputs & 0 obsolete map-outputs from tasktracker and 0 map-outputs from previous failures
2008-04-02 17:17:44,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Got 0 known map output location(s); scheduling...
2008-04-02 17:17:44,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200804021200_0337_r_000008_0 Scheduled 0 of 0 known outputs (0 slow hosts and 0 dup hosts))
) repeated over and over again
It seems that this is triggered if different nodes are configured to use a different dfs name, including the following

on the TaskTracker and JobTracker instances

fs.default.name = "foo.example.com:50001"

on the DataNode instances

fs.default.name = "hdfs://foo.example.com:50001/"

This seems to cause the reducers to be unable to find the mapper output, I'm also getting this behaviour with 0.17.1, on an EC2 hosted cluster, 14 worker (m1.small) and 1 control (NameNode/JobTracker) VMs.

The job was rather small (40 map tasks, with minimal data to each, they fetch their input internally from S3), 20 reducers. streaming.jar.

As far as I have been able to notice:

-) no progress on reducers (I had cases where jobs have been hanging for multiples of 12 hours)
-) the configuration is exactly the same on all nodes, so the DFS name idea from the previous comment does not apply.
-) killing the reduce tasks restarts them on the same node. There they go to the same state (with the same number of needed outputs) very quickly, the job being not that big. Then they hang at the same place.
-) the only remedy that I have found if one call this so is restarting the cluster. So this hang is not 100% input dependant.

Andreas, Andreas, the next time you notice this could you please run:

$ bin/hadoop job -events <jobId> 0 1000000

and provide us the output? Thanks!, We're also running into this on 0.16.2. The job has 50 mappers, 895 reducers, and all mappers finished exactly once (no speculative execution).

When the failures occur, they seem to effect all the reduce tasks assigned to a tasktracker; they all get stuck waiting for the same number of map outputs. The number does vary from tasktracker to tasktracker (and many tasktrackers complete the reducers successfully). The task's syslog reports:

{code}
2008-08-02 06:10:02,686 INFO org.apache.hadoop.mapred.ReduceTask: task_200807311630_0007_r_000006_0 Ignoring obsolete copy result for Map Task: task_200807311630_0007_m_000001_0 from host: HOST-1131
{code}

The number of those lines will be equal to the number of map outputs they (later) get stuck waiting for. The result that is claimed to be obsolete has already been copied, though:

{code}
2008-08-02 06:09:21,461 INFO org.apache.hadoop.mapred.ReduceTask: task_200807311630_0007_r_000006_0 Copying task_200807311630_0007_m_000001_0 output from HOST-1131
....
2008-08-02 06:10:02,683 INFO org.apache.hadoop.mapred.ReduceTask: task_200807311630_0007_r_000006_0 done copying task_200807311630_0007_m_000001_0 output from HOST-1131
{code}

As a work-around, manually killing the task until it gets assigned to a working tasktracker allows the job to complete.

Attachment {{task_200807311630_0007_r_000006_0.syslog.gz}} is the full syslog from one such failed task, I'll upload the output of {{./bin/apollo-hadoop job -events job_200807311630_0007 0 10000000}} next.
, Thanks for the logs, Rick.
Are there lost task trackers during the job?
Can you attach JobTracker log and TaskTracker log on which task_200807311630_0007_r_000006_0 ran?
And if you upload log for another stuck reducer, that will be very useful., Here are debug patches logging task completion events at the tasktracker for branches 0.16 and 0.17.
Could you guys please apply the respective patches and run applications.

And if this problem occurs again, please upload the tasktracker logs for a _good_ task tracker as well as the logs for a tasktracker on which reducers get stuck.

And also upload the result of _kill -3  <taskTrackerPID>_ for the problematic tasktracker., Took me awhile to assemble the logs, but here they are. Included are:

- all reducer task syslog files from two failed tasktrackers and one good tasktracker
- the tasktracker logs from the same hosts
- the jobtracker log entries for this job
- stackdumps (kill -QUIT output) from the tasktracker and task VM created on a hung tasktracker while the job was running., Thanks Rick for the effort.

One more question: What sort of failures did you observe on bad tasktrackers? I dont see any failures in tasktracker logs or the job tracker logs., Thanks Amareshwari. These tasks don't fail --- they just don't make any progress. In the case of job 200807311630_0007, the stuck task attempts were eventually manually killed until they were assigned to a non-stuck tasktracker.

We've patched the tasktrackers to log the extra debugging, though since restarting them to deploy the patch, this problem hasn't recurred (we saw failures like this for two jobs in a row before the update, so it seems related to some kind of state accumulated in either the tasktrackers or jobtracker)., {code}
"Map-events fetcher for all reduce tasks on tracker_HOST-6026:localhost.localdomain/127.0.0.1:53932" daemon prio=10 tid=0x7ff5d800 nid=0x510b in Object.wait() [0x81e0a000..0x81e0a1b0]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x87981ee0> (a java.util.TreeMap)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.mapred.TaskTracker$MapEventsFetcherThread.run(TaskTracker.java:511)
	- locked <0x87981ee0> (a java.util.TreeMap)
{code}
The above _kill -QUIT_ output of the tasktracker shows that MapEventsFetcherThread is in WAITING state, which is abnormal with the scenario.

It says that the Thread is waiting at line number 511:
{code}
509            while (((fList = reducesInShuffle()).size()) == 0) {
510              try {
511                runningJobs.wait();
512              } catch (InterruptedException e) {
513                LOG.info("Shutting down: " + getName());
514                return;
515              }
516            }
{code}
MapEventsFetcherThread will wake up from here if there are any reducesInShuffle.  
One scenario is the thread saw that there are no reducesInShuffle and is waiting, and the notifications are missed. But, 
Since the reducers have fetched some map outputs, that says that the thread woke up and fetched some events. And since the reduces are still in shuffle, the fetcherThread should not go to wait again.

If I'm not missing something, there could be a bad data structure that is causing the issue.
, Facebook job hang from 10/08, hey folks - i just uploaded the output from bin/hadoop job -events <jobId> 0 1000000

we are beginning to reproduce this problem once in a while and it's hugely disruptive. the cluster is in this state right now - we will try to not restart it if any information can be extracted that can help debugging this .. Please let us know., btw - one observation - failing the reduce task does not fix the problem. the restarted reducer ends up hanging at precisely the same point (waiting for same N number of map-outputs - although i can't tell if it's the exact same map-outputs - but it's the exact same 'N' all the time). That makes me suspect that the issue is not a race condition (which was one of the possibilities discussed in hadoop-4360)., Some more information from facebook:

Completion Events from TASKTRACKER: (NOTE: line 1-3 and line 4-6 are duplicates)
NextId: 57
ID	Event ID	Task ID	ID within Job	Type	Status
0	0	task_200810061724_5371_m_000019_0	19	Map	SUCCEEDED
1	0	task_200810061724_5371_m_000023_0	23	Map	SUCCEEDED
2	0	task_200810061724_5371_m_000008_0	8	Map	SUCCEEDED
3	0	task_200810061724_5371_m_000019_0	19	Map	SUCCEEDED
4	0	task_200810061724_5371_m_000023_0	23	Map	SUCCEEDED
5	0	task_200810061724_5371_m_000008_0	8	Map	SUCCEEDED
6	0	task_200810061724_5371_m_000012_0	12	Map	SUCCEEDED
7	0	task_200810061724_5371_m_000017_0	17	Map	SUCCEEDED
8	0	task_200810061724_5371_m_000021_0	21	Map	SUCCEEDED
9	0	task_200810061724_5371_m_000011_0	11	Map	SUCCEEDED
10	0	task_200810061724_5371_m_000015_0	15	Map	SUCCEEDED
11	0	task_200810061724_5371_m_000020_0	20	Map	SUCCEEDED
12	0	task_200810061724_5371_m_000003_0	3	Map	SUCCEEDED
13	0	task_200810061724_5371_m_000009_0	9	Map	SUCCEEDED
14	0	task_200810061724_5371_m_000004_0	4	Map	SUCCEEDED
15	0	task_200810061724_5371_m_000005_0	5	Map	SUCCEEDED
16	0	task_200810061724_5371_m_000013_0	13	Map	SUCCEEDED
17	0	task_200810061724_5371_m_000014_0	14	Map	SUCCEEDED
18	0	task_200810061724_5371_m_000006_0	6	Map	SUCCEEDED
19	0	task_200810061724_5371_m_000002_0	2	Map	SUCCEEDED
20	0	task_200810061724_5371_m_000007_0	7	Map	SUCCEEDED
21	0	task_200810061724_5371_m_000010_0	10	Map	SUCCEEDED
22	0	task_200810061724_5371_m_000001_0	1	Map	SUCCEEDED
23	0	task_200810061724_5371_m_000016_0	16	Map	SUCCEEDED
24	0	task_200810061724_5371_m_000018_0	18	Map	SUCCEEDED
25	0	task_200810061724_5371_m_000018_1	18	Map	KILLED


Completion Events from JOBTRACKER:
ID	Event ID	Task ID	ID within Job	Type	Status
0	0	task_200810061724_5371_m_000019_0	19	Map	SUCCEEDED
1	1	task_200810061724_5371_m_000023_0	23	Map	SUCCEEDED
2	2	task_200810061724_5371_m_000008_0	8	Map	SUCCEEDED
3	3	task_200810061724_5371_m_000024_0	24	Map	SUCCEEDED
4	4	task_200810061724_5371_m_000022_0	22	Map	SUCCEEDED
5	5	task_200810061724_5371_m_000000_0	0	Map	SUCCEEDED
6	6	task_200810061724_5371_m_000012_0	12	Map	SUCCEEDED
7	7	task_200810061724_5371_m_000017_0	17	Map	SUCCEEDED
8	8	task_200810061724_5371_m_000021_0	21	Map	SUCCEEDED
9	9	task_200810061724_5371_m_000011_0	11	Map	SUCCEEDED
10	10	task_200810061724_5371_m_000015_0	15	Map	SUCCEEDED
11	11	task_200810061724_5371_m_000020_0	20	Map	SUCCEEDED
12	12	task_200810061724_5371_m_000003_0	3	Map	SUCCEEDED
13	13	task_200810061724_5371_m_000009_0	9	Map	SUCCEEDED
14	14	task_200810061724_5371_m_000004_0	4	Map	SUCCEEDED
15	15	task_200810061724_5371_m_000005_0	5	Map	SUCCEEDED
16	16	task_200810061724_5371_m_000013_0	13	Map	SUCCEEDED
17	17	task_200810061724_5371_m_000014_0	14	Map	SUCCEEDED
18	18	task_200810061724_5371_m_000006_0	6	Map	SUCCEEDED
19	19	task_200810061724_5371_m_000002_0	2	Map	SUCCEEDED
20	20	task_200810061724_5371_m_000007_0	7	Map	SUCCEEDED
21	21	task_200810061724_5371_m_000010_0	10	Map	SUCCEEDED
22	22	task_200810061724_5371_m_000001_0	1	Map	SUCCEEDED
23	23	task_200810061724_5371_m_000016_0	16	Map	SUCCEEDED
24	24	task_200810061724_5371_m_000018_0	18	Map	SUCCEEDED
25	25	task_200810061724_5371_r_000000_0	0	Reduce	SUCCEEDED
26	26	task_200810061724_5371_r_000005_0	5	Reduce	SUCCEEDED
27	27	task_200810061724_5371_r_000009_0	9	Reduce	SUCCEEDED
28	28	task_200810061724_5371_r_000007_0	7	Reduce	SUCCEEDED
29	29	task_200810061724_5371_r_000004_0	4	Reduce	SUCCEEDED
30	30	task_200810061724_5371_r_000030_0	30	Reduce	SUCCEEDED
31	31	task_200810061724_5371_r_000026_0	26	Reduce	SUCCEEDED
32	32	task_200810061724_5371_r_000024_0	24	Reduce	SUCCEEDED
33	33	task_200810061724_5371_r_000018_0	18	Reduce	SUCCEEDED
34	34	task_200810061724_5371_r_000021_0	21	Reduce	SUCCEEDED
35	35	task_200810061724_5371_r_000015_0	15	Reduce	SUCCEEDED
36	36	task_200810061724_5371_r_000019_0	19	Reduce	SUCCEEDED
37	37	task_200810061724_5371_r_000022_0	22	Reduce	SUCCEEDED
38	38	task_200810061724_5371_r_000008_0	8	Reduce	SUCCEEDED
39	39	task_200810061724_5371_r_000028_0	28	Reduce	SUCCEEDED
40	40	task_200810061724_5371_r_000001_0	1	Reduce	SUCCEEDED
41	41	task_200810061724_5371_r_000014_0	14	Reduce	SUCCEEDED
42	42	task_200810061724_5371_r_000025_0	25	Reduce	SUCCEEDED
43	43	task_200810061724_5371_r_000003_0	3	Reduce	SUCCEEDED
44	44	task_200810061724_5371_r_000010_0	10	Reduce	SUCCEEDED
45	45	task_200810061724_5371_r_000017_0	17	Reduce	SUCCEEDED
46	46	task_200810061724_5371_r_000016_0	16	Reduce	SUCCEEDED
47	47	task_200810061724_5371_r_000002_0	2	Reduce	SUCCEEDED
48	48	task_200810061724_5371_r_000011_0	11	Reduce	SUCCEEDED
49	49	task_200810061724_5371_r_000023_0	23	Reduce	SUCCEEDED
50	50	task_200810061724_5371_r_000029_0	29	Reduce	SUCCEEDED
51	51	task_200810061724_5371_r_000020_0	20	Reduce	SUCCEEDED
52	52	task_200810061724_5371_r_000027_0	27	Reduce	SUCCEEDED
53	53	task_200810061724_5371_m_000018_1	18	Map	KILLED
54	54	task_200810061724_5371_r_000012_0	12	Reduce	SUCCEEDED
55	55	task_200810061724_5371_r_000006_0	6	Reduce	KILLED
56	56	task_200810061724_5371_r_000006_1	6	Reduce	SUCCEEDED


Basically TaskTracker get some duplicates for some mysterious reasons. Dhruba also finds that at the time of these duplicate we see RPC time out on the task tracker.
, The problem is that when the TaskTacker re-initializes its own data structures, it interrupts the fetcher thread but does not wait for the fetcher thread to exit. This means that multiple instances of the fetcher thread can be active at the same time, thus inserting duplicate entries in the fetched list., Review comments welcome., Catch Interrupts while waiting in the getTaskCompletionEvents RPC., +1
, Good catch Dhruba! 

Can you please verify (via extra logging if necessary) that this is indeed the cause? Just to be safe! *smile*

OTOH, shouldn't we also go ahead and purge _all_ state in RunningJobs etc. when in TaskTracker.close? i.e. clear TaskTracker.runningJobs (thus the previous 'FetchStatus' etc.) That probably is a related bug worth fixing too?, Dhruba - i just realized based on the previous comments internally that this patch will not work.

the issue is that interruptedexceptions are ignored by ipc/Client.java. So they are not a reliable way of shutting down the mapeventsfetcher.  once consumed by the wait() code inside Client.java - the interrupt is gone and the thread will never terminate.

So we need to set some flag as well and check that at the top of the loop. If the code is blocked in RPC layer - it will eventually time out. If it is blocked in the wait() call inside the fetcher thread - then the interrupt will work. In either case - it should check for a shutdown flag at the top of the loop and use that to trigger termination., it would seem that the problem is not so much a race between the shutdown and the initialize of the mapeventsfetcher - as perhaps that mapeventsfetcher is not shutdown at all (because of the interrupts being swallowed)., @Arun: I deduced this scenario from looking at the tasktracker logs. So, my theory is indeed validated. In out cluster, it typically ocurs during periods of high load. The TaskTracker.close() already clears most data structures. In this case, after the data strctures are cleared, the first instance of the fercher thread re-populated the data structure and then exited. The second instance of the fetcher thread continued using the data structure. Hence the duplicate values. Does this make sense?

@Joydeep: I did see that the code in the ipc layer ignored InterruptedExceptions. The question is why does the ipc code catch and ignore InterruptedExceptions? Shouldn's it just throw InterruptedException?, I am tentatively marking this issue for 0.19 because my belief is that it causes most of the tasktrackers to freeze up during times of heavy load on JT. I am guessing this might  have occured with Christian Kunz's cluster earlier., @Joydeep: do you mean the following piece of code in waitForProxy function?
{code}
      try {
        Thread.sleep(1000);
      } catch (InterruptedException ie) {
        // IGNORE
      }
{code}

waitForProxy is called by the main thread (that calls TaskTracker.initialize()). It's not called by the FetchThread. So FetchThread will never ignore InterruptedException, correct?
, i don't know why the ipc layer ignores interruptedexceptions. but the practical matter is that it does - and my naiive suspicion is that changing that would be non trivial. also - since the ipc layer always times out anyway - interrupts are not required to break out of RPC calls.

as i mentioned - it's no longer a race condition. the assumption that the thread.interrupt() works to kill the mapeventsfetcher is inherently wrong. my speculation is that both the threads run forever - which is why new jobs/tasks also hit the same problem when they run on this TT (i am assuming this since we had many many jobs that hit the same problem - and they started hours apart).

we need a shutdown flag in addition to the interrupt and need to check that flag in the eventsfetcher loop. otherwise thread.join will hang. (also - if u look at general documentation on the web about using interrupts in java - everyone advises using a flag in addition as general practice).

i would be curious if there are other parts of the hadoop code base that suffer from the same assumptions (about thread.interrupt being caught and thrown)., @Joydeep: I see what you mean. It's the Client.java. 

There are 3 methods named "call" in Client.java. We are calling the one that actually throws InterruptedException.

{code}
  public Writable call(Writable param, InetSocketAddress address)
  throws InterruptedException, IOException {

  public Writable call(Writable param, InetSocketAddress addr, 
                       UserGroupInformation ticket)  
                       throws InterruptedException, IOException {

  public Writable[] call(Writable[] params, InetSocketAddress[] addresses)
    throws IOException {
{code}

The swallowing of InterruptedException definitely needs to be fixed, but it seems it does not affect this patch?
, @Zheng: The ipc code in Client.java swallows InterruptedExceptions. So, if the fetcher thread is blocked inside the RPC code, and then it gets interrupted, it will swallow the InterruptedException. If this occurs, the fetcher thread will not exit., @Zheng - my assumption (from looking at the code a long time back) is that the Proxy code (invoked by calls like getTaskcompletion) winds it's ways to calls to the ipc/Client.java routines. If u thumb through ipc/Client.java - the code ignores Interrupts. This is safe since it always has timeouts - so it doesn't need interrupts as a way of unblocking. but it also means that one cannot use Interrupted exception as a way of detecting shutdown request.

use interrupts to unblock. use flags to signal and check for shutdown., @Joydeep:

I just saw the line "Connection connection = getConnection(addr, ticket);" inside this "call" method that is called by RPC.Invoker.invoke. 
Inside the getConnection method there is indeed a catch block ignoring InterruptedException.

So it's true that we can be ignoring InterruptedException during the calling of getTaskcompletion -> RPC.Invoker.invoke -> call(Writable param, InetSocketAddress addr, UserGroupInformation ticket)  -> getConnection.

{code}
  public Writable call(Writable param, InetSocketAddress addr, 
                       UserGroupInformation ticket)  
                       throws InterruptedException, IOException {
    Connection connection = getConnection(addr, ticket);
    Call call = new Call(param);
    synchronized (call) {
      connection.sendParam(call);                 // send the parameter
      long wait = timeout;
      do {
        call.wait(wait);                       // wait for the result
        wait = timeout - (System.currentTimeMillis() - call.lastActivity);
      } while (!call.done && wait > 0);

      if (call.error != null) {
        throw new RemoteException(call.errorClass, call.error);
      } else if (!call.done) {
        throw new SocketTimeoutException("timed out waiting for rpc response");
      } else {
        return call.value;
      }
    }
  }
{code}
, GetConnection does swallow InterruptedException. To verify if this is the culprit of the problem, check the TaskTracker log if you see a message starting with "Retrying connect to server: "., Before rethrowing InterruptedException, make sure to remove the connection out of the connections queue., A modified patch that exits the fetchLoop as soon as the fetcher thread is interrupted.

Can somebody please provide me review comments?, bq. I deduced this scenario from looking at the tasktracker logs. So, my theory is indeed validated. In out cluster, it typically ocurs during periods of high load. The TaskTracker.close() already clears most data structures. In this case, after the data strctures are cleared, the first instance of the fercher thread re-populated the data structure and then exited. The second instance of the fetcher thread continued using the data structure. Hence the duplicate values. Does this make sense?
Dhruba, I don't quite see how this could happen (maybe I am missing something and I hope so *smile*). Even if you have two threads live at some point, both of them would be sharing the same object (FetchStatus) and hence should be seeing the same fromEventId, no?
So if the first thread repopulated the datastructure, the second thread should be seeing the same fromEventId object (and hence would be seeing the same value), no? Also, note that two concurrent fetching of map completion events for the same job cannot happen since the fetch is synchronized on the fromEventId object.

I do agree that we should not have two concurrent fetcher threads live simultaneously., Another thing I forgot to ask - did you "Lost tracker  " lines in the JobTracker log for those trackers where this problem started showing up. During normal execution, TaskTracker.close will be invoked only when a tracker is lost. , 1. say two threads start with same fromEventId.
2. Both get same set of events
3. now each of them increments fromEventId - so we get duplicate events and miss out on getting the real ones.

in 17.1 - there is no synchronized clause on fromEventId. it's possible that this problem does not exist in 19 (or since whenever the synchronized block was added), bq. in 17.1 - there is no synchronized clause on fromEventId. it's possible that this problem does not exist in 19 (or since whenever the synchronized block was added)
Oh that's true! 

+1 for the patch, Maybe we should make this patch work for 0.18.3 as well. Thoughts? 
True that this problem might not show up with 0.19 due to the synchronization around fromEventId, but it makes sense to join on the thread before starting a new one., Have to merge with latest trunk, Merged with latest trunk.

@Devaraj: thanks for reviewing this patch. I am currently targeting this for trunk only (not for 0.18 branch). If it needs to go into 0l18, please let me know., I think we should get a patch for 17/18 as well.. In 0.19, this problem may not be there at all due to the synchronization introduced around the fromEventId object. It makes sense to fix the bug where it surely shows up..., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12392114/fetcherThread.patch
  against trunk revision 704732.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3462/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3462/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3462/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3462/console

This message is automatically generated., Fix findbugs error., Fix findbugs issue., I am marking this as a blocker for 0.19., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12392194/fetcherThread.patch
  against trunk revision 704989.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3468/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3468/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3468/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3468/console

This message is automatically generated., I just committed this. Thanks, Dhruba!, Integrated in Hadoop-trunk #636 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/636/])
    . Ensure that there is only one thread fetching TaskCompletionEvents on TaskTracker re-init. Contributed by Dhruba Borthakur.
, Experienced this issue on 0.18.3 as well. I've attached a patch against this branch.]