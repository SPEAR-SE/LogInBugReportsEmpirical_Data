[I don't see a circular dependency in the stack trace. Is there one?, Raghu,
right, there is no real circular dependency. However, all threads are waiting for the DataStreamer that blocks all other threads since the java.net.SocketOutputStream.socketWrite0(Native Method) seems to got stuck for no real reason. Other DFS Clients are able to write to the DFS cluster without any delays etc. and all datanodes are active & alive.
I finally killed the application (after taking this ThreadDump) since there was no progress at all for more than 12 hours. No Timeout or any other exception has been thrown.
AFAIK socketWrite is a blocking call. However, even when the reader on the other end is slow, there should be at least some progress?!?, It is possible that you have a slow datanode that is participating in this pipeline only. This might have caused the DataStreamer thread to block on the socketWrite. You would also see that this thread is marked as RUNNABLE, probably because it is writing out its bufefr slowly., Also there is a known bug and we are trying to catch these nodes 'in action' :) .See HADOOP-3132.  When this happens again, could you get the stack trace on the datanodes also? Right now it is not easy to know the datanodes involved in the write pipeline. I am planning to include datanodes involved in the DataStreamer thread description., @dhruba: All datanodes were up, on the same network link and CPU utilization was normal as usual etc. Also, there were 7 more DFS clients writing files of the same size ( with throughputs of about 6 MB/s) and none of them got stuck :-/. We have 13 datanodes in total and replication factor is 3.
Digging the logs didn't reveal anything useful. However I will keep you in the loop..., I don't think this is related to HADOOP-3132. For this jira we need to find out what datanodes are doing when a client is blocked like this. We also need to differentiate between a pause (mostly because of heavy disk load) and real hang., I recently identified a "bad" datanode in our cluster - bad in the sense that the JVM (IBM Java 6 for PPC) on that datanode seemed to "consume" more open file handles than the "regular" SUN JRE. So this caused a lot of  "too many open files" exceptions where all writers got blocked when this specific datanode was involved in the pipelining. Maybe this is related to HADOOP-3051 for some JVMs? Takeing out this datanode seemed to have resolved the issue., > Maybe this is related to HADOOP-3051 for some JVMs? 
HADOOP-3051 does not affect 0.16.1., Right, I forgot that - however, the IBM JVM seems to have an unusually behavior with open file handles.
It would be great is the WebUI would have a special page that shows all files that are "in flight" - the filename and all with that file assoicated/allocated blocks and the involved datanodes... , I am closing this because this is on a very old release and most of the error recovery code has changed. Please reopen if you see this problem again. Regarding printing in-flight files/blocks, "hadoop fsck / -files -blocks -locations -openforwrite" will give you all the goodies about files/blocks being written.]