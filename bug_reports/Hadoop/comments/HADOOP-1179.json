[This is related to HADOOP-1158, This is a duplicate of HADOOP-1158, Actually, it isn't really a duplicate of HADOOP-1158. Part of what is going on is that the MapOutputServlet is using a lot of memory. In particular, each thread is using:
  1. A file to read the index file
  2. A file to read the index file crcs
  3. A file to read the map outputs
  4. A file to read the map output crcs
All 4 files use the io.file.buffer.size, which we usually set to 64k. Since the servlet is lazy about closing them, all 4 files stay open during the entire call. Since we have ~40 threads, that is 10MB of file buffers in the TaskTracker's JVM.

A couple of steps would make this much better:
  1. Use smaller buffers for reading crc files (io.file.buffer.size * sizeof(crc) / sizeof(crc block) ?)
  2. Close the index file before opening the data files

, Yeah Owen, i totally missed the open-file issues., I have attached a patch containing the part to do with closing index file as soon as the read is done. Regarding the crc thing, I think what needs to change is the method ChecksumFileSystem.getSumBufferSize, but was not sure whether it would affect the other parts of the code. So didn't touch that part., 
This issue is related to HADOOP-1158 but not a dup.
Two things need to be investigated/fixed: why out of memory in Jetty (or in TT). Early comments and the attached patch may address that partially. However, I am not sure that accounts all.

The second thing is proper handling of outOfMemory. Normally, when this happens, it may be desirable to kill the TT and hope to be restarted.

, Note that running out of file handles can cause OutOfMemoryExceptions, even when there's lots of memory left.  So this could just be a file-handle leak.  It'd be great to log the heap size somehow (e.g. simply by turning on verbosegc, which has little performance impact) so we had an idea of whether this is really a memory issue or a file handle issue., For all the nodes with org.mortbay.jetty.servlet.ServletHandler: java.lang.OutOfMemoryError, they each had one instance of OutOfMemoryError from inmemory file.

2007-03-28 16:30:58,204 WARN org.apache.hadoop.mapred.TaskRunner: task_0334_r_000606_0 Intermediate Merge of the inmemory files threw an exception: java.lang.OutOfMemoryError
  at java.io.FileOutputStream.writeBytes(Native Method)
  at java.io.FileOutputStream.write(FileOutputStream.java:260)
  at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:166)
  at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
  at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
  at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
  at java.io.DataOutputStream.write(DataOutputStream.java:90)
  at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.write(ChecksumFileSystem.java:391)
  at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:38)
  at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
  at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
  at java.io.DataOutputStream.write(DataOutputStream.java:90)
  at org.apache.hadoop.io.SequenceFile$CompressedBytes.writeCompressedBytes(SequenceFile.java:492)
  at org.apache.hadoop.io.SequenceFile$RecordCompressWriter.appendRaw(SequenceFile.java:903)
  at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2227)
  at org.apache.hadoop.mapred.ReduceTaskRunner$InMemFSMergeThread.run(ReduceTaskRunner.java:838)
, +1, I've just committed this. Thanks Devaraj!, Integrated in Hadoop-Nightly #48 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/48/)]