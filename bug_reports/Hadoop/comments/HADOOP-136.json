[The UTF8 class was originally written to be compatible with Sun's DataOutput.writeUTF:

http://java.sun.com/j2se/1.4.2/docs/api/java/io/DataOutput.html#writeUTF(java.lang.String)

Long-term it would be good to support strings with more than 2^16 characters and to support 4-byte UTF8 characters, but this will be hard to do back-compatibly.

Short term, would you or someone else like to submit a patch improving UTF8's handling of long strings?  Some unit tests demonstrating these problems would also be welcome.  Thanks!

, I need a fix for this. (Serialization of long UTF8 strings)

I have two proposals.
I am not addressing 4-byte UTF8 characters.
What would others recommend here?


Option 1. 

An alternate encoding for potentially long Strings. 
Code must explicitely choose to write and read back the "large" version.

To share as much code as possible, 
just add an optional argument to the UTF8 constructors: boolean large
If large mode:
 the length encoding is a VarLenShort (see below)
else:
 the length encoding is a short (the current format)

Note about static methods in class o.a.h.io.UTF8:
This change requires instance state (for the boolean large flag)
So the static versions of the UTF8 methods would ignore this change.
This should not be a problem since the code mentions that 
the static methods are deprecated and will go away,
   
   
   
Option 2. A semi-backward-compatible change.

In fact this is the same change as Option 1, 
except that we always assume large = true

in UTF8 change this:
  int bytes = in.readUnsignedShort();
to this:
  int bytes = in.readVarLenShort(); 
(and similarly for writes)

This is word-level variable-length encoding:
if the highest bit of the length word (16th bit) is set, 
then there is an extension word for the length. 
Total payload: 30 bits worth of length, which is enough.

For short enough Strings, the length encoding is unchanged 
This is why it is semi-backwards-compatible.

What inputs are currently accepted:
 Unicode strings, clipped at 0xffff/3=21845 characters.

What would be backwards compatible:
 Strings of encoded length <= 32767 bytes
 This includes: 
  o content with average character length less than 32767/21845 = 1.5 bytes
  o in partic. all single-byte UTF-8 (ASCII, iso-8859)
 
, What about simply introducing a new UTF8_String class that does the right thing for the complete range of characters and deprecating the use of the current class?, I would vote for option 2 above.  I'm having difficulty imagining an existing file that has the following three properties:

1: too large to convert if need be with a one-off

2: has UTF8's larger than 32767 bytes encoded but smaller than 21845 characters

3: hasn't already been trashed by a UTF8 that was truncated.

I woul dlike to go after the 4-byters as well.  There's no compatibility issue here.

-dk
, I think we should aim for 100% back-compatibility.  Perhaps if the 16th bit is set, and the next byte is invalid UTF-8 (e.g., a null) then we can assume the fourth byte is part of the length, otherwise the third and fourth are payload data.  That would permit up to 2^23 bytes.  In a subsequent release we could drop this "feature", permitting up to 2^31 bytes.  Could something like that work?
, OK, given the need for 100% backward-compatibility and given that I will only use this format internally:
I will go with Sameer's suggestion: 
have a separate class org.apache.hadoop.io.LargeUTF8.java

I will just make its length field 4-bytes, rather than var-length:
Otherwise this would complicate things like the ad-hoc offset computations in UTF8.Comparator.


Should I make a TestLargeUTF8 based on TestUTF8?

---

Concerning 4-bytes-long UTF-8 characters:
it seems that this situation does not occur in "Java-modified-UTF8"

The 4-byte chars are represented as 3+3.
See Modified UTF-8 in:
http://java.sun.com/developer/technicalArticles/Intl/Supplementary/

, Here is the patch, two new files:

org.apache.hadoop.io.TestLargeUTF8
org.apache.hadoop.io.LargeUTF8

The only difference with the UTF8 string format is that 
the length is stored on 4 bytes rather than 2 bytes.

TestLargeUTF8 tests serialization of larger strings up to 1MB


, If we can't fix the UTF8 class back-compatibly, then I'd prefer we provide a replacement class and deprecate UTF8.  Long-term we don't want two classes around that do almost the same thing.  We could name the new class Text or Chars.  It could use a variable number of bytes to indicate its length, using the 1 byte for lengths less than 127, two-bytes for lengths less than 2^14, etc., then encode the content with UTF-8.  Note that the length should be the number of bytes of content, not the number of encoded characters.  Does this sound reasonable?
, The static methods for converting integers and longs to zero-encoded representation (similar to what you have suggested) already exists in the record IO Util class., Just to verify, which length-encoding scheme are we using for class Text (aka LargeUTF8)

a) The "UTF-8/Lucene" scheme? (highest bit of each byte is an extension bit, which I think is what Doug is describing in his last comment) or 
b) the record-IO scheme in o.a.h.record.Utils.java:readInt

Either way, note that:

1. UTF8.java and its successor Text.java need to read the length in two ways: 
  1a.  consume 1+ bytes from a DataInput and 
  1b. parse the length within a byte array at a given offset 
(1.b is used for the "WritableComparator optimized for UTF8 keys" ).  

o.a.h.record.Utils only supports the DataInput mode.
It is not clear to me what is the best way to extend this Utils code when you need to support both reading modes

2 Methods like UTF8's WritableComparator are to be low overhead, in partic. there should be no Object allocation. 
For the byte array case, the varlen-reader utility needs to be extended to return both: 
 the decoded length and the length of the encoded length.
 (so that the caller can do offset += encodedlength)
   
3. A String length does not need (small) negative integers.

4. One advantage of a) is that it is standard (or at least well-known and natural) and there are no magic constants  (like -120, -121 -124)

, FYI:
some info on Java-modified UTF-8 
(this was previously posted)
See Modified UTF-8 in: 
http://java.sun.com/developer/technicalArticles/Intl/Supplementary/ 

As far as i understand:
The bottom-line is that supplementary UTF-8 characters:
o would be encoded as 4+ bytes in non-Java programs
o but they are already encoded as two Java char-s (i.e. two-bytes) when our converter code sees them.
o and so the conversion to UTF-8 proceeds on these two chars independently.
o So all the existing Java UTF-8 code that only handles 1..3-bytewide chars is already compliant with Java-modified UTF-8.

What do the java-i18n experts think?

---
Earlier comment:

Concerning 4-bytes-long UTF-8 characters: 
it seems that this situation does not occur in "Java-modified-UTF8" 

The 4-byte chars are represented as 3+3. 
See Modified UTF-8 in: 
http://java.sun.com/developer/technicalArticles/Intl/Supplementary/ 
, this was fixed in 0.5]