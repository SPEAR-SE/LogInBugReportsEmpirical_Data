[The Replication Monitor got a Runtime Exception as described in HADOOP-1232. The Namenode server threads do not catch RuntimeExceptions. The real fix is to find the cause of HADOOP-1232, but there are a few additional things that we can do to address in this issue:

1. Make the namenode exit when a system thread encounters a RuntimeException. Have another deamon that monitors HDFS processes and restarts them if they die.

2. Make the namenode fall into safemode when a system thread encounters a runtime exception.

3. Make the namenode exit when a system thread encounters a RuntimeException. It will remain dead until administrator manually intervenes.

I prefer option 1., The ReplicationMonitor thread catches all types of exceptions, logs them, sleep for 5 seconds and then continue from the beginning., This one might merit going into the 0.13 release as early as possible. Marking it as a blocker pending further discussion,., > The ReplicationMonitor thread catches all types of exceptions, logs them, sleep for 5 seconds and then continue from the beginning. 

This solution makes sure that ReplicationMonitor does not go away in case of RuntimeErrors. But is it possible that this solution leaves namenode in an inconsistent state? What if ReplicationMonitor is in the middle of updating some data structures when RuntimeError occurs. If this is possible, option 1 might be a safer solution. , merged patch with latest trunk., I think this patch alleviates this problem in the short term. Longer term, we can experiment with Java Service Wrapper (HADOOP-1525) to create a service process that monitors and recreates hadoop daemons as and when necessary., +1

http://issues.apache.org/jira/secure/attachment/12360507/catchThrowable2.patch applied and successfully tested against trunk revision r549977.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/327/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/327/console, Like Hairong, I am not completely comfortable with this patch.  Wouldn't it be safer to, in the added catch clause, set fsRunning to be false so that the namenode exits when an unexpected exception is encountered?  And, also, shouldn't we explicitly try to fix the IllegalArgumentException problem that caused this?, I too like the option 1 that I had listed earlier: make the namenode exit when it encounters a runtime exception. The question that remained unanswered is whether to have a monitoring daemon that restarts namenode automatically without any manual intervention. Do you have any suggestions in this regard?

Also, i would rather fix the real cause of this bug: HADOOP-1232. But we do not yet have a fix for that one. Leaving the namenode up and running without the ReplcaitionMonitor thread is not an option because blocks do not get replicated.

, Should we log StringUtils.stringifyException(t) instead of "t"?
, > whether to have a monitoring daemon that restarts namenode automatically

It seems safe to restart the namenode in this case.  I'd simply add a loop to NameNode.main() that creates and starts a new NameNode when the existing namenode exits unexpectedly.  We should only restart if it's stopping due to an error, and not due to an explicit call to stop().  So perhaps NameNode#join() could return a boolean indicating whether it's exiting normally or should be restarted, and the catch in the ReplicationMonitor should call a NameNode method to trigger that kind of exit.  Does this sound workable?, I think it might be dangerous to make NameNode.main() create a new NameNode object if the original one dies. The original instance of the Namenode would have used up lots of old-memory. If we create a new instance of the NameNode within the same JVM, then the GC process might take a while before the memory situation stabilizes. Is it ok if I exit the namenode-jvm completely and leave it to the administrator to restart the namenode if necessary?, > If we create a new instance of the NameNode within the same JVM, then the GC process might take a while before the memory situation stabilizes.

That's possible, I suppose, it's also possible that the GC might handle this well.  GC time is often proportional to the amount of non-garbage, which would be small on restart.

> Is it ok if I exit the namenode-jvm completely and leave it to the administrator to restart the namenode if necessary?

Sure, that'd be okay.  But, if the namenode auto-restarts slowly, the admin can always kill & restart it manually, so I don't see the harm in it attempting to auto-restart.  Restarting slowly isn't worse than being down, is it?  So my instinct would be to try auto-restarting.  It shouldn't cause data loss, and might indeed help in many cases, so, why not?, The Replication Monitor catches the RuntimeException and signals the namenode to restart. The namenode gracefully shuts down existing threads and starts all over again.

A unit test to test this feature is attached., Exit the JVM when the ReplicationMonitor thread encounters a runtime exception. Restarting the namenode within the same JVM instance was not easy, especially because not all resources were getting released by NameNode.stop().
, Exit the Namenode when the ReplicationMonitor thread encounters a RuntimeException. It would have been nice to be able to restart the namenode within the context of the same JVM, but a lot of work is needed to gracefully release all previously allocated resources. , Changed two invocations of throw new RuntimeException() from FSEditLog because they actually need to exit the JVM. These were introduced by HADOOP-1414., +1

http://issues.apache.org/jira/secure/attachment/12361532/namenodeRestart2.patch applied and successfully tested against trunk revision r555077.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/392/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/392/console, I just committed this.  Thanks, Dhruba!, Integrated in Hadoop-Nightly #152 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/152/])]