[As a temporary solution I replaced the listSubPath method of Jets3tFileSystemStore class with this new mathod, which simply check if there is any tail after delimiter and also does not pass zero to s3Service.listObjects:

public Set<Path> listSubPaths(Path path) throws IOException {
    try {
      String prefix = pathToKey(path);
      if (!prefix.endsWith(PATH_DELIMITER)) {
        prefix += PATH_DELIMITER;
      }
      S3Object[] objects = s3Service.listObjects(bucket, prefix, PATH_DELIMITER);
      Set<Path> prefixes = new TreeSet<Path>();
      for (int i = 0; i < objects.length; i++) {
    	if(objects[i].getKey().replaceAll(prefix,"").indexOf(PATH_DELIMITER)<0)    	
        prefixes.add(keyToPath(objects[i].getKey()));
      }
      prefixes.remove(path);
      return prefixes;
    } catch (S3ServiceException e) {
      if (e.getCause() instanceof IOException) {
        throw (IOException) e.getCause();
      }
      throw new S3Exception(e);
    }
  }, Mike,

I agree we should use the listObjects call that doesn't take a maxListingLength. JetS3t retrieves them in batches of 1000 behind the scenes for us. (Note that listDeepSubPaths uses the correct form of the method.)

Does dropping the URL encoding in Jets3tFileSystemStore cause any problems with root ("/") - I seem to remember it may have. However, it's wrong to double encode, so I would suggest removing the encoding from Jets3tFileSystemStore and seeing if the tests pass. If they don't we should go from there (rather than using double encoding because it happens to work).

Would you be able to create a patch for this? Thanks!, Tom, 

I made the patch that fixes the listSubPaths. Also, I removed the enconding from Jets3tFileSystemStore and there was no problem and it worked fine and there was no problem with root '/'. But, I didn't add that in the patch because that change is not backward compatible. If someone removes the encoding, then the previous built files are not readable anymore. It would be nice to make a tool to migrate the double encoded files to single '/' files when we make a patch to remove encoding from Jets3tFileSystemStore .
, S3 files need to be written with a version number of the client library that wrote them (as suggested in HADOOP-930). If we did this now, then we can detect when there is a mismatch and fail fast (and informatively). While it would be possible (but tricky) to support both versions, I don't feel that we should do that since there is a workaround for data migration: copy your S3 data from the old file system to a local file or HDFS (on EC2 preferably, but this isn't necessary) using the old version of Hadoop, then copy it back to a new S3 file system using a new version of Hadoop. I'd be happy to write this.

(I'm not saying that version-aware code will never be needed, just that it isn't yet since not that many people are using this feature yet.)

Thoughts?, We could start adding the version number property with this patch, so that one might write an automatic migration tool later.  Adding the version number ASAP is a good idea anyway., Yes, the patch should include

 * Fixing listObjects call
 * Removing URL encoding.
 * Adding version numbering metadata to files. This should use the S3 metadata feature so we can detect mismatches (and throw an exception) before reading the data.
 * Adding filesystem type metadata to files. Again using S3 metadata. This is easy to do at the same time as version and is a pre-requisite for HADOOP-930.

I'll have a go at this., Patch for review., Should we check not just the FS version, but also the name and type?

Also, to be clear, this is not back-compatible, right?  Is that what we want?  Perhaps instead, if something has none of the metadata properties set, we could interpret it as the prior version, for back-compatibility, so folks can still read their S3 data.  We might later drop that, as long-term, we might make it so that objects with none of these properties set are treated as simple HTTP resources., No this is not back-compatible. It's not actually that hard to make it back-compatible - so I'll produce another patch. I'll check for name and type too.

(I was thinking a similar thing about the long-term support for HTTP.), Tom, 

I checked the second patch and it works fine and detects the version mismatch. Also, it seems "not stopping at the path_delimiter" problem is fixed after removing double encoding. 

 , > It's not actually that hard to make it back-compatible

It's a bit harder than I thought when I wrote this, since we have changed the key format. So, you would have to check for both forms of the key before before saying the file or directory doesn't exist. I feel this would complicate the code somewhat.

I would suggest going with this new patch (v3) which additionally checks name and type. People will have to migrate old data as I originally described above. If this proved problematic, or there was demand, we could write a migration script. (the benefit of this is that it keeps the core S3FileSystem code unencumbered by version migration logic.), Thanks Tom, 

I wondering if you've got a chance to write any migration script. I do have some data on the old scheme that I cannot afford to lose.
, I haven't written a migration script yet, but I'll try to do so before 0.13., This patch (HADOOP-1061-v4.patch) includes MigrationTool to migrate the data written using an older version of S3FileSystem to the latest version. I have successfully tested it on an old filesystem., +1

http://issues.apache.org/jira/secure/attachment/12356557/HADOOP-1061-v4.patch applied and successfully tested against trunk revision r533233.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/97/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/97/console, +1 This looks good to me., I've just committed this., Integrated in Hadoop-Nightly #77 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/77/)]