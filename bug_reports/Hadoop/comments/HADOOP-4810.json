[When invalid replicas (ones from A1.A2.A3.A4 and B1.B2.B3.B4) were reported, NN was able to detect that the replicas were corrupted and mark them as invalid by adding them to recentInvalidates. However the attempts to add them to recentInvalidates failed because of SafeMode. However, the replicas were also left in blocksMap. When NN were out of SafeMode, corrupted replicas were counted as valid ones and the block was treated as over-replicated. Unfortunately when removing excessive replicas, valid replicas (ones from C1.C2.C3.C4 and D1.D2.D3.D4) were chosen to be removed. Thus valid data got lost.

It seems to me that when corrupted replicas are first detected in block report, they should be put in CorruptedBlockMap instead of invalidating them directly. At least NN would not count them as valid ones so false over-replication won't happen. , The data got lost when we upgraded our cluster from 0.17 to 0.18. The root cause of the problem is HADOOP-4663 which introduced a lot of corrupted blocks into dfs and thus polluted the data., This patch does
1. addStoredBlock marks a block as corrupted instead of deleting it.
2. addStoredBlock also check if a zero length block is truncated or not.
3. invalidateBlock can be executed in the safe mode because it only changes in-memory data structure.
4. markBlockAsCorrupt adds the corrupted block into corruptedBlockMap no matter even if the block is not under-replicated.
, I think that just removing the safe mode condition from invalidateBlock() (point 3) should solve this particular problem.
During startup (in safe mode) the block with smaller length will be added to recentInvalidates, and will not be scheduled for deletion until after the safe mode is turned off - this is how ReplicationMonitor works now.
In general I agree with Hairong that shorter blocks should be considered corrupt, but doing it now and synchronizing it in 3 versions seams too complicated. This adds one more step before incorrect block gets into recentInvalidates. Namely you first place them into corruptReplicas, and when the blocks are fully replicated they will move into recentInvalidates. Verifying that all works correctly on that path is hard. I just tried. And I would rather incorporate this into HADOOP-4563., OK, I agree with Konstantin. The minimum the change, the better. , I tried Konstantin's suggestion and found a problem with it. NN preserves a block's size persistently. So when the first truncated block report, NN is able to detect it and tries to invalidate it. But the invalidation fails because this is the only copy for this block. So this corrupted replica slips in and becomes a valid one.

I will keep the first patch., 
Looks good. Konstantin's suggestion would have made the patch much simpler but given that it still has an issue, we could go with the patch. 

One minor nit : There are two messages with "Corrupt block ... from" in two different places. It will be better if there is something in the message that can can tell these two apart.
, Looks good. 4 things: +1 modulo the following

1) Change the comment
              // Delete new replica.
   to 
           // mark  new replica as corrupt

2) for each of the cases check to see if the lease is open. If lease is not open, log an error that we got a length mismatch even when the file was not open.
  Also file a jira for the case when the lease is not open to perhaps write to edits log to record the new length (I am not sure if writing the new length
is right or wrong but we can  debate this on that jira).

3) Your fix will not let us distinguish  between true corruption caused by some bug in HDFS,  and the normal mismatch that can occur during appends when 
a client dies (I am not sure of this but that is my recollection from the append discussions with Dhruba last year at yahoo).
This is okay for now. But let us file a jira to fix this so that we can distinguish.
The easy code fix for this is to add a field to internal data structure to record the original length in fsimage - but this will increase the usage of memory in the system
since the 4 bytes will be multiplied by the number of  logical blocks in the system.

4) In my opinion the correct behavior for shorter blocks (but longer than the fsimage recorded length) is to invalidate as in the original code - however our invalidation code does not handle the case because if the "corrupt" block is the last one it keeps it as valid.  Thus your patch is a good emergency fix to this very critical problem.
 I suggest that we  file a jira to handle invaliding such invalid blocks correctly.
Note here I am distinguishing between *corrupt* blocks (caused by hardware errors or by bugs in our software) and *invalid* blocks (those lenght mismatches that
can occur due to client or other failures). Others may not share the distinction I make - lets debate that in the jira; we need to get this patch out ASAP.


, After some more thinking, lets  do item 2 as a separate jira to allow us to further discuss it., An updated patch for trunk., The patch for 0.18 branch., Ant test-patch result:
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     -1 Eclipse classpath. The patch causes the Eclipse classpath to differ from the contents of the lib directories.

Ant test-core result:
   Only org.apache.hadoop.mapred.TestJobTrackerRestart failed.

All failures are known and not related to this patch., I've just committed this., Integrated in Hadoop-trunk #698 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/698/])
    , +1 on this patch, albeit belatedly. 

Block report processing should ignore blocks that are being written to  via HADOOP-5027.]