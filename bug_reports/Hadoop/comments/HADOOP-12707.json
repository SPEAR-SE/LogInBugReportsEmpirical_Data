[My suggested solutionis for UserGroupInformation, the hashCode method should be
  public int hashCode() {
    return subject.hashCode();
  }
instead of 
  public int hashCode() {
    return System.identityHashCode(subject);
  }, Hello, [~haitao-tony].  Thank you for filing the issue, but the current implementation of {{UserGroupInformation#hashCode}} and {{UserGroupInformation#equals}} is by design.  Please see issue HADOOP-6670 if you're interested in more background.

I have 2 suggestions to offer for the memory leak problem that you noticed:
# Consider using the {{FileSystem#closeAllForUGI}} method, which would close all cached instances for a user.
# If it's not possible to change code, then consider disabling the cache by setting configuration property {{fs.hdfs.impl.disable.cache}} to {{true}} in core-site.xml.  This would bypass the cache completely, which might be appropriate depending on your application., Hi Chris Nauroth,Many thanks for your reply, this two option can solve the problem.
For the current design, even if the same user ,if the user calls this method twice then the filesystem ojbect will be created twice.
The CACHE can't have any effect except buffers a lot of unreferenced filesystem object.
Isn't there any plan to fix this?
, Hi Chris Nauroth,Many thanks for your reply, this two option can solve the problem.
For the current design, even if the same user ,if the user calls this method twice then the filesystem ojbect will be created twice.
The CACHE can't have any effect except buffers a lot of unreferenced filesystem object.
Isn't there any plan to fix this?
, bq. For the current design, even if the same user ,if the user calls this method twice then the filesystem ojbect will be created twice.

Just a small clarification: it happens only if those 2 calls use different {{UserGroupInformation}} instances that really represent the same "logical" underlying user ({{Subject}}).  This is a much less common usage pattern compared to usage of a single {{UserGroupInformation}} instance that always represents that "logical" user.

There are no plans to change this behavior at this time.  The standard solution is to do one of the two things I described in my last comment: disable the cache in configuration or use {{FileSystem#closeAllForUGI}}.

There is a lot of legacy behind the current behavior of the {{FileSystem}} cache.  Some of it is described in HADOOP-6670, and some of it is described in other old JIRAs.  I wish we could make some big changes there, but a lot of applications are coded to expect the current behavior., ...we could consider using weak references, or at least having some idleness, bq. ...we could consider using weak references, or at least having some idleness

I thought about weak references at one point but abandoned it for a few reasons:

# Weak references would provide different semantics compared to the current cache implementation.  Right now, applications can get a {{FileSystem}}, use it, abandon it for a long time (no longer holding a reference to it), and then come back and still find that instance sitting in the cache.  With weak references, there would be a race condition if a GC swept away the instance before the application came back to claim it again.  This could change performance and load patterns in negative ways if clients need to reconnect sockets.  Whether or not it would really be problematic in practice is unclear, but there is a ton of ecosystem and application code that would need testing.  Any such testing would be difficult and perhaps unconvincing, because it would be dependent on external factors like heap configuration, and ultimately, the timing of GC is non-deterministic anyway.
# The cache goes beyond just resource management and is actually coupled to some logic that implements the API contract.  Delete-on-exit becomes problematic.  With weak references, I suppose we'd have to trigger the deletes from {{finalize}}.  I've experienced a lot of bugs in other codebases that rely on a finalizer to do significant work, so I have an immediate aversion to this.  I also worry about tricky interactions with the {{ClientFinalizer}} shutdown hook.  I guess an alternative could be to somehow "resurrect" a reaped instance that still has pending delete-on-exit work, but I expect this would be complex.
# Even assuming we do a correct bug-free implementation of delete-on-exit from a finalizer, it still changes the semantics.  The deletions are triggered from the {{close}} method.  Many applications don't bother calling {{close}} at all.  In that case, the deletes would happen during the {{ClientFinalizer}} shutdown hook.  Effectively, these applications expect that delete-on-exit means delete-on-process-exit.  They might even be calling {{cancelDeleteOnExit}} to cancel a prior queued delete.  If a weak-referenced instance drops out because of GC, then the deletes would happen at an unexpected time, all of that prior state would be lost, and the application has lost its opportunity to call {{cancelDeleteOnExit}}.

An idleness policy would have similar challenges.  It's hard to identify idleness correctly within the {{FileSystem}} layer, because current applications expect they can come back to the cache at any arbitrary time and get the same instance again.

Effectively, this cache isn't just a cache.  It's really a set of global variables where clients expect that they can do stateful interactions.  Pitfalls of global variables, blah blah blah...  :-)

I don't like the current implementation, but I also don't see a safe way forward that is backwards-compatible.  If I had my preference, we'd re-implement it with explicit reference counting, require by contract that all callers call {{close}} when finished, and perhaps try to scrap the delete-on-exit feature entirely.  There has been pushback on the reference-counting proposal in the past, but I don't remember the exact JIRA.

Maybe explore backwards-incompatible changes for 3.x?, We hit this issue also and it impacts us a lot. Our Hadoop cluster is pretty big and Hadoop security plays a big part of it. So please consider it high priority.

Yes, disabling cache or calling closeAll() would prevent leaking but we lose the benefit of cache. We would like it to be fixed so that we can have a performant service. 

The use case for us is we have to create proxy user and get FileSystem in doAs(). The code is as below. 

UserGroupInformation ugi = UserGroupInformation.createProxyUser(proxyUser, UserGroupInformation.getCurrentUser());

fs = ugi.doAs((PrivilegedExceptionAction<FileSystem>) () -> FileSystem.get(conf));

Because ugi is different object even for same proxy user, the FileSystem#Cache#Key would be different for same proxy user. 

It would be great to fix it. HADOOP-6670 does have a valid reason that mutable object but simply using identityHashCode() is a bold decision and impact the usage of it. 

 , Added proxy users to the title.
 [~cnauroth]: what are your thoughts here?

[~xinlishang] I think you may want to consider what you could do with  your own cache, as that would isolate your code & you could be clever here. That is; you can cache filesystems outside FileSystem.cache if you really want to]