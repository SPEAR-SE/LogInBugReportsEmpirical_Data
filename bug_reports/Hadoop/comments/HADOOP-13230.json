[# Have you tested this against branch-2 yet? HADOOP-11694 covers changes there
# This should be testable: you'll need to bypass the s3a code after a mkdir and PUT up a file; listing the dir probably won't find the path.

I don't see us changing the policy of creating those empty paths, it's how we emulate empty dirs, and there is a core assumption in the Hadoop FS APIs, after a call to fs.mkdirs(path), then exists(path). Holds


But, 

* HADOOP-13208 proposes making listFiles(recursive) do a bulk list call; that would bypass the directory walk. We'll take a patch there, with tests,
* We sort of do that in rename already. Does playing with that make any difference? Maybe rename() is copying the empty/ dir entries too, even though there are children, so propagating the problem. Again, we'll take a patch there.

Finally, there is always the possibility of bypassing that HEAD for the empty dir and going straight to a listing. 
# That listing will need to recognise the diff between an empty dir entry and the children
# you have to consider that the cost of list operation is >> than a HEAD, due to the need to parse the XML response. That means it may get bounced for cost reasons. On the other hand, if you can can show that overall, on a populated directory path, things come out lower (and we are now counting individual operations for you to add those measurements to your tests), then they will get in.

Note that any patches against S3 will need to be tested by you before anyone will look at them:

https://wiki.apache.org/hadoop/HowToContribute#Submitting_patches_against_object_stores_such_as_Amazon_S3.2C_OpenStack_Swift_and_Microsoft_Azure

That's a policy which we ourselves have to abide by.


, -actually, a {{touch}} into the dest dir should be sufficient to trigger a cleanup, at least until some optimisations go in.

What may be viable here is actually to add some specific cleanup entry point/command to hadoop-aws, which cleans a bucket out of spurious false empty directories. After something starts mixing s3a and raw aws cli calls, this could be used to purge fake directories up the tree, I think something has been lost in the conversation on this issue so far: the original bug reports involved an unexpected interaction with delete, not listStatus.  If you click through to the linked Hive and Cloudera JIRAs, then you'll see the sequence of events is more like:

# mkdir
# Use AWS CLI to load files into the directory externally.
# delete directory
# Now, the data loaded externally via AWS CLI is still there despite the delete.

This is because the logic of {{S3AFileSystem#delete}} detects the empty fake directory and issues only a single object delete for that fake directory:

{code}
      if (status.isEmptyDirectory()) {
        LOG.debug("Deleting fake empty directory {}", key);
        s3.deleteObject(bucket, key);
        instrumentation.directoryDeleted();
        statistics.incrementWriteOps(1);
{code}

I expect the logic of {{listStatus}} is actually fine, though I haven't tested integration with AWS CLI yet.  I don't see any special case logic for empty fake directories in {{listStatus}}.  From what I can tell, it will always try the S3 listing, so files won't go missing.

Based on that, perhaps what we need here is a code change in {{delete}} to scan a listing and delete all children even if the base object looks like a fake empty directory.  This is extra S3 calls, so there is a potential performance impact.  In the common case where it really is a fake empty directory and no files were loaded externally, it's one extra listing call, and the results will be empty, so hopefully that's not too much of a hit on the XML parsing.  If we want to be conservative, then we could introduce a boolean configuration property like {{fs.s3a.external.bucket.access}}.  It could default to {{false}}, and users who want to opt in to external integration can flip it to {{true}}.  When it's {{false}}, we have opportunities for optimization, such as sticking with the existing {{delete}} logic of trusting the fake empty directory object and skipping the full scan.

I have looked only at delete, not a comprehensive review of all APIs, so maybe there are similar problems with external integration lurking there.  If we find additional similar situations, where there is a trade-off between enabling external integration and getting optimal performance, then the same {{fs.s3a.external.bucket.access}} property might be applicable there too.

What are your thoughts on this approach?, I know this surfaced in a delete, but suspected it would surface elsewhere; anything which bailed out early on an empty directory.

we'd effectively skip that special case for empty directories ...it'd need to list all child entries, just as a normal delete would do...it'd also have to delete the fake dir.

This would be more expensive than before, as for a "real" empty dir, there'll be one extra list call, then the existing delete operation issued.

I don't see it adding any costs to non-empty directories though.
, Could this be implemented replacing the HEAD request for the fakedir entry with a listObjects call? That would be the same number of api calls in the 'empty fakeDir' case, but no more work in the populated directory case.

 

Recently I ran into this issue using PRESTO to insert into hive partitions. Presto does not use the S3a driver, and does not delete the fakedir objects. ]