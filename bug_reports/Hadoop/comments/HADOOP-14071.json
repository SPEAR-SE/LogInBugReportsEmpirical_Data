[
What's happened is that the HTTP connection failed (is this long haul?), aws tried to reset the pointer (using mark/reset) and the buffer couldn't go back that far.


We saw this before, thought I'd eliminated it by not buffering the input stream, but instead sending the file input stream up direct. I'll review that code.

It may be we need to address this differently, simply by recognising the specific exception and retrying to send the block. That is: we implement the retry logic., For the ByteArrayInputStream & ByteBufferInputStream I don't think we currently set  {{request.getRequestClientOptions().setReadLimit}}. My understanding is that, based on the above, we should do this. Is that correct?
, That AWS SDK issue does look relevant.

For the specific case of data source being a file, we could have the MPU request using that, rather than opening it ourselves. Will need some changes in the code, as currently the BlockOutputStream assumes the source is always some input stream...it'll have to support the option of a File, and if supplied, prefer that as the upload option., This is short-haul (EC2 in us-east-1 to S3 in us-standard)., OK. I'm redoing the HADOOP-14028 patch with the File ref being passed down to AWS. Due to some technical issues ("laptop is toast") my dev time is somewhat crippled this week, so I'm not going to give a schedule for that being available. Hopefully in the next day or two â€”I just haven't got hadoop building locally right now, can we move discussion to HADOOP-14208, and I resolve this as a dupe? Keep discussion in one place, For sure.]