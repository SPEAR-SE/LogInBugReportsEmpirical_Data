[Change 1:

I plan to remove:
org.apache.hadoop.mapred.TaskInProgress.totalTaskIds (String[])& 
org.apache.hadoop.mapred.TaskInProgress.usableTaskIds (TreeSet)

and replace them with:
ArrayList<String> usableTaskIds

totalTaskIds isn't used anywhere except in org.apache.hadoop.mapred.TaskInProgress.init() and we don't need usableTaskIds to be a TreeSet, and ArrayList should suffice...

(I'll keep updating this issue with proposed changes as I glean more info from memory profiles of the JobTracker.), Attached are 2 netbeans' memory profiles of the JT after executing about 250 jobs & 500 jobs respectively (each job had 300 maps and 2 reduces i.e. ~75k tasks & ~150k tasks respectively) ... at this point the JT was consuming ~500Mb & ~1Gb of memory respectively., Here is a very early patch while I keep testing; appreciate any feedback... , Here is an updated patch (reflecting changes to trunk and some preliminary review by Devaraj).

After some thought I have done away with the call to jobtracker.removeTaskEntry from JobInProgress.failedTask and instead let the JobInProgress.garbageCollect -> jobtracker.finalizeJob handle it at the end of the job (success/failed/killed). Thoughts?, Another take while I continue further testing... appreciate any f/b., Ok, here is a reasonably well-tested patch...

List of changes:
  a) Fixed HADOOP-740 i.e. ensure task entries are cleaned up on completion.
  b) Fixed HADOOP-787 i.e. ensure we keep only 100jobs per user; rest are available via jobhistroy anyway.
  c) Fixed both JobTracker & TaskTracker to ensure lost status-updates/heartbeatResponses due to lost rpcs are resent by both TaskTracker & JobTracker; and also that the JobTracker can detect that duplicate 'TaskTrackerStatus' updates and ignore them, which otherwise are fatal.
  d) Some miscellaneous fixes like using ArrayList instead of TreeSet and array for 'usableTaskIds' in TaskInProgress.java
  
  Results:
  Currently after running smallJobsBenchmark with 750 jobs each with 300 maps & 2 reduces (i.e. total of ~225,000 tasks) the memory footprint of the JobTracker is ~1.5Gb after 'RETIRE_JOB_INTERVAL' (which I suspect also leads to degeneration of JT's performance as in HADOOP-843 since each of the JT's datastructures are extremely bloated leading to sluggishness). With this patch the memory-footprint is down to ~150MB after 'RETIRE_JOB_INTERVAL', yes, that's 150Mb! :) (and seems to solve HADOOP-843 too).

  Appreciate any feedback...
, -1, because the patch command could not apply the latest attachment (http://issues.apache.org/jira/secure/attachment/12348120/jt_memory_profiles.tgz) as a patch to trunk revision r489707. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., Looks good (except some whitespace changes which should be removed)., 1. The TaskInProgress.usableTaskIds should be removed and a new task id generated when needed. That list has been bothering me for a while. *smile*

2. You replace a new style for loop with an old style for loop on TaskTracker.java line 597, which should stay a new style loop.

3. The trackerToMarkedTaskMap isn't synchronized by a lock, so has race conditions.

4. There are spacing diffs., > There are spacing diffs.

On formatting, indentation is also four-spaces per level rather than the preferred two.

http://wiki.apache.org/lucene-hadoop/HowToContribute
, > 1. The TaskInProgress.usableTaskIds should be removed and a new task id generated when needed. That list has been bothering me for a while. *smile*
  Ok, coming up. :)


> 2. You replace a new style for loop with an old style for loop on TaskTracker.java line 597, which should stay a new style loop.
  Subtle change due to the fact that org.apache.hadoop.mapred.TaskTrackerStatus.getTaskReports returns an 'Iterator'. Shud i fix the (public) api to return 'List<TaskStatus>' instead?


> 3. The trackerToMarkedTaskMap isn't synchronized by a lock, so has race conditions.
  The functions manipulating 'trackerToMarkedTaskMap' assume that the JobTracker itself is locked on entry (much like existing 'removeTaskEntry'), would it help if I put in a comment there? Or shud I explicitly mark the function as 'synchronized' ?

>On formatting, indentation is also four-spaces per level rather than the preferred two. 
  Ok, I'll fix it. Spent time hitting the 'tab' key to ensure compliance with existing indentation in some functions. *rueful smile*

  Related thought: should we open a new issue to track and ensure 'all' code is indented with 2 spaces instead of 4 (as in some places)? :), Ok, that was me with the a new patch incorporating:
> 1. The TaskInProgress.usableTaskIds should be removed and a new task id generated when needed. That list has been bothering me for a while. *smile* 
>On formatting, indentation is also four-spaces per level rather than the preferred two. 
 (I've made all 'new' code indented with 2 spaces, while any code added to functions which had 4 are kept as before... does that sound reasonable?)

I'll serve-up new patches post-discussion on points 2 & 3., Please change the TaskTrackerStatus.taskReports() method to be depricated and make a new:

public List<TaskStatus> getTaskReports() { ... }

I'm pretty sure there are some contexts where the JobTracker is not locked. I'll take a look., > Please change the TaskTrackerStatus.taskReports() method to be depricated and make a new: 
> public List<TaskStatus> getTaskReports() { ... } 
Ok, will do. Another patch coming up...

> I'm pretty sure there are some contexts where the JobTracker is not locked. I'll take a look.
I've run Devaraj through the whole synchronization logic here, basically the functions assume that the JobTracker is locked on entry (as in the javadoc) and happens so since most of the calls emanate from

public synchronized JobTracker.heartbeat() -> JobTracker.processHeartbeat() -> JobTracker.updateTaskStatuses

I'd definitely appreciate another closer look at this... Thanks!, Here is a new patch incorporating Owen's comments and have added:
public List<TaskStatus> getTaskReports() { ... } 

, Here are the paths that don't lock the JobTracker as far as i can see...

ExpireLaunchingTask.run -> 
  JobInProgress.failedTask (line 670) ->
  JobInProgress.updateTaskStatus ->
  JobInProgress.failedTask (line 592)
  JobTracker.markCompletedTaskAttempt

and:

ExpireTrackers.run ->
  JobTracker.lostTaskTracker ->
  JobInProgress.failedTask (line 670) -> from above

and a couple of similar variations. I think we are heading the wrong way with this, because if it is this hard to verify that it is right, we will not be able to keep it from breaking when we change the code later., Let me expand on the point. I think it would be easier to keep the locking straight-forward if we keep the calls going:

JobTracker.* -> JobInProgress.* -> TaskInProgress.*

and stop making calls to the upper levels. The backwards calls in the TaskTracker have also caused lots of trouble for us. Thoughts?, Yeah, I agree that JobTracker does not get locked for some flows in the code (and the patch, I think, doesn't worsen/improve this situation). Arun and I were thinking that a new Jira should be raised to handle these bugs. , I think it does make it worse by adding a new global collection that is being updated by the lower levels of the call stack. There was previously a race condition on one of those paths as shown in HADOOP-600, but I think this patch makes us more vulnerable. , I think you're right that after we fix HADOOP-600, this one should be ok. We do need to consider a long term solution for how to break the cyclic dependencies, since they make the straightfoward locking so challenging. I've moved HADOOP-600 to 0.10.1, so +1., Thanks for the review Owen... 

HADOOP-600 should indeed fix the only (long-standing) issue wrt synchronization (via ExpireTrackers.run -> JobTracker.lostTaskTracker -> JobInProgress.failedTask; for clarification purposes: the other one via ExpireLaunchingTasks is 'ok' since it locks the JobTracker explicitly)., I'll submit a patch soon for HADOOP-600...

Meanwhile I'll articulate my take on a long-term solution for these cyclic dependencies; I completely agree we need a better solution., I just committed this.  Thanks, Arun!]