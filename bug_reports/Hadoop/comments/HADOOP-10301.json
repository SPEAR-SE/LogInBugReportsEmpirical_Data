[{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12625672/HADOOP-10301.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-auth.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3489//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3489//console

This message is automatically generated., The 0.23 patch causes problems for oozie's use of auth cookies.  Oozie caches the cookies on the local fs until they are invalid and expects fallback to spnego to occur.

Currently, an uncaught AuthenticationException in a servlet sends the client an illegal 401 with no WWW-Authenticate header.  The existing behavior:
# 2.x catches and ignores AuthenticationException while validating auth cookies - expired, wrong secret, wrong type, etc.  A valid 401 + negotiate header is sent to trigger spnego.  No problem here.
# 0.23 does *not* catch AuthenticationException while validating auth cookies.  Servlet returns an illegal 401 with no auth header causing a client NPE.
# Neither 2.x nor 0.23 catch AuthenticationExceptions if spnego fails or proxy authorization fails.  Servlet returns an illegal 401.  Client NPE.

These patches fix all these issues by converting uncaught AuthenticationExceptions from 401 to 403 Forbidden which is entirely appropriate for #3.  However, for 0.23 (#2), the client does not revert to spnego for invalid auth cookies.  I'm studying AuthenticatedURL to see how the invalid 401 ever could have worked for oozie.  There's a tangle of issues with how webhdfs vs. oozie expects this to work that I'm investigating.
, Modified patches to always return 401 when auth cookie validation fails.   The response message of the 401 is the reason., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12626749/HADOOP-10301.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-auth.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3524//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3524//console

This message is automatically generated., [~tucu00], can you take a look?  This is a 2.3 blocker., [~daryn], I may be missing something but it seems that we can still return a 401 without the WWW-Authenticate header right?

Shouldn't we define instead?

{code}
    int errCode = HttpServletResponse.SC_NOT_ACCEPTABLE
{code}
, Under what condition(s) do you think the 401 sans header can occur?  The unit tests should cover the various scenarios, but I'm double checking all the code paths again., It's proving very tedious to trace and logically document all the code paths.  In the spirit of keep it simple, how about in the block where {{httpResponse.sendError}} is invoked:  if the response is going to be 401 but the auth handler didn't set the expected WWW-Authenticate header that I convert it to a 403?  If nothing else, it's a failsafe for future bugs in auth handlers.  I'll post a patch shortly., I was about to suggest something like that, the filter itself should never return a 401, that is up to the authhandler (and it must be accompanied of a WWW-Authenticate: header), makes sense., Sigh.  Nothing is ever as easy as it seems.

I first was going to specifically check if response.status=401 and !response.containsHeader(WWW-Authenticate).  HttpResponse appears to let you set the status, but not query it...  So I fell back to checking errCode=401 and !response.containsHeader(WWW-Authenticate).  Now all the mocked unit tests fail because they don't persist any state from the response.setHeader, so containsHeader always returns false. :|

So, I can:
# Try to change the unit tests and hope other projects' tests don't break.
# Call it a day and unblock 2.3 because I can't find an existing code path that will cause an illegal 401.

If you agree with #2, we really should file another jira to cleanup the filter + handler interactions.  Among other things, the auth cookie should exclusively be a feature of the auth filter.  Auth handlers shouldn't know anything about cookies.  That should make the logic much simpler and easier to enforce proper auth handler behavior, but it's too much for me to do right now., +1, From what I understand, this is an existing issue with 2.2 and is NOT a regression. This can patch can go in if need be, but I am moving it to 2.4. Please revert back if you disagree. Thanks!, Is this ready to go? Or, should it be blocking 2.4? Thanks., I had to merge up the patch and fix up some tests.  Relevant code is the same., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12635358/HADOOP-10301.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-auth.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3678//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3678//console

This message is automatically generated., The patch generally looks good to me.

My concern is on the testing side. The change is going to affect all downstream projects in secure set ups. Given the fact that 2.4 is coming up pretty soon, I wonder, is it a good idea to put it in at the last minute? It seems to me that there is insufficient time to test it in the 2.4 timeframe, which might break downstream projects like Oozie unexpectedly.

Since we have move it as a blocker from 2.3 to 2.4, is it okay to move it to 2.5? More precisely, we can continue to make progress on this patch, but commit it only to branch-2 for now. That way this change will be extensively tested in the 2.5 timeframe, and leave the downstream projects enough time to fix any bugs if they occur., I agree that it is not a good idea to have this and HDFS-4564 as the last minutes changes to 2.4.  How about moving them to 2.5?, I move this to 2.5. [~daryn], please revert it back if you think it's necessary for 2.4., Yes, this must be in 2.4 to prevent client-side NPEs.  We've been running the patch internally for 2 months.  It's been tested with oozie and other internal tools., For client-side NPEs, do you mean HADOOP-9363?  Then, do we also need HADOOP-9363?  There is no patch in HADOOP-9363., The patch for trunk and branch 2.x looks good to me. +1 for the patch., This jira and HDFS-4564 are sufficient to fix webhdfs.  HADOOP-9363 should remain open to fix the authenticated url issues. The hdfs jira doesn't pass true to the url factory so it bypasses the use of authenticated url which avoids another raft of problems.  The use of the auth cookie is completely unnecessary with webhdfs because it obtains a token and then uses it for all subsequent operations., SUCCESS: Integrated in Hadoop-trunk-Commit #5426 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5426/])
HADOOP-10301. AuthenticationFilter should return Forbidden for failed authentication. Contributed by Daryn Sharp. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1582883)
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestPseudoAuthenticator.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, I've committed the patch to trunk, branch-2, and branch-2.4. I have not committed it to branch-0.23. Please feel free to commit it if you think it's ok [~daryn]., FAILURE: Integrated in Hadoop-Mapreduce-trunk #1741 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1741/])
HADOOP-10301. AuthenticationFilter should return Forbidden for failed authentication. Contributed by Daryn Sharp. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1582883)
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestPseudoAuthenticator.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Yarn-trunk #524 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/524/])
HADOOP-10301. AuthenticationFilter should return Forbidden for failed authentication. Contributed by Daryn Sharp. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1582883)
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestPseudoAuthenticator.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1716 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1716/])
HADOOP-10301. AuthenticationFilter should return Forbidden for failed authentication. Contributed by Daryn Sharp. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1582883)
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationFilter.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestPseudoAuthenticator.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestAuthenticationFilter.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
, This breaks how the Oozie Client was checking for expired auth tokens (OOZIE-2485).  I was looking into something related, and saw that when using Kerberos and an expired auth token, Oozie Client wasn't getting a new token.  

I didn't notice this problem until I really dug into the code because the Oozie client commands would still succeed, it would always just log a warning in the Oozie Server from hadoop-auth about the token, and use Kerberos.

After a lot of debugging, I figured out the cause.  Currently, Oozie does this in {{AuthOozieClient}} to determine if a token has expired:
{code:java}
        if (currentToken.isSet()) {
            HttpURLConnection conn = (HttpURLConnection) url.openConnection();
            conn.setRequestMethod("OPTIONS");
            AuthenticatedURL.injectToken(conn, currentToken);
            if (conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
                AUTH_TOKEN_CACHE_FILE.delete();
                currentToken = new AuthenticatedURL.Token();
            }
        }
{code}
Previously, the response code would be 401 when the token expired.  Oozie Client would clear out {{currentToken}} and some later code would get a new one after using the {{KerberosAuthenticator}}.  However, it's now 200 here and returns a new token (in the header) after successfully doing SPNEGO without (Oozie explicitly) calling the {{KerberosAuthenticator}} at all.  To fix this, Oozie has to modify the above to do this:
{code:java}
        if (currentToken.isSet()) {
            HttpURLConnection conn = (HttpURLConnection) url.openConnection();
            conn.setRequestMethod("OPTIONS");
            AuthenticatedURL.injectToken(conn, currentToken);
            if (conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
                AUTH_TOKEN_CACHE_FILE.delete();
                currentToken = new AuthenticatedURL.Token();
            } else {
                try {
                    AuthenticatedURL.extractToken(conn, currentToken);
                } catch (AuthenticationException ex) {
                    AUTH_TOKEN_CACHE_FILE.delete();
                    currentToken = new AuthenticatedURL.Token();
                }
        }
{code}
Here it will try to extract the new token if one was given using {{AuthenticatedURL.extractToken}}, which will update {{currentToken}}., Given that the previous behavior wasn't following the spec, I guess we have to keep this though, and update Oozie to handle this.  , Pseudo Auth also behaves a little differently than it used to.  It gives a 403 instead of a 401 in the above code.  {{extractToken}} throws an {{AuthenticationException}} if it doesn't see a 20X, so the updated code will also handle this case.  In any case, the behavior has changed and its also different than Kerberos Auth, in that it does not give a new token here and gives 403 instead of 200.]