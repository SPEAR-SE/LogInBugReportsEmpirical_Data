[In the datanode logs, there are a lot of "IOException: Connection reset by peer". For example,
{noformat}
2008-06-30 23:13:17,848 WARN org.apache.hadoop.dfs.DataNode: DatanodeRegistration(xx.xx.xx.xx:50297, storageID=DS-603925314-xx.xx.xx.xx-50297-1214866035317,
 infoPort=51131, ipcPort=50020):Got exception while serving blk_-590855607842175534_2046 to /yy.yy.yy.yy:
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
	at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:418)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:519)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:199)
	at org.apache.hadoop.dfs.DataNode$BlockSender.sendChunks(DataNode.java:1841)
	at org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1938)
	at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:1096)
	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1024)
	at java.lang.Thread.run(Thread.java:619)
{noformat}, The "java.io.IOException: Connection reset by peer" is very easy to reproduce.  Promote this to a 0.18 blocker.
, > The "java.io.IOException: Connection reset by peer" is very easy to reproduce. Promote this to a 0.18 blocker.

Is this relevant yet?

The client is stuck in an RPC to NameNode. Currently RPCs can be wait for a long time if the server is busy. , > Is this relevant yet?

I have tried running randomwriter and sort on a 120-node cluster.  There is no RPC stuck but a lot of "Connection reset by peer".  I see the same thing in Arun's Datanode logs., > I have tried running randomwriter and sort on a 120-node cluster. There is no RPC stuck but a lot of "Connection reset by peer". I see the same thing in Arun's Datanode logs.

Does it have stuck clients? DFS is expected to handle connection errors. from the stacktraces in the description of this jira, I think the main question is why the RPC is stuck. , > The client is stuck in an RPC to NameNode. 
correction : "recoverBlock()" seems to be an RPC to datanode. 

Could this be related to HADOOP-3673? I am not yet familiar with the datanode RPCs.. , > Could this be related to HADOOP-3673?
HADOOP-3673 may be the cause of recoverBlock() stuck.
, Arun, could you set dfs.datanode.handler.count to a 20?  It might gets around the deadlock., > The "java.io.IOException: Connection reset by peer" is very easy to reproduce. Promote this to a 0.18 blocker.
These messages can be ignored. Filed HADOOP-3678 to ignore these., Thought to be resolved by HADOOP-3673. No longer observed say Arun and Raghu.]