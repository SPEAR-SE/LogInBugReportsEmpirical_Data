[Venkatesh, 

Hadoop 2.3+ works on windows without needing to do tricks like this...so if there are problems here they should go away if you get a native windows build (including the native libraries for performance, and {{winutils.exe}} for chmod. You should be able to submit jobs to a CDH5. x cluster running on Linux with it.

If you do want to stick with CDH client side, then this is going to have to be something to take up with them. From ASF side, I'm going to close as a INVALID. Sorry., Hi Steve, I changed my Maven pom file to replace all dependencies having cdh with the apache version like. 
		<!-- setup for CDH 5.x -->
 <!--  
		<jdk_version>1.7</jdk_version>
		<avro_version>1.7.5-cdh5.1.0</avro_version>
		<cdh_version>2.3.0-cdh5.1.0</cdh_version>
		<cdh_ZK_version>3.4.5-cdh5.1.0</cdh_ZK_version>
		<cdh_HB_version>0.96.1.1-cdh5.1.0</cdh_HB_version>
 --> 
 		<!-- setup for non-CDH 5.x but hadoop 2.3.0-->
  
		<jdk_version>1.7</jdk_version>
		<avro_version>1.7.5</avro_version>
		<cdh_version>2.3.0</cdh_version>
		<cdh_ZK_version>3.3.1</cdh_ZK_version>
		<cdh_HB_version>0.96.1.1</cdh_HB_version>
 

but still get the same null pointer exception. I'm compiling and executing the MR job via eclipse as a simple Java Application (Right Click on Driver class --> Run--> run as Java Application).
, Github user velo commented on the pull request:

    https://github.com/apache/incubator-tinkerpop/pull/207#issuecomment-178164276
  
    the windows profile is to work around
    https://issues.apache.org/jira/browse/HADOOP-10969
    and
    https://issues.apache.org/jira/browse/SPARK-6961
]