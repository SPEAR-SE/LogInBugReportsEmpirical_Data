[(Source: https://issues.apache.org/jira/secure/EditComment!default.jspa?id=12665400&commentId=14106278)

Given this:

{code}
$ export HADOOP_COMMON_HOME=$(pwd)/$(ls -d hadoop-common-project/hadoop-common/target/hadoop-common-*/)
$ export HADOOP_HDFS_HOME=$(pwd)/$(ls -d hadoop-hdfs-project/hadoop-hdfs/target/hadoop-hdfs-*/)
$ export PATH=$HADOOP_COMMON_HOME/bin:$HADOOP_HDFS_HOME/bin:$PATH
$ hdfs
ERROR: Unable to exec (path)target/hadoop-hdfs-3.0.0-SNAPSHOT/bin/../libexec/hadoop-functions.sh.
{code}

How do we make hdfs work properly?

First, what is happening?

The code tries to find where to look for hdfs-config.sh is located.  It does this by looking for ../libexec, where it finds it.  It now makes the (false) assumption that this must be the one, true libexec dir.  So it now tries to fire up hadoop-config.sh and hadoop-functions.sh which fail.

There are a couple of different ways to solve this:

* Look to see if HADOOP_COMMON_HOME is defined and look for hadoop-config.sh/hadoop-functions.sh is there as well.
* Throw caution to the wind and see if this stuff is in our current path.
* Recalculate HADOOP_LIBEXEC_DIR in hadoop-config.sh might work too, since clearly hdfs found it.
* Do the full gamut of checks for HADOOP_HDFS_HOME, etc, for hdfs-config.sh + some of the stuff above.

One sticking point is what happens if hadoop-layout.sh redefines the directory structure?  The code is sort of in a catch-22., OK, hdfs-config.sh does to the right thing (although it could be argued the order should be reversed):

{code}
if [ -e "${HADOOP_LIBEXEC_DIR}/hadoop-config.sh" ]; then
  . "${HADOOP_LIBEXEC_DIR}/hadoop-config.sh"
elif [ -e "${HADOOP_COMMON_HOME}/libexec/hadoop-config.sh" ]; then
  . "${HADOOP_COMMON_HOME}/libexec/hadoop-config.sh"
elif [ -e "${HADOOP_HOME}/libexec/hadoop-config.sh" ]; then
  . "${HADOOP_HOME}/libexec/hadoop-config.sh"
else
  echo "ERROR: Hadoop common not found." 2>&1
  exit 1
fi
{code}

So it's really hadoop-config.sh that's broken here:

{code}
# get our functions defined for usage later
if [[ -f "${HADOOP_LIBEXEC_DIR}/hadoop-functions.sh" ]]; then
  . "${HADOOP_LIBEXEC_DIR}/hadoop-functions.sh"
else
  echo "ERROR: Unable to exec ${HADOOP_LIBEXEC_DIR}/hadoop-functions.sh." 1>&2
  exit 1
fi

# allow overrides of the above and pre-defines of the below
if [[ -f "${HADOOP_LIBEXEC_DIR}/hadoop-layout.sh" ]]; then
  . "${HADOOP_LIBEXEC_DIR}/hadoop-layout.sh"
fi
{code}

This is going to be a relatively easy fix, I think.  We just need to add checks for HADOOP_COMMON_HOME prior to using HADOOP_LIBEXEC_DIR., Patch that fixes hadoop-config.sh to use HADOOP_COMMON_HOME/libexec if it can't find it in HADOOP_LIBEXEC_DIR as well as fixes two bugs in HADOOP_HDFS_HOME and HADOOP_MAPRED_HOME definitions when they aren't defined., FWIW, I opted to reverse the order because I remembered why I did it in the other code as well:  in NORMAL operating modes, HADOOP_LIBEXEC_DIR is the correct place., 

-01:  Wait... wait... wait... We should NOT be using HADOOP_HOME for anything! So let's fix that too., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12663573/HADOOP-10996-01.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.ha.TestZKFailoverController
                  org.apache.hadoop.ha.TestZKFailoverControllerStress
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4533//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4533//console

This message is automatically generated., Cancelling patch -01.

After working with it, I've found some edge and not-so-edge cases that either:

a) are made worse (e.g., usage of *_HOME should be viewed as exceptions to _PREFIX, not as an all or nothing scenario)
b) aren't covered (e.g., etc/hadoop/*-site.xml comes from *_HOME)

, -02:

Changed up the order. Reduces the amount of stat calls needed by checking if some of the *_HOME vars are defined.

I started to poke around at enabling *_HOME/etc/hadoop (or whatever), but decided it probably isn't worth it since it will likely lead to unpredictable results.

[~andrew.wang], please try this out and see if it fixes your specific issue. Thanks!, Hey Allen, thanks for working on this. What's your recommendation on where to put config files? Right now I do something like:

{noformat}
<above stuff>
cp -r ~/configs/* $HADOOP_HDFS_HOME/etc/hadoop/
hdfs namenode
{noformat}

This still works, but your comments about *_HOME/etc/hadoop being unpredictable made me wonder.

+1 regardless though, thanks again., TL;DR: Absolute best bet is to put configs some place and assign HADOOP_CONF_DIR to it so that you have absolute certainty on where Hadoop is pulling settings.  

Longer story:

Currently, if HADOOP_CONF_DIR isn't defined, it uses a bit of interesting logic to locate it:

1. Figure out where HADOOP_PREFIX is at. Is HADOOP_PREFIX defined? If not, then let's assume it's "what's called us/..".
2. Does HADOOP_PREFIX/conf/hadoop-env.sh exist? OK, then that must be HADOOP_CONF_DIR
3. No? OK, then HADOOP_CONF_DIR must be HADOOP_PREFIX/etc/hadoop.

What's fun about this and what you're doing is that HADOOP_CONF_DIR will get defined differently depending upon which bin dir you are using. :D

Fine, you say!  Let's just treat all _HOME/etc/hadoop and _HOME/conf as potentially valid.  Now we have a very interesting problem:  how do you define HADOOP_CONF_DIR?  Other stuff past Hadoop depends upon this being one directory.  We could pick the first one and then just shove the rest in the classpath and none would be the wiser!

Aha! But they would.  Which one takes precedence? What happens if there are conflicts? etc, etc. It gets messy very very fast. So... ABORT! ABORT!

(BTW, this is pretty much the same logic from branch-2. It could be argued that there should be a check to see if etc/hadoop is 'real' too and abort on it.  Here's the fun part: the shell code works perfectly fine if -env.sh is empty now... the NN will still crash though.  That said, if HADOOP-10879 gets finished, this will almost certainly need to get revisited.  Probably better to look for core-site.xml, honestly, since all of the sub-projects all depend upon that.  In other words, we could run through all of the _HOME, HADOOP_PREFIX, etc, and use the first core-site.xml we find as the 'real' HADOOP_CONF_DIR.), Thanks! I'll commit this as soon as the git repo opens up!, Cool, thanks for the explanation :) will do that in the future.]