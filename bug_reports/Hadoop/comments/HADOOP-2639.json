[I ran sort-100 with a bunch of maps not serving output to a bunch of reducers, here 10 maps blocking 5 reducers. The job ran to completion. I am not able to reproduce the error.  Is there a way to reproduce the error. , Finally found the bug that causes this effect. HADOOP-2247 introduced a new strategy for killing the maps i.e kill the map if {{(fetch-failure-notification/num-running-reduce-tasks) > 0.5}}. It seems that {{num-running-reduce-tasks}} can achieve negative value thus breaking the overall strategy and stalling the whole job by not killing the maps. This is because the reduce count is incremented if the TIP is not running and decremented for every task in the TIP. Providing a patch that addresses this issue by incrementing the counter for every task that gets scheduled. , -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12373955/HADOOP-2639.patch
against trunk revision 614721.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests -1.  The patch failed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1667/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1667/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1667/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1667/console

This message is automatically generated., The test {{TestFsck}} has failed. All the tests passed on my system., I'm uncomfortable with this patch... 

It is moving some really old code around, I'm not sure we understand all the implications to do this before a major release... should we consider adding another field to track the information you want to fix the bug introduced by HADOOP-2247?, How can you explain the negative values of {{runningReduceTasks}}? I can use a different variable and open another issue but still that will be a blocker bug, no? {{runningReduceTasks}} is not what is should be! By the way this counter does not affect anything in the framework and in the trunk the values of {{runningReduceTasks}} and {{runningMapTasks}} are just junk!!, bq. should we consider adding another field to track the information you want to fix the bug introduced by HADOOP-2247?
 HADOOP-2247 did not introduce any bugs. It used some unused (by framework) counters that was buggy., I feel that the bug fix should go in (it doesn't affect any public API, and fixes a big inconsistency). Also, I am not convinced that this change will have negative implications., I'm pretty worried by this patch. I remember putting those updates in the branch precisely because there was a race condition on the order of status update messages. I think it you occasionally get double updates with this patch, which would be very bad., HADOOP-2247 introduced code which relied on a buggy counter, so that was buggy indeed.

I'm not super sure, but looks to me with this patch that running{Maps/Reduces}Tasks are incremented more than once if we have speculative tasks, so it bothers me. 

Like Owen commented, there is a reason why this code is so - maybe the fix is to figure out why {{numRunningReduces}} is getting negative? That is why I think we can fix this differently... 
, Oh sure I want this bug fixed for 0.16.0, maybe differently..., I am missing something I think - isn't runningMapTasks/runningReduceTasks supposed to include speculative tasks also? This patch increments those values for speculative tasks also. If it is not supposed to increment the values for new speculative tasks, then we should fix those places where the values are decremented. This is because the decrement happens even for killed tasks (and this fix is more involved). Assume this scenario with just one map task in the job in the order of time (in the current codebase without the patch):
1) obtainNewMapTask returns a task - runningMapTasks becomes 1
2) obtainNewMapTask gets called again - this time a speculative task is returned, and runningMapTasks is not incremented since _wasRunning_ is true
3) The first task completes - runningMapTasks becomes 0 (in _completedTask_)
4) The speculative task is killed - runningMapTasks becomes -1 (in _failedTask_) --- this is *bad!*

I don't see a good reason behind why we should have runningMapTasks/runningReduceTasks not count speculative tasks. Did code analysis and didn't seem to find any place where this would have any effect on the existing semantics.

[A comment on the patch - the patch should check whether the _task_ returned by _getTaskToRun_ is non-null and then increment the runningMapTasks/runningReduceTasks.], {quote} I'm pretty worried by this patch. I remember putting those updates in the branch precisely because there was a race condition on the order of status update messages. I think it you occasionally get double updates with this patch, which would be very bad.
{quote}
I dont think this has any thing to do with {{runningReduceTasks}}. Can you explain in more detail?
{quote}
Oh sure I want this bug fixed for 0.16.0, maybe differently...
{quote}
Yeah. One thing we can do is increment and decrement the count only once per TIP. Then the appropriate name would be {{runningReduceTIPs/runningMapTIPs}} or else increment it for every scheduled _task_ since currently we are decrementing it for every task, in that case its the same as the submitted patch. , Ok, looks like there is some disconnect...

While implementing HADOOP-2247, I remember that the following code: 

{noformat}
    float failureRate = (float)fetchFailures / runningReduceTasks;
{noformat}

is to mean: _Check if there are too many currently running reduce TIPs are complaining about this map._

I had a discussion with Amar and he clarified that he was considering the counter as _currently running reduce task-attempts_ and not _currently running reduce TIPs_ as was the original intention... and hence this debate/disconnect.

Given that, I think the right fix is for us to figure _why_ the *runningReduceTasks* counter is wrongly turning -ve and fix the wrong decrement... 

Thoughts?, I still don't clearly see why we should not consider all *running tasks* when we refer to runningReduceTasks/runningMapTasks. Even for the above failureRate calculation, we should consider _tasks_, no? Why should we not consider speculative tasks when it comes to reporting fetchfailures?, To be honest the details are hazy, but I do remember us considering writing a simple loop to figure out that count rather than use _runningReduceTasks_; anyway it does seem right that the denominator should be 'task-attempts' rather than TIPs since the numerator is task-attempts too... Owen pointed this too.

The original patch needs to incorporate Devaraj's comment and I also need to check if running{Map|Reduce}Tasks variables need to fiddled with in JobInProgress.completedTask. There is a if-check where we check if the TIP is already complete there, if so we don't manipulate the aforementioned variables... I'll double check., Ok, this is Amar's original patch after incorporating comments from Devaraj, poring with a lens through findNewTask, obtainNew{Map|Reduce}Task and finally completedTask to ensure it works; along with some helpful comments.

I've also reproduced the original problem (-ve values for running{Map|reduce}Tasks) via some complicated shenanigans and also checked that this patch fixes them. 

Appreciate feedback while I'm running/monitoring some large-scale benchmarks on with this patch., Wondering about the last comment in the patch. After going over the JT and JIP code multiple times we found that the code
{code}
// Sanity check: is the TIP already complete? 
    if (tip.isComplete()) {
      // Mark this task as KILLED
      tip.alreadyCompletedTask(taskid);

      // Let the JobTracker cleanup this taskid if the job isn't running
      if (this.status.getRunState() != JobStatus.RUNNING) {
        jobtracker.markCompletedTaskAttempt(status.getTaskTracker(), taskid);
      }
      return false;
    } 
{code}
will never be executed since {{completedTask()}} is called once for a TIP and that too for the first _SUCCEEDED (COMMIT_PENDING -> SUCCEEDED)_  task and hence {{tip.isComplete()}} will always be false in this case, no?, Looks like there was a timing issue which was thankfully solved by HADOOP-2726. *smile*, +1, Submitting this patch to Hudson with the hopes that Arun is good with it., +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12374231/HADOOP-2639_1_20080128.patch
against trunk revision 615723.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1696/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1696/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1696/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1696/console

This message is automatically generated., I just committed this. Thanks, Amar and Arun!, Integrated in Hadoop-trunk #384 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/384/])]