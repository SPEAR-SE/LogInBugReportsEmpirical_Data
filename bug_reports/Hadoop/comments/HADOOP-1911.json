[chooseDataNode() has bug that is triggered when all the replicas of a file are bad. The value of "failures" in DFSClient.chooseDataNode is always zero. When there are no more good nodes, bestNode() generates an exception that is caught inside chooseDataNode. "failures" is still zero; it clears the deadnodes, refetches block locations and starts all over again. Hence the infinite loop.
, This bug still happens after 0.14 upgrade.

If this file is part of distcp, the job won't finish., I just hit this on hadoop 0.15.3...

, There is a very simple "fix" to this, i.e. make the "failures" count an instance var on DFSInputStream rather than a local variable in chooseDataNode. This would make the semantics of MAX_BLOCK_ACQUIRE_FAILURES to be a cap on the number of total block acquisition failures for the life of the stream, which is not exactly correct, but it is a fix we could easily get into 0.17. It will yield false negatives for a particularly problematic stream, but for applications like distcp it should be sufficient.

After consulting with Dhruba, the longer-term fix will track failures not using a list of "deadnodes", but rather a map of blocks to a list of deadnodes and- to preserve the retry semantics- a map of blocks to full acquisition failures. Right now, a datanode that fails to serve a block is blacklisted on the stream until there are no replicas available for some block, when the list is cleared. The false negatives this yields require the existing, problematic retry semantics. After confirming this approach with Koji, I'll file a JIRA for the more correct fix and submit the sufficient one for 0.17, I ran the Hudson validation on my dev box and- excluding the absence of unit tests- everything passes. If someone can +1 this then I can commit it and file a separate JIRA for a unit test., This code change looks good. , I just committed this., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12379436/1911-0.patch
against trunk revision 643282.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests -1.  The patch failed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2170/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2170/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2170/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2170/console

This message is automatically generated., The test that failed is related to HADOOP-3139 (see Nicholas's last comment). The count of threads tracking the creation of lease threads has a race that only showed up in our tests for Java 1.5 on mac and linux on 0.16, but not in Java 1.6 or in trunk. Hudson might be hitting the same condition with trunk. We might consider putting the sleep in the test into trunk, as we did for 0.16., Integrated in Hadoop-trunk #451 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/451/])]