[[~stevel@apache.org] You've discussed reasons for not taking this route before: https://issues.apache.org/jira/browse/HADOOP-13826?focusedCommentId=15705101&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15705101. I'm wanting to investigate that a little further since I did see it have a significant impact on a Spark job. Regarding your comment there, do we not get much of the benefit of having everything == block size by ensuring that part sizes are at least multiples of that, so boundaries still line up nicely?, Just to add to the numbers above, 256 MB / 512 MB seemed to be the right spot when I was computing in us-west-2  and storing in us-west-1. If I do both in us-west-1, it would seem about 100 MB is where multipart uploads become faster for both upload and rename. Would be good to get more data on how that changes - I'll post the script I'm using in case other folks want to try it for their own set ups.

Given that I'm not seeing a huge discrepancy between upload and rename speeds (I was originally considering the 512 MB that was proving the right transition point for multipart renames vs. the 16 MB that is the default in core-default.xml), maybe the right thing to do here is indeed to keep the configuration together, but consider bumping the value in core-default.xml up (I definitely haven't seen performance fluctuations in S3 over time quite that large, so I'm reluctant to write it off as that). </thinking out loud>, always risky changing defaults, but as the fast output stuff hasn't shipped yet, that's something which could be played with., FYI, I was mistaken about the current defaults - looking at the wrong repo / branch. The default size is currently 100 MB and the threshold is 2 GB. I've done some more testing (I've covered all the US regions), and as long as I'm using a bucket in that region, I'm consistently seeing that around 128 MB or so is where multi-part uploads start being faster (even though that actual raw throughput can vary significantly). I also compared the time to upload 1GB using AWS CLI (24.8s), hadoop fs -cp with these settings (27.7s), and hadoop fs -cp with the current defaults (53.1s)., Attaching a patch with my proposed defaults, and a script I used to gather data (that assumes you've set BUCKET to the bucket to use and HADOOP to the path to the Hadoop executable to run) in case anyone wants to verify., 128MB seems a reasonable increase. But could the patched values be of the form 128M, rather than the multiplied out number. That way it's easier for people reading it to see what the actual number means, Attaching a patch ising the M suffix where possible. I thought it would be cool to tweak things so the Constants.java values could also be in that format and they could be consistent everywhere, but that requires changing a bunch of functions used elsewhere to accept Strings. Probably not worth it., {quote}128MB seems a reasonable increase{quote}

Just to be clear, it's a decrease. I was mistaken about what the previous defaults were in trunk. But the current value is also significantly sub-optimal (at least in all the US regions I tested, despite significantly varying raw performance between them)., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  9m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 13s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 18s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 10m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 10m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 43s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 26s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 34s{color} | {color:green} hadoop-aws in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 37s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 77m  9s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HADOOP-13868 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12842566/HADOOP-13868.002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  xml  findbugs  checkstyle  |
| uname | Linux dfd8afad53d8 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 80b8023 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11236/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-tools/hadoop-aws U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11236/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Just pinging on this - I'd like to resolve it soon.]