[I believe that this is triggered by much faster block replication scheduling introduced by HADOOP-2606. Currently when the namenode choosing a replication target, it only looks at the remaining disk space provided by a datanode through the previous heartbeat but do not look at the space that will be taken by the blocks that have been scheduled to replicate to this datanode. If the previous heartbeat says it has space for 5 blocks but the namenode can schedule as many as 50 blocks to this datanode, the overassigned 45 blocks will fail with the DiskOutOfSpaceException. It looks that the namenode should look at the scheduled blocks as well when choosing a replication target., Yes. Considering load scheduled also avoids related problems like HADOOP-3633 and HADOOP-3685. , Proposal that Hairong and I discussed : (this seems safe enough for 0.17 and 0.18) :

* Each datanode maintains a counter {{approxBlocksSheduled}}.
** incremented each time a block is scheduled to a datanode
** decremented when datanode receives 'block received' message from a datanode.
** No list of block ids is maintained.
** Not every block scheduled will eventually receive a 'block received' message. it will be corrected over time, as described later below.

* disk space left on datanode will be ({{freespace_reported_in_heartbeat - (approxBlocksScheduled+prevApproxBlockScheduled)*defaultBlockSize}})

* 'approx' in the name of the variable is deliberate since it is not expected to be very accurate. It will be handled like this :
** another variable 'prevApproxBlocksScheduled' is maintained.
** Every 5 minutes or so, value of 'prev' will be ignored. 'prev' will be set to current value and current will be set to zero. 
** So if there are some blocks that are not reported back by the datanode, they will eventually get adjusted (usually 10 min; bit longer if datanode is continuously receiving blocks).
** Its not an error if NameNode receives 'block received' message and this counter is zero.

* This count will also be useful for throttling number of blocks scheduled for replication.. (may be the limit could be something large like 50 or 100).
, patch for branch-0.17 is attached. It seems to work well., Updated patch adds a test case., Raghu, could you please upload a patch for the trunk?, Patch for trunk is attached., This is a nice idea, but I am not happy that 
- it is rather complex, 
- it is very approximate,
- it is not localized in one place of the code, and therefore hard to maintain.

One disadvantage of the approach is that the name-node forgets about scheduled replication after a while (10 min?).
Which means that if you do a lot of slow writes or replications then you can schedule a lot for one node then forget about itand then schedule the same amount again.

# I think you don't need the approximation , and hence can remove prevApproxBlockScheduled, because we know exactly when to increment and when to decrement the scheduling counters.
#- increment when block is scheduled for replication and when a new block is allocated, this can be in one common place at chooseTargets().
#- decrement when blockReceived() and when the block moves from pendingReplications back to neededReplications. Need to find one common place for the call.
# May be we can solve this using simpler things. Like
#- slow down the replication scheduler. 
#- use bigger values for dfs.datanode.du.pct or dfs.datanode.du.reserved., 
> I think you don't need the approximation ,

I don't see how it can possibly be accurate with out having lot more code and more memory to maintain more state in NameNode. For. e.g. say NameNode returns 3 datanodes to a client to write to, and then client immedicately dies. If there are no such errors, then it is accurate. 

bq. increment when block is scheduled for replication and when a new block is allocated, this can be in one common place at chooseTargets().

chooseTarget() is not always called to schedule writes. And blocks allocated may not be used by the caller of chooseTargets(). It calls in two places instead of one.. as an alternative, we could have a common method that is invoked whenever NN asks a DN or a client to write to a DN.

> May be we can solve this using simpler things. Like
May be. Though I don't see why NameNode can't do this itself.. like this patch.

> slow down the replication scheduler.
I think slowing down activities is pretty defensive. Also replication is not the only cause: should we slow down client writes too?

I don't mean to say this is the best solution. Is there a better solution that is comparably simple and safe for 0.17 and 0.18?
, Advantages of the current patch :

* fixes a real problem observed by users and increases robustness of DFS.
* is certainly an improvement over what we have.
* has no regressions.
* does not slow down any NameNode activity.
* in my opinion does not increase or decrease the complexity or change the nature of the big beast "FSNamesystem".
* once it works well, the counter can be used for other scheduling activities.
* I don't think "approx" in the name should distract much.. it is as accurate as it can be.. and we deal with small departures from accuracy in case of errors. It is only guilty of living with uncertainty :).

Of course we can change the patch, for e.g. we can increase the "roll interval" from 5 minutes., Attaching patch for 0.17.2. It has two cosmetic changes compared to earlier 0.17 patch. Diff between the patches :

{noformat}
-  private final int BLOCKS_SCHEDULED_Roll_INTERVAL_MSEC = 300 * 1000; // 5 min
+  private static final int BLOCKS_SCHEDULED_ROLL_INTERVAL = 300 * 1000; // 5 min
...
-        BLOCKS_SCHEDULED_Roll_INTERVAL_MSEC) {
+        BLOCKS_SCHEDULED_ROLL_INTERVAL) {
...
{noformat}

Konstantin, 

for 0.18 and trunk, I am looking into if the counter could be decremented when a block moves from {{neededReplications}} to {{pendingReplications}} and when the lease expires.. which makes the counter more accurate and we could increase the "roll interval" to something like 1 hour.

If you think patch for branch 0.17 is good enough (i.e. it does not make anything worse), could you +1 it for 0.17? 0.17.2 release is waiting only on this patch.

, 'ant test-patch' and 'ant test-core' succeeded for 0.17.
, committed this patch to 0.17. I will temporarily remove 0.17 from the 'fix versions' so that it does not appear as an unresolved blocker for 0.17.
, Attached patches for 0.18 and trunk.

Increased the 'roll time' to 10 minutes from 5 minutes.

As Konstantin suggested, we could improve the count by updating the count when a block moves from 'pendingReplications' to 'neededReplications' and when a file is abandoned. I am not sure yet sure of a clean patch for that. For now we could file another jira to improve this. I think these improvements will make the counter more dependable and might be used in more scheduling decisions.

I hope the attached patches fix this jira and improve current block allocation. , +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12386028/HADOOP-3707-trunk.patch
  against trunk revision 677127.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2867/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2867/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2867/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2867/console

This message is automatically generated., I just committed this., Integrated in Hadoop-trunk #581 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/581/])]