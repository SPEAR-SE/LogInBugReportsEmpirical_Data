[In order to ensure the minimal change of the original code and keep the SocketTimeoutException, add additional IOException catch statement to catch other IOException except the SocketTimeoutException., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12575838/HADOOP-9440.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2373//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2373//console

This message is automatically generated., Any comments, Suresh?, Why do we get a protobuf exception in the first place? We shouldn't be modifying the test to capture an IOException as we don't expect that to happen. Please correct my understanding if am wrong., Harsh, you are right that we don't expect this exception. But it is really thrown rather than SocketTimeoutException. If we use hadoop2.4.0a, it runs well. So can we think it as a protobuf2.5.0+ problem?, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12575838/HADOOP-9440.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2548//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2548//console

This message is automatically generated., I've tried to compile hadoop-commons' trunk with protobuf 2.5.0 and succeeded to pass TestIPC though I needed to run "mvn clean" command explicitly under hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common directory.  Therefore, the test failure seems to be caused the mixture of old jar file compiled with protobuf 2.4.x and new jar file compiled with protobuf 2.5.0. If this assumption is correct, this ticket should be marked as "Not a problem". Did you "mvn clean" before recompile?, And, let me know if I am wrong. Thanks!, Thanks Tsuyoshi OZAWA for your comment. I agree with what you said. So close it as "Not a problem"., [~ozawa] I have the same issue even after I run "mvn clean".]