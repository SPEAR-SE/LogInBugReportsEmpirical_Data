[Konstantin, builds don't stay around forever on Hudson.  I suggest to copy the relevant pieces into a text file and attach it to this issue., Here is the log of the failed test., After examining the log, it looks that we got the following scenario:
1. blk_167544198419718831 was replicated to datanode 1, datanode 2, and datanode 3;
2. Datanode 1 lost contact with the namenode and datanode 2 is scheduled to be decomissioned.
3. Datanode 1 reregistered with the namenode; but the block report came in before its network location was resolved; so its block report was dropped.
4. Because the namenode does not know that datanode 1 has the blk_167544198419718831, it schedules to replicate the block to datanode 1 and datanode 4.
5. The replication of the block failed because it already has the block.
6. No additional block report was received until the end of the log. So the block replication kept on failing. , Looks that the problem is caused by the flag indicating if a block report is processed not seting to be false when a datanode re-registers. Therefore, the namenode does not ask for a block report when the datanode's network location is resolved., I have run TestDecommission with the patch for 50 times in a row on my linux box without seeing any failure., I ended up fixing more problems associated with sending block reports.
1. Datanode does not send the inital block report until requested by the namenode;
2. namenode asks a datanode to send a block report when the datanode's network location is resolved as a reply to a heartbeat;
3. Add a static field R of type Random to DataNode and replace all the use of new Random() with R., This patch makes sure that the initial block report is sent once and only once., The code is fine.
+1, -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12379315/blockReport2.patch
against trunk revision 643282.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2151/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2151/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2151/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2151/console

This message is automatically generated., TestDecommision triggers this bug once a while. So no unit test is provided., I just committed the patch., Integrated in Hadoop-trunk #451 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/451/])]