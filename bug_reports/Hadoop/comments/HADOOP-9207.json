[See HADOOP-8924 for some earlier discussion on this topic.  We could either calculate a single MD5 checksum across the whole project and use it for both HDFS and YARN, or calculate separate checksums per module, but with inclusion of the Common code in each one.
, Quoting an earlier comment from HADOOP-8924:

{quote}
The most important use of the checksum is to enforce the constraint that all the servers talking to each other be running code compiled from the same source tree.
{quote}

Considering this, I think the best solution is to calculate a single global checksum for the whole project and use the same value across all modules.
, A possible way (a bit peculiar) of doing this would be to have the version plugin defined only in hadoop-common and have as fileset something like:

{code}
            <configuration>
              <source>
                <directory>${basedir}/../..</directory>
                <includes>
                  <include>**/src/main/java/**/*.java</include>
                  <include>**/src/main/proto/**/*.proto</include>
                </includes>
              </source>
            </configuration>
{code}

And we should get rid of the YarnVersionInfo.java class and use the VersionInfo.java instead., Is this actually still valid?  , This problem still exists, but it's minor in my opinion.]