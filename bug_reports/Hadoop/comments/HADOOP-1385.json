[This patch adds a new function MD5Hash.quarterDigest() that gives the user the first 4 bytes as a int. It replaces the current function with a call to quarterDigest. It adds test cases for the old halfDigest, the new quarterDigest, and a test of the collision., +1

http://issues.apache.org/jira/secure/attachment/12357585/1385.patch applied and successfully tested against trunk revision r539135.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/156/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/156/console, This looks good except I was confused by the collision test as there doesn't seem to be a collision. Shouldn't closeHash1 and closeHash2 share the same first four bytes but differ in (some of) the others? Then they would not be equal (according to the equals method) but the hash codes would be equal.

Also, this looks like a 0.14.0 fix since the existing code isn't broken, just inefficient in some cases., 
> Shouldn't closeHash1 and closeHash2 share the same first four bytes but differ in (some of) the others? Then > they would not be equal (according to the equals method) but the hash codes would be equal. 

Maybe I should remove that test. I basically wanted to check the non-existence of the current bug. In the new hash function the hash codes are different. In the old code the hash codes are the same.

> Also, this looks like a 0.14.0 fix since the existing code isn't broken, just inefficient in some cases.

It is really badly broken if you have more than 256 reduces. You'll basically have extremely heavy loading on 256 of the reduces (depending on the precise number of reduces). If you run with 2000 reduces, you'll have the majority of your workload done by a 1/8 of your cluster. Certainly for my cluster, I'll fix this for 13. If the majority of people feel like this can push to 14, that is fine., I would keep that test (it's good to test for regressions!), but also add the one I describe.

> It is really badly broken if you have more than 256 reduces.

OK - I'm convinced., I think it wouldn't be a good test case to make sure the collision happens if the fifth bytes are different. If at some point we xor all of the words together, that would be ok and should cause regression failures., Do we need a for loop and an extra function call for hashCode()? Why not just return 
{{((digest[0] & 0xff)  << 24)  | ((digest[1] & 0xff) << 16) | ((digest[2] & 0xff) << 8) | (digest[3] & 0xff)}}, The extra function was to give an explicit name to what I was doing for hash. Clearly there are lots of things that you could use as a hash code, so if someone just wants the first 4 bytes they can call the new function rather than hash code.

The loop is easier to read and less likely to get wrong and a decent optimizer could unroll the loop for you. Granted, I expect in Java it is not done, but I don't think this function to be a performance bottleneck., The fact that you have good tests for quarterDigest() (which hashCode uses) and halfDigest() answers my concern, I think, so +1 for the current patch., I just committed this.  Thanks, Owen!, Integrated in Hadoop-Nightly #97 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/97/)]