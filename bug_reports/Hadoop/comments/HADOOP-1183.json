[Retrials of map output fetches might overwrite the new events got from the JT for the same maps. Lets assume that a tasktracker is lost while we are in the process of fetching map outputs from it. There is a timing issue between when a mapoutput fetch completes with a failure, and when a new event for the same map task is obtained. If the latter is got before the former, and if the fetch corresponding to the new event is not scheduled before the former, then it will lead to loss of this new event (overwritten with the retrial for the old failed fetch).

The attached patch should handle this issue - here the FAILED events are explicitly handled. Please review it (while i am testing it on a big cluster)., This patch does a slightly better handling of failed maps. It records only those failed maps whose outputs we haven't fetched yet., Indentation changes (as per Owen's comments. Thanks Owen)., I'm uneasy about this patch. The underlying code is very complex, the patch is adding substantial complexity, and it isn't clear to me that this is the right direction. I think we should post-pone this fix and likely redesign the fetcher in 0.13.

One possible approach to simplifying this section of code would be to make an array of states for each of the map outputs (INITIAL, LOCATED, FETCHING, DONE, FAILED) and process the map outputs using a DFA. Another structure that might make sense is an array of the best MapOutputLocation for each map.

Thoughts?, I agree with Owen that the fetcher needs some redesigning on those lines. But in any case, should anyone see reduce(s) hanging (with the fetcher continuously trying to fetch output from a failed/lost map), this patch should be applied., The original bug is a duplicate of HADOOP-1270. The comment on redesigning shuffle is now a new issue - HADOOP-1337.]