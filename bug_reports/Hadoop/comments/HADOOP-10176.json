[This one is going to be trouble. There's no expectation that "/" can be in a path, or that the empty string can be a path, so "//" isn't a valid filename or path to a subdir under root.

We could be ruthless and say "don't" -you can't create paths like this from Hadoop itself, after all.
Even so, some recognition of duplicates is probably useful.

How about on a directory list, if an entry of the specific name is found, its somehow merged in or flagged as a problem?, That sounds like a reasonable approach. Is there a precedent in hadoop filesystems for flagging a file as problematic?  , well, files with a .crc extension are invisible in the nativeFS (and HDFS?), while HDFS 2.4 will hide subdirs with the .snapshot extension, which is going to force in a migration plan. Also, I doubt that you can have files called COM1 or LPT1 on the windows native FS due to [historical reasons|http://msdn.microsoft.com/en-us/library/aa365247(VS.85).aspx]. 

The S3N filesystem uses {{"_$folder$"}} as its folder separator, so it builds up a very different model of a directory tree -and accordingly, won't see a lot of paths in an s3 bucket as directories, Hiding the mis-named objects seems likely to be confusing for the user (when ls shows /foo in the list, is it referring to the object "foo" or "/foo"?).  

We could mangle the name of the invalid object somehow - perhaps append something like .!!! or .@@@.  The mangling algorithm would need to be reversible, otherwise there's a whole can of worms when a user requests an object with a mangled name. ]