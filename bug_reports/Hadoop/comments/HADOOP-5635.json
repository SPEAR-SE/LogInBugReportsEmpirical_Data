[This patch removes the check for HDFS and lets any file system through. The onus is on the user to ensure that the file system is globally available on all nodes., Wouldn't it be generally better if Hadoop was configured with a list of shared file systems. Then, when the administrator permitted, users could use shared NFS filesystems as sources and targets for map reduce jobs. E.g., in our setup, /local/ and /users/ are shared to all nodes. If we wanted to do a quick map reduce test on stuff storied in /local/ we would have to copy to the DFS, when it would be OK to run as is.

{noformat}
<name>fs.shared.filesystems</name>
<value>hdfs://,file://users/,file://local/ </value>
{noformat}, It sounds like what you want is a new featue whereas this patch is just to fix a bug.

Currently the behavior is not right. If a user specifies a non-HDFS URI for distributed cache then the job will fail because the tasks look for the file in HDFS. This patch fixes that for cases when the user specifies a URI to another distributed file system. With the patch, if a user specifies KFS or S3N (and the file system is properly configured) then the job will succeed. The behavior for specifying a URI not accessible on every machine remains unchanged: the job will fail as tasks are unable to reach the URI.

I think a feature for administrators to restrict distributed cache access to certain file systems should be a new Jira., Andrew,

This looks like a good change to me. Have you thought how to write a unit test for this?

Also, the documentation in DistributedCache should be updated to remove HDFS assumptions. , I've updated the patch. It includes a unit test, fixes the error message in StreamJob, and updates some Javadocs that I had previously missed., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408837/HADOOP-5635.patch
  against trunk revision 778388.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/403/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/403/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/403/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/403/console

This message is automatically generated., The failing tests seem unrelated., I've just committed this. Thanks Andrew!, Integrated in Hadoop-trunk #863 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/863/])
    ]