[While there's certainly scope for improving resilience, the "a file is as long as it says it is" is called out as [[an invariant of any hadoop-compatible filesystem|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/fsdatainputstream.md]]

bq.  the size of the data stream equals the size of the file as returned by FileSystem.getFileStatus(Path p)

Changing that breaks a lot; there's assumptions that things like {{seek(length-1}}; read()}} is valid for all lengths > 0. thinks like splitting files are all based on the assumption there's a 1:1 mapping between offsets and data.

Which is why the HDFS-at-rest encryption reports a different file length to the caller than its actual length: clients see how many bytes they can read, not how many there are.

what is doing the encyption? This is amazon's own S3 encryption?, Yes, this is in Amazon S3.  We are using the AWS sdk to do client side encryption.

I've fixed the issue locally by forcing the 'x-amz-meta-x-amz-unencrypted-content-length' flag on file write and then checking to see if it's there.

The fact that the amazon sdk doesn't set this metadata seems to be a bug in the aws sdk as it should have set it for me.

Our custom InputStream extends PositionedReadable and Seekable and I've overloaded all relevant functions to return proper codes up until EOF.  So {{seek(length-1); reead()}} will always work.
Since the stream is block buffered we make sure the stream is reset properly on reverse., moved to hadoop common, tagged as an fs/s3. Corby, there's a new "s3a FS client that uses the AWS APIs directly" -could you try that to see if it behaves better?, Took a look at s3a.
Not going to work for us short term because it was put into 2.6 which won't be available for AWS EMR for a while.

The other major problem is that s3a doesn't support client side encryption (which is a MUST for us).  So it's a dead end., Corby: if you are using Amazon's own S3 client then know that it is not the Apache one; at some point they forked, Update: there's apparently a header which declares the unencrypted length: {{x-amz-unencrypted-content-length}} , see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/package-summary.html

Trouble is, this doesn't show up in listing the parent path, which means partitioning is broken. It may be something which could be added to getFileStatus(), that is: grab the header and use that for the declared length. The difference between published and actual values would still break the spec, but maybe, just maybe, more things would work.]