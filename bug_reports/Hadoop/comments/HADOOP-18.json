[Wish there was an "edit" option in JIRA.  Obviously it was within the SegmentReader -- though I don't believe it does anything special to try to read the data file off disk.

I do not and have not seen this exception with a single local directory configuration., I have sucessfully used mapred.local.dir with multiple values on many occasions.

Can you please try to distill this to an easy to reproduce test-case?  Thanks., I enabled multiple local directories on 50% of my machines. This error, which does not appear otherwise (we processed a few hundred million pages without error), happens across all machines BUT always on part-##'s which were created on the split directory boxes.

Secondly, the error only occurs during the segread command, org.apache.nutch.segment.SegmentReader, while retrieving the data out of common storage (NAS in our case).

060203 222553 task_m_4x4zan 0.7223333% /opt/sitesell/sbider_data/nutch/segments/segmentset-2006-02-02254/20060202181143-3/content/part-00001/data:0+21025408
060203 222553 task_m_4x4zan  Problem reading checksum file: java.io.EOFException. Ignoring.
060203 222553 task_m_4x4zan  Error running child
060203 222553 task_m_4x4zan java.lang.ArrayIndexOutOfBoundsException
060203 222553 task_m_4x4zan     at java.util.zip.CRC32.update(CRC32.java:43)
060203 222553 task_m_4x4zan     at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)
060203 222553 task_m_4x4zan     at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)
060203 222553 task_m_4x4zan     at java.io.BufferedInputStream.read1(BufferedInputStream.java:254)
060203 222553 task_m_4x4zan     at java.io.BufferedInputStream.read(BufferedInputStream.java:313)
060203 222553 task_m_4x4zan     at java.io.DataInputStream.readFully(DataInputStream.java:176)
060203 222553 task_m_4x4zan     at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)
060203 222553 task_m_4x4zan     at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)
060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)
060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)
060203 222553 task_m_4x4zan     at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)
060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)
060203 222553 task_m_4x4zan     at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)
060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)
060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)
060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)
060203 222553 task_m_4x4zan     at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:603)

060203 222557 task_m_4x4zan done; removing files., Finally figured it out. One temp directory was filling up (different sizes) and the fetch was aborting BUT I didn't see this in the logs for the longest time.

The patch in NUTCH-143 helped track down the issue because it caused the error to be noticed be the scripts driving the code in a location close to where the error took place rather than several steps (possibly many hours) later..

Please close this bug., Resoving at Rod's request.]