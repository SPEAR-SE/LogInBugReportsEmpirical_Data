[Currently, the strategy used to allocate a file to a block is exploring the whole FSDir tree. So it calls a lot of addBlock() methods, thus the StackOverflowError is thrown if there is many subdirectories...

I will provide a patch ASAP.
, This patch does not use intensive method calls, but a while loop instead.

Anyway, there is still some places where a loop would be better than "in-depth" method calls in the FSDir class... 
, This patch is not perfect, but it should solve the issue., 
Do you know how many blocks you have on this node?
, +1, because http://issues.apache.org/jira/secure/attachment/12351902/patch-StackOverflowError-HADOOP-1035 applied and successfully tested against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/511100. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch, We have about 21000 blocks stored on each datanodes., Also we use BEA JRockit jvm on a 64 bit system., Raghu, do you intend to review this patch?  Thanks., 
yes,  I will take a closer look at the patch. Currently DataNode's addBlock() recurses with depth equal to number of blocks.. which is bad.
, 
This is probably good time to get rid of siblings[] array in FSDataset. I will first try to remove siblings variable. I think it will simplify the this patch also.
, Phillippe,

I your patch, we ever call createChildren() only for children[0], right? This results in tree growing only in depth and not in breadth. I think getting rid of sibling[] variable will make the code more intuitive.

, > I your patch, we ever call createChildren() only for children[0], right? This results in tree growing 
> only in depth and not in breadth. I think getting rid of sibling[] variable will make the code more intuitive. 

btw, there is a similar problem in current code : it creates children only in the last child (subdir63).
, Yes, you're right. In fact, the tree is growing the same way as in the 'old' addBlock method.

+1 for removing siblings., 
1035-1.patch is attached. Here addBlock's recursion is limited to depth of the directory tree. Also we now create children under all the sub directories and this will keep tree depth in check.

Each directory remembers last child that had room for a new block. This way most of the calls to addBlock would only traverses one line in the tree.

clearPath() also had equally had recursion. This is fixed too.

Phillippe, please take look at the patch and use it if you feel confident about it.
, 
will make patch available once this is reviewed.
, Your patch is good, i've tested it and it's working fine :)

, Attached 2.patch. This includes improvement to clearPath().

clearPath() is called for each block deleted and it does File.compareTo() and every directory. In this patch, we first guess the likely child from "subdirXY". The guess would be correct unless layout of the directories changes in future.



, +1 Code reviewed.

I like the optmization in clearPath. Also, since we are using ext3 filesystem, and it has support for large directories, we could possibly think about increasing the number of files per subdirectory (currently 64)., Thanks Dhruba.

attached 3.patch. This removes excessive whitespace on one of the lines in 2.patch.
, -1, because 3 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12352492/HADOOP-1035-3.patch against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/513993. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch, -1, because 3 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12352492/HADOOP-1035-3.patch against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/513993. Please note that this message is automatically generated and may represent a problem with the automation system and not the patch. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch, I just committed this.  Thanks, Raghu!]