[A fat client makes it harder to provide back compatibility and makes implementations in multiple languages harder to maintain. We should avoid fattening the client whenever possible. I vote for proposal 1.

, +1 on proposal 1.

Is getDistance() the only reason that makes these two methods heavyweight?, +1 for proposal 1. We should have zero string operations in getDistance().. only pointer comparisions.

, 
One more thing, I think we should calculate distance once for each replica and use the value in sort. This would cut calls to getDistance by half in the case of 3 replicas and more  in the case of more replicas. To facilitate this we could have an int in DatanodeDesscriptor that is set before the sort.

, Do we need to sort datanodes by distance? Why not just do a linear scan for the on node and on rack instances, put them at the front of the pipeline and leave the rest in random order?

Another option would be to do a linear scan and bucket nodes by distance, sorting just seems unnecessary here. , Sameer, your suggestion works when the distances between any two racks are the same, but I guess it is good for now., +1 on Raghu's suggestion. The # of calls on getDistance could be reduced in sortedByDistance.

Another improvement to chooseTarget is to remove a data node from clusterMap when the data node is being decommissioned. So a decomissioning node will not be chosen as a replication target. This idea also came from Raghu. See his comment to HADOOP-1070., This patch optimizes the performance  of getDistance by changing clusterMap data structures. I will submit a seperate patch to incorprate other suggestions.,          if( node1 == node2 || node1.equals(node2)) {
             return 0;
         }

I think we should remove equals() as well. Most real clusters will not have two data nodes on the same node.. even if someone did, usually transfer between two nodes on the same switch is just as fast as transfer from one port to another port on the same node.
, Raghu, I would rather keep equals. We do have test clusters that run two datanodes on the same machine. Although now we do data exchange through ports on the same machine, I think we could optimize it by a disk copy later on. So it is important to make a difference., This patch optimizes getDistance, contains, and isOnSameRack., +1. Code looks good. I am running it on my cluster., 
I still think we don't need to optimize uncommon case at the expense of common case. I doubt if there is any real benefit to copying locally compared transferring from another node on the same switch. Note that the data still needs to flow through DFSClient, for CRC etc.

, > still think we don't need to optimize uncommon case at the expense of common case.

You convinced me! Here is the new patch that reflects your suggestion.

> I doubt if there is any real benefit to copying locally compared transferring from another node on the same switch. Note that the data still needs to flow through DFSClient, for CRC etc. 

This could be done if we change the client/datanode protocol. We could have a special opCode indicating that the block content is in the local disk along with the source path., Will resubmit after HADOOP-702 is committed.
, The patch applies with the latest trunk., +1, because http://issues.apache.org/jira/secure/attachment/12353899/getDistance2.patch applied and successfully tested against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/525290. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch, I've just committed this. Thanks Hairong!]