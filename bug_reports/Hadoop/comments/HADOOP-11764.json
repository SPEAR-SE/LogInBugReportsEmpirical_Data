[As per http://grepcode.com/file/repo1.maven.org/maven2/org.fusesource.leveldbjni/leveldbjni-all/1.8/org/fusesource/hawtjni/runtime/Library.java if we define the property library.${name}.path we can avoid it using the temporary directory. 
{noformat}
The file extraction is attempted until it succeeds in the following directories.
1. The directory pointed to by the "library.${name}.path" System property (if set)
2. a temporary directory (uses the "java.io.tmpdir" System property)
{noformat}

So we can fix this by setting -Dlibrary.leveldbjni.path=$(pwd) in the nodemanager options., One way to add the variable. [~aw] appreciate your review for the best way to do this., We really can't read this in via the core-site.xml file?  I could have sworn we populated the system properties with the content of the xml files on startup.  It's really less than ideal to set this up on the command line as it is already quite long and there is a risk that we'll overflow the buffer, preventing things from running.

That issue aside:

a) We need to avoid using daemon-specific and/or project-specific environment variables.  This is the #1 reason why the branch-2 shell code is just utter chaos.  This should be HADOOP_ something.

b) Additionally, this should be set for everything, not just the nodemanager in order to keep this consistent if/when other stuff uses this functionality. The add_param should probably happen in finalize_hadoop_opts

c) The default should be set with an actual value in hadoop_basic_init.  $(pwd) is a *terrible* default value for this, because there is no guarantee what the pwd actually is.

d) There needs to be a stanza added to hadoop-env.sh that actually explains what your new environment variable does, how to set it, etc.

All/most of this is covered on http://wiki.apache.org/hadoop/UnixShellScriptProgrammingGuide .  I'm now going to go check to make sure I didn't miss anything on that page and add it if I did. :), Addressed feedback. Thanks [~aw] for the very specific feedback, patch looks good, but have we verified we can't set this in core-site.xml yet?, I am not sure which value in core-site would fix this after going through the core-default documentation. The only way i can think about is using the appropriate environment variable for eg setting the YARN_NODEMANAGER_OPTS value for Nodemanager , bq. I am not sure which value in core-site would fix this after going through the core-default documentation.

I'm afraid we can't set it in config file, because config file is read by the daemon, but we need to start the daemon with this opt.

And IMHO, {{-Dlibrary.leveldbjni.path}} alone cannot fix the problem. If the temporal native lib is redirected to another dir, we also needs to add that dir to {{JAVA_LIBRARY_PATH}}. Otherwise, we may still end up with native lib not found., Update the jira a bit, as the issue doesn't limit to NM only. All components that are using leveldb for storage will be affected., bq If the temporal native lib is redirected to another dir, we also needs to add that dir to JAVA_LIBRARY_PATH. Otherwise, we may still end up with native lib not found.
Hi Zhijie, I am guessing that this is not something needs to be done in this jira which tries to address the /tmp noexec problem, right?. , bq. I'm afraid we can't set it in config file, because config file is read by the daemon, but we need to start the daemon with this opt.

It just needs to be set as a system property prior to invoking the class.  That's all putting it on the command line does, so why can't we do this in Configuration?

bq.  If the temporal native lib is redirected to another dir, we also needs to add that dir to JAVA_LIBRARY_PATH.

This is sounding more and more like a complete mess, with no real thought as to how admins are supposed to deal with it., bq. That's all putting it on the command line does, so why can't we do this in Configuration?
[~aw] The solution demands writing to the environment and the proposed solution does that. Isnt that better than adding code in each daemon to read from the configuration and set it to the environment? 

bq. If the temporal native lib is redirected to another dir, we also needs to add that dir to JAVA_LIBRARY_PATH.
[~zjshen] This is an unrelated problem to the one this jira is solving. The current jira is not solving native lib not found. Rather the jar cannot be extracted and executed because of noexec permissions., bq. It just needs to be set as a system property prior to invoking the class. That's all putting it on the command line does, so why can't we do this in Configuration?
bq. Isnt that better than adding code in each daemon to read from the configuration and set it to the environment?

That sounds true. However,  I agree with [~adhoot] that it's better to setup env vars instead of modifying each daemon to set system property programatically.

bq.  This is an unrelated problem to the one this jira is solving. The current jira is not solving native lib not found. Rather the jar cannot be extracted and executed because of noexec permissions.

The reason is that just moving tmp file generated for leveldbjni out of /tmp is not going to fix the problem. Instead of {{Operation not permitted}}, we will encounter {{not found}} problem, and finally the native code is still not linked if {{JAVA_LIBRARY_PATH}} doesn't include {{library.leveldbjni.path}} (by default it doesn't library.leveldbjni.path is some customized path).

I don't think it's a complete solution if now we have BUG A, and after we fix it, we have BUG B. Therefore, my proposal is if {{HADOOP_LEVELDBJNI_DIR}} is specified, it will set to {{library.leveldbjni.path}} as well as be added to {{JAVA_LIBRARY_PATH}}. Thoughts?

bq. This is sounding more and more like a complete mess, 

Would you mind elaborating what is the complete mess? The admin just needs to set HADOOP_LEVELDBJNI_DIR to somewhere else than /tmp if /tmp is mounted as noexec. If HADOOP_LEVELDBJNI_DIR is not set, it will just work as what it is now., bq. However, I agree with Anubhav Dhoot that it's better to setup env vars instead of modifying each daemon to set system property programatically.

... except, you don't have to.  All of the daemons run through the Configuration class (and probably other classes as well).  It could get set then, *if it isn't already*.

bq. Would you mind elaborating what is the complete mess? The admin just needs to set HADOOP_LEVELDBJNI_DIR to somewhere else than /tmp if /tmp is mounted as noexec. If HADOOP_LEVELDBJNI_DIR is not set, it will just work as what it is now.

I'm specifically referring to:

bq. Therefore, my proposal is if HADOOP_LEVELDBJNI_DIR is specified, it will set to library.leveldbjni.path as well as be added to JAVA_LIBRARY_PATH.

So not only do we have an extra -D parameter, -Djava.library.path also has the exact same content which increases the line length even more.  This will almost certainly break some users at some point in time., bq. All of the daemons run through the Configuration class (and probably other classes as well). It could get set then, if it isn't already.

I'm not sure it's a good idea. Configuration is used by a couple of other projects in Hadoop ecosystem. It's likely to be a more risky change and expose something unnecessary to other projects.

bq. So not only do we have an extra -D parameter, -Djava.library.path also has the exact same content which increases the line length even more. This will almost certainly break some users at some point in time.

I'm not sure if I understand your point here, but "increasing line length" doesn't sound like something that will almost certainly break some users at some point in time, unless those some users are very special to be CLI length sensitive. One step back, JAVA_LIBRARY_PATH is defined by Hadoop to let users to customize JNI lib path, and it's not something that brings scaring breakage. My proposal is to relieve users from manually adding leveldb jni lib path into JAVA_LIBRARY_PATH, which they anyway have to do to make leveldb work in Hadoop.
, bq. I'm not sure it's a good idea. Configuration is used by a couple of other projects in Hadoop ecosystem. It's likely to be a more risky change and expose something unnecessary to other projects.

Fine. What about the service code?  There are lots and lots of options here...

But Configuration should probably populate the system properties when it reads in configs though....

bq. I'm not sure if I understand your point here, but "increasing line length" doesn't sound like something that will almost certainly break some users at some point in time, unless those some users are very special to be CLI length sensitive.

You've just described every version of Unix ever. It's called ARG_MAX and it becomes a real problem at extremely long lengths.  It's something we need to be vigilant about to make sure it doesn't grow to big.  With shell profiles, it's going to get even bigger as 3rd parties add more content..., bq.  There are lots and lots of options here...

Yeah, among the lots of options, isn't it the most decent way to fix the env issue in the scope where we work on evn?

bq.  it becomes a real problem at extremely long lengths.

In most cases, we don't need to specify JAVA_LIBRARY_PATH. leveldb works fine with the default path. In the case that is described above and that we need to specify JAVA_LIBRARY_PATH to point to leveldb jni libary dir, the reasonable path length should usually not exceed the arg max length. In addition, AFAIK, ARG_MAX is not a small number (e.g., 2621440). JAVA_LIBRARY_PATH alone is not that easy to exceed it, but it's true that there will be the extreme cases. 

However, the point in my previous comment is that whether JAVA_LIBRARY_PATH is set by users manually or automatically by the script, it has to be set to fix leveldb JNI issue. That said, automatic setting JAVA_LIBRARY_PATH is *NOT* generating longer args than users' manual setting. Your argument sounds more like agains using JAVA_LIBRARY_PATH. If you have concern that JAVA_LIBRARY_PATH will break ARG_MAX, please feel free to start a separate thread to discuss it. This jira is focusing on fixing leveldb jni., I'm starting to think more and more that using leveldb is a HUGE mistake.

a) There's this complete nonsense about requiring all this pre-configuration.

b) What prevents a user from inserting a malicious .so into this shared directory?  Given that we have to default some where like /tmp or even hadoop.tmp.dir, this is a massive security hole that directly impacts the running daemons.

c) HADOOP-11790 means we've effectively broken the build for probably non-linux, non-x86., bq. the native code is still not linked if JAVA_LIBRARY_PATH doesn't include library.leveldbjni.path (by default it doesn't library.leveldbjni.path is some customized path).
[~zjshen] The custom library.leveldbjni.path does not need to be added to JAVA_LIBRARY_PATH.
Copied below by sample test where i created a new directory and set only the HADOOP_LEVELDBJNI_DIR to point to it

mkdir /testleveldb
export HADOOP_LEVELDBJNI_DIR=/testleveldb

/usr/java/default/bin/java -Dproc_nodemanager -Dhadoop.log.dir=/home/adhoot/src/mr2-pseudo-dist/hadoop-3.0.0-SNAPSHOT/logs -Dyarn.log.dir=/home/adhoot/src/mr2-pseudo-dist/hadoop-3.0.0-SNAPSHOT/logs -Dhadoop.log.file=yarn.log -Dyarn.log.file=yarn.log -Dyarn.home.dir= -Dyarn.id.str= -Dhadoop.root.logger=INFO,console -Dyarn.root.logger=INFO,console -Dyarn.policy.file=hadoop-policy.xml -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8001 -Dhadoop.home.dir=/home/adhoot/src/mr2-pseudo-dist/hadoop-3.0.0-SNAPSHOT -Dhadoop.id.str=root -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender -Dlibrary.leveldbjni.path=/testleveldb org.apache.hadoop.yarn.server.nodemanager.NodeManager

[root@anuonebox ah]# dir /testleveldb/
libleveldbjni-64-1-219738139685490287.8, [~adhoot], thanks for sharing the example. Does it mean if "library.leveldbjni.path" is defined, leveldb will programatically not only replace "java.io.tmpdir" with it, but also add it to "java.class.path"? I used to have some experiment: if I change "java.io.tmpdir" only to dir_abc, but not add it to "java.class.path" manually, JNI lib loading is still wrong., I think this just means the code first tries using the custom folder instead of /tmp for extracting and loading the library from the jar as per the code http://grepcode.com/file/repo1.maven.org/maven2/org.fusesource.leveldbjni/leveldbjni-all/1.8/org/fusesource/hawtjni/runtime/Library.java#216
It does not need to change java.io.tmpdir or anything else.
, [~zjshen] does this seem ok? , [~adhoot], sorry for late reply. I tried the patch. It worked fine locally. But would you mind my take one or two more days to see if I can reproduce may aforementioned issue before?, [~vinodkv], this is the JIRA we were discussing at Hadoop Summit.  Please take a look at it.

... and just so it's official, I'm very much a (binding) -1 on pushing this into an environment variable until it can proven otherwise that this can't be handled via *-site.xml., bq. ... until it can proven otherwise that this can't be handled via *-site.xml.
This can also mean *-env.sh.

[~adhoot] / [~zjshen], it seems weird that we have an option just for leveldb. In the past, we used hadoop.tmp.dir for stuff like this. And if needed, we could also use HADOOP_OPTS/ HADOOP_YARN_OPTS etc at each site. Why wouldn't that work?, I used to run into a scenario where I need to change both HADOOP_OPTS and JAVA_LIBRARY_PATH to point the same dir . Having HADOOP_LEVELDBJNI_DIR defined and applied to both evn vars in *-evn.sh is quicker way for op. However, after it, I couldn't reproduce it, and it seems that changing HADOOP_OPTS only is enough fix the issue. Given only one evn var needs to be changed, defining HADOOP_LEVELDBJNI_DIR is no obviously cheaper than customizing HADOOP_OPTS. , bq. In the past, we used hadoop.tmp.dir for stuff like this.
This use case of a tmpr is different than normal use cases of tmp directory as it is executing code from there instead of treating as a directory for data. 

yes we can do it via *-env.sh or HADOOP_OPTS. The current solution was doing it via customizing HADOOP_OPTS in a convenient way for the leveldb setting just like all other examples of "hadoop_add_param HADOOP_OPTS" in hadoop-functions.sh. Would be good to document in the shell scripting guide some conventions on when do we - a) document how to modify HADOOP_OPTS, or b) or add it to *-env or c) create a new option - as essentially they all end up adding a System property at the end of the day.
, bq. This can also mean *-env.sh.

I'm not sure how you got that from the statement "until it can proven otherwise that this can't be handled via *-site.xml."  I specifically mean: push this into the XML files and leave the shell environment alone. As we discussed at Summit\-\-and you tentatively agreed\-\-populating the system property for leveldb from the *-site.xml files could be done at service start time, given that there are generic abstractions for actually starting daemons in Hadoop that large chunks of the system already use.

It's worth pointing out that putting the leveldb settings in a generic shell env will impact end users, and it would have to be done in daemon specific env vars.  So no, one can't modify HADOOP_OPTS here.  You'll break apps that use their own leveldb bits.


, Committed to trunk and branch-2.1, SUCCESS: Integrated in Ambari-trunk-Commit #3187 (See [https://builds.apache.org/job/Ambari-trunk-Commit/3187/])
HADOOP-11764. [ HADOOP-11764] NodeManager should use directory other than tmp for extracting and loading leveldbjni (aonishuk) (aonishuk: http://git-wip-us.apache.org/repos/asf?p=ambari.git&a=commit&h=4c27db62466b08962ee6b9dae44cac2fd22a3c8f)
* ambari-server/src/main/resources/stacks/HDP/2.3/services/HDFS/configuration/hadoop-env.xml
* ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/configuration/hadoop-env.xml
* ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py
* ambari-server/src/main/resources/stacks/HDP/2.2/services/HDFS/configuration/hadoop-env.xml
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/params.py
* ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog211.java
* ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog211Test.java
* ambari-server/src/test/python/stacks/2.0.6/hooks/before-ANY/test_before_any.py
, SUCCESS: Integrated in Ambari-branch-2.1 #285 (See [https://builds.apache.org/job/Ambari-branch-2.1/285/])
HADOOP-11764. [ HADOOP-11764] NodeManager should use directory other than tmp for extracting and loading leveldbjni (aonishuk) (aonishuk: http://git-wip-us.apache.org/repos/asf?p=ambari.git&a=commit&h=8acac954e43e321a6c446beb3ad6cb73c8856c28)
* ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/configuration/hadoop-env.xml
* ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog211Test.java
* ambari-server/src/main/resources/stacks/HDP/2.3/services/HDFS/configuration/hadoop-env.xml
* ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog211.java
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/params.py
* ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py
* ambari-server/src/test/python/stacks/2.0.6/hooks/before-ANY/test_before_any.py
* ambari-server/src/main/resources/stacks/HDP/2.2/services/HDFS/configuration/hadoop-env.xml
, Reverting change.

This will break on Windows., Actually, this was committed to Ambari, not Hadoop.  Meh. Whatever. I'll keep it closed then., FAILURE: Integrated in Ambari-branch-2.1 #319 (See [https://builds.apache.org/job/Ambari-branch-2.1/319/])
Revert "HADOOP-11764. [ HADOOP-11764] NodeManager should use directory other than tmp for extracting and loading leveldbjni (aonishuk)" (aonishuk: http://git-wip-us.apache.org/repos/asf?p=ambari.git&a=commit&h=43b4a049a1544c9ecd23a898fbf321de3e1b833c)
* ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog211Test.java
* ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog211.java
* ambari-server/src/main/resources/stacks/HDP/2.3/services/HDFS/configuration/hadoop-env.xml
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py
* ambari-server/src/test/python/stacks/2.0.6/hooks/before-ANY/test_before_any.py
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/params.py
* ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/configuration/hadoop-env.xml
* ambari-server/src/main/resources/stacks/HDP/2.2/services/HDFS/configuration/hadoop-env.xml
* ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py
, FAILURE: Integrated in Ambari-trunk-Commit #3220 (See [https://builds.apache.org/job/Ambari-trunk-Commit/3220/])
Revert "HADOOP-11764. [ HADOOP-11764] NodeManager should use directory other than tmp for extracting and loading leveldbjni (aonishuk)" (aonishuk: http://git-wip-us.apache.org/repos/asf?p=ambari.git&a=commit&h=a06a3243866b5938f6c0972f91222bb0998c776a)
* ambari-server/src/main/resources/stacks/HDP/2.3/services/HDFS/configuration/hadoop-env.xml
* ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog211Test.java
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py
* ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/params.py
* ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/configuration/hadoop-env.xml
* ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog211.java
* ambari-server/src/test/python/stacks/2.0.6/hooks/before-ANY/test_before_any.py
* ambari-server/src/main/resources/stacks/HDP/2.2/services/HDFS/configuration/hadoop-env.xml
, FAILURE: Integrated in Ambari-trunk-Commit #3427 (See [https://builds.apache.org/job/Ambari-trunk-Commit/3427/])
HADOOP-11764. [ HADOOP-11764] NodeManager should use directory other than tmp for extracting and loading leveldbjni (aonishuk) (aonishuk: http://git-wip-us.apache.org/repos/asf?p=ambari.git&a=commit&h=1c9df2d3f9fbfb98eeb7be651cd511cbe6727358)
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/params.py
* ambari-server/src/main/resources/stacks/HDP/2.2/services/YARN/configuration-mapred/mapred-env.xml
* ambari-server/src/test/python/stacks/2.0.6/hooks/before-ANY/test_before_any.py
* ambari-server/src/main/resources/stacks/HDP/2.1/services/YARN/configuration/yarn-env.xml
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py
* ambari-server/src/main/resources/common-services/YARN/2.1.0.2.0/package/scripts/params_linux.py
* ambari-server/src/main/resources/stacks/HDP/2.3/services/YARN/configuration/yarn-env.xml
* ambari-server/src/main/resources/common-services/YARN/2.1.0.2.0/configuration/yarn-env.xml
, SUCCESS: Integrated in Ambari-branch-2.1 #518 (See [https://builds.apache.org/job/Ambari-branch-2.1/518/])
HADOOP-11764. [ HADOOP-11764] NodeManager should use directory other than tmp for extracting and loading leveldbjni (aonishuk) (aonishuk: http://git-wip-us.apache.org/repos/asf?p=ambari.git&a=commit&h=d9f600e23d42544a3d8f67e2da75dc3db98bc555)
* ambari-server/src/main/resources/stacks/HDP/2.2/services/YARN/configuration-mapred/mapred-env.xml
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py
* ambari-server/src/main/resources/common-services/YARN/2.1.0.2.0/package/scripts/params_linux.py
* ambari-server/src/main/resources/stacks/HDP/2.3/services/YARN/configuration/yarn-env.xml
* ambari-server/src/main/resources/common-services/YARN/2.1.0.2.0/configuration/yarn-env.xml
* ambari-server/src/test/python/stacks/2.0.6/hooks/before-ANY/test_before_any.py
* ambari-server/src/main/resources/stacks/HDP/2.1/services/YARN/configuration/yarn-env.xml
* ambari-server/src/main/resources/stacks/HDP/2.0.6/hooks/before-ANY/scripts/params.py
]