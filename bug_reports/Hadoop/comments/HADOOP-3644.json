[-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12384696/testmem.patch
  against trunk revision 671563.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2739/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2739/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2739/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2739/console

This message is automatically generated., I'd propose that we mark this "don't fix", because I just committed HADOOP-3655 that lets you set the memory size on the ant command line. Does that solve the need?, Owen - that sounds good, though it might lead to a bit of confusion as more people start working with 64-bit Java., * Java 64 bit often requires 1.5X memory as its pointers are bigger. JRockit uses "short" pointers for references in the nearest 4GB memory, so avoids a lot of memory bloat. But it has other "issues".

* I've debated tweaking Ant for a different multiplier on 64 bit JVMs, so the memory can get scaled up. , HADOOP-3655 seems to address the need and changing the default to suit particular platform/JVM considerations doesn't seem correct.

That said, it's very tempting to just raise the limit. Are there any reasons why we shouldn't? The unit tests aren't so long-lived or memory-intensive that I'd fear masking errors with too high a limit. I'm going to close this as "Won't fix" for now, since it's been in patch queue limbo for a couple weeks and there is a solution already committed, but wouldn't object if it were reopened should someone want to argue that it's too low for platform-agnostic reasons., It's up to you whether to raise the limit by default or not, but the reason I'd suggest doing it is to avoid confusing new users. It's very confusing to check out a stable release of Hadoop and see it failing unit tests. That's what happened to me - I checked out Hadoop, I made some small changes, I tried running the tests, and this completely unrelated test was failing. 64-bit Linux is not an esoteric platform these days and raising the limit to 512 will probably work fine on other platforms, so why have the unit tests fail on purpose for this configuration?, What we could do is have the build file work out (how?) that its a 64 bit JVM and automatically ask for more memory. But that test would be tricky. 

Defaulting to a bigger -xMX would not be too harmful to 32 bit JVMs (its only a maximum; JVMs are short lived). , "resolved as won't fix" means that "fixed version" should not be set.]