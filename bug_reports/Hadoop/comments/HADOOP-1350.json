[Can consistently reproduce this on our cluster., Investigation in progress. Should have something soon. Worst case is that we go back to the hack where we don't do chunked encoding for content-length < 2GB. That will handle all typical cases and also handle the extreme cases of large content-lengths with some performance loss., Changed the chunked-encoding handling. Now the servlet doesn't set the header for chunked-encoding explicitly. Instead it leaves it to jetty to do chunked-encoding if required. Looks like there is an issue if we set that header explicitly.
sort500 works fine with this patch. I am yet to test the huge map output problem (the original problem reported in HADOOP-1176)., "sort500 works fine with this patch" - this should have been "sort350", Tested this patch for the case where a map generates more than 2 GB of output for a single reduce. Worked perfectly. I am also attaching the source code for simulating the big map output problem. It might prove useful for future tests/benchmarks., By the way, sort350 ran as expected with shuffle proceeding normally., +1

http://issues.apache.org/jira/secure/attachment/12357159/bigmapoutput.patch applied and successfully tested against trunk revision r537295.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/137/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/137/console, I just committed this.  Thanks, Devaraj!, Integrated in Hadoop-Nightly #89 (See http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/89/)]