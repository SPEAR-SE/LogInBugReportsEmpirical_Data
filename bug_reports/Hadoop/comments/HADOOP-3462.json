[We have a long running job with multiple waves of maps. It occasionally occurs that a relatively high number of TaskTrackers on nodes with failing disks repeatedly fail, and their number is high enough to let reduce tasks run out of attempts before these TaskTrackers get blacklisted.

Wouldn't it be better to only count reduce task failures against number of attempts when the reduce application is actually running, and not already during shuffling?, Christian, are you seeing the reduce task failures being caused by merge failures. Looking at the code, it didn't seem that reduce task fails when a fetch fails due to bad disk., No, not merge failures.
What I see is:

Task task_200804260028_0027_r_000093_12 failed to report status for 1214 seconds. Killing!

or

2008-05-26 04:11:22,354 INFO org.apache.hadoop.mapred.TaskRunner: Communication exception: java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:514)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)
        at org.apache.hadoop.mapred.$Proxy0.statusUpdate(Unknown Source)
        at org.apache.hadoop.mapred.Task$1.run(Task.java:294)
        at java.lang.Thread.run(Thread.java:619)

or


2008-05-28 09:26:51,105 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.IllegalStateException: Shutdown in progress
        at java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:39)
        at java.lang.Runtime.addShutdownHook(Runtime.java:192)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1195)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:150)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)
        at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:763)
        at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:696)

Maybe some other kinds of errors.

What is common that the failures occurred on nodes with disk errors., I think it makes sense to not count failures during shuffle against the task. , Currently, the task attempts failed due to factors such as disk errors, network errors, shuffle errors, fs errors etc. will mark the attempt as FAILED. If there are four such failures on a task tracker, the tracker will be blacklisted.  And if the attempts of a tip failed four times due to such factors, that will kill the tip thereby kill the job. 

To avoid such failures killing the job, the attempts should be marked something like FAILED_INTERNAL and these failures should be considered for blacklasting the trackers, but not kill the job.

Thoughts?, There could be some problems in the suggested approach. For example, there could be a faulty task which is writing to scratch space and making the disk out of space. Currently such tips would get failed in four attempts, thereby kill the job; which is the intended behavior. But making them FAILED_INTERNAL will not kill the job and just blacklist all the task trackers. 
And also if there are map tasks generating large map output files or reduce tasks generating large merge files, the job should get killed, instead of trying to run the map or reduce on all the tasktrackers.

To address this,
1. One solution could be: we can have a configuration property _mapred.map/reduce.max.internal.failures_. And a tip can be killed if the number of internal failures of the attempts exceed the _mapred.map/reduce.max.internal.failures_. Then we have to decide on the default number for this. But, this approach could take more time to kill the job.
2. Another solution could be to limit the disk space available for a task (something in the lines of process-ulimit?). And fail the task if it is exceeding the allotted space. But here, it would be difficult to keep track of disk space used by the task.   

Thoughts?, *Please ignore my previous comment*. Let this issue look at failures during shuffle only.  
So, errors/exceptions during shuffle phase of the task will mark the attempt FAILED_INTERNAL. And these failures will be considered for blacklasting the trackers, but not kill the job.
, 
Shuffle failures with DiskOutOfSpaceException on four different task trackers should not say that the failure is because of framework. For example, sort benchmark on 500 nodes run with single reducer will definitely fail because of out of memory. Such tasks which require large disk space should not make the job infinitely running because of no space left. Considering this, here is patch for review.

In the patch:
1. Errors/exceptions during shuffle phase of the task, except DiskOutOfSpaceException, will mark the attempt *FAILED_FRAMEWORK*. DiskOutOfSpaceException will mark the task as FAILED.
2. The FAILED_FRAMEWORK attempts of tip blacklist the tracker, but not kill the job.
3. Adds a public api isDiskOutOfSpaceException(Throwable th) in org.apache.hadoop.util.DiskChecker
4. jsp files are changed to show the FAILED_FRAMEWORK attempts as part of job failures.
, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12387177/patch-3462.txt
  against trunk revision 680823.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2983/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2983/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2983/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2983/console

This message is automatically generated., Cancelling patch to incorporate offline review comments from Devaraj.

Comments include:
1. FAILED_FRAMEWORK attempts should make TaskCompletionEvent status as KILLED.
2. These attempts should be logged as KILLED attempts in JobHistory, instead of FAILED.
3. Web UI should show these attempts in KILLED attempts.
, Here is patch incorporating the review comments.

Unresponsive tasks during shuffle are marked as FAILED attempts, since this could be because of User's faulty comparator., Found a nit in Reducetask.., updated patch., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12387341/patch-3462.txt
  against trunk revision 681912.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3001/console

This message is automatically generated., Patch updated with trunk, Cancelling patch to add ComparatorException around comparator in merge and reducetask will be FAILED if it fails with ComparatorException., Patch incorporating review comments to add ComparatorException, test-patch result on my machine:
{code}
     [exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
{code}

All core tests and contrib tests passed on my machine., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12387546/patch-3462.txt
  against trunk revision 682593.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3002/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3002/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3002/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3002/console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12387546/patch-3462.txt
  against trunk revision 682978.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3007/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3007/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3007/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3007/console

This message is automatically generated., Patch updated with trunk, I'm sorry to be coming in so late on this one, but I think the fix for this is much worse than the problem. We could easily end up with *stuck* jobs that never finish and never fail. That is much worse than having jobs fail. I think we need to find another way to address the observed problem, such as keeping black lists between jobs. I'm just worried that any implementation of this request will make the system less stable and that isn't ok., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12388021/patch-3462.txt
  against trunk revision 685425.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3055/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3055/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3055/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3055/console

This message is automatically generated., I agree with Owen on this. The patch assumes that there are no framework bugs, but while ideally we should not have framework bugs, that's not a guarantee *sigh*. Framework bugs could lead to jobs getting stuck., Shall we have maximum allowed FAILED_FRAMEWORK attempts per job? Say, if 10\% of the cluster tasktrackers get blacklisted because of internal failures, then kill the job.
Thoughts?, I think we need a better approach to the problem. Making this change would introduce the substantial risk of hung jobs in the presence of framework bugs.]