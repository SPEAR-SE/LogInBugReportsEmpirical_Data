[Similar issue was also found for HDFS related services, ref. HDFS-5087.

It looks like we have two approaches to set JAVA heap max, by configuring JAVA_HEAP_MAX or adding -Xmx option directly in related *_OPTS. We might avoid using the both and the conflict.

Also looked at other components like YARN. To be consistent, we could fix this by I would suggest:
1. Add variable HADOOP_CLIENT_HEAPSIZE, and user can set it either in hadoop-env.sh or hadoop-config.sh;
2. In appropriate script, check HADOOP_CLIENT_HEAPSIZE and set the value to JAVA_HEAP_MAX if any.
, [~drankye] I agree with your approach. 
Just one concern: $HADOOP_CLIENT_OPTS tries to wrap all configurations from user-side and pass to hadoop command. If we introduce another variable $HADOOP_CLIENT_HEAPSIZE, it may complex the original mechanism.

Another possible approach may be:
1. In {{hadoop-env.sh}}, before {{export HADOOP_CLIENT_OPTS="-Xmx512m $HADOOP_CLIENT_OPTS"}}, we can check whether $HADOOP_CLIENT_OPTS contains -Xmx. If has, ignore the default -Xmx512; otherwise, take the -Xmx512m.
2. In {{hadoop}}, before {{exec "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS "$@"}}, we can check whether $HADOOP_OPTS containers -Xmx. If has, don't include $JAVA_HEAP_MAX in the command; otherwise, take the $JAVA_HEAP_MAX.

Let's wait for some comments from other guys.
, Is there something inherently wrong with letting the JVM make the decision?  Are we worried about a JVM that doesn't follow the same set of rules?  (which, at this point, is a de facto API), [~aw]Yes. I haven't found any document talking about multiple -Xmx configurations in the command line. Not sure whether all JDKs always utilizes the last -Xmx. Correct me if I'm wrong here., we have similar solution applied to our repo, I will submit the patch soon., Great, [~jhsenjaliya]., attaching patch, that considers HADOOP_HEAPSIZE if set.

I haven't included corresponding updates to .cmd (for Windows Env) as there are no existing framework to test the changes.
However, I also have a patch that has .cmd file updated, Please advise if that is preferred over the attached patch.

Thanks
Jayesh, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12613265/HADOOP-9870.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3276//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3276//console

This message is automatically generated., Hi Jayesh,

The patch looks good, but it looks like you are only changing the HADOOP_CLIENT_OPTS here.
Were you planning to address JAVA_HEAP_MAX here too? 

If not, then this patch looks good from what Konstantin and I noticed in HADOOP-9211., Thanks for reviewing Plamen,

I have updated patch that addresses JAVA_HEAP_MAX in hadoop shell script and also includes windows .cmd file update.

Please review and vote if it looks good.

Thanks
Jayesh, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12614739/HADOOP-9870.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3300//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12614776/HADOOP-9870.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3301//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3301//console

This message is automatically generated., AFAIK, java process, processes its commandline VM args sequentially. If the same VM argument is set multiple times,it will choose the last one as its value.
Even though hadoop have many -Xmx configurations, it will take the last one in the list. 
User need not confuse about the other JVM argument (-Xmx1000m) as that will help if user didnot configure anything ( in case HADOOP_CONF_DIR is different and it dont have hadoop-env.sh file).

So I am not seeing any issue here., [~vinayrpet]. As I said above, I haven't found any documents that said the jvm would pick the last one. Correct me if I'm wrong here. So this patch wants to make this clear, instead of letting jvm make the decision to choose which as the jvm setting., bq. I haven't found any documents that said the jvm would pick the last one
Yes you are right. I too dint find any explicit document in hadoop mentioned about that. But we tested it and found that later argument value only it will use. And we are using in our clusters by configuring higher value than default of 1000m. 
User specied opts are added at last of the command line list just before the classname just make sure that their parameters take effect, Vinay, That is exactly why we need this fix, people dont necessarily need to hack this things ( including Xmx in the end or > 1000m).
It should be configurable and clear on how things will be used in launching hadoo process, which this patch provides., Closing this as HADOOP-9902 contains a fix for this issue.]