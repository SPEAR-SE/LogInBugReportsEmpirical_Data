[The attached patch does the following:
1) Modifies hadoop-default.xml; sets the value of fs.inmemory.size.mb to 75

2) SequenceFile.Sorter.MergeQueue.merge is modified to cleanup empty segments right at the beginning where the segments are obtained from the sorted map datastructure. Segments are extracted from the sorted map until we have collected the desired number of segments to merge or have no more segments to look at. This optimizes cases where we have lots of empty segments to merge.

3) InMemoryFileSystem class has been modified to have synchronization blocks wherever pathToFileAttribs map is touched. Some of them may be unnecessary but don't harm. It also has what Owen pointed out as a comment on HADOOP-1014. renameRaw has also been modified to throw an exception if the dst already exists.

4) The changes in ReduceTaskRunner are:
  4.1) MERGE_THRESHOLD, a number signifying the limit on the max number of files we will accumulate before initiating inmem merge, has been introduced. This is set to 500.
  4.2) The check for whether to initiate inmem merge has been modified to take into account the MERGE_THRESHOLD
  4.3) The on-disk spill file is now created prior to invoking merge (to take care of the case where we may not have any files left in the ram fs to cloneFileAttributes from; this will happen if all map outputs are empty)
  4.4) InMemFileSys.close is called at just one place now, This is the correct patch., Ack! Jira ate my comment.

Limiting the fan-in of the merge to 500 works, but means that the "spill size" may be small. WIth 200k maps, it means that you are pretty guaranteed to have 400 spills and thus a 2 level merge, which is unfortunate. If I remember right, the SequenceFile merge code has huge input buffers for the files, which is totally unnecessary for ramfs. Can we disable the buffers? Just the keys for the merge shouldn't be that big of a burden on the jvm.
, Good point regarding the buffersize. However, the problem with the buffersizes is that it cannot be completely disabled without, probably, major changes in the FileSystem part of the framework. This is because things like checksum depend on the buffersize. As an instance, the minimum buffersize should be at least as much as io.bytes.per.checksum, otherwise the checksum algo won't work, etc.... So, changing the buffersize, although it makes good sense for the ramfs, doesn't guarantee we won't run into the OutOfMemory errors when we increase the number of maps from, lets say, 9000 to 18000 (and all outputs can still fit in the ramfs). One more thing worth noting here is that we will have ramfs, Pririty Queue, and other merge datastructures proportional to the number of files we will merge at once.
So, chose a middle ground for this. In the merge code, used the minimum buffersize which is same as the value of io.bytes.per.checksum for the case where merge is trying to open a file in the ramfs. Also, added a config value mapred.inmem.merge.threshold; it has a default value of 0 which signifies that we DON'T want to have the file-count-threshold-based merging. If we want it (coz of OutOfMemory errors), then configure some appropriate value for that (like 5000 or so). This should give optimal performance for the typical use cases. 
Makes sense?, Forgot to mention that the patch 1027-new2.patch implements my comments., Attached is a patch that applies cleanly to the current trunk. Also, set the merge threshold count (for inmem merge) to 1000 files in hadoop-default.xml. 
I think it makes sense to have merge based also on the count of the number of files accumulated in ramfs, by default. We will then have a better control of the memory usage. It will avoid the various datastructures growing out of bounds (and thereby giving OutOfMemory errors) when there are too many maps (generating small outputs) and the size of the ramfs is large (so many many map outputs can be accomodated). 
Hopefully, since we can accomodate so many map outputs in the ramfs in the first place, the sizes of the outputs are, quite likely, small and hence the on-disk spills are small too. So although disk IO would be there, think it would not be the major bottleneck. 
Thoughts?, 
I notice that this patch contains if statements without the braces.  Quoting from the coding guidelines http://java.sun.com/docs/codeconv/html/CodeConventions.doc6.html#449:

  Note: if statements always use braces {}. Avoid the following error-prone form:

    if (condition) //AVOID! THIS OMITS THE BRACES {}!
        statement;
, This hopefully addresses the coding convention related comment., +1, because http://issues.apache.org/jira/secure/attachment/12351949/1027-new4.patch applied and successfully tested against trunk revision http://svn.apache.org/repos/asf/lucene/hadoop/trunk/511100. Results are at http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch, > This hopefully addresses the coding convention related comment.

Indeed it does.  Thank-you.

, +1, I just committed this.  Thanks, Devaraj!]