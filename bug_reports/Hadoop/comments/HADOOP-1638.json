[Here is the patch I mentioned...

Sorry about the nasty formatting in the original issue., +1

http://issues.apache.org/jira/secure/attachment/12362158/hadoop-1638.patch applied and successfully tested against trunk revision r557790.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/435/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/435/console, The hadoop-site.xml is created on node startup using the hostname passed in as user data (which comes from the env file). See src/contrib/ec2/bin/image/hadoop-init. Is the problem that something is corrupted - what are the #</property> lines above? Or is it that the wrong hostname is substituted?, (fixed the confusing formatting)

The hadoop-init script gets the MASTER_HOST value correctly and places it in hadoop-site.xml, but the problem is that the master node will not bind to the MASTER_HOST value. Since this is the address that gets put in hadoop-site.xml, the jobtracker and namenode will not start.
, There doesn't seem to be much action around HADOOP-1202, but it should make this a non-issue., I abandoned HADOOP-1202 because people didn't seem to see any value in it,
and I changed the way I use hadoop on ec2 around the same time.  You're
welcome to pick it up and port the patch to trunk; it shouldn't be too much
work.

-Michael



, This problem was caused by the changes made in Amazon EC2 addressing: previously instances were direct addressed (given a single IP routable address) and now they are NAT-addressed (by default, for later tool versions). The key point is that NAT-addressed instances can't access other NAT-addressed instances using the public address. Direct addressing is going to be phased out. See http://developer.amazonwebservices.com/connect/entry.jspa?externalID=682&categoryID=100 for more details. 

Tools versions ec2-api-tools-1.2-9739 and later use NAT addressing, and I have been using ec2-api-tools-1.2-7546 (although I thought I had been using ec2-api-tools-1.2-9739) which still uses direct addressing.

I don't think HADOOP-1202 will make this a non-issue since EC2 NAT instances cannot route to the public address of other instances. So even if the namenode and job tracker could bind to the public address that would not be much help to the slaves since they have to connect to the internal address - so this patch would still be needed.

Stu, I agree that it would be nice to fix this problem more thoroughly but until we have a better solution I think this approach is fine.

I've tested with the last three versions of ec2-api-tools and have successfully run the grep example on small multi-node clusters. When NAT-addressing is used however the webservers on datanodes and task trackers are not accessible since non-routable addresses are used. Apart from this limitation (which can be worked around by logging in to the relevant machine to browse logs) jobs ran OK.

So I vote to commit this (along with HADOOP-1635, HADOOP-1634) - I'll have some time to do this tomorrow., > This problem was caused by the changes made in Amazon EC2 addressing: previously instances were direct addressed (given a single IP routable address) and now they are NAT-addressed (by default, for later tool versions). The key point is that NAT-addressed instances can't access other NAT-addressed instances using the public address.

I don't use the hadoop ec2 scripts, but I filed HADOOP-1202 specifically because of this issue.

The solution I intended with HADOOP-1202 was to make the namenode and jobtracker bind to 0.0.0.0 using my HADOOP-1202 patch, but use the internal addresses in the hadoop configs.  I set up an http proxy to view logs for the datanodes and tasktrackers (I have my httpd.conf if anybody is interested).  It is then possible to view the jobtracker & namenode website normally (you have to submit jobs from inside the cluster though, since submitting a job writes to the dfs).  The problem is that you can't use the dfs from outside the cluster; instead you have to use some proxying solution which will be much slower (in our case it took longer to copy data back than to compute it).  

If you need to use dfs, the real solution is to make all datanodes bind to 0.0.0.0, make the namenode aware that each datanode has two addresses, and make sure the namenode knows when to use which one.  This would require significantly more work than my HADOOP-1202 patch though.
, I've just committed this. Thanks Stu!

(I also fixed another instance of the column 7 problem in start-hadoop.)]