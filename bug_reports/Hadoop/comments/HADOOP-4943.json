[Tested on an 8-node cluster with (4 map, 2 reduce) on half of the cluster, and (2 map, 4 reduces) on the other half.

Tested with a streaming job of 24 zero-length input + 24 reducers (map/reduce='sleep 60'). We are able to schedule all of maps at the same time (also for reducers).

Tested with a setting of 12, we are able to uniformly spread the load.

Tested with a setting of 48, we are able to throttle at running 24 maps (instead of going beyond limit) (also for reducers).

The test was done on hadoop 0.17.
, Looks good to me. This is an issue that actually affects other schedulers too, because that "max load" code was taken from the implementation of the default scheduler. (Unless it has been fixed until then). Do you think it might be possible to create a unit test for this using the fake tasktrackers in the existing test class?, Here's a patch that includes a unit test., +1. Looks good to me.
Do you want to fix the default scheduler (as you mentioned above) in the same transaction?
, I just committed this. Thanks Zheng!

I also looked at the default and capacity schedulers, but the default scheduler already seems to have this logic as part of the patch for HADOOP-3136, and the capacity scheduler doesn't try to do this kind of load balancing when there are fewer tasks than slots so I think this should be a separate JIRA.]