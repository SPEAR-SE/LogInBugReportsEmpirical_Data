[The blockSize a persistent attribute of the file. It is stored in the INode in the fsimage. It can be specified only at file creation time. HDFS makes every effort to chunk a file into blocks of size specified by blockSize. , Added a unit test to test blockSize for files., Added unit tests to test blocksize., I think we should be careful introducing new persistent fields. It is really simple to add a field, 
but it may be hard to remove or support it in the future.
Why do we need the block size to be persistent? What is the use case?
Right now we create files with the default block size, at least that is what map-reduce does.
Block size is used by map-reduce to calculate splits. I don't know whether this patch will break the
semantics of generating splits.
On the other hand if we store the block size per file, what is the semantics of that field?
Currently we have flexibility to create blocks of different sizes within the file, do we loose that flexibility
from now on? And if we don't why do we need to store it?
This looks like one of those simple changes that can lead to big consequences.

Currently if there is more than one block in the file the block size is returned correct;y. The problem is with one block files only.
I'd propose one of the 3 variants in this case:
# keep it as it is: return the first block size;
# return the default block size;
# return -1 as the block size from the name-node, and let DFSClient return its default size further up to the application. 
Most probably that will the size this file was created with.

On a side note we need to deprecate getBlockSize() both in DFSClient and ClientProtocol because it is never used.
The correct way is to call getFileInfo().getBlockSize()., The blocksize is a heuristic that HDFS uses to chunk up a file. HDFS makes every effort to chunk a file where most chunks are of size specified by blocksize. This means HDFS can still create blocks of a size other than the specified blocksize if it needs to (maybe in the case of appends). Another requirement is that if an application specified the blocksize while creating the file, it should have the ability to retrieve that *precise* value by invoking getBlockSize(). Given the above definition and requirements, the above proposal 1-3 might not fit the needs. I do not see any other way of achieving this other than persisting the blocksize attribute in the inode.



, A question came up during the review that the blocksize is really a heuristic; the file system could still have blocks in the file that are different from the specified blocksize. This means that the specified block size is really the "preferred" block size.

HDFS now implement getPreferredBlockSize(). This preferred block size is persisted in the file system image. The FileSystem API has not changed., merged with latest trunk., I could only find one piece of code that could be a problem: dfs.FileDataServlet::pickSrcDatanode. For a file longer than the reported blocksize, it assumes a length of n*blocksize will return n blocks (unless it's a zero length file, when it asks for only one block). It will still work, but the number of blocks surveyed is no longer as claimed.

The only other case is irrelevant, in TestDFSShell where querying the blocksize of a zero-length file need only not throw. That it expects zero doesn't matter., merged patch with latest trunk, The patch looks good except for some minor comments:
1. FSConstants.java
line 161: should update the comment  for the new layout version.
1. FSEditLog.java
line 260: the error message of IOException should be updated.
line 502: it uses toLogTimeStamp to convert a block size to a UTF8. It should be clearer to change the method name to be something like to LogLong., Incorporated all of Hairong's review comments., +1 The patch looks good., -1, build or testing failed

2 attempts failed to build and test the latest attachment http://issues.apache.org/jira/secure/attachment/12364784/blockSize6.patch against trunk revision r570983.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/650/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/650/console

Please note that this message is automatically generated and may represent a problem with the automation system and not the patch., I just committed this.]