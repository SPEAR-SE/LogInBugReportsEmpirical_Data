[Owen has argued that libhdfs's build should be re-written to use autoconf.

https://issues.apache.org/jira/browse/HADOOP-1410?focusedCommentId=12497947#action_12497947

We probably need a separate issue for that.  Should we fix this independently?
, This was an issue for me for testing fuse-dfs - FUSE is installed to match the kernel, so on a 64bit kernel, I have to use 64bit java and 64bit libhdfs. If an autoconf build system would be ready for 0.18 then perhaps we should try to move to one?, Craig: it doesn't sound like anyone is yet working on an autoconf build for libhdfs, but, yes, this would be a welcome contribution for Hadoop 0.18., Ok, this is first attempt at an autotools build system.

Notes/issues:
 * It doesn't yet work. I havent figured out how to get .so shared files built, so building the tests fail
 * The Java-related autoconf macros came from Apache Commons, Daemon. Hope this is OK?
 * I still need to test on other linux architectures. Will need other volunteers for alternative platforms. 
 * Requires autoconf 2.61, for the AC_TYPE_INT64_T and similar macros

To build configure & Makefile, do:
{noformat}
autoreconf;  libtoolize && aclocal -I ../utils/m4/ &&  automake -a --foreign && autoconf
{noformat}
For normal building:
{noformat}
./configure && make
{noformat}

Can anyone provide assistance in completing this?, > Can anyone provide assistance in completing this?

Sorry, I have no experience with autotools.  Anyone else?, This patch is an improved patch - libhdfs now builds OK, but doesn't build as a .so file. Added doc and test targets, and updated build.xml to call configure.

Will test on 64bit platforms and improve. Work in progress., I'm getting this error ---

relocation  against `a local symbol' can not be used when making a shared object; recompile with -fPIC

This seems to have been a problem for others on amd64 too, but I don't know what the fix is other than adding -fPIC to the library file build, but that has bad implications on non amd64 platforms.
, fPIC rings a bell on amd64. What are the disadvantages to adding -fPIC (a) for amd64 only, (b) for all platforms?, bq. What are the disadvantages to adding -fPIC (a) for amd64 only, (b) for all platforms?

Just the extra indirection required for being independent. I guess on another platform if you are going static it could cost something although not much - especially here/

I had to remove the -shared from the configure.ac, add -fPIC to the CPPFLAGS and also weirdly add hdfs_read_LDADD = libhdfs.la                                                                                                                                                                                   
 for each of read, write and test. I thought it would be automatically added.  other than that, everything else works for me on amd64.
, Here we have the v2 version of the patch available which as well *requires autoconf-2.61*. This is an improved version over v1 submited by Craig. Many thanks to Craig.

I 've used a small piece of java code along with the m4 macros to detect the jvm arch. 
{code:title=getArch.java|borderStyle=solid}
class getArch {
  public static void main(String []args) {
     System.out.println(System.getProperty("sun.arch.data.model", "32"));
  }
}
{code} 
If somebody can suggest me of a better way I would be more than happy to implement it

This patch addresses all the three scenarios
    * 32bit OS, 32bit java => libhdfs should be built 32bit, specify -m32
    * 64bit OS, 32bit java => libhdfs should be built 32bit, specify -m32
    * 64bit OS, 64bit java => libhdfs should be built 64bit, specify -m64

{noformat}
To Build libhdfs.so use    ant compile-c++-libhdfs -Dcompile.c++=true
To Test  libhdfs.so use    ant test-c++-libhdfs -Dcompile.c++=true 
{noformat}

I have tested this patch on amd64 with 32 bit and 64bit jvm.  
Please help me by testing in other platforms as necessary and let me know your comments. 

Thanks



, bq. I 've used a small piece of java code along with the m4 macros to detect the jvm arch. 

Giridharan, you can pass the requisite java properties (sun.arch.data.model etc.) straight from build.xml without adding getArch. Please take a look at the compile-core-native target in hadoop/trunk/build.xml, we use that for compiling the native hadoop compression libraries (libhadoop.so), version v3 has the changes incorporated. v3 doesn't have the java snippet anymore to find the JVM Arch
Thanks to Arun.

Please help me in testing this v3 patch; I 've the patch tested on amd64 with 32 and 64 bit java.

Thanks
, I have tested on Mac OS X 32bit powerpc 10.4. Will test on Linux in due course (10.4 Mac OS X is now unsupported platform for trunk, as Java 6 is not provided by default).

Some minor comments:

1. Mac OS X does not have error.h, so my compile fails. hdfsJniHelper.c includes error.h, added by HADOOP-3549. As far as I can see, it is superfluous. At least on Linux, there are no functions in error.h which hdfsJniHelper.c uses. Should i reopen HADOOP-3549, or file a new issue? Note that error.h should not be confused with the standard errno.h

2. Will create-c++-configure be called for every compile? It customary to include the generated configure/Makefile, as people compiling need not have autoconf/automake.

3. (related to 2) Your patch includes configure/Makefile etc, which is great for testing the patch atm, but the version committed should NOT include these.

Craig, Thanks for your comments Craig.

{quote}
2. Will create-c++-configure be called for every compile? It customary to include the generated configure/Makefile, as people compiling need not have autoconf/automake.
{quote}

Nope - It doesn't gets called. 
During the compilation we just call the configure and then the make install.

{quote}
3. (related to 2) Your patch includes configure/Makefile etc, which is great for testing the patch atm, but the version committed should NOT include these.
{quote}
I did "make distclean" before creating this patch. I understand this patch has an empty Makefile which we have to delete after submitting the patch to svn. 

Please correct me if my understanding is wrong anywhere.

Thanks
Giri, Craig, 
Any update on testing with linux? Based on that I can make the patch available..
thanks,
Giri, steps to build libhdfs 

{noformat}
ant compile -Dcompile.c++=true -Dlibhdfs=true 
By using libhdfs=true flag we build libhdfs with other c++components. 
{noformat}

This patch addresses all the three scenarios
    * 32bit OS, 32bit java => libhdfs should be built 32bit, specify -m32
    * 64bit OS, 32bit java => libhdfs should be built 32bit, specify -m32
    * 64bit OS, 64bit java => libhdfs should be built 64bit, specify -m64

Here is the local test-patch result

[exec]
     [exec]
     [exec]
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 10 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 Eclipse classpath. The patch retains Eclipse classpath integrity.
     [exec]
     [exec]
     [exec]
     [exec]
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]     Finished build.
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]
     [exec]

Thanks,
Giri, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12395943/HADOOP-3344.patch
  against trunk revision 726129.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 10 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3734/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3734/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3734/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3734/console

This message is automatically generated., The reason for the failure is the missing autoconf-2.61 on the build server.
Im going to resubmitting the patch, With this new patch c++-create-configure target is added with the libhdfs flag.
By doing so it doesn't touch the libhdfs 's configure/compile/ or test target. (unless the flags are set 
ie -Dcompile.c++=true & -Dlibhdfs=true)
-Giri
, Build doesn't do anything to the libhdfs target's unless the two flags are set. This is implemented b'coz this *libhdfs requires autconf-2.61* and the build server seems to have autoconf-2.59.

Thanks,
Giri, +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12396056/HADOOP-3344.patch
  against trunk revision 726129.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 10 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3743/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3743/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3743/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3743/console

This message is automatically generated., Looks like patch requires some merging , as there are some conflicts with the ivy porting
resubmitting the patch
-giri, thanks,
giri, I just committed this.  Thanks Giri!, Incompatible due to:

1) autconf-2.61 required to compile now
2) location of libhdfs is now c++/<os_osarch_jvmdatamodel>/lib, Edit release note for publication.

To build libhdfs use the following command.

ant compile -Dcompile.c++=true -Dlibhdfs=true 
But make sure you have  "autoconf-2.61" installed 

By using libhdfs=true flag we build libhdfs with other c++components.
and the resulting .so file will be installed in  c++/<os_osarch_jvmdatamodel>/lib directory

This patch addresses all the three scenarios

*32bit OS, 32bit java => libhdfs should be built 32bit, specify -m32 
*64bit OS, 32bit java => libhdfs should be built 32bit, specify -m32 
*64bit OS, 64bit java => libhdfs should be built 64bit, specify -m64 , Thanks, Craig, for the correction!]