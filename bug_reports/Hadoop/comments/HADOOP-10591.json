[This change was made in HADOOP-10047. [~gopalv] or [~cmccabe] any thoughts on this one?, We have two ways we could go on this one.  One is to implement a buffer pooling scheme.  Another is to manually free the direct buffers.

The buffer-pooling scheme initially might seem more attractive, but it's problematic.  We don't know that all the buffers we're creating will be the same size, so we end up with the same kind of problems you get when implementing {{malloc}}.  It's also unclear how long we should hang on to buffers when they're not in use.

Manually freeing the buffers is possible through a Sun-specific API.  We do this in a few other cases-- for example, to {{munmap}} a memory segment.  This is probably the simpler route to go., The [CodecPool|https://hadoop.apache.org/docs/r2.4.0/api/org/apache/hadoop/io/compress/CodecPool.html] class implements pooling of direct buffers for compression codecs., (well, it pools the Compressor/Decompressor, but the intent is to pool the direct buffers), Hmm.  The JIRA talks about "direct buffers allocated by compression codecs like Gzip (which allocates 2 direct buffers per instance)."
I assume this is a reference to {{ZlibDecompressor#compressedDirectBuf}} and {{ZlibDecompressor#uncompressedDirectBuf}}.  Those are buffers inside {{Decompressor}} objects, not buffers inside {{Codec}} objects.

*However*... {{CodecPool}} has a cache for {{Compressor}} and {{Decompressor}} objects, but it seems to be optional, not mandatory.  For example, this code in {{SequenceFile}} is careful to use the {{Decompressor}} cache:
{code} 
         keyLenDecompressor = CodecPool.getDecompressor(codec);
          keyLenInFilter = codec.createInputStream(keyLenBuffer, 
                                                   keyLenDecompressor);
 {code}

On the other hand, there are also one-argument versions of the {{createInputStream}} functions that always create a new {{Decompressor}} (and similar one-argument versions for {{createOutputStream}}).

What's the right resolution here?  Is it just to mark the one-argument versions as deprecated and audit HDFS and Hadoop client programs to remove usages?  That certainly seems like a good idea, if we want to cache these {{ByteBuffers}} without adding more caching mechanisms., [~andrew.wang]: HADOOP-10047 was a change which avoided the need to allocate direct buffers by the decompressors implementing the DirectDecompressor interface.

DirectDecompressor::decompress(ByteBuffer src, ByteBuffer dst)

was meant to avoid allocating objects in the decompressor object's control. That does a decompress from src into dst without an intermediate allocation or copy.

Before that ORC couldn't use own the buffer pools for src/dst. 

The issue in this bug pre-dates HADOOP-10047., Thanks, Gopal.  I agree that this is a pre-existing issue, definitely not introduced by HADOOP-10047.  And, in fact, that JIRA should improve the situation in many cases by eliminating the need for the {{Decompressor}} to allocate its own direct buffer.

semi-related: One thing that I notice in the constructor for {{ZlibDirectDecompressor}} is that it invokes the superclass constructor ({{ZlibDecompressor}}) with {{directBufferSize = 0}}, causing us to call {{allocateDirect}} with a size of 0.  I do wonder what this actually does... I didn't manage to find any documentation for this case (maybe I missed it?)., This is a patch which makes the one-argument forms of {{Codec#createOutputStream}} and  {{Codec#createInputStream}} take the codecs from the global {{CodecPool}}, rather than allocating a new one each time., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12651774/HADOOP-10591.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4132//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4132//console

This message is automatically generated., Latest patch looks pretty good to me. Two nits:

# Any reason we can't make {{CompressionOutputStream.trackedCompressor}} private?
# The javadoc for {{createInputStreamWithCodecPool}} says "The codec to use to create the *output* stream."

+1 once these are addressed., bq. Any reason we can't make CompressionOutputStream.trackedCompressor private?

Yeah, let's make it private.

bq. The javadoc for createInputStreamWithCodecPool says "The codec to use to create the output stream."

Fixed., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12656112/HADOOP-10591.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.fs.TestSymlinkLocalFSFileContext
                  org.apache.hadoop.ipc.TestIPC
                  org.apache.hadoop.fs.TestSymlinkLocalFSFileSystem

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4295//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4295//console

This message is automatically generated., Test failures are unrelated.  TestIPC is timing out resolving a hostname (looks like a jenkins problem), and the symlink tests have been failing for some other patches too., Agreed.

+1, the latest patch looks good to me. Thanks, Colin., FAILURE: Integrated in Hadoop-trunk-Commit #5900 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5900/])
HADOOP-10591. Compression codecs must used pooled direct buffers or deallocate direct buffers when stream is closed (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1611423)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionInputStream.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionOutputStream.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DefaultCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/Lz4Codec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/SnappyCodec.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #616 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/616/])
HADOOP-10591. Compression codecs must used pooled direct buffers or deallocate direct buffers when stream is closed (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1611423)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionInputStream.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionOutputStream.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DefaultCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/Lz4Codec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/SnappyCodec.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1835 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1835/])
HADOOP-10591. Compression codecs must used pooled direct buffers or deallocate direct buffers when stream is closed (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1611423)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionInputStream.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionOutputStream.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DefaultCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/Lz4Codec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/SnappyCodec.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1808 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1808/])
HADOOP-10591. Compression codecs must used pooled direct buffers or deallocate direct buffers when stream is closed (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1611423)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionInputStream.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionOutputStream.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DefaultCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/Lz4Codec.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/SnappyCodec.java
]