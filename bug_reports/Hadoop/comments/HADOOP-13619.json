[
you aren't overwriting blobs with new ones of different sizes? that can cause visible consistency problem.

An other possible cause is that these are clearly multipart files, which are trouble in their own ways. Specifically, I've seen mismatches between the size returned in FileSystem.getFileStatus and the actual file length as returned in reads.

Can you replicate this locally? That is, repeatedly run getStatus() against the file and verify that it always returns the length of file just written

Now the bad news: you are probably going to have to be the one to identify and, if it is possible fix the problem. I would also recommend you work on on the branch-2 branch, because that and trunk is where fixes will go, maybe backported to 2.8 or 2.7.4.


I've been writing some spark & object store tests in SPARK-7481, albeit with a focus on S3; In HADOOP-11694 we've been adding scale tests for s3 too.

One limitation we have there is the lack of large public datasets, and the time/cost it takes to set up transient ones for a single test. Would you be able to serve up the specific file causing problems here as a public object? That way some of our integration tests could work with it direct, the way we do for some AWS tests today.
]