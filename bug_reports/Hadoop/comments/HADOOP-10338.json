[I don't see what this has to do with the globber.  The globber can resolve the root inode just fine.

In fact, there's a unit test for this in {{TestGlobPaths}}:
{code}
    Path root = new Path(USER_DIR);
    status = fs.globStatus(root);
    checkStatus(status, root);
{code}

The problem is not on the client side.  I ran a "chmod /" and traced it back all the way to the {{FSNamesystem}}, which seems to be calling {{FSDirectory#setPermission}} on src=(empty string).  Then {{FSDirectory#unprotectedSetPermission}} locates the root inode and calls {{INode#setPermission}} on it.  For some reason, that doesn't work / take effect.

There seems to be a more general problem where modifying the root inode fails.  I can't change the owner or group, for example.  The INode stuff has been modified a lot over the last few releases.  The inclusion of snapshots, the Feature work, and probably some other stuff I'm forgetting.  I think someone needs to go in there and do a deep dive to fix this.

By the way, I verified that I could chmod / chown snapshottable directories.  I was a little concerned about that after looking at this problem.  But the issue seems to be specific to root., Thanks to [~atm] for figuring out that this is the globber after all.  It turns out that there is a bug which causes the Globber to return a bogus {{FileStatus}} when asked about root.  This doesn't affect what I was testing (i.e. that we can chmod or chown the root directory).

So during my test, the root *was* actually being modified, but I just couldn't see it because the globber continued to tell me that it had the same status as before.  atm wrote a program which bypassed the globber and proved that the inode was being modified after all., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12627683/HDFS-5888.002.patch
  against trunk revision .

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6076//console

This message is automatically generated., build failed because:
{code}
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 1078712 bytes for Chunk::new
# An error report file with more information is saved as:
# /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hs_err_pid17045.log
{code}, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12627683/HDFS-5888.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.TestGlobPaths
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6097//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6097//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12627683/HDFS-5888.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.TestGlobPaths

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6102//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6102//console

This message is automatically generated., I poked around to debug the failing test. It turns out we have some FC only code in TestGlobPaths#TestGlobFillsInScheme:

{code}
      if (fc != null) {
        // If we're using FileContext, then we can list a file:/// URI.
        // Since everyone should have the root directory, we list that.
        statuses = wrap.globStatus(new Path("file:///"),
            new AcceptAllPathFilter());
        Assert.assertEquals(1, statuses.length);
        Path filePath = statuses[0].getPath();
        Assert.assertEquals("file", filePath.toUri().getScheme());
        Assert.assertEquals("/", filePath.toUri().getPath());
      }
{code}

The tricky part here is that the default filesystem for this FileContext is an HDFS, which is why Jenkins is picking up "localhost:port" for the authority in Globber#authorityFromPath:

{code}
        authority = fc.getDefaultFileSystem().getUri().getAuthority();
{code}

If I change it to this, the test passes:

{code}
        authority = fc.getFSofPath(path).getUri().getAuthority();
{code}

I think the error stems from how file:// URIs have a null authority, and we shouldn't fill it in. I think the fix is to use getFSofPath for both FC and FS in authorityFromPath., thanks, Andrew.  I fixed that, +1 pending jenkins, nice work, Thanks a lot for taking care of this guys., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12628290/HDFS-5888.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.TestGlobPaths
                  org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.ha.TestHAAppend

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6110//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6110//console

This message is automatically generated., I re-uploaded the 003 patch to fix a small bug I found.  The preceding test results refer to the old version.  Sorry for reusing the same number., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12628317/HDFS-5888.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6113//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6113//console

This message is automatically generated., test failure is HDFS-5936, not related to this patch.  committing.  thanks for the reviews, Andrew., +1 for the latest latest version, for super clarity :), moved this to common, to reflect the fact that the globber is in common, SUCCESS: Integrated in Hadoop-trunk-Commit #5150 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5150/])
HADOOP-10338. Cannot get the FileStatus of the root inode from the new Globber (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1567497)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Globber.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #479 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/479/])
HADOOP-10338. Cannot get the FileStatus of the root inode from the new Globber (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1567497)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Globber.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1671 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1671/])
HADOOP-10338. Cannot get the FileStatus of the root inode from the new Globber (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1567497)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Globber.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1696 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1696/])
HADOOP-10338. Cannot get the FileStatus of the root inode from the new Globber (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1567497)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Globber.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java
]