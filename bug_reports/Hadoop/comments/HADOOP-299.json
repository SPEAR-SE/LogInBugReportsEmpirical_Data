[This patch does a couple of things:
  1. It makes it more obvious that the task slot padding is only done on clusters bigger than 3 nodes.
  2. Remove the dead avgMaps and avgReduces variables.
  3. Change the default value of PAD_FRACTION from 0.1 to 0.01. (This variable is not in hadoop-default.xml, so the code controls the default value.)
  4. Fix a typo that was counting all of the running jobs' tasks instead of the the running jobs' running tasks. (totalMaps versus totalNeededMaps and totalReduces versus totalNeededReduces), It's the same issue that I reported earlier on the mailing list (e.g. http://www.mail-archive.com/hadoop-dev@lucene.apache.org/msg01510.html, http://www.mail-archive.com/hadoop-dev@lucene.apache.org/msg01524.html and http://www.mail-archive.com/hadoop-dev@lucene.apache.org/msg01557.html).

Your patch, although it improves things, could perhaps go one step further if you have some time to spare... ;) I'm thinking specifically about the following:

* don't schedule reduce tasks from jobs, where map tasks had no chance of running yet. This happens when there are a couple available slots, and map tasks cannot be scheduled yet, but the code in JobTracker:743 still allocates reduce tasks from the next job.

* allow simple priority-based preemption, i.e. jobs with a higher priority (presumably short-lived) could be favored in task allocation over already running jobs.

* alternatively, allow setting limits on the min/max % of cluster capacity the job is willing to accept. This gives some leeway to the scheduler to allocate the flexible portion of remaining tasks to old/new jobs.

* allow setting different priorities for maps and reduces - e.g. in Nutch fetcher job, map tasks are very long running and in many cases need to fit within a specified time-frame (e.g. during the night). However, reduce tasks, which simply reshuffle the data, are not so time-critical.
, *smile* I don't have that much free time! I certainly agree with you that the right long term solution would involve:
  1. Using task killing to make slots available if they were needed.
  2. Not starting reduces until at least one of the maps feeding it has finished and generated output.
  3. For bonus points you could prefer to run a reduce that has local input. (In general that won't help, but there is a sub-class of problem where most of the input for each reduce is coming from a small number of maps.), I just committed this.  Thanks, Owen!]