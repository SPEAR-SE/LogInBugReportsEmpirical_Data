[I had trouble reading old compressed SequenceFiles using the new block-compressing code, with similar kinds of problems. I haven't been able to characterize why, yet, so I don't have anything useful to add to this bug except "I've seen this general sort of thing, too!". In my case, it's with a class that's similar to BytesWritable., The compression codec is not reading the entire value buffer, but it is getting the correct value. (I suspect the unread bytes are a crc.) This error message is the SequenceFile complaining that the entire buffer was not used.

This patch:
  1. extends the unit test to use bigger values so that we detect the problem
  2. allows the user of the org.apache.hadoop.io.TestSequenceFile main program to control the random seed (and prints out the seed value, even if it is random).
  3. check that the stream is done by trying to read the next byte on the input stream.
  4. removes some redundant buffering of the already buffered value stream.
  5. marks the start of the value in non-block compressed sequence files and does a reset at the front of getCurrentValue., I just committed this.  Thanks, Owen!]