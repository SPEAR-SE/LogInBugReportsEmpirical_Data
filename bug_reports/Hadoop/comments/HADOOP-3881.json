[This is by design.  The client used to time out, but, when servers got slow and clients retried, it led to server meltdown.  Think of this another way, if the call were local, not remote, should it time out?  At the RPC layer, nothing exceptional has happened: both ends are still up, responding to pings, etc.  So I don't see the current behavior as obviously wrong.  If the service hangs, and clients depend on that service, then clients will hang too.

That said, there may be a case, with non-singleton services, where a service could get hung and the client might reasonably retry a different service.  That doesn't currently apply much to Hadoop.  I can find only two protocols where clients talk to peers rather than superiors (InterDatanodeProtocol and ClientDatanodeProtocol).  In these cases, if the remote end were to hang indefinitely, its not clear what the client should do.  It would probably be bad, so perhaps we should add a way to time things out in these specific cases.
, Yes, retries in this situation would not be ideal. Throwing some exception "timeout invoking InterTracker.heartbeat() on /127.0.0.1/8083 -possible deadlock" would be enough for developers. But production, well, it shouldn't show up. Shall I close this issue as INVALID?

If retry load is an issue then the whole client retry operations in TaskTracker and DataNode need to be looked at. There's a sleep, with the sleep time hard coded in the source. Which means that if the whole datacentre is synchronzied -as you get if the power gets toggled and they all boot up at the same time, there's a risk that all the nodes in the datacentre will hit the tracker/namenode simultaneously. Even exponential backoff doesnt work if the clocks are fully synchronized. it helps, but a bit of jitter is needed too just to round things off. There's enough complexity/duplication here that this could be pushed into a reused class.

Also, maybe the IPC and design decisions could be documented in the wiki
, > a bit of jitter is needed [ ... ]

There is jitter in block reports. and in ExponentialBackoffRetry.  I have not heard of folks having problems on cluster restart.

> Also, maybe the IPC and design decisions could be documented in the wiki

The problem with detailed code documentation separate from the code is that it quickly goes stale.  The internal design is dynamic.  What's better for this is good documentation in the code, since that is more naturally maintained as the code changes.  It's best to only use separate documentation for slower-moving targets like end-user API documentation and high-level architectural documentation.
, > If retry load is an issue then the whole client retry operations in [...]

The current implementation makes retry less relevant and not needed in most cases, right? One of the main motivation was handle burst-y load gracefully at the server. BlockReports do an exponential backoff right now, but it is not required and does not get triggered with the current  IPC implementation. It needs to be removed.

We could add a FAQ entry in Wiki briefly stating that IPC calls don't timeout. , This can arise if the JT isn't yet live, which can happend because the DFS isn't live. Is hanging forever the right tactic here? , I propose having some possible timeout in RPC, but have a default of some hours, so it does not surface in normal outages -unless the caller changes the properties to make it fail sooner rather than later. , Closing this as stale.]