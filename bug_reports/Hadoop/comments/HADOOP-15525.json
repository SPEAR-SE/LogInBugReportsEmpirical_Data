[Assigning to me for now. I'd like to write a doc here to describe an example with actual IAM policies so we can talk concretely about it.  Coincidentally, I'm about to go on vacation for two weeks but will try to post something when I get back. Meanwhile, comments welcomed., [~fabbri] FWIW AWS IAM can do this with IAM policies.


https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/


Which is to say that a certain IAM policy attached to (probably) an IAM role can only read/list/delete something under a specific keyspace in S3.

The solution for this in AWS EMR is to provide a mapping in configuration that EmrFS (EMR's internal Hadoop-compatible S3 filesystem) uses to assume those roles to get different credentials before making requests to S3.


https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-emrfs-iam-roles.html


On EMR, this is a little simpler because the service controls provisioning of the cluster. It is likely to be a more ambiguous problem to solve in open source, where any users can deploy it anywhere.

I was responsible for the AWS EMR solution to this problem, I would be happy to involve myself as much as I'm allowed in preparation of this feature., Thanks [~poeppt]. We do have assume role support in S3A (HADOOP-15176). I'd like to add:
 # Documentation with IAM policy examples on how to achieve the scenario listed here.
 # Probably: some integration tests that confirm it works as expected–and keeps working in the future.
 # Along the way, any features we think we need to simplify usage, etc. can get new JIRAs.

This will greatly simplify things for end users that are trying to achieve this because the way directories are emulated, and the implication for required permissions, is not obvious., The existing docs [here|https://hadoop.apache.org/docs/current3/hadoop-aws/tools/hadoop-aws/assumed_roles.html] give the basic permissions needed.  Will probably link to some additional examples from there., Actually, refreshing my memory here, [~stevel@apache.org] has already done most of this--though I'm proposing a more restrictive example.

The test case {{org.apache.hadoop.fs.s3a.auth.ITestAssumeRole#testRestrictedWriteSubdir}} ([link|https://github.com/apache/hadoop/blob/fba1c42adc1c8ae57951e1865ec2ab05c8707bdf/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java#L389]) is a good example.

I'm wondering if adding explicit create and delete permissions on parent directory keys (e.g. allow deleteObject on "/a/" "/a/b/" "/a/b/c/") would avoid the "cannot update directory markers" problem. Since there is no wildcard after these keys, they should only grant access to manipulating the actual empty directory markers, right?

Will try to play around with that when I get back from break., As you note, *most of this is done* in HADOOP-15176. See [Assumed Roles|https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/assumed_roles.md]; while {{org.apache.hadoop.fs.s3a.auth.RoleModel}} gives you a declarative model to set up a role for testing; makes it relatively straightforward to generate S3A filesystem instances with reduced rights and see that things fall back.

# the post-create actions are happy with perm failures, they just end up leaving markers around. So I'm not worried about 
# the pre-create actions look for the parent entry & make sure it isn't a file; maybe handle a permission failure there? 
# big issue is rename, especially with S3Guard involved

HADOOP-15183 covers that. If you try to rename a directory and you don't have read or delete perms to a path underneath, rename() fails *without updating s3guard as to its current state*. It needs to sync up & add tombstones for all deleted entries, new entries for created stuff. 

* Delete will also need to add the tombstones for all successful deletes, 
* createFakeDirectory to handle failure on the GET or the PUT
* maybe also: add specific metric for permissions failures 

rename() is what worries me the most with s3guard, as its the one which will end up most inconsistent. 

Everything is setup to add tests for these; harder part is fixing. Thank you for volunteering :) 





, +the other permissions issue to handle is *caller doesn't have access to the KMS key to read a file. rename() will fail there in the COPY; delete & HEAD checks will be OK. I think the error is slightly different...it'd be good to actually expand that text better to make clear its a KMS permission, so not mislead people into thinking it is bucket access. I don't know about other people, but it's confused me in the past, Also: we assume caller has full read access up the stack. May not hold...don't know what to do there]