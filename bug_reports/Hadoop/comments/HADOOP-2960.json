[Computing the number of unique keys is not free. In particular, it will cost O(N) comparisons. Even worse, this doesn't scale. Currently, combiners are only applied on the original spill, where your approach could be done. However, we plan to apply combiners every time you write to disk during the merge sort. There, you certainly can't count the duplicated keys without a prohibitive cost. 

-1

Once we have HADOOP-2399, almost any reduction in the cost of network and disk i/o should be worth the cost of the combiner., 
Uniq key counting can be done as a part of sort. YOu don't need extra computing at  all.

I don't think the overhead of calling combiner can be dismissed.
It does not make sense to call it most keys are unique, which is very common if the number of reducers is large.
, Closing at won't fix, given the -1.]