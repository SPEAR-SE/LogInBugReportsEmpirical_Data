[We can't always rely on cleanup/finally stuff to run.  JVMs can exit unexpectedly.  We hope it doesn't happen often, but we must be able to handle that situation.  If we need to, e.g., clean up temp files, we do that on startup.

The reason this was added was to handle the case where the tasktracker has exited and the child is somehow hung.  We must not leave stray, hung, JVMs around.  Thread.interrupt() is not reliable enough.  When a thread is hung, it will not recieve an interrupt.  I've seen this frequently when fetching, where socket read()  requests hang indefinitely, despite the socket having a short read timeout.

So I'd be happy to have this first try to exit more gracefully, but, after a time, it should still call exit().  The child processes do not have a pid file.  Once their parent has died, nothing tracks them, so they must reliably exit fairly quickly when their parent dies., Ok, I can see wanting to try gently first and then pull out the exit sledgehammer if it doesn't work.

Checking back with the taskTracker once a second seems too frequent.

I'd also propose changing the ping message to return a boolean that means the task should exit, so that IOException can mean that there was a communication error. If there is a communication error, it would make sense to retry a few times., Sure, that would be safer, but recall that this communication is all on the same host.  A tasktracker shouldn't have more than a handful of children, so per second pings should not be a great burden.  And communications problems to localhost seem unlikely.  I've seen nodes with loads over 100, timing out all sorts of requests from other hosts, and I've never seen "Parent died" logged when a tasktracker was really still alive.  But, still, it shouldn't hurt to try a few times., That said, it's possible that the exit() somehow swallows the "Parent died" message and all that we see is the "Child died with nonzero exit status" message.  We do see those more than we'd like.  I don't see how the message could get swallowed, however.  The parent has a thread reading the standard error of the child to EOF.  The logged message should get flushed to standard error before the child process exits.  And the kernel shouldn't throw away the last buffer written when a process exits.  So I doubt that's happening., I've been assuming that the "Child died with nonzero exit status" with no other error messages is coming from the failure of a ping. I will change that message to include the actual exit status and change the ping-failure exit code to 123 or something., Sounds like a good plan., The results are interesting. Based on a 45 minute run (on 195 linux nodes) of my random writer, I got:

226 instaces of exit == 143
11 instances of exit == -113
  2 instances of exit == 65   // exception thrown on the ping
  0 instances of exit == 66  // task tracker not recognizing the task

So there is a lot of dying going on that I don't understand. Tomorrow, I'll make a patch for my code that  changes ping from "void ping(string)" to "boolean ping(string)" where false means that the task is unknown and exceptions get a second chance. Do you happen to recognize either 143 or -113? Doing a quick search in eclipse, I didn't see them., Searching around a bit, I found a few folks who suggest using -Xrs if you're seeing exit code 143, since this can be caused by signals.  So you might try adding -Xrs to mapred.child.java.opts and see if that helps any., The 143's could be normal.  We call process.destroy() to stop a task. This probably sends a SIGTERM, and SIGTERM causes an exit 143, according to http://scv.bu.edu/SCV/FAQ/batchcode.txt.

, Here is a patch that allows retries for communication problems on the ping from the Task to the TaskTracker. It changes the interface of ping from returning nothing to returning a boolean. A false return value means the child should immediately go away. Exceptions are treated as temporary problems and given 3 chances. This patch also changes the exit code to be either 65 (asked to kill self) or 66 (ping exception)., I just committed this.  Thanks, Owen!]