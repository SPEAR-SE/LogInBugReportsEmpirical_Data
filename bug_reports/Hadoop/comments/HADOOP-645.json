[Not sure if it's just mis-stated, but -reducer NONE doesn't imply no output by reducer, it implies that the mapper itself writes out its output as a dfs file, and the reducer is never even launched. It save transfer of data over the network, sorting by the reducer etc.
-Yoram, Is this a streaming specific discussion, or does general map-reduce really imply that map outputs go directly to HDFS when no reducers are run?  I'm not sure that is a viable scalable behavior.  This may simply create way too many outputs and kill the FS.

Do we believe this is the current behavior or an ask for an extension?, it's streaming specific.

That said, with each map task normally working on one entire dfs block but sometimes working on an entire file (like in the case of gzipped data), generating one file per map will result in fairly large files on output. If the imput was a bunch of small files to begin with, the output is no worse than the input.

With iterative jobs in particular, where the job output is the input to the next job and is really temporary, it is very reasonable to skip shuffling the data and sorting it if possible., This fixes the Exception generated by the streaming job that is submitted with -reducer NONE. The outut of map goes directly into files in HDFS., This fixes the Exception that was being raised when a streaming job with -reducer NONE was submitted., I just committed this.  Thanks, Dhruba!, This issue is not completely fixed. My fix improves the situation but does not solve all of the issues reported:

it is completely broken --
(a) the task fails       ---  THIS PART IS FIXED BY MY PREVIOUS PATCH
(b) a file is create for the output instead of a directory.
(c) there is no way to understand what is going on from the client output

I can produce an example for you, if you like -- but the behavior is consistent, so $HSTREAM -mapper /bin/cat -reducer NONE should show the problem, Let's create a new bug for the other issues.]