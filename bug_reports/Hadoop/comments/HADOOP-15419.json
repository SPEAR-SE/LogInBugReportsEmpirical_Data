[[~Tao Jie] 
{quote}In viewfs situation, we can just obtain delegation token from specific namenode rather than all namenodes.{quote}
It sounds a good idea, but I am confused that how to take out `specific namenode` we need to obtain delegation token from.
In fact, some request path may be found after task running (for one simple instance: hard code path in MapReduce/Spark job), if we don't obtain delegation token for that NameNode, the Job will be fail due to all tasks can not pass authentication.
In one word, It seems that we have to obtain all delegation token for all namespace under current token mechanism.
If I am wrong please correct me., [~hexiaoqiao] thank you for your comment.
{quote}
In fact, some request path may be found after task running (for one simple instance: hard code path in MapReduce/Spark job), if we don't obtain delegation token for that NameNode, the Job will be fail due to all tasks can not pass authentication.
{quote}
Agree. For the cluster, it is not easy to know which namenodes a certain job would access. I think the mechanism could be more flexible and obtaining tokens from all namenodes seems to be too crude.
1, We can have a option maybe {{fs.viewfs.use.specific.filesystem}}, only when this option is true, the following logic works.
2, When submit a mr/spark job, if the input/output path is a viewfs path, instead of obtaining token from all namenode, we would visit and fetch token from only a SET of filesystem.
3, The raw filesystem of the input/output path should be in the SET
4, We may have a global option like {{fs.viewfs.global.filesystem}} which defines filesystems that all jobs may visit(Eg. the filesystem of tmp dir, scratch dir), and it should be added into the SET
5, Job-level option like {{fs.viewfs.additional.filesystem}} which defiles extra filesystem that the certain job need.
Since obtaining delegation tokens happens on the client side, the effect of the modification would be controllable.  
Any thought?, [~Tao Jie], I echo [~hexiaoqiao]'s concerns. Your recent proposal sounds like it is defeating the point of ViewFS, which is to hide the physical location details of data. If you have to be aware of physical locations of all of the data you are reading, and set configurations accordingly, it seems to me that you have lost the advantages of ViewFS. If you aware you are only using a single physical cluster, why not just bypass ViewFS and use the URI directly..?, [~xkrogen] thank you for your comment. I understand your concerns. Actually I don't want to break any existing logic or assumptions for viewfs. We just want add another option for the user which somehow like a {{hint}} to viewfs which could help to reduce request to the cluster.  ]