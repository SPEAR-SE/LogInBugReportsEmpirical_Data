[Mostly this sounds good to me.

> 1. change reportChecksumFailure parameter crc from int to FSInputStream

I'm confused by this one.  There's already an FSInputStream parameter.  In the DistributedFileSystem implementation of this method, one can cast this to DFSInputStream and then access whatever implementation-specific state is needed (like the datanode where the block in question resides).  So I see no need to alter the reportChecksumFailure signature.
, What I meant by that change is to pass in two FSInputStreams (and range information).  We have an FSInputStream for the data file, but we also need one for the checksum file to be able to delete the corrupted block from the checksum.   

In my first pass, when the checksum doesn't match, both the data and checksum blocks will be deleted.  Should we also try to figure out which of the two was corrupt?   Will the extra effort be worth the gain in not rewriting a block that was not actually corrupt?

  
, > we also need one for the checksum file

Ah, I get it.  That sounds reasonable.  Thanks for clarifying!

> Should we also try to figure out which of the two was corrupt?

I don't think it's worth it, at least not for the first pass.

What should we do if the replication level is one for the corrupt block?  Keep it, I think., If the replication level is one, then we will keep the corrupt block and report the error.   , DFSInputStream also needs a method to return the current block. We could either have a memember variable to keep track of the current block or calculate the block based the member variables pos and blocks., I've added a member variable to keep track of the current block.  , We should probably call the protocol method something like invalidateBlock() rather than deleteBlock(), since it only deletes it if has replicas.

Also, does this issue include HADOOP-731 or not?  If so, then it needs more work.  If not, we should re-open HADOOP-731., This patch does not include HADOOP-731, I've re-opened it., 1. src/java/org/apache/hadoop/dfs/NameNode.java
     In deleteBlocks, I would not enforce one location per block.
2. src/java/org/apache/hadoop/dfs/FSNamesystem.java
     In deleteBlocks, should we make sure that the remaining containing datanodes contain at least one non-decomissoned/ing datanode when deciding if a block should be deleted? We need to check if the block is underreplicated before putting it to needReplications. Also need to check if the block should be taken out of excessReplicationMap., Thanks Doug and Hairong for looking over the patch. 

a. I'll change the name from deleteBlocks to invalidateBlocks
b. I'll take away the one location enforcement in NameNode
c. I'm going to call removeStoredBlock to update the data structures in invalidateBlock.  We need to do everything it does.  (It updates blocksMap, neededReplications, and excessReplicateMap)
d. I agree with Hairong that we should not delete the corrupt copy if it is the only one on a live node, in case the decommissioned nodes are taken down.  I will implement it this way unless people disagree. , I've attached another patch which includes the points mentioned in my previous comment with one change.  Instead of invalidateBlocks, I called it reportBadBlocks because others pointed out the namenode can choose to do whatever it wants with this information from the client, not necessarily delete or invalidate the blocks.
, +1, because http://issues.apache.org/jira/secure/attachment/12348878/hadoop-855-7.patch applied and successfully tested against trunk revision r495045., Wendy,

This patch has fallen out of date, since HADOOP-803 and HADOOP-842 were committed.  Can you please update it?  Thanks!

Doug, Updated the patch., I just committed this.  Thanks, Wendy!]