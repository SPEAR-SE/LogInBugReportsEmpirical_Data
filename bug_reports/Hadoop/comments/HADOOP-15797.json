[Let's see what [~aw] says about -builtin vs -optional.

# we need to document all this stuff a bit better
# if hadoop_add_to_classpath picks up the -optional files, then everything will be consistent. I'd be happy with that.
, First, let's put the side the issue of s3guard. It breaks things, as we'll see in a bit.

Second, let's also remember that the whole point of this code is to pull things OUT of the default classpath.  

Now, what does 'builtin' and 'optional' mean?

builtin = required by a command.  For example, hadoop distcp requires it's jar at runtime.  It is not needed any other time, so it doesn't make sense to put it AND ANY DEPENDENCIES on the classpath all the time.

optional = optional features the USER wants to enable.  All of these features need to always be available at runtime. Prior to s3guard, this was ALL of the non-core file systems: S3, Azure, etc, etc.  Users enable these features using the HADOOP_OPTIONAL_TOOLS environment variable. Again, if I don't access S3 from my cluster, I don't want the AWS jars AND ANY DEPENDENCIES on the classpath.  

It's also worthwhile pointing out that removing all of these jars from the default classpath, in addition to allowing more user freedom, greatly speeds the system up when measured across all java launches.

That said, it is now easy to see the problem that s3guard presents and how it is an outlier.  s3guard is a built-in command that depends upon components are also optional. IMO: using s3guard to determine any sort of functionality for the rest of the system is completely and totally wrong.

That said, what makes anyone think that "hadoop_add_to_classpath_tools hadoop-azure" should work?  optional bits come as shellprofiles, not as hooks for built-ins.  I mean the documentation here literally says:

{code}
## @description  Run libexec/tools/module.sh to add to the classpath
## @description  environment
{code}

If you want per-user settings for this (which is also weird, but whatever), then modifying .hadoop-env is the way to go., Allen, thank you for your explanations. 

bq. if I don't access S3 from my cluster, I don't want the AWS jars AND ANY DEPENDENCIES on the classpath.

+1. It's only a source of pain, and, with a 50MB shaded JAR, startup hit even though you do at least get to avoid jackson version conflict.


FWIW, I quite like the {{hadoop_add_to_classpath_tools}}  command, because it makes it easier to add multiple tools to the CP in separate scripts. working with the classpaath env var seems like a step backwards.

Anyway, the use case the object store ops are trying to address is "add support for a non-HDFS filesystem" to the CP. That may be optional (s3a), or, as in the case of wasb/adl/abfs with Hadoop running in Azure, absolutely mandatory for anything to work.

The metric for CP setup is always the same:

Does {{hadoop fs -ls <my store URL>}} work? If it does, distcp and everything else should also kick in, maybe even HBase.


, 
bq. working with the classpaath env var seems like a step backwards.

Just to be explicit about it, enabling Azure, etc, is not about users modifying the classpath directly.  That's always dangerous.  But it is about modifying the HADOOP_OPTIONAL_TOOLS var as documented in hadoop-env.sh (as well as a few other places):

{code}
# Enable optional, bundled Hadoop features
# This is a comma delimited list.  It may NOT be overridden via .hadooprc
# Entries may be added/removed as needed.
# export HADOOP_OPTIONAL_TOOLS="hadoop-aliyun,hadoop-aws,hadoop-azure,hadoop-azure-datalake,hadoop-kafka,hadoop-openstack"
{code}

bq. The metric for CP setup is always the same:
bq. Does hadoop fs -ls <my store URL> work? If it does, distcp and everything else should also kick in, maybe even HBase.

Using .hadooprc, hadoop fs -ls can work but the others may not since distcp, HBase, etc require the entire cluster to be configured.  This is why these settings are hadoop-env.sh config and not a hadooprc config., Makes sense to me. I'll close this and file a separate one to see if there's a good way to have the "hadoop s3guard" CLI command exist without hadoop-aws being marked as a required module. If the command exists but fails with a nice error message if someone tries to use it without adding hadoop-aws to HADOOP_OPTIONAL_TOOLS, that would be good enough for me., See HADOOP-15803.]