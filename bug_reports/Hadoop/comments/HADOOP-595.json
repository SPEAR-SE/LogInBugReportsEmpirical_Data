[it works fine for me, using the default config: namenode uses info port 50070, datanode uses 50075, My steps:
1. Unpack hadoop-0.7.0.tar.gz into /root/hadoop-0.7.0
2. Set export JAVA_HOME=/usr in hadoop-env.sh
3. Change hadoop-site.xml:

<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
  <name>fs.default.name</name>
  <value>localhost:65408</value>
  <description>The name of the default file system.  Either the
  literal string "local" or a host:port for DFS.</description>
</property>

<property>
  <name>mapred.job.tracker</name>
  <value>localhost:65472</value>
  <description>The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  </description>
</property>

<property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication. 
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>

</configuration>

4. cd /root/hadoop-0.7.0/bin
5. ./start-dfs.sh   -> OK
6. Connect http://localhost:50070   -> OK
7. Click "Browse the filesystem"   ->

An error occurred while loading http://localhost.localdomain:50070/nn_browsedfscontent.jsp: 
Could not connect to host localhost.localdomain (port 65535). 
, Has anyone seen this recently?, This is a really old bug (0.7.0). Webui is expected to work and there are no known bugs about simple uses. I am closing this. ]