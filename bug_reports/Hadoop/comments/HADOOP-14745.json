[So i think we should call getFileStatus with explicitly expect,  e.g.
getFileStatus(path,true)
true means we think the path we input is a directory

In internal of getFileStatus, i think the call like above will skip the first two getObjectMetadata call, and only list the object under this given path, which also benefit the performance of this network consuming function. 

, On the other hand, I saw hdfs implementation not allow existing any file with the same name of directory under it's parent path.
 , You shouldn't have a file and path with the same name: we don't like it & our attempt to make S3 look like a filesystem depends on this.

I don't believe you could have created this path structure through S3A itself. Try it to see: create a dir then create a file with the name and the / prefix removed. The getFileStatus() call inside open() should pick up the dir and reject the call.

if that doesn't happen: bug in S3A. If it does happen, closing this as a WONTFIX. Sorry, I create that path structure through s3cmd, and yes , we can't do this by s3a itself. 
But, s3 or s3-compatible storage allow them existed within the same folder, and the common workload running on Hadoop on Ceph is data analysis, which means the data in these storage should be store by other way instead of s3a, and read data from storage through s3a, which can't stop this issue happen again.  , Sorry, no. it is possible to create s3 data through other tooling with names s3a can't handle

* data in path/
* data in path////path2
* +probably some characters in path elements we don't allow

Not only that, s3a can and will delete what it thinks are mock directories, e.g /path/, without even checking to see if they contain data. 

You're going to have to set up a process which doesn't create paths which confuse S3a. Hadoop s3a makes sure it does this itself. For other tools, you are going to have to be aware of the limitations and avoid them.

Sorry. Closing as a WONTFIX, for reasons discussed.]