[Hi [~lushuai] thanks for reporting the issue. You can try one of the following:

# What you described seems to suggest YARN-2964, which is fixed in Hadoop 2.7.0. So upgrade to that version may solve the issue.

# Can you try to set the property {{mapreduce.job.complete.cancel.delegation.tokens}} to {{false}} and see if that works? https://hadoop.apache.org/docs/r2.6.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml

# Check if any of your hosts in the cluster has clock screw., Hi  Wei-Chiu Chuang  thanks for fix the issue.  

The cause of this problem is  Multiple Instances of KMS， Behind a Load-Balancer or VIP 。  if  only one Instances of KMS its OK, Multiple Instances of KMS not work, and throw the above  exeption logs.

1.   YARN-2964 is not the question;
2.   set the property mapreduce.job.complete.cancel.delegation.tokens to false   Not solve the problem
3.   the cluster  clock is OK.

, Instead of running multiple KMS instances behind load balancer, you can use LoadBalancingKMSClientProvider to achieve KMS HA. This is an undocumented feature added in HADOOP-11620. I'm working to document this feature soon.

Basically, your hadoop.kms.key.provider.uri in kms-site.xml should be something similar to:
{{kms://https@kms01.example.com;kms02.example.com:16000/kms}}. That is, use semicolon to separate multiple kms instances., bq. Basically, your hadoop.kms.key.provider.uri in kms-site.xml should be something similar to: kms://https@kms01.example.com;kms02.example.com:16000/kms. That is, use semicolon to separate multiple kms instances.
One correction is the parameter is called {{dfs.encryption.key.provider.uri}}, and should be set across-cluster (e.g. in hdfs-site.xml), not just kms-site.xml since the clients need it as well.

bq. The cause of this problem is Multiple Instances of KMS， Behind a Load-Balancer or VIP 。 if only one Instances of KMS its OK, Multiple Instances of KMS not work, and throw the above exeption logs.
Feels to me that this either means the load balancer behind the KMS needs to be further looked at, or somehow multiple KMS' weren't setup correctly to share the secrets. The secret sharing should be done using zookeeper. See https://hadoop.apache.org/docs/r2.6.2/hadoop-kms/index.html#HTTP_Authentication_Signature for an example., [~xiaochen] thanks, you're right! It's a client-only configuration. So it's the {{dfs.encryption.key.provider.uri}} in hdfs-site.xml that should be configured.]