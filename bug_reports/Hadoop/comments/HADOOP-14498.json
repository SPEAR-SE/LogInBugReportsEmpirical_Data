[OK, using 'hadoop --debug classpath' we can see exactly what is happening with the specific construction of "hadoop-azure,hadoop-aws,hadoop-azure-datalake"

{code}
$ bin/hadoop --debug classpath 2>&1 | grep PROFILES
DEBUG: HADOOP_SHELL_PROFILES accepted hadoop-aws
DEBUG: HADOOP_SHELL_PROFILES accepted hadoop-azure-datalake
DEBUG: HADOOP_SHELL_PROFILES declined hadoop-azure
DEBUG: HADOOP_SHELL_PROFILES accepted hdfs
DEBUG: HADOOP_SHELL_PROFILES accepted mapred
DEBUG: HADOOP_SHELL_PROFILES accepted yarn
{code}

hadoop-azure is getting rejected by the shell profiles code because it is getting up in the dedupe pattern match code.  Converting this to use the new array code added in HADOOP-13595 will probably fix this.  Flipping the order will also make it pass.
, Thanks [~aw] for the prompt debugging. Your analysis makes sense to me. I'll have a look at [HADOOP-13595]., Now that HADOOP-13595 is in I had a look at this. It is indeed the regex matching logic in hadoop_add_param being overzealous. That function does already state an assumption that it is space-delimited, so I used spaces or line boundaries to ensure a full-word patch instead of just a partial one. Rather than have it use arrays, let's fix this function for all the other places it's used instead of using something from HADOOP-13595 instead.

Attaching a patch. I got a clean Yetus run locally, but it wasn't running bats tests for some reason. I ran the hadoop_add_param tests manually but not the others, and some unrelated Java test failures seem to be blocking it (unless there's a way to bypass that and easily run all bats tests?), The thing is, is that add_param was really meant for dealing with de-duping stuff like HADOOP_OPTS.  arrays are probably a better choice here because we know we can do an exact match., The -00 patch will likely break the world.

e.g.:

{code}
hadoop_add_param HADOOP_OPTS Xmx "-Xmx${HADOOP_HEAPSIZE_MAX}"
{code}

will now add multiple Xmx lines., Aah that makes sense. I guess that's what the comment means by "different syntaxes". Testing a patch now that uses an array for HADOOP_SHELL_PROFILES and a corresponding test., Attaching said patch.... It'd be nice to replace that inline loop with a join function. I don't immediately see one - do we have a file for non-Hadoop-related helper functions like that? Or should I just throw it in hadoop-functions.sh? (logging off for the night - will iterate tomorrow)., bq. I guess that's what the comment means by "different syntaxes". 

Yeah.  In the beginning I was trying really really really hard to avoid arrays, for lots of reasons.  One of the big ones that I'm willing to write down was that not all of the array functions are available in our target bash 3.2.  (e.g., associative arrays, mapfile, etc).   Plus backward compatibility with the raw string format of HADOOP_OPTS + trying to solve the duplicate parameter problem led to add_param.  It's not pretty and I'm not proud of it.  But it works.

Sidenote:

HADOOP_OPTS is probably at this point the biggest hindsight-20-20 mistake in Hadoop.  I don't think people really understand how much of an impact it's had on literally everything in the system.  For example, it's *the* reason that spaces in file paths are a complete nightmare.  HADOOP-13365 is my attempt at fixing it.  I'm not sure if it makes it worse or better though.

bq.  It'd be nice to replace that inline loop with a join function. 

Yeah, I'd love for someone to take another whack at it. I can't remember what all I tried before I ended up just settling on the loop.  I seem to recall I had a better way, but it only worked with bash 4.x.  I guess we could always put a version check in there.  (There's one or two other places like that already.)

bq. Or should I just throw it in hadoop-functions.sh?

I've just been throwing everything into hadoop-functions.sh, as it ends up creating one big API doc at mvn site time.  Pretty convenient. , | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 23s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 23s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 15s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} shellcheck {color} | {color:red}  0m  4s{color} | {color:red} The patch generated 4 new + 20 unchanged - 0 fixed = 24 total (was 20) {color} |
| {color:green}+1{color} | {color:green} shelldocs {color} | {color:green}  0m  9s{color} | {color:green} There were no new shelldocs issues. {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 50s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 15s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 19m 46s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HADOOP-14498 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12880140/HADOOP-14498.002.patch |
| Optional Tests |  asflicense  mvnsite  unit  shellcheck  shelldocs  |
| uname | Linux de9b9a42bfa8 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 79df1e7 |
| shellcheck | v0.4.6 |
| shellcheck | https://builds.apache.org/job/PreCommit-HADOOP-Build/12934/artifact/patchprocess/diff-patch-shellcheck.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12934/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12934/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, .003 patch address shellcheck issues and refactors the loop into a shared hadoop_join_array function., | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 27s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} shellcheck {color} | {color:green}  0m  4s{color} | {color:green} There were no new shellcheck issues. {color} |
| {color:green}+1{color} | {color:green} shelldocs {color} | {color:green}  0m  9s{color} | {color:green} There were no new shelldocs issues. {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  2m  6s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 20m 46s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HADOOP-14498 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12880256/HADOOP-14498.003.patch |
| Optional Tests |  asflicense  mvnsite  unit  shellcheck  shelldocs  |
| uname | Linux b531bc262252 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 293c74a |
| shellcheck | v0.4.6 |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/12943/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12943/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Hey [~aw], do you have time to review Sean's patch?, I'm dropping this in priority after offline discussion from Sean., I think this fixes the immediate problem, but I'm wondering if we shouldn't purge the usage of hadoop_add_entry and hadoop_verify_entry while we're here... especially since it is only used by the HOT code anyway.]