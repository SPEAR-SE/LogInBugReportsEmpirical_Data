[The default ports that Hadoop uses are completely unreasonable.  I understand that we can change these ports.  Ideally, Hadoop should be promoting good practices out of the box., I think the history is that originally Hadoop scanned from an initial port, so each was effectively the beginning of a range.  But that scanning has mostly (entirely?) been removed., The problem is that out of the box Hadoop uses 8020, 50010, 50075, 50020, 50070, ....  Is there some reason these ports can't be in order.  Say 50010, 50011, 50012, etc?  Is there a dartboard somewhere with the 500x0 numbers on it? :)

, > Is there some reason these ports can't be in order. Say 50010, 50011, 50012, etc? 

Not that I know of, except back-compatibility.
, Although port 8020 may be in the code, it's not explicitly mentioned and/or overridden in hadoop-defaults.xml:

{code:xml}
<property>
  <name>fs.default.name</name>
  <value>file:///</value>
</property>

<property>
  <name>dfs.datanode.ipc.address</name>
  <value>0.0.0.0:50020</value>
  <description>
    The datanode ipc server address and port.
    If the port is 0 then the server will start on a free port.
  </description>
</property>
{code}

The quickstart doc uses port 9000 for fs.default.name:
{code:xml}
<property>
  <name>fs.default.name</name>
  <value>localhost:9000</value>
</property>
{code}

We need to make sure that when we consolidate, we make changes in the code, the default configs and the forest docs simultaneously.
, Ops teams are expected to reconfigure the ports via the configuration files instead of using the defaults.  Closing as won't fix.]