[in 0.13.x multiple close() was allowed unintentionally I think, second close was hidden from FSInputStream by BufferedInputStream in between.

Many Java streams seem to tolerate closing more than once, may be Hadoop should too., A work around for user code affected by this is to use a {{BufferedInputStream(fsDataInputStream)}}., This has been the behavior for 3 releases now, closing this as wont fix. , > closing this as wont fix. 

+1, I've been seeing this when terminating services that have a DFS client. There's  no isOpen() call so all we can do is try and close any non-null dfs refereence and print the exception.

The exception could be caught and swallowed, but how do you differentiate a harmless "Already closed" exception from a harmful "we couldn't write the data and your work is lost" exception?

A backward compatible solution would be to have a special subclass of IOException to indicate DFS already closed, something callers could catch and ignore., > A backward compatible solution would be to have a special subclass of IOException to indicate DFS already closed, something callers could catch and ignore.

Or it would be simpler to just swallow multiple closes as other java streams. 

Ideally caller of the close() should be careful not to close twice (such a thing a really bad bug while dealing with file descriptors in a C program, for e.g.). May be the intention behind resolving this "Won't Fix" is to force this good practice on the user.  But unfortunately some java libraries don't make that easy (they just close the stream, though they haven't opened it). My vote is to go with the convention. 

 , The current behaviour is at odds with the java.io.Closeable interface, which states that " If the stream is already closed then invoking this 
method has no effect. ". This is the interface that DfsClient.close() implements. ]