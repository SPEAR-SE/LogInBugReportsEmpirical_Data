[Debugging this issue, I found the cause to be a timing problem that does not happen on all machines.

In the HodRing, we have code that determines if a launched Hadoop command has exited with a non-zero error code, and in such cases an error is reported back to the ringmaster. The check is made soon after the command is launched. On some machines, the time limit between launching the command and its exit with the error code is a few 100s of milliseconds. On such machines, the code determining whether the Hadoop command exited thinks that all is fine, and fails later when it tries to check if the JobTracker's Jetty server is up. In the process it loses about a minute's time.

If the max-master-failures variable is > 1, a second attempt is made to launch the JobTracker. On similar hardware and configuration, the same timing issue shows up. By the time 2 machines have failed, the HOD client times out waiting for the JobTracker URL and the cluster is deallocated by deleting the Torque job.

This is a fairly serious issue, because it nullifies the enhancement made in HADOOP-3184, as the JobTracker is not launched on enough machines to give it a chance of coming up on a good machine.

Introducing a minor delay of just a second in the HodRing code fixed the problem that is described above. It seems fair to wait a bit for the Hadoop command to actually exit (if there are errors) before checking for it's error code., Inserting delays to fix synchro problems is a perilous course.   Can we do something better, like alternate between checking for an error code, and checking if the jetty interface came up?  Is there at least a principled way to pick the waiting period here?  , Hemanth, this was marked a blocker, but had no release assigned.  Setting Fix Version to 0.18.  Please correct that if I'm wrong.

, The attached patch addresses the issue of missing the hadoop command's exit code due to timing issues., bq. Inserting delays to fix synchro problems is a perilous course. Can we do something better, like alternate between checking for an error code, and checking if the jetty interface came up?

Ari, excellent suggestion. I have done just that in the attached patch. So, what happens now is that even if the command's exit code is missed the first time, I continue to check for exit status while checking for the jetty interface status. Thus, at some point if the command is found to have exited, the code returns immediately with a good error message.

I tested this patch by introducing spurious failures on a test cluster. I was able to validate that the correct code paths were covered. However, some testing in a more controlled environment where we can control the allocated nodes and set them up for failure could help. Karam, can you please take this patch, and re-run your tests., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12383920/3531.patch
  against trunk revision 667040.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2649/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2649/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2649/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2649/console

This message is automatically generated., To verify this issue did the following -:
1. Tried a scenario where --gridservice-mapred.pkgs and --gridservice-hdfs.pkgs paths correct on three nodes and max-master-failure=12. Tried successfully hod allocation with 15 nodes three times and monitored the ringmaster log-:
    a. namenode came up  in 2nd retry. jobtracker came in 4 retry after 3 failures.
    b. namenode came up in 9th retry  after 8 failures. jobtracker came in 1st try.
    c. namenode came up in first try. Jobtracker came up in 3 retry after 2 failures.
2. Tried a scenario where --gridservice-mapred.pkgs path correct on two nodes and max-master-failure=13 using static dfs. Tried successfully hod allocation with 15 nodes 4 times and monitored the ringmaster log-: jobtracker came in first try for 3 allocations. In 4th allocation jobtracker came up in 8th retry after 7 failures.
3. Tried a scenario where --hodring.java-home correct only on ringmaster, with max-failures=12. namenode came up on ringmaster node. All other 14 hodrings failed to start with "Invalid --hodring.java-home" error (observed from ringmaster log). ringmaster waited 2 mins for mapred before giving up
3, Tried a scenario where --hodring.java-home correct on 3 nodes , with max-failures=12. Tried hod allocate 15 nodes. namenode came up on ringmaster node.12 hodrings failed with invalid --hodring.java-home error.  jobtracker, dn and tt came up on remaining two nodes
    
Also tried some negative test with max-failures= 2-:
1. Provided wrong --hodring.pkgs. Verified that hod allocation fails as ringmaster failed with proper error message.
2. Provided wrong path for --gridservice-mapred.pkgs and --gridservice-hdfs.pkgs. Verified that proper error message from ringmaster log displayed at hod client side. Also tried with invalid tarball
3. Tried a scenario with --gridservice-mapred.pkgs and --gridservice-hdfs.pkgs path correct only ringmaster node with max-master failures =2
    Tried two times -:
    a. hod allocation failed as jobtracker failed to start with proper error message and ringmaster log also showing -:Detected errors (3) beyond allowed number of failures (2). Flagging error to client
   b. hod allocation failed as namenode  failed to start with proper error message and ringmaster log also showing -:Detected errors (3) beyond allowed number of failures (2). Flagging error to client
, I just committed this. Thanks, Hemanth!, Integrated in Hadoop-trunk #521 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/521/])]