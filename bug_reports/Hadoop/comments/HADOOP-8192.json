[While debugging this issue it was observed that the order by which the racksToBlocks HashMap gets populated seems to matter. As per Robert Evans and Devaraj Das, it appears by design that the order should not matter. 

 The reason order happens to play a role here is that as soon as all the blocks are accounted for, getMoreSplits() stops iterating through the racks, and depending upon which rack(s) each block is replicated on, and depending upon when each rack is processed in the loop within getMoreSplits(), one can end up with different split counts, and as a result fail the testcase in some situations.

Specifically for this testcase, there are 3 racks that are simulated where each of these 3 racks have a datanode each. Datanode 1 has replicas of all the blocks of all the 3 files (file1, file2, and file3) while Datanode 2 has all the blocks of files file2 and file 3 and Datanode 3 has all the blocks of only file3. As soon as Rack 1 is processed, getMoreSplits() exits with a split count of the number of times it stays in this loop. So in this scenario, if Rack1 gets processed last, one will end up with a split count of 3. If Rack1 gets processed in the beginning, split count will be 1. The testcase is expecting a return value of 3 which is the value returned if running on Sun JVM, but a value of 1 or 2 may be returned depending on when rack1 gets processed.
, One option is to make the relevant hashmaps sorted in CombineFileInputFormat, and then fix the testcase to check the correct (and consistent) values in the asserts. Would that fix the testcase problem? The other option is to make the testcase more robust so that it can tolerate the fact that ordering could be different on different JVMs. , Basically somehow make the order of inserts/gets consistent across JVMs by using appropriate datastructures...,  I think it would be extremely useful to understand if CombineFileInputFormat's getSplits() is working as designed -- Should the method  exit out of the loop as soon as all blocks are accounted for. If so, the next question is - Is this is a testcase scenario issue? --  In a production environment are there scenarios where a Datanode will contain all the blocks in a cluster?

 We would like to make sure that we are not fixing a non-existent problem to pass a test case, and inadvertently break the split functionality when it is working without issues. If this just happens to be a behavior issue between two JVMs for a given scenario then it might be more prudent to create separate test paths for each JVM. 
, I suggest that we split the JIRA into 3 separate ones, as they are technically unrelated, each requiring patches for trunk and branch-1. I might have some cycles to work on this in the coming week.

For CombineFileInputFormat, I'm -1 on creating separately test paths for each JVM, as it's a maintenance nightmare., Luke, please explain what you mean by "I suggest that we split the JIRA into 3 separate ones, as they are technically unrelated"..
, @Amir, 3 test failures have different root causes and require different patch review cycles. E.g., TestCombineFileInputFormat failure is a result of relying on undefined HashMap iteration order in the implementation of CombineFileInputFormat; TestStreamingBadRecords, AFAICT, only fails on plinux (you guys need to figure out the root cause for this); TestCapacityScheduler failures is a bit more serious, as it expose a JLS compliance bug (that can crash JobTracker) in the current code base as well as the HashMap order issue. Combining all these together is confusing for the patch review process., Luke - I need to correct a statement you made. These tests fail on IBM Java 6 on x86_64 as well, and the reason for the failure is as described in my earlier comment. So this is an IBM Java vs Sun JVM behavior rather than a plinux one.
I would still like to get a better understanding than just code comments - If getSplits() method in CombineFileInputFormat is working as designed. Can you suggest someone who has expertise on this?, @Kumar, Sorry, faulty memory :) I really meant TestStreamingBadRecords only fails on plinux after the compliance bug fix., @Luke, Thx. -- I am not sure what you mean by the compliance bug fix. , I meant JLS (Java Language Specification) compliance, which mandates certain properties for equality and comparison. ]