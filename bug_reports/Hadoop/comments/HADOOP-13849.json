[# you can see what code is used in the logs; if it says "java builtin" then it's using the java one; if it says system, then its using system.
# there are other factors in performance, like disk bandwidth. you may not get speedup.
# compare the decompress times too.

Closing as invalid, sorry
https://wiki.apache.org/hadoop/InvalidJiraIssues , Hi Tao Li!

Thanks for your effort to benchmark the two implementations. Are you proposing to make one faster than the other?, [~stevel@apache.org]
1. I saw the "using java builtin" or "using system-native" in my test cases log, so I am sure my test cases are correct.
2. My hardware CPU/Memory/Network bandwidh/Disk bandwidh are not bottleneck
3. I have also tested decompress speed. I even found that the "java builtin" is faster than "system native", Yes. I think the "system native" should have better compress/decompress performance than "java builtin"., Well, if you want to work on it, feel free. 

however, know that the native codec uses the standard {{libbz2}}; there's not much that can be done in the Hadoop code to speed that up other than any improvements in how data is moved between the Java memory structures and those of libbz...if there are memory copies taking place then that could be hurting performance. Anything that can help there would be good.


bq. I think the "system native" should have better compress/decompress performance than "java builtin".

That's something to explore. The latest Java 8 compilers are fast, and if the algorithms aren't doing lots of object creation, then bit operations in Java should be on a par with C-language actions against general registers. Where you would expect differences is if the native code uses some special CPU registers and operations (example, Intel SSE2) for significant performance. I don't know if bzip does that.

The fun part in benchmarking is isolating things. For codec performance, maybe have some test data being pre generated in CPU & cached in RAM. in standard formats (avro, orc), and the different codecs, then compressing that to RAM not HDD, so that the compression code is isolated from Disk IO, etc, etc. 

If the isolated native code is faster than the java one, then the implication is that the bottleneck is elsewhere in the workflow, not the codec. Again: that's interesting information.

bq. My hardware CPU/Memory/Network bandwidh/Disk bandwidh are not bottleneck

one of them is. Always â€”and it can be things like CPU cache latencies, excess synchronization in the code, even branch-misprediction in the CPU can hurt efficiency. FWIW, Flamegraphs are current the tool of choice for visualising performance during microbenchmarks



, That makes sense Steve and Tao Li! Thanks for your efforts. Please keep us updated if you find any bottlenecks. , [~stevel@apache.org] Thanks Steve. I will do some profiling in the future to find the bottleneck.]