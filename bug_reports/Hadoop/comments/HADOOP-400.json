[I do see one possible hole.  If a machine loses its TaskTracker, it gets a new one.  Can we arrange for the new TaskTracker to inherit the task failures from its predecessor?  That would be a bit hard ... but for this to work at all the tasks have to know what TaskTrackers they've flunked on.  All the TaskTracker has to know is who its predecessors are to refuse tasks that have flunked on its TaskTracker site [usually, on its machine].

-dk
, This patch does:
  1. It limits each TaskTracker to running
        min(tasksPerTracker, ceil(tasksLeftToRun/numTaskTrackers))
      this will prevent the problem that we saw where the last 2 reduces scheduled were put on the same node rather than different empty ones
  2. It refactors obtainNewMapTask and obtainNewReduceTask to call a common utility function. It also replaces the two parallel loops with one.
  3. Only allowed tasks that have failed on this task tracker to run if we have exhausted the cluster., I just committed this.  Thanks, Owen!]