[Snappy is currently un-detectable code wise, at least AFAICT, since it lacks a container format. However, yeah, we should check for SEQ magic header (and the likes) first I think., Yup. I'm cool with using extensions, but after we check for SEQ. Today, it's reversed. It checks the extension before checking for the magic header., This patch ought to take care of this. The reverse order is what Hue does as well, as I remember from my https://issues.cloudera.org/browse/HUE-1 patch.

I could not find tests for this command (or others) so haven't added any., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12530258/HADOOP-8449.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.fs.viewfs.TestViewFsTrash
                  org.apache.hadoop.ha.TestZKFailoverController

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1059//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1059//console

This message is automatically generated., As Harsh pointed out, there are no tests for the FsShell commands. The two failing tests look unrelated, so I'm +1 on the patch., bq. there are no tests for the FsShell commands.

There are many tests for the shell commands, but they're unfortunately in the HDFS sub-project, even though FsShell is implemented in Common. See: hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java, Thanks ATM. Totally missed that class. I'll improve the Text test in it to catch regressions in future, and provide a new patch. Cancelling current patch., Please link the hdfs jira to this one.  Pending tests, I think this looks good.  One trivial suggestion would be to move the codec stuff into a default case for the switch., bq. Please link the hdfs jira to this one.

I don't think there's a need for a separate JIRA, now that test-patch.sh supports cross-project patches., Attached patch addresses Aaron and Daryn's comments. Added test bits fail without the Display.java changes, expectedly. Passes otherwise., Which {{Display}} changes are you referring to?, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533528/HADOOP-8449.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 2 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.viewfs.TestViewFsTrash
                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1141//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1141//console

This message is automatically generated., I think he means he ran the new tests without the fix to Display.java and it failed as expected, but passes with the patched Display.java as desired., Looks good, please check if the javadoc warnings are related., The javadocs are from MAPREDUCE-4355 and will be addressed from its follow ups.

Thanks Aaron, Daryn and Joey! Committing by EOD to trunk and branch-2 unless there's any other comment., +1, Thank you Daryn. Committed to branch-2 and trunk., Integrated in Hadoop-Hdfs-trunk-Commit #2482 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2482/])
    HADOOP-8449. hadoop fs -text fails with compressed sequence files with the codec file extension. (harsh) (Revision 1355636)

     Result = SUCCESS
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355636
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java
, Integrated in Hadoop-Common-trunk-Commit #2414 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2414/])
    HADOOP-8449. hadoop fs -text fails with compressed sequence files with the codec file extension. (harsh) (Revision 1355636)

     Result = SUCCESS
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355636
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java
, Integrated in Hadoop-Mapreduce-trunk-Commit #2431 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2431/])
    HADOOP-8449. hadoop fs -text fails with compressed sequence files with the codec file extension. (harsh) (Revision 1355636)

     Result = FAILURE
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355636
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java
, Integrated in Hadoop-Hdfs-trunk #1092 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1092/])
    HADOOP-8449. hadoop fs -text fails with compressed sequence files with the codec file extension. (harsh) (Revision 1355636)

     Result = FAILURE
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355636
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java
, Integrated in Hadoop-Mapreduce-trunk #1125 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1125/])
    HADOOP-8449. hadoop fs -text fails with compressed sequence files with the codec file extension. (harsh) (Revision 1355636)

     Result = FAILURE
harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355636
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java
, Hi

We found the changes in order of switch and guard block in 

{code}private InputStream forMagic(Path p, FileSystem srcFs) throws IOException{code}

Because of this change, return value of {code}codec.createInputStream(i){code} is changed if codec exists.

h4. cdh3u3
{code}
private InputStream forMagic(Path p, FileSystem srcFs) throws IOException {
    FSDataInputStream i = srcFs.open(p);

    // check codecs
    CompressionCodecFactory cf = new CompressionCodecFactory(getConf());
    CompressionCodec codec = cf.getCodec(p);
    if (codec != null) {
      return codec.createInputStream(i);
    }

    switch(i.readShort()) {
       // cases
    }
{code}


h4. cdh3u5
{code}
private InputStream forMagic(Path p, FileSystem srcFs) throws IOException {
    FSDataInputStream i = srcFs.open(p);

    switch(i.readShort()) { // <=== index (or pointer) processes!!
      // cases
      default: {
        // Check the type of compression instead, depending on Codec class's
        // own detection methods, based on the provided path.
        CompressionCodecFactory cf = new CompressionCodecFactory(getConf());
        CompressionCodec codec = cf.getCodec(p);
        if (codec != null) {
          return codec.createInputStream(i);
        }
        break;
      }
    }

    // File is non-compressed, or not a file container we know.
    i.seek(0);
    return i;
  }
{code}, Thanks Muddy, silly mistake of mine. I filed https://issues.apache.org/jira/browse/HADOOP-8833]