[Fix it., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12487140/HADOOP-7478.patch
  against trunk revision 1147971.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/753//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/753//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/753//console

This message is automatically generated., It doesn't seem like FileUtil#copy should always delete the destination if there's an IOE doing the copy. For example, what if there's an error reading the source, overwrite is true, and the dest exists. We will have removed the dest but not necessarily replaced it with a new file, ie copy(x, y) should never result in y not existing right?  Please add tests that cover the current behavior (what happens when there's an IOE during copy) so we can see how the tests modify the behavior., I'm a bit uneasy about this change.  It feels like a lower level issue.  What's a real use case that causes the problem?  Is it something like the source file can't be opened due to permission issues?  If so, then the more appropriate change would be to open the source before opening the dest., The change is definitely not good to go (see my last comment). However I think the jira itself (not polluting the fs with 0 byte files when a put fails) is totally reasonable., In trunk we have o.a.h.fs.Options.Rename.OVERWRITE. We could modify fs -put to write to a .tmp file in the same directory, then rename with OVERWRITE to the final destination., Wouldn't that possibly lead to false quota issues?, Thanks a lot for your comments,@Daryn & @Eli & @Todd.
To Daryn:
   Reproduction steps:
   1)start DFS.
   2)kill all the datanodes. 
   3)put files into DFS.
     # ls /work/xg1
     -rw-r--r-- 1 root root 363 Jul 19 15:49 /work/xg1
     # hadoop dfs -put /work/xg1 /user/hadoop/
   4)# hadoop dfs -ls /user/hadoop
       Found 2 items
       -rw-r--r--   1 root supergroup         32 2011-07-26 10:05 /user/hadoop/xg
       -rw-r--r--   1 root supergroup          0 2011-07-26 10:05 /user/hadoop/xg1

To Todd:
   Thanks for your suggestion,but i think the problem will still exists while there`s an exception during putting file to the .tmp file.

To Eli:
   Before copying bytes really,the function checkDest() will check the problems such as "target already exists" and "target is a dir". And i think it is necessary to delete the dest file while there`s an error reading the src,because it is easy to mislead users if 0-byte file retained.

, Is the -put not generating an exception?  That implies that whatever is opening the src is not throwing an exception, which leads to the dest being opened & truncated.  The problem might be solved if the dst is never opened when the src fails to open., The exception is as follows(and i think it wasn't caused by opening src file but making the DataNode's block):

[root@localhost logs]# hadoop dfs -put /work/xg1 /user/hadoop
11/07/26 10:05:19 INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.ConnectException: Connection refused
11/07/26 10:05:19 INFO hdfs.DFSClient: Abandoning block blk_7177429611982124425_1004
11/07/26 10:05:25 INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.ConnectException: Connection refused
11/07/26 10:05:25 INFO hdfs.DFSClient: Abandoning block blk_9193825693957629504_1004
11/07/26 10:05:31 INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.ConnectException: Connection refused
11/07/26 10:05:31 INFO hdfs.DFSClient: Abandoning block blk_-7406363576735183504_1004
11/07/26 10:05:37 INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.ConnectException: Connection refused
11/07/26 10:05:37 INFO hdfs.DFSClient: Abandoning block blk_1387010999141076236_1004
11/07/26 10:05:43 WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

11/07/26 10:05:43 WARN hdfs.DFSClient: Error Recovery for block blk_1387010999141076236_1004 bad datanode[0] nodes == null
11/07/26 10:05:43 WARN hdfs.DFSClient: Could not get block locations. Source file "/user/hadoop/xg1" - Aborting..., Interesting.  Is the -put command returning non-0 to indicate failure?  I doubt it is since it's not generating an error message of its own...  Ex. {{put: /work/xg1: something}}.  That would be another bug in and of itself., Exec'ing {{fs -put}} should return back a proper code in trunk, I think, given Daryn's work on the whole revamp.

I'd argue that this behavior is OK, since it indicates that the file's inode was successfully created, but none of the writing of bytes passed., No longer an issue on trunk after the shell changes.

{code}
$ hadoop fs -put /etc/foo foo
…
13/07/14 05:40:44 ERROR hdfs.DFSClient: Failed to close file /user/test/foo._COPYING_
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/test/foo._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 2 node(s) are excluded in this operation.
…
$ hadoop fs -ls /user/test/foo._COPYING_
ls: `/user/test/foo._COPYING_': No such file or directory
$ hadoop fs -ls /user/test/foo
ls: `/user/test/foo': No such file or directory
{code}]