[I gathered simulated measurements on the time to process a single heartbeat on the namenode versus the number of blocks to replicate. Here is the data:

pending Blocks to replicate           time to process one heartbeat (millisec)
100,000                                         2
500,000                                         5
600,000                                         6

100,000 blocks typically corresponds to about 12 TB. This is analogous to the capacity of three typical datanodes., Thus, if three datanodes go down at the same time, the namenode spends about 2 ms of CPU time to process a single incoming heartbeat. The global FSNamesystem lock is kept for this entire 2 ms.

In the current implementation, heartbeats are sent by the datanode once every 3 seconds. A 1500 node cluster will cause the namenode to spend about 3 seconds (2ms * 1500) processing just heartbeat requests. The current DFS's scalabilty could be limited to 1500 datanodes.

The above results could vary depending on the type of hardware and communcation link that is being used.

 

, Introduce a new DatanodeProtocol call named  sendBlockModifications(). The namenode returns the blocks that are to be replicated or deleted as part of this call. The existing method sendHeartbeat() just updates the heartbeat array in the namenode, it does not send back the list of blocks that are pending replication or the blocks that are to be deleted.

The Datanode invokes the sendHeartbeat RPC once every 3 seconds. The Datanode invokes the sendBlockModifications RPC once every 10 heartbeats.

The namenode acquires only the heartbeat lock while processing the sendHeartbeat call. The namenode acquires the global FSnamesystem lock while processing the sendBlockModifications call.  

The above change ensures that heartbeats processing time does not depend on the amount of blocks that are pending to be replicated.
, Overall this sounds like a great direction.

> new DatanodeProtocol call named sendBlockModifications().

This seems more get-like than send-like.  So maybe it should be called 'getBlockModifications()'  or 'getBlockInstructions()?, A first version of the patch. Review comments needed.

I compared the two implementation 
Approach 1. keeping a single heartbeat RPC and making the namenode transparently return the list-of-blocks-to-be-transferred once every nth heartbeat call.
Approach 2. Introducing a new RPC call that is invoked by the datanode to retrieve list-of-blocks-to-be-transferred. (default 20 seconds).

This current patch implements Approach 2. This approach allows for having a dedicated namenode thread to process heartbeats (if the need arises)., A background thread that computes pendingTransfers and chooseTargets. It stores the computed work into the relevent DatanodeDescriptor. A heartbeat from a datanode retrieves this pre-computed work from the Datanode Descriptor., Two comments:
1. I feel that it is not neccessary to balance # of transfers when the heartbeat thread picks up the replication work. First the background thread that computes pendingTransfers has already balanced the load. Second block replication work needs to be done asap to avoid data loss. Since the datanode has been assinged the block replication work, no other datanode is able to pick up the work. If the work does not get to send to the datanode in the current heartbeat, it has to wait for at least another heartbeat interval.

2. The background thread that computes pendindingTransfer scans only 100 datanodes per interation and then sleep for 3 seconds. I feel that the approach does not scale well. For example, when a cluster size becomes 2000, a datanode's work gets computed every 2000/100*3=1min if we ignore the computation overhead, which is far less frequently than what we do now (every 3 seonds). Another minor flaw is that the thread uses the index to record the next node to be checked. But if the heartbeat queue gets updated between two consecutive interations, the index may not point to the right node., Incorporated review comments. the change from the previous version is in method FSNamesystem.computeDatanodeWork()., Code has been reviewed by Hairong., I just committed this.  Thanks, Dhruba!]