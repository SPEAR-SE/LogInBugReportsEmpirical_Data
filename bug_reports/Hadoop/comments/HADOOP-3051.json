[What is the fd limit you have for your JVM? 0.17 uses NIO sockets. Looks like JVM uses selector to wait for connect, and each selector in Java eats up 3 extra fds :(. DataNode will also use selectors if it needs to wait on sockets..

> (50 dfs clients with 40 streams each writing concurrently files on a 8 nodes DFS cluster) 
2000 writes across 8 datanodes? 

yes, 0.17 eats more file descriptors, especially with loads that result in a lot of threads waiting on sockets (as opposed to disk i/o). If this is big problem we might default to not using extra fds and provide a config option to enable it. One would enable such an option if 'write timout' is required on sockets (HADOOP-2346).
, fd limit is 1024 according to "ulimit -aH"

exactly 2000 writes across 8 datanodes..., > fd limit is 1024 

Seems quite low for a systems applications that are io intensive like this. 1024 probably made sense years ago. 

Even with 16, you are close to the limit. Assuming the replication is 3, at any time the datanodes are writing 3 * 2000 blocks => each datanode is writing 750 blocks. With uniform distribution, each block write takes 2.66 fds => each datanode needs 2000 fds.

Irrespective the limit, looks like most users may not want 'write timeout'. May be by default HDFS should not make DataNode take more fds  than before (may be DFSClient too)., I increased the fd limit to 4096 :-) It performs way better now...

Anyway I would recommend to set the default to "not to use extra fds" since I assume that a bunch of other users will have the same experience when upgrading from earlier releases...?!?
, 
Even 4k is not very large. I would certainly recommend higher number.

I am marking this jira for 0.17.  I need to add a config option and a unit test so that non-default value works as expected.

btw, anyone has an idea how much it would take to implement a custom (bare bones) Selector, extending AbstractSelector?. If we could just use regular poll() (instead of epoll) and not support wakeUp(), then selector will not require any fds., I am seeing this exception on current trunk running TestCrcCorruption w/ 1024 descriptors (the default).  We should certainly be able to pass unit tests with 1024 descriptors, no?, yes, since there is no reason to think TestCrcCorruption keeps lots of files open. I am trying to see why hundreds of datanode threads are waiting on socket reads., > I am seeing this exception on current trunk running TestCrcCorruption w/ 1024 descriptors (the default).
This will be fixed in HADOOP-3067., Looks like this is handled by HADOOP-3124, which adds a configuration variable that lets using standard sockets in DataNode. But default behavior uses NIO sockets.]