[I have mentioned a similar problem (with the same stack trace) in HADOOP-1158. 
I suspect the cause might be : 
  1. Tasktrackers run the maps successfully
  2. Map outputs are served by jetty servers on the TTs.
  3. All the reduce tasks connects to all the TT where maps are run. 
  4. since there are lots of reduces wanting to connect the map output server, the jetty servers run out of threads (default 40)
  5. tasktrackers continue to make periodic heartbeats to JT, so that they are not dead, but their jetty servers are (temporarily) down. 
 
Are you using trunk version (including HADOOP-1158 )? If not, could you please give it a try. , I am using trunk version, but I will analyze the logs for this possibility., Enis, are you saying that the jetty issue cause lost tasktrackers. That doesn't seem to be the case since the tasktrackers, in fact, are doing the heartbeats and hence they won't be lost. 
So I think Srikanth is not hitting HADOOP-1158. IMHO we might be hitting some scalability issue - it might help in the short term to see how things behave with an increased number of JobTracker RPC server handlers. In the code, the number of RPC handlers is hardcoded to 10. It would be interesting to see if things become better with 40.  , Attached is a straightforward patch that increases the number of rpc server handlers from 10 to 40. Pls try things out with this patch and let us know the results., This patch makes the number of RPC server handlers in the JobTracker configurable. I personally found that configuring the number of RPC server handlers to 50 for a 1400 node cluster worked pretty well - the sort benchmark ran to successful completion. , +1

http://issues.apache.org/jira/secure/attachment/12364832/1763.patch applied and successfully tested against trunk revision r571275.

Test results:   http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/659/testReport/
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/659/console, Devaraj, have you checked ipc call queue sizes? 
{{Server.Listener#doAccept()}} method prints this info as debug log. 

It seems configuring the right number of IPC/RPC server handler will always be a fragile design issue. But alternatively I think we could make the number of handlers dynamic. We can monitor the queue usage and opt for best number of handlers? any thoughts ? , Enis, I think we should close this for now by having the JobTracker's handlers configurable. The number of NameNode's RPC server's handlers can be configured with dfs.namenode.handler.count. This patch introduces this feature in the JobTracker's server. We could open a new jira issue to investigate making the #handlers dynamic and handle the problem for all servers (NameNode, JobTracker and maybe TaskTracker).
What do others think?, I'm usually scared about doing smart/dynamic scaling of threads/handlers, clearly once we have something well thought out and tested I'd tag along.

I'll go with the current patch for now. +1, Could this patch be committed? We are waiting for this one., As part of another jira, I think we should make RPC Q length configurable. There is nothing magical about 100 per thread. I am pretty sure if we half the number of threads and double the max q length we get exactly same benifit. On a 8 cpu node, CPU wise, 60 threads cann't do any more work than 20 threads can. , I also give +1 for this patch to be committed, but just wondered if things could have been made easier. I just wanted to raise some discussions before opening another issue, to ensure that dynamic number of handler threads is reasonable. , +1, Enis, please raise a jira for it either way. 
We can discuss things there and it will be easier to track/record... thanks!, Here we go : HADOOP-1830, I just committed this., > On a 8 cpu node, CPU wise, 60 threads can't do any more work than 20 threads can.

Not quite.  The ipc.Server reads requests asynchronously in the listener thread, but responses are written synchronously in handler threads.  So handler threads are not just doing computation, they're also doing i/o, writing responses.  If, e.g., responses are large, and/or clients are slow to read them, then more handler threads will increase throughput.
, > If, e.g., responses are large, and/or clients are slow to read them, then more handler threads will increase throughput.

true, more threads can help in this case. Do you think this is the cause of failure in this jira? I doubt it.

Handling slow clients with more threads is inherently problematic and not scalable IMHO. IPC already has all the serialized data, IPC could write data asynchrnously (with future enhancement)., > Do you think this is the cause of failure in this jira? I doubt it.

Probably not, but I don't really know.  With 2000 nodes, if even 1% get bogged down simultaneously then the jobtracker would run out of handler threads.  And hadoop can slam nodes pretty hard, so they're often not very responsive.  Or if a switch is temporarily saturated, all responses to it might get hung up for a bit.

Perhaps this could be improved by buffering entire responses in handler threads, then having a separate, non-handler thread that uses non-blocking writes to asynchronously write queued responses., bq. Perhaps this could be improved by buffering entire responses in handler threads, then having a separate, non-handler thread that uses non-blocking writes to asynchronously write queued responses.

+1 to this. Just like we have selectors for reading data, we should write data via selectors too. , I've added HADOOP-1841 to address this further.]