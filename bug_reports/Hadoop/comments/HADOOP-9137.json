[As observed in HDFS-4233, NN's socket connections can use up all FDs leaving none for rolling journals or other normal files. 
I could not find separate limits for regular files and tcp connections in Linux.
The rpc layer needs to limit the number of tcp connections to be a little less than the max fds., We've another case that affects edit logging: The edit rolling was successful, but after downloading checkpoint fsimage from the secondary namenode, FileJournalManager#purgeLogsOlderThan() failed due to the system's opendir() failed. 

bq. The rpc layer needs to limit the number of tcp connections to be a little less than the max fds.

There are also multiple log files and connections through servlets beside RPC requests. Do we want NN to figure out the limit automatically? Or leave it to users set a config variable? Automatic setting may be a bit tricky to do in a platform independent way.

The following is a breakdown of open file descriptors of a running namenode excluding sockets.

1 gc log
2 /dev/random, /dev/urandom
2 in-use lock files
2 edit log files
3 stdin, stdout, stderr
3 nn log, auth log, audit log
9 epoll fd
18 pipes
157 jar files

Limiting RPC layer to not accept beyond configured limit will be straightforward. , This is a good find, Sanjay.

One problem with trying to do connection throttling automatically is that (at least on Linux) there is both a global number of file descriptors, and a process-specific number of file descriptors.  Since we can't know what else is running on the system, we can't really tailor ourselves to respect the global number of FDs.

Even if we only consider the per-process limit, there are difficult race conditions involved.  For example, if there are 3 per-process fds left, the connection throttler can try to limit itself to 3 more connections-- but then, if another thread jumps in and opens a file for some reason-- you are unable to open the last socket.  Consequently, if we did do this, we'd have to have a high "slop factor."

I'm also a little bit afraid that if we automatically throttle ourselves in the case of a low per-process FD limit, this will result in users not realizing why their performance has degraded.  At the very least, if we do implement such throttling, we should prominently log an ERROR message when throttling kicks in.  I have a feeling that doing this kind of thing will hurt us significantly in benchmarks.  I also don't see a clear benefit, since running with a low number of max fds is almost always a misconfiguration.

So for these reasons, I would advocate a manually configured connection throttle.  The default should be either no limit or something very large., bq. So for these reasons, I would advocate a manually configured connection throttle. The default should be either no limit or something very large.

+1 on this idea., I moved it to common, since it involves IPC server changes. , The patch adds a way to specify and enforce  the maximum number of concurrent connections for IPC servers., The change is generic for all IPC servers with the limit disabled by default. Although it is still usable to address the original problem, but I expect HADOOP-9136 will make it easy to config different servers differently.
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560642/hadoop-9137.trunk.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1845//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1845//console

This message is automatically generated., Updating the dependency.  Once it is done, I will post a new patch., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560642/hadoop-9137.trunk.patch
  against trunk revision 89b0749.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5535//console

This message is automatically generated., Attaching a new patch.  This does not have direct dependency on HADOOP-11518, but this is difficult to use without HADOOP-11518., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12695562/HADOOP-9137.patch
  against trunk revision f2c9109.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ipc.TestRPCWaitForProxy

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5544//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5544//artifact/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5544//console

This message is automatically generated., The audit warning was not generated by this patch. It's a yarn's static web content.

{noformat}
 !????? /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/hadoop-yarn-project/
hadoop-yarn/hadoop-yarn-common/src/main/resources/webapps/static/dt-1.9.4/images/Sorting icons.psd
Lines that start with ????? in the release audit report indicate files that do not
 have an Apache license header.
{noformat}, bq. org.apache.hadoop.ipc.TestRPCWaitForProxy
The test case failed because the server was trying to bind to port 20 and couldn't.  When run on my machine, it passes., I think the best approach would be accepting, returning a RetriableException, and closing.  Alas, it's not trivial but shouldn't be too hard.

The current approach of relying on the idle scan doesn't work because outstanding call metrics are wrong.  Out of band RPC messages (ex. sasl) don't increment on read, but do decrement on response.  I think I filed a jira a long time ago about it.  The acceptor will go into a spin loop scanning for connections that will never be considered idle, all the while spewing garbage and increasing GC pressure.

The easiest middle ground is probably to just accept & close the connection.  It's not very friendly to clients, but it's a lot better than the NN dying., The new patch closes the connection right away., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12695621/HADOOP-9137.v2.patch
  against trunk revision 951b360.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5546//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5546//artifact/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5546//console

This message is automatically generated., +1  Looks good.  It's odd that the pre-commit is complaining about a photoshop file in the yarn project..., Thanks for the reviews! I've committed it to trunk and branch-2 and also updated the title to better reflect the nature of the change., FAILURE: Integrated in Hadoop-trunk-Commit #6972 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6972/])
HADOOP-9137. Support connection limiting in IPC server. Contributed by Kihwal Lee. (kihwal: rev 8dc59cb9e0f8d300991a437c1b42f1e4e495cfe4)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java
* hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #90 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/90/])
HADOOP-9137. Support connection limiting in IPC server. Contributed by Kihwal Lee. (kihwal: rev 8dc59cb9e0f8d300991a437c1b42f1e4e495cfe4)
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #824 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/824/])
HADOOP-9137. Support connection limiting in IPC server. Contributed by Kihwal Lee. (kihwal: rev 8dc59cb9e0f8d300991a437c1b42f1e4e495cfe4)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java
* hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2022 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2022/])
HADOOP-9137. Support connection limiting in IPC server. Contributed by Kihwal Lee. (kihwal: rev 8dc59cb9e0f8d300991a437c1b42f1e4e495cfe4)
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #87 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/87/])
HADOOP-9137. Support connection limiting in IPC server. Contributed by Kihwal Lee. (kihwal: rev 8dc59cb9e0f8d300991a437c1b42f1e4e495cfe4)
* hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
* hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #91 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/91/])
HADOOP-9137. Support connection limiting in IPC server. Contributed by Kihwal Lee. (kihwal: rev 8dc59cb9e0f8d300991a437c1b42f1e4e495cfe4)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
* hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2041 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2041/])
HADOOP-9137. Support connection limiting in IPC server. Contributed by Kihwal Lee. (kihwal: rev 8dc59cb9e0f8d300991a437c1b42f1e4e495cfe4)
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java
* hadoop-common-project/hadoop-common/src/main/resources/core-default.xml
]