[Maybe we can take it even further and share a single TransferManager (as advised by AWS) with a custom threadpool of tunable size (default is FixedThreadPool(10), which is probably too low). Thus, construct a final TransferManager in S3AFileSystem.initialize() and pass it around (to S3AOutputStream)?, Thanks for the comment, Thomas.
AmazonS3Client is used to construct TransferManager. I replaced it with TransferManager in S3AOutputStream ctor.

Introduced the following parameters which control threadpool used by TransferManager:
fs.s3a.threads.max
fs.s3a.threads.core
fs.s3a.max.total.tasks
fs.s3a.threads.keepalivetime, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12689078/hadoop-11446-001.patch
  against trunk revision 4f18018.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-aws.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5341//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5341//console

This message is automatically generated., Hi Ted,
Looks good to me. Some minor remarks:

- The parameters should be defined (and documented) in Constants.java. Your default of fs.s3a.threads.core=256 means up to 256 parallel (part)Uploads. That should fill up your network pipe :p. However, the number of concurrent http connections opened by the underlying AmazonS3Client (fs.s3a.max.connections) is set to a much lower value by default (too low?). Could you elaborate on the default values? I think we should tweak these a bit to give a good "out of the box" experience and/or document some tuning guidelines for different network conditions (use cases).

- Also use the shiny new single TransferManager for purging at the end of initialize() in S3AFileSystem, replacing the following code path
{code}
if (purgeExistingMultipart) {
      TransferManager transferManager = new TransferManager(s3);
{code}

- I like that you went for a low-level implementation for the Executor instead of using Executors.newFixedThreadPool. The ability to block submitting threads by setting fs.s3a.max.total.tasks  is nice tool for limiting memory consumption. Out of curiosiity: can you envision use cases where setting different values for core.threads and max.threads would be important? 
, w.r.t. comment #1 above:
{code}
    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, 
      DEFAULT_MAXIMUM_CONNECTIONS));
{code}
The default value is from current setting.
Do you think setting max connections as value for fs.s3a.threads.core makes sense ?

w.r.t. last comment, having different values for core.threads and max.threads would get the full potential out of ThreadPoolExecutor.

Next patch would address remaining comments., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12689269/hadoop-11446-002.patch
  against trunk revision 1454efe.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-aws.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5351//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5351//console

This message is automatically generated., Ted, could you pull "fs.s3a.max.total.tasks" & its default value into the constants class.

I think we'll need the doc updated with the config options too.

test-wise, I can't see an easy way to test that this works, except by you repeating your experiment, Patch v3 addresses Steve's comment.

Once the set of new parameters has been agreed upon, I will update release note., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12689537/hadoop-11446-003.patch
  against trunk revision 6621c35.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-tools/hadoop-aws.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5352//testReport/
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5352//console

This message is automatically generated., +1, FAILURE: Integrated in Hadoop-trunk-Commit #6805 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6805/])
HADOOP-11446 S3AOutputStream should use shared thread pool to avoid OutOfMemoryError (stevel: rev 27d8395867f665fea1360087325cda5ed70efd0c)
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AOutputStream.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
, Some remarks: 
- I guess that at the end of "purging" the TransferManager should not be shut down?
- Which got me thinking: Should we override FileSystem.close() in S3AFileSystem to call TransferManager.shutDownNow(true)? This will shut down the thread pool and AmazonS3Client. Otherwise, these resources may still leak? 

Regarding my previous comments: I see no harm in giving users full control, although probably a linear relation between fs.s3a.max.connections and fs.s3a.threads.core (etc.) is suitable (at least that's what I derive from the aws-sdk source code, I haven't tested this yet). However, we should at least document their interplay as f.i. increasing threads while keeping connections fixed will likely not yield the (linear) performance improvement users might expect. 

, Thomas:
Addendum addresses your comment., Relying on FileSystem.close() to be called would not always work - ExportSnapshot from hbase doesn't call close().

FYI, is this addendum something you want to get in? If so, it's going to have to be another patch I'm afraid, FAILURE: Integrated in Hadoop-Yarn-trunk #799 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/799/])
HADOOP-11446 S3AOutputStream should use shared thread pool to avoid OutOfMemoryError (stevel: rev 27d8395867f665fea1360087325cda5ed70efd0c)
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AOutputStream.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #65 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/65/])
HADOOP-11446 S3AOutputStream should use shared thread pool to avoid OutOfMemoryError (stevel: rev 27d8395867f665fea1360087325cda5ed70efd0c)
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AOutputStream.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
* hadoop-common-project/hadoop-common/CHANGES.txt
, @ [~tedyu]: 
- I fear we have overlooked this: copyFile and copyLocalFile still use a method-local TransferManager transfers object (instead of the class member object aka this.transfers). The addendum removes the shutDown() calls to the local transfermanager there but we should entirely abolish the local objects and only use this.transfers.
- concerning close():  I guess calling close is left to the end user. However, I think we do not leak memory as long as fs.s3a.threads.keepalivetime > 0. Because you set tpe.allowCoreThreadTimeOut(true), the TransferManager will be garbage collected after it goes out of scope AND all (core) threads have timed out. Correct?

@[~stevel@apache.org]: I fear we should. Without the addendum: if the purge code is hit, the next fs command will throw an error as the TransferManager has been shut down. Furthermore,  my first remark above hints at an addendum-002 , FAILURE: Integrated in Hadoop-Hdfs-trunk #1997 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1997/])
HADOOP-11446 S3AOutputStream should use shared thread pool to avoid OutOfMemoryError (stevel: rev 27d8395867f665fea1360087325cda5ed70efd0c)
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AOutputStream.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk-Java8 #62 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/62/])
HADOOP-11446 S3AOutputStream should use shared thread pool to avoid OutOfMemoryError (stevel: rev 27d8395867f665fea1360087325cda5ed70efd0c)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AOutputStream.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #66 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/66/])
HADOOP-11446 S3AOutputStream should use shared thread pool to avoid OutOfMemoryError (stevel: rev 27d8395867f665fea1360087325cda5ed70efd0c)
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AOutputStream.java
* hadoop-common-project/hadoop-common/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2016 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2016/])
HADOOP-11446 S3AOutputStream should use shared thread pool to avoid OutOfMemoryError (stevel: rev 27d8395867f665fea1360087325cda5ed70efd0c)
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java
* hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AOutputStream.java
]