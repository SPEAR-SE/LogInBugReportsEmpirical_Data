[Looks like you are hit by S3's eventual consistency.

Check out S3Guard which should help with your problem:
https://blog.cloudera.com/blog/2017/08/introducing-s3guard-s3-consistency-for-apache-hadoop/
https://hortonworks.com/blog/s3guard-amazon-s3-consistency/, But our production environment doesn't offer a Dynamo DB instance for S3 Guard. Is there a way to tune the options for distcp to copy the huge files. I'm looking for below information,

1) How to select the number of  map and it's size. I have a directory which has ~10000+ files with total size of ~250 GB. When I run with below option, it is taking ~1.30 hours.

hadoop distcp -D HADOOP_OPTS=-Xmx12g -D HADOOP_CLIENT_OPTS='-Xmx12g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' -D 'mapreduce.map.memory.mb=12288' -D 'mapreduce.map.java.opts=-Xmx10g' -D 'mapreduce.reduce.memory.mb=12288' -D 'mapreduce.reduce.java.opts=-Xmx10g' '-Dfs.s3a.proxy.host=edhmgrn-prod.cloud.capitalone.com' '-Dfs.s3a.proxy.port=8088' '-Dfs.s3a.access.key=XXXXXXX' '-Dfs.s3a.secret.key=XXXXXXX' '-Dfs.s3a.connection.timeout=180000' '-Dfs.s3a.attempts.maximum=5' '-Dfs.s3a.fast.upload=true' '-Dfs.s3a.fast.upload.buffer=array' '-Dfs.s3a.fast.upload.active.blocks=50' '-Dfs.s3a.multipart.size=262144000' '-Dfs.s3a.threads.max=500' '-Dfs.s3a.threads.keepalivetime=600' '-Dfs.s3a.server-side-encryption-algorithm=AES256' -bandwidth 3072 -strategy dynamic -m 200 -numListstatusThreads 30 /src/ s3a://bucket/dest

2) I'm not seeing the throughput of 3gbps even after configuring the -bandwidth as 3072. 

3) How to configure the Java heap and map size for the huge file, so that distcp will give better performance.

4) WIth fast upload option, I'm writing the files to S3 using threads. Could you please help me in providing some tuning option for this.

Appreciate Your Help., This is consistency, but not one you need s3guard. Looks more like HADOOP-13145; the stack is exactly the same as HADOOP-11487. Closing as a duplicate of those.

This was fixed a while back. What version of CDH are you using?

* Hadoop 2.8 or the recent HDP and CDH releases have the higher performance upload
* for config : [https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.2/bk_cloud-data-access/content/using-distcp.html]

make sure you aren't trying to use: --atomic  or any of the -poptions.

bq. I'm not seeing the throughput of 3gbps 

I'd be surprised if s3 gave you that. Anyway, it's a "maximum per node", not any guarantee of actual B/W.

Are you trying to write to S3 from a physical cluster, or inside EC2 itself. 

250 GB in 1h30 is 800 KB/s; 6-7 MBits. For a long-haul link, well, its conceivable that is the bandwidth. For in-EC2, its pretty bad.

It does a lot of throttling for writes to specific buckets and paths in it. You may find you get better performance by actually cranking back on how aggressive the bandwidth per node is, reducing the # of mappers. Try cutting it in half and seeing what happens. Then do it again.

bq.  WIth fast upload option, I'm writing the files to S3 using threads. Could you please help me in providing some tuning option for this.

https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.2/bk_cloud-data-access/content/s3a-fast-upload.html

If want to benchmark your upload speed better, download and run https://github.com/steveloughran/cloudup ; for a bulk upload of local data. This isolates all network traffic for the upload, prioritises large files first, and shuffles the filenames to reduce throttling at the back end. Your bandwidth per node will not be > that]