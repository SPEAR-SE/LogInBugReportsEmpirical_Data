[Uploading initial patch.., Makes sense, and the patch looks good to me. +1 pending Jenkins.

Thanks, Arun., Updating patch
* Modified {{ValueQueue}} to prestart the executor core threads only when needed (on first request to generateEncrytedKey).. , Hi [~asuresh], do we have to check if provider == null in DFSClient#close?

I see elsewhere in DFSClient we check if provider == null before use., Updating patch..
Nice catch [~schu] .. thanx, I am little concerned that we are introducing a lifecycle method close() without a real lifecycle definition for KeyProviders.
It seems the latest update to the patch is speaking to my concern.

Once you close a KeyProvider - is it capable of being used or do you need to get a new one?

Do we need an initialize()?
Do all consumers of all KeyProviders - not just the crypto extension - need to call close when done?
The algorithm of traversing a configured provider.path would need to incorporate any lifecycle methods.

If we need to manage internal state of the providers than we need a full lifecycle story - I think.

Thoughts?, [~lmccay], thank you for the review and your feedback..
but, I don't think of {{close()}} as a full lifecycle method per se. But yes, I do understand your concern about making it clear to the client that once closed, it should not be used.. Would having {{KeyProvider}} implement {{java.io.Closable}} address your concern ?

bq. If we need to manage internal state of the providers than we need a full lifecycle story 
Not really sure I understand why.. consider Streams, the ByteArrayInput/OutputStream has a NO-OP close method, while most of the others have..  all of them manage some internal state.. without full lifecycle methods..
, hmmm...
It seems to me that if you shouldn't use the provider after calling close() that it is a lifecycle method - you just killed it. That is how I read what you said - that it shouldn't be used again. If that isn't true then I am less concerned.

Otherwise, I just wanted to make sure that if we were introducing a lifecycle that each stage is represented. Seems like the need for the close() may be pointing out  a smell in the original design of KeyProvider or of the extension for crypto. We just need to address the whole smell. :)

If there is no larger issue and closing and throwing away a provider after each use solves all such circumstances - then that may be just fine.

Just some thoughts., I think in general a lot of classes support a {{close()}} method to indicate that internal resources can be released, that don't need to go whole-hog with a life cycle state machine. That's why java has the {{Closeable}} interface in the first place.

In any case, I think we can entirely reasonably add a {{close()}} call now, and revisit the need for an {{init()}} method at a later time. Given the fact that we're introducing a no-op concrete method, and not an abstract method, there should be no compatibility implications here.

Larry, to be explicit, are you OK with me committing the latest patch as-is?, Yes, I think it is reasonable to address the need for init when it arises.
Thank you for asking.

Can someone just confirm my assertion that providers should only be used once given this change?

[~owen.omalley] - do you have an opinion here?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12672808/HADOOP-11161.3.patch
  against trunk revision 7f6ed7f.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4861//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/4861//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4861//console

This message is automatically generated., [~lmccay],
bq. .. confirm my assertion that providers should only be used once given this change ?
Short answer : Yes
Clients must assume that calling the {{close()}} method implies that the KP should not be used again. But the actual behavior would depend on the implementation. To quote my previous example, Input/Output streams should in general not be used after calling {{close()}}, but the method has no effect on {{ByteArrayInput/OputputStream}}
, Heads up that if there are no further comments on this I'm going to commit the latest patch some time tomorrow most likely., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12672808/HADOOP-11161.3.patch
  against trunk revision 7b29f99.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4883//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/4883//artifact/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4883//console

This message is automatically generated., +1, the latest patch looks good to me. Writing a test to exercise this leak would be quite difficult, and I'm confident that the release audit and test failure are unrelated. The tests fails locally for me without the patch, and the RAT warning has been showing up in other builds as well and I believe is referencing a file unrelated to this patch.

I'm going to commit this momentarily., I've just committed this to trunk, branch-2, and branch-2.6.

Thanks a lot for the contribution, Arun, and thanks also to Larry and Stephen for the reviews., FAILURE: Integrated in Hadoop-trunk-Commit #6219 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6219/])
HADOOP-11161. Expose close method in KeyProvider to give clients of Provider implementations a hook to release resources. Contribued by Arun Suresh. (atm: rev 2a51494ce1b05fc494fb3a818a7a3526f3f40070)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #706 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/706/])
HADOOP-11161. Expose close method in KeyProvider to give clients of Provider implementations a hook to release resources. Contribued by Arun Suresh. (atm: rev 2a51494ce1b05fc494fb3a818a7a3526f3f40070)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1896 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1896/])
HADOOP-11161. Expose close method in KeyProvider to give clients of Provider implementations a hook to release resources. Contribued by Arun Suresh. (atm: rev 2a51494ce1b05fc494fb3a818a7a3526f3f40070)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1921 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1921/])
HADOOP-11161. Expose close method in KeyProvider to give clients of Provider implementations a hook to release resources. Contribued by Arun Suresh. (atm: rev 2a51494ce1b05fc494fb3a818a7a3526f3f40070)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java
]