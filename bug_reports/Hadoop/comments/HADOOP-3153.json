[Attaching a patch. This results in the following behaviour:
   - Any operation fails right away, when user cluster state file has no read permissions.
   - Allocation fails if user cluster state has no write permissions.
   - Deallocation deallocates the cluster, but doesn't(cannot) update user cluster state when the file has no write permissions. After this, info and list still report old clusters, but saying information cannot be found about the clusters. Agreeable. (?)
    - The last step of allocation, writing to state file, is now enclosed in a try-catch block to handle any unknown error conditions. Not sure when this can trigger, but putting it just in case.

Added test cases. Thinks to check - behavior, error messages and error codes. Deallocation with invalid cluster state file couldn't have a test case now, 'coz of  lack of enough testing framework in place to do this (a successful allocation followed by a failing deallocate)., Basic fix seem right.

Some comments:

- If the $HOME/.hod directory does not exist, (and so the clusters.state file does not exist), allocate is not working.
- I think we can make some small changes to the error messages. Currently, in the error scenario, for allocate we say:
Invalid Permissions on clusters' data file : /home/yhemanth/.hod/clusters.state. It has to be a readable file.
Instead, we can say:
Requested operation cannot be performed. Cannot read /home/yhemanth/.hod/clusters.state: Permission denied. 
This is somewhat similar to what Unix errors report.
Likewise, for deallocate, when you don't have write permissions, we say:
Invalid Permissions on clusters' data file : /home/yhemanth/.hod/clusters.state. It has to be a writable file. info and list operations might show old cluster data.
Instead, we can say:
Cannot update /home/yhemanth/.hod/clusters.state: Permission denied.\n Cluster is deallocated, but info and list operations might show incorrect information.
- In the except block of the last part of the allocate command, the deallocation is still not happening, right ? It should be, as per your comment ?

Minor points:
- For the checkStateFile, I think we can pass in some const strings instead of the hardcoded numbers. In fact, it would be great if we can reuse os.R_OK etc. Can we not OR os.R_OK and os.W_OK to accomodate mode 3 ?
- Likewise the error strings, if possible should be only at one place. That way, it will be easier to change.

Test cases:
- Move setupConf to a library type function.. maybe in a util class in the same module.
- One more test case I can think of is what happens if the cluster state directory does not have write and execute permissions. This is actually the scenario that we have fixed
, Attaching a second patch incorporating above suggestions.

    - Allocate works now, even if .hod directory does not exist.
    - Made changes to error messages.
    - Incorporated minor points and suggestions for changes in test-cases.
    - Now, deallocation happens when the except block of the last part of the allocate command is triggered. Tested this by putting in a hard-coded exception, and verified that cluster is deallocated.
    - To implement the above, re-factored code in hadoop.py a bit. Tested the re-factored code too(by giving an interrupt after ringmaster comes up).
    - Made changes to error codes so that 1 is returned when failing because of an invalid state file. ( <1><Configuration error><Incorrect configuration values specified in hodrc, or other errors related to HOD configuration. The error messages in this case must be sufficient to debug and fix the problem> ), - I am not sure we need the refactoring. Why can't we call hadoopCluster.deallocate from the except code block ? Even if for some reason we cannot, I think we must refactor this differently. 
{noformat}    
  def shutdown_job(self, ringClient=None):
    if ringClient is not None:
      self.__log.debug("Calling rm.stop()")
      ringClient.stopRM()
      self.__log.debug("Returning from rm.stop()")
      self.__log.info("Job Shutdown by informing ringmaster.")
    else:
      self.delete_job(self.jobId)
      self.__log.info("Job %s removed from queue directly." % self.jobId)
{noformat}
And there must be a way to get the ringClient from hadoopCluster in hodRunner.py

- In checkStateFile: I think we should check that self.__store is writable. Alternatively, can we check if the file does not exist, by using errno to differentiate permission errors.
- Provide an accessor for _hodState__stateFile in hodState and use that.
- testAllocateWithInvalidStateStore - we can add a test case where the directory has no write permissions.,    * Knocking off the refactoring code. Using hadoopCluster.deallocate instead, as suggested.
   * Incorporated the rest of the changes suggested.

Attaching patch., +1. Patch looks good now., +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12379829/HADOOP-3153.2
against trunk revision 645773.

    @author +1.  The patch does not contain any @author tags.

    tests included +1.  The patch appears to include 3 new or modified tests.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2204/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2204/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2204/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2204/console

This message is automatically generated., I committed this patch to trunk, and Devaraj committed it to the 0.17 branch. Thanks, Vinod !, Integrated in Hadoop-trunk #457 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/457/])]