[Still an issue in 0.17.1:

Sample job with 100,000 maps:

9,000  reduces: map executes on average in 1hr
18,000 reduces: map executes on average in 1:30 hr, Christian, how big are the map outputs? What's the value of io.sort.mb? This will give a rough estimate on the number of spills a map does. If there are multiple spills while the map is running, one suspect is that we are spending too much time in the final merge across the spill segments (merges to produce the final map output). We'd also do a lot of seeks during that merge. It'd be nice to remove the seeks, before the merge of the segments belonging to a certain partition, since we are ultimately reading the files sequentially anyway.

But again, the above depends on whether the map does multiple spills or not. , io.sort.mb = 200.

The average map output is 3-4 GB.

Your suspicion about time difference in the final merge might be correct. I checked one task, which spent about 12 minutes in the final merge with 9000 reduces, but close to 30 minutes with 18000 reduces (the execution time of the map itself was basically the same)., Does this job run with a combiner? That would also add time spent in each spill in proportion to the number of reducers. Is there large variation in the map execution time, or are most fairly close to the average time?, Sorry for the late response.
No, this job does not have a combiner. And the job execution times of the map tasks do not have large variations., Checked similar job in hadoop-0.18.1 with block compression of transient data turned on.

Merge-sort of map spills still depends strongly on number of reduces, but less than in earlier releases.

On the average a map task took:
1hr 13min with 9000 reduces
1hr 19min with 18000 reduces
Of this time on the average 51 minutes were taken up by the application, i.e. the merge-sort of the map spills increased from 22min to 28min when doubling the number of reduces.

Overall the time spent in merge-sort of the map spills increased because of compression (before 0.18 block compression of transient data could not be used at that scale), but the dependence on the number of reduces decreased, In my view this is no longer much of an issue., Closing the issue as it got fixed as Christian illustrated.]