[This results in potential data corruption, but is not a regression. We should fix it sooner rather than later., If the namenode receives a block confirmation with a block size that differs from the blocksize in the blocksMap, then all those replicas that have a smaller size are deleted.

I will create a separate JIRA to enhance the DatanodeBlockScanner to trigger a corrupted block if the entire size of the crc file does not match the size of the block data file., Looks good. Could we add check to delete blocks only if they are smaller than current reported (larger) size., In the case when the new replica is larger in size that what is stored in the blocksMap, then all those replicas in the blocksMap are deleted. The new replica is stored in the blocksMap. The size of the block in the blocksMap needs to be updated in this case., +1 Looks good. , -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12377632/inconsistentSize.patch
against trunk revision 619744.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs -1.  The patch appears to cause Findbugs to fail.

    core tests -1.  The patch failed core unit tests.

    contrib tests -1.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1951/testReport/
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1951/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1951/console

This message is automatically generated., There was a compilation error in the previous patch. Uploading new patch., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12377952/inconsistentSize.patch
against trunk revision 619744.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1974/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1974/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1974/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1974/console

This message is automatically generated., I just committed this!, Integrated in Hadoop-trunk #431 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/431/]), Hi dhruba borthakur, isn't this problem is because that NN incorrectly uses the block object used in RPC to queue to neededReplication queue instead of using internal block object ?  Because 134217728 is actually 128k. I got this message from HADOOP-5605]