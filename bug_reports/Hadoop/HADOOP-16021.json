{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "id": "12330961",
            "name": "common",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12330961"
        }],
        "created": "2018-12-27T12:08:22.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "asin",
            "key": "xinkenny",
            "name": "xinkenny",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=xinkenny",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2018-12-27T14:21:51.048+0000",
        "customfield_12310222": "1_*:*_1_*:*_554853350_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "4.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|u00c1c:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Jan 03 03:13:04 UTC 2019",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": " \r\n I want append the data in a file , when i use SequenceFile.appendIfExists , it throw NullPointerException at at org.apache.hadoop.io.SequenceFile$Writer.(SequenceFile.java:1119)\r\n\r\nwhen i remove the 'appendIfExists', it works, but it will cover old file.\r\n\r\n \r\n\r\nwhen i try use CompressionType.RECORD or CompressionType.BLOCK throw \"not support\" exception\r\n\r\n \r\n{code:java}\r\n// my code\r\nSequenceFile.Writer writer = null; \r\n\r\nwriter = SequenceFile.createWriter(conf, \r\n    SequenceFile.Writer.file(path), \r\n    SequenceFile.Writer.keyClass(Text.class), \r\n    SequenceFile.Writer.valueClass(Text.class), \r\n    SequenceFile.Writer.appendIfExists(true) );\r\n{code}\r\n \r\n{code:java}\r\n// all my code\r\npublic class Writer1 implements VoidFunction<Iterator<Tuple2<String, String>>> {\r\n    private static Configuration conf = new Configuration();\r\n    private int MAX_LINE = 3; // little num,for test\r\n\r\n    @Override\r\n    public void call(Iterator<Tuple2<String, String>> iterator) throws Exception {\r\n        int partitionId = TaskContext.get().partitionId();\r\n        int count = 0;\r\n        SequenceFile.Writer writer = null;\r\n        while (iterator.hasNext()) {\r\n\r\n            Tuple2<String, String> tp = iterator.next();\r\n            Path path = new Path(\"D:/tmp-doc/logs/logs.txt\");\r\n\r\n            if (writer == null)\r\n                writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(path),\r\n                        SequenceFile.Writer.keyClass(Text.class),\r\n                        SequenceFile.Writer.valueClass(Text.class),\r\n                        SequenceFile.Writer.appendIfExists(true)\r\n                        );\r\n\r\n            writer.append(new Text(tp._1), new Text(tp._2));\r\n            count++;\r\n\r\n            if (count > MAX_LINE) {\r\n                IOUtils.closeStream(writer);\r\n                count = 0;\r\n                writer = SequenceFile.createWriter(... // same as above\r\n            }\r\n        }\r\n        if (count > 0) {\r\n            IOUtils.closeStream(writer);\r\n        }\r\n        IOUtils.closeStream(writer);\r\n    }\r\n}\r\n{code}\r\n // above code call by below\r\n{code:java}\r\nimport com.xxx.algo.hadoop.Writer1\r\nimport com.xxx.algo.utils.Utils\r\nimport kafka.serializer.StringDecoder\r\nimport org.apache.spark.sql.SparkSession\r\nimport org.apache.spark.streaming.kafka.KafkaUtils\r\nimport org.apache.spark.streaming.{Durations, StreamingContext}\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\n\r\n\r\nobject KafkaSparkStreamingApp {\r\n  def main(args: Array[String]): Unit = {\r\n    val kafka = \"192.168.30.4:9092,192.168.30.5:9092,192.168.30.6:9092\"\r\n    val zk = \"192.168.30.4:2181,192.168.30.5:2181,192.168.30.6:2181\"\r\n    val topics = Set(\"test.aries.collection.appevent.biz\")\r\n    val tag = \"biz\"\r\n    val durationSeconds = 5000\r\n    val conf = new SparkConf()\r\n    conf.setAppName(\"user-log-consumer\")\r\n      .set(\"spark.serilizer\",\"org.apache.spark.serializer.KryoSerializer\")\r\n      .set(\"spark.kryo.registrationRequired\", \"true\")\r\n      .set(\"spark.defalut.parallelism\",\"2\")\r\n      .set(\"spark.rdd.compress\",\"true\")\r\n      .setMaster(\"local[2]\")\r\n    val sc = new SparkContext(conf)\r\n    val session = SparkSession.builder()\r\n      .config(conf)\r\n      .getOrCreate()\r\n    val ssc = new StreamingContext(sc, Durations.milliseconds(durationSeconds))\r\n    val kafkaParams = Map[String, String](\r\n      \"metadata.broker.list\" -> kafka,\r\n      \"bootstrap.servers\" -> kafka,\r\n      \"zookeeper.connect\" -> zk,\r\n      \"group.id\" -> \"recommend_stream_spark\",\r\n      \"key.serializer\" -> \"org.apache.kafka.common.serialization.StringSerializer\",\r\n      \"key.deserializer\" -> \"org.apache.kafka.common.serialization.StringDeserializer\",\r\n      \"value.deserializer\" -> \"org.apache.kafka.common.serialization.StringDeserializer\"\r\n    )\r\n    val stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\r\n      ssc,\r\n      kafkaParams,\r\n      topics\r\n    )\r\n    val timeFieldName = \"log_time\"\r\n    stream.foreachRDD(rddMsg => {\r\n      rddMsg.map(msg => {\r\n        val value = msg._2\r\n        val time = Utils.getTime(value, timeFieldName)\r\n        new Tuple2(time + \",\" + tag, value)\r\n      })\r\n        .toJavaRDD().foreachPartition(new Writer1()) // here\r\n    })\r\n    ssc.start()\r\n    ssc.awaitTermination()\r\n  }\r\n}\r\n{code}\r\n{{more info see:[https://stackoverflow.com/questions/53943978/hadoop-sequencefile-createwriter-appendifexists-codec-cause-nullpointerexception]}}",
        "duedate": null,
        "environment": "windows10 or Linux-centos , hadoop2.7.3, jdk8",
        "fixVersions": [],
        "issuelinks": [{
            "id": "12551112",
            "outwardIssue": {
                "fields": {
                    "issuetype": {
                        "avatarId": 21133,
                        "description": "A problem which impairs or prevents the functions of the product.",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                        "id": "1",
                        "name": "Bug",
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                        "subtask": false
                    },
                    "priority": {
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
                        "id": "2",
                        "name": "Critical",
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/2"
                    },
                    "status": {
                        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                        "id": "5",
                        "name": "Resolved",
                        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                        "statusCategory": {
                            "colorName": "green",
                            "id": 3,
                            "key": "done",
                            "name": "Done",
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
                        }
                    },
                    "summary": "Unable to append to a SequenceFile with Compression.NONE."
                },
                "id": "12968232",
                "key": "HADOOP-13138",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/12968232"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12551112",
            "type": {
                "id": "12310000",
                "inward": "is duplicated by",
                "name": "Duplicate",
                "outward": "duplicates",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"
            }
        }],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": ["bug"],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
            "id": "4",
            "name": "Minor",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/4"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095"
            },
            "id": "12310240",
            "key": "HADOOP",
            "name": "Hadoop Common",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310240"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "asin",
            "key": "xinkenny",
            "name": "xinkenny",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=xinkenny",
            "timeZone": "Etc/UTC"
        },
        "resolution": {
            "description": "The problem is a duplicate of an existing issue.",
            "id": "3",
            "name": "Duplicate",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/3"
        },
        "resolutiondate": "2019-01-02T22:15:55.000+0000",
        "status": {
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "id": "5",
            "name": "Resolved",
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "SequenceFile.createWriter appendIfExists codec cause NullPointerException",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2019-01-03T03:13:04.000+0000",
        "versions": [{
            "archived": false,
            "description": "2.7.3 release",
            "id": "12334005",
            "name": "2.7.3",
            "releaseDate": "2016-08-25",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12334005"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-16021/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-16021/watchers",
            "watchCount": 3
        },
        "workratio": -1
    },
    "id": "13206598",
    "key": "HADOOP-16021",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13206598"
}