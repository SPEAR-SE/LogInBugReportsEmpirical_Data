{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Mahadev konar",
            "key": "mahadev",
            "name": "mahadev",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=mahadev",
            "timeZone": "Etc/UTC"
        },
        "components": [],
        "created": "2006-06-08T08:28:11.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Michel Tourn",
            "key": "michel_tourn",
            "name": "michel_tourn",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michel_tourn",
            "timeZone": "America/New_York"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2006-06-20T07:30:22.000+0000",
        "customfield_12310222": "10002_*:*_1_*:*_1302129000_*|*_1_*:*_2_*:*_7214477000_*|*_6_*:*_1_*:*_0_*|*_5_*:*_1_*:*_1899439000",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "7.0",
        "customfield_12310320": null,
        "customfield_12310420": "80662",
        "customfield_12310920": "107198",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i0ipbz:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Sep 14 22:11:37 UTC 2006",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "RFC: Efficient file caching \n(on Hadoop Task nodes, for benefit of MapReduce Tasks)\n------------------------------------------------------\n\nWe will start implementing this soon. Please provide feedback and improvements to this plan.\n\nThe header \"Options:\" indicates places where simple choices must be made.\n\n\nProblem:\n-------\no MapReduce tasks require access to additional out-of-band data (\"dictionaries\")\n\nThis out-of-band data is:\n\no in addition to the map/reduce inputs.\no large (1GB+)\no broadcast (same data is required on all the Task nodes)\no changes \"infrequently\", in particular:\noo it is always constant for all the Tasks in a Job. \noo it is often constant for a month at a time \noo it may be shared across team members\no sometimes used by pure-Java MapReduce programs\no sometimes used by non-Java MapReduce programs (using Hadoop-Streaming)\no (future) used by programs that use HDFS and Task-trackers but not MapReduce.\n\nExisting Solutions to the problem:\n---------------------------------\nThese solutions are not good enough. The present proposal is to do Sol 1 with caching.\n\nSol 1: Pure Hadoop: package the out-of-band data in the MapReduce Job jar file.\nSol 2: Non  Hadoop: for each task node run rsync from single source for data.\nSol 3: Non  Hadoop: use BitTorrent, etc.\n\nSol.1 is correct but slow for many reasons:\n The Job submitter must recreate a large jar(tar) file for every Job.\n  (The jar contains both changing programs and stable dictionaries)\n The large Jar file must be propagated from the client to HDFS with \n a large replication factor. \n At the beginning of every Task, the Task tracker gets the job jar from HDFS \n and unjars it in the working directory. This can dominate task execution time.\n \nSol.2 has nice properties but also some problems.\n It does not scale well with large clusters (many concurrent rsync read requests i.e. single-source broadcast)\n It assumes that Hadoop users can upload data using rsync to the cluster nodes. As a policy, this is not allowed.\n It requires rsync.\n \nSol.3 alleviates the rsync scalability problems but \n      It is a dependency on an external system. \n      We want something simpler and more tightly integrated with Hadoop.\n      \n\nStaging (uploading) out-of-band data:\n------------------------------------\nThe out-of-band data will often originate on the local filesystem of a user machine \n (i.e. a MapReduce job submitter)\nNevertheless it makes sense to use HDFS to store the original out-of-band data because:\no HDFS has (wide) replication. This enables scalable broadcast later.\no HDFS is an available channel to move data from clients to all task machines.\no HDFS is convenient as a shared location among Hadoop team members.\n\n\nAccessing (downloading) out-of-band data:\n----------------------------------------\nThe non-Java MapReduce programs do not have or want[1] APIs for HDFS.\nInstead these programs just want to access out-of-band data as \n local files at predefined paths.\n([1] Existing programs should be reusable with no changes. \n This is often possible bec. communication is over stdin/stdout.)\n\n\n\nJob's jar file as a special case:\n--------------------------------\nOne use case is to allow users to make the job jar itself cachable.\n\nThis is only useful in cases where NOTHING changes when a job is resubmitted\n (no MapRed code changes and no changes in shipped data)\nThis situation might occur with an 'extractor' job (gets data from an external source: like Nutch crawler)\n\nCurrently the Hadoop mapred-jar mechanism works in this way:\n the job jar data is unjarred in the \"working directory\" of the Task \n the jar contains both MapRed java code (added to classpath)\n\n\n\nCache synchronization:\n---------------------\n\nThe efficient implementation of the out-of-band data distribution\nis mostly a cache synchronization problem.\nA list of the various aspects where choices must be made follows.\n\n\nCache key:\n---------\nHow do you test that the cached copy is out-of-date?\n\nOptions: \n1. the archive/file timestamp \n2. the MD5 of the archive/file content\n\nComparing source and destination Timestamps is problematic bec. it assumes synchronized clocks.\nAlso there is no last-modif metadata in HDFS (for good reasons, like scalability of metadata ops)\n\nTimestamps stored with the source ('last-propagate-time') do \n not require synchronized clocks, only locally monotonic time. \n(and the worse which can happen at daylight-savings switch is a missed update or an extra-update)\n\nThe cache code could store a copy of the local timestamp \nin the same way that it caches the value of the content hash along with the source data.\n \n\n\nCachable unit:\n-------------\nOptions: individual files or archives or both.\n\nNote:\nAt the API level, directories will be processed recursively \n(and the local FS directories will parallel HDFS directories)\nSo bulk operations are always possible using directories.\nThe question here is whether to handle archives as an additional bulk mechanism.\n\n\nArchives are special because:\no unarchiving occurs transparently as part of the cache sync\no The cache key is computed on the archive and preserved although \n  the archive itself is not preserved.\nSupported archive format will be: tar (maybe tgz or compressed jar)\nArchive detection test: by filename extension \".tar\" or \".jar\"\n\nSuppose we don't handle archives as special files:\nPros:\n o less code, no discussion about which archive formats are supported\n o fine for large dictionary files. And when files are not large, user may as well\n   put them in the Job jar as usual.\n o user code could always check and unarchive specific cached files\n   (as a side-effect of MapRed task initialization)\nCons:\n o handling small files may be inefficient \n  (multiple HDFS operations, multiple hash computation, \n   one 'metadata' hash file along with each small file)\n o It will not be possible to handle the Job's jar file as a special case of caching \n\n\n\nCache isolation: \n---------------\nIn some cases it may be a problem if the cached HDFS files are updated while a Job is in progress:\nThe file may become unavailable for a short period of time and some tasks fail.\nThe file may change (atomically) and different tasks use a different version.\n\nThis isolation problem is not addressed in this proposal.\nStandard solutions to the isolation problem are:\n\no Assume that Jobs and interfering cache updates won't occur concurrently.\n\no Put a version number in the HDFS file paths and refer to a hard-coded version in the Job code.\n\no Before running the MapRed job, run a non-distributed application that tests\n  what is the latest available version of the out-of-band data. \n  Then make this version available to the MapRed job.\n  Two ways to do this. \n  o either set a job property just-in-time:\n    addCachePathPair(\"/mydata/v1234/\", \"localcache/mydata_latest\"); \n    (see Job Configuration for meaning of this)\n  o or publish the decision as an HDFS file containing the version.\n    then rely on user code to read the version, and manually populate the cache:\n    Cache.syncCache(\"/hdfs/path/fileordir\", \"relative/local/fileordir\");\n    (see MapReduce API for meaning of this)\n\n\nCache synchronization stages:\n----------------------------\nThere are two stages: Client-to-HDFS and HDFS-to-TaskTracker\n\no Client-to-HDFS stage.\nOptions: A simple option is to not do anything here, i.e. rely on the user.\n\nThis is a reasonable option given previous remarks on the role of HDFS:\n HDFS is a staging/publishing area and a natural shared location.\nIn particular this means that the system need not track \nwhere the client files come from.\n\n\no HDFS-to-TaskTracker:\nClient-to-HDFS synchronization (if done at all) should happen before this.\nThen HDFS-to-TaskTracker synchronization must happen right before \nthe data is needed on a node.\n\n\n\nMapReduce cache API:\n-------------------\nOptions:\n\n1. No change in MapReduce framework code:\nrequire the user to put this logic in map() (or reduce) function:\n\n in MyMapper constructor (or in map() on first record) user is asked to add:\n \n    Cache.syncCache(\"/hdfs/path/fileordir\", \"relative/local/fileordir\");\n    Cache.syncCache(\"...\"); //etc.\n  \n-----\n\n2. Put this logic in MapReduce framework and use Job properties to\n   communicate the list of pairs (hdfs path; local path)\n \nDirectories are processed recursively.\nIf archives are treated specially then they are unarchived on destination.\n\n \nMapReduce Job Configuration:\n---------------------------\nOptions:\n\nwith No change in MapReduce framework code (see above)\n no special Job configuration: \n   it is up to the MapRed writer to configure and run the cache operations.\n\n---\nwith Logic in MapReduce framework (see above)\n some simple Job configuration\n\nJobConf.addCachePathPair(String, String)\nJobConf.addCachePathPair(\"/hdfs/path/fileordir\", \"relative/local/fileordir\");\n\n",
        "duedate": null,
        "environment": null,
        "fixVersions": [{
            "archived": false,
            "description": "",
            "id": "12312051",
            "name": "0.7.0",
            "releaseDate": "2006-10-06",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12312051"
        }],
        "issuelinks": [{
            "id": "12313281",
            "inwardIssue": {
                "fields": {
                    "issuetype": {
                        "avatarId": 21141,
                        "description": "A new feature of the product, which has yet to be developed.",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                        "id": "2",
                        "name": "New Feature",
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                        "subtask": false
                    },
                    "priority": {
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                        "id": "4",
                        "name": "Minor",
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/4"
                    },
                    "status": {
                        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                        "id": "5",
                        "name": "Resolved",
                        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                        "statusCategory": {
                            "colorName": "green",
                            "id": 3,
                            "key": "done",
                            "name": "Done",
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
                        }
                    },
                    "summary": "Adding caching to Hadoop which is independent of the task trackers."
                },
                "id": "12347835",
                "key": "MAPREDUCE-458",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/12347835"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12313281",
            "type": {
                "id": "10030",
                "inward": "is related to",
                "name": "Reference",
                "outward": "relates to",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
            }
        }],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095"
            },
            "id": "12310240",
            "key": "HADOOP",
            "name": "Hadoop Common",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310240"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Michel Tourn",
            "key": "michel_tourn",
            "name": "michel_tourn",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michel_tourn",
            "timeZone": "America/New_York"
        },
        "resolution": {
            "description": "A fix for this issue is checked into the tree and tested.",
            "id": "1",
            "name": "Fixed",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1"
        },
        "resolutiondate": "2006-09-14T22:11:37.000+0000",
        "status": {
            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
            "id": "6",
            "name": "Closed",
            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "RFC: Efficient file caching",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2006-10-06T21:48:56.000+0000",
        "versions": [{
            "archived": false,
            "description": "",
            "id": "12312025",
            "name": "0.6.0",
            "releaseDate": "2006-09-08",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12312025"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-288/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-288/watchers",
            "watchCount": 1
        },
        "workratio": -1
    },
    "id": "12344101",
    "key": "HADOOP-288",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12344101"
}