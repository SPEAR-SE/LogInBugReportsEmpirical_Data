{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "id": "12310687",
            "name": "io",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12310687"
        }],
        "created": "2012-05-11T20:33:28.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Catalin Alexandru Zamfir",
            "key": "catalinalexandru.zamfir@gameloft.com",
            "name": "catalinalexandru.zamfir@gameloft.com",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2012-05-12T05:23:57.655+0000",
        "customfield_12310222": "1_*:*_1_*:*_50300369_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": "memory leak, hadoop",
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "239224",
        "customfield_12310920": "43597",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i07ttb:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Sat May 12 10:31:48 UTC 2012",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "We're trying to write about 1 few billion records, via \"Avro\". When we got this error, that's unrelated to our code:\n\n10725984 [Main] INFO net.gameloft.RnD.Hadoop.App - ## At: 2:58:43.290 # Written: 521000000 records\nException in thread \"DataStreamer for file /Streams/Cubed/Stuff/objGame/aRandomGame/objType/aRandomType/2012/05/11/20/29/Shard.avro block blk_3254486396346586049_75838\" java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:657)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:612)\n        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:184)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1202)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1046)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.getProtocolVersion(Unknown Source)\n        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)\n        at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:160)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3117)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2586)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2790)\n10746169 [Main] INFO net.gameloft.RnD.Hadoop.App - ## At: 2:59:03.474 # Written: 522000000 records\nException in thread \"ResponseProcessor for block blk_4201760269657070412_73948\" java.lang.OutOfMemoryError\n        at sun.misc.Unsafe.allocateMemory(Native Method)\n        at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:117)\n        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:305)\n        at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:75)\n        at sun.nio.ch.IOUtil.read(IOUtil.java:223)\n        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)\n        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:55)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)\n        at java.io.DataInputStream.readFully(DataInputStream.java:195)\n        at java.io.DataInputStream.readLong(DataInputStream.java:416)\n        at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2964)\n#\n# There is insufficient memory for the Java Runtime Environment to continue.\n# Native memory allocation (malloc) failed to allocate 32 bytes for intptr_t in /build/buildd/openjdk-6-6b23~pre11/build/openjdk/hotspot/src/share/vm/runtime/deoptimization.cpp\n[thread 1587264368 also had an error]\n[thread 1111309168 also had an error]\n[thread 1820371824 also had an error]\n[thread 1343454064 also had an error]\n[thread 1345444720 also had an error]\n# An error report file with more information is saved as:\n# [thread 1345444720 also had an error]\n[thread -1091290256 also had an error]\n[thread 678165360 also had an error]\n[thread 678497136 also had an error]\n[thread 675511152 also had an error]\n[thread 1385937776 also had an error]\n[thread 911969136 also had an error]\n[thread -1086207120 also had an error]\n[thread -1088251024 also had an error]\n[thread -1088914576 also had an error]\n[thread -1086870672 also had an error]\n[thread 441797488 also had an error][thread 445778800 also had an error]\n\n[thread 440400752 also had an error]\n[thread 444119920 also had an error][thread 1151298416 also had an error]\n\n[thread 443124592 also had an error]\n[thread 1152625520 also had an error]\n[thread 913628016 also had an error]\n[thread -1095345296 also had an error][thread 1390799728 also had an error]\n\n[thread 443788144 also had an error]\n[thread 676506480 also had an error]\n[thread 1630595952 also had an error]\npure virtual method called\nterminate called without an active exception\npure virtual method called\nAborted\n\nIt seems to be a memory leak. We were opening 5 - 10 buffers to different paths when writing and closing them. We've tested that those buffers do not overrun. And they don't. But watching the application continue writing, we saw that over a period of 5 to 6 hours, it kept constantly increasing in memory, not by the average of 8MB buffer that we've set, but my small values. I'm reading the code and it seems there's a memory leak somewhere, in the way Hadoop does buffer allocation. While we specifically close the buffers if the count of open buffers is above 5 (meaning 5 * 8MB per buffer) this bug still happens.\n\nCan it be fixed? As you can see from the strack trace, it writes a \"fan-out\" path of the type you see in the strack trace. We've let it execute till about 500M records, when this error blew. It's a blocker as these writers need to be production-grade ready, while they're not due to this native buffer allocation that when executing large amounts of writes, seems to generate a memory leak.\n\n",
        "duedate": null,
        "environment": "Ubuntu 64bit, 4GB of RAM, Core Duo processors, commodity hardware.",
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [
            "DataStreamer",
            "I/O",
            "OutOfMemoryError",
            "ResponseProcessor",
            "hadoop,",
            "leak",
            "memory",
            "rpc,"
        ],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
            "id": "1",
            "name": "Blocker",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/1"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095"
            },
            "id": "12310240",
            "key": "HADOOP",
            "name": "Hadoop Common",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310240"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Catalin Alexandru Zamfir",
            "key": "catalinalexandru.zamfir@gameloft.com",
            "name": "catalinalexandru.zamfir@gameloft.com",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com",
            "timeZone": "Etc/UTC"
        },
        "resolution": {
            "description": "The problem isn't valid and it can't be fixed.",
            "id": "6",
            "name": "Invalid",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/6"
        },
        "resolutiondate": "2012-05-12T10:31:47.000+0000",
        "status": {
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "id": "5",
            "name": "Resolved",
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "DataStreamer, OutOfMemoryError, unable to create new native thread",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2012-05-12T10:31:48.000+0000",
        "versions": [{
            "archived": false,
            "description": "maintenance release on branch-1.0",
            "id": "12320152",
            "name": "1.0.2",
            "releaseDate": "2012-04-03",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12320152"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-8396/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-8396/watchers",
            "watchCount": 4
        },
        "workratio": -1
    },
    "id": "12554971",
    "key": "HADOOP-8396",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12554971"
}