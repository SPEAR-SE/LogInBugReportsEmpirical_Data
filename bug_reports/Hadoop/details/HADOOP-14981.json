{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13111989","self":"https://issues.apache.org/jira/rest/api/2/issue/13111989","key":"HADOOP-14981","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":null,"customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Oct 25 14:50:35 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-14981/watchers","watchCount":3,"isWatching":false},"created":"2017-10-25T14:39:03.690+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329058","id":"12329058","description":"2.8.0 release","name":"2.8.0","archived":false,"released":true,"releaseDate":"2017-03-22"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12334219","id":"12334219","description":"2.9.0 release","name":"2.9.0","archived":false,"released":true,"releaseDate":"2017-11-17"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12338416","id":"12338416","description":"3.0.0-beta1 release","name":"3.0.0-beta1","archived":false,"released":true,"releaseDate":"2017-10-03"}],"issuelinks":[{"id":"12518465","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12518465","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13009690","key":"HADOOP-13684","self":"https://issues.apache.org/jira/rest/api/2/issue/13009690","fields":{"summary":"Snappy may complain Hadoop is built without snappy if libhadoop is not found.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-10-25T14:53:13.067+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12330961","id":"12330961","name":"common"}],"timeoriginalestimate":null,"description":"I'm getting a stack trace on trunk because there is no native lib on the path and Snappy has been invoked\r\n{code}\r\n  at java.lang.Thread.run(Thread.java:745)\r\n  ...  Cause: java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy()Z\r\n  at org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy(Native Method)\r\n  at org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:63)\r\n{code}\r\nHADOOP-13684 touched these lines in the past, ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"SnappyCodec.checkNativeCodeLoaded raises UnsatisfiedLinkError if native code is missing","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13111989/comment/16218802","id":"16218802","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Full stack. I believe what is happening is that because {{NativeCodeLoader.buildSupportsSnappy()}} is now being called before {{NativeCodeLoader.isNativeCodeLoaded()}}, the (native) method is being called even when its not there, hence the link failure.\r\n\r\nPossible solutions\r\n* revert HADOOP-13694\r\n* Catch the Link exception and convert to a meaningful error text \"no hadoop native lib\"\r\n\r\n{code}\r\n landsat pipeine *** FAILED ***\r\n  org.apache.spark.SparkException: Job aborted.\r\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\r\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\r\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\r\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n  ...\r\n  Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 14, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy()Z\r\n\tat org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy(Native Method)\r\n\tat org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:63)\r\n\tat org.apache.hadoop.io.compress.SnappyCodec.getCompressorType(SnappyCodec.java:136)\r\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150)\r\n\tat org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131)\r\n\tat org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:102)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$$anonfun$createOutputStream$1.apply(CodecStreams.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$$anonfun$createOutputStream$1.apply(CodecStreams.scala:84)\r\n\tat scala.Option.map(Option.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:84)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CSVFileFormat.scala:135)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:77)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask.org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(FileFormatWriter.scala:418)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2.apply(FileFormatWriter.scala:451)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2.apply(FileFormatWriter.scala:440)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.sql.catalyst.util.AbstractScalaRowIterator.foreach(AbstractScalaRowIterator.scala:26)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask.execute(FileFormatWriter.scala:440)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\r\n\t... 8 more\r\n\r\nDriver stacktrace:\r\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n  at scala.Option.foreach(Option.scala:257)\r\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n  ...\r\n  Cause: org.apache.spark.SparkException: Task failed while writing rows\r\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\r\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\r\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\r\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n  at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n  at java.lang.Thread.run(Thread.java:745)\r\n  ...\r\n  Cause: java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy()Z\r\n  at org.apache.hadoop.util.NativeCodeLoader.buildSupportsSnappy(Native Method)\r\n  at org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:63)\r\n  at org.apache.hadoop.io.compress.SnappyCodec.getCompressorType(SnappyCodec.java:136)\r\n  at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150)\r\n  at org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131)\r\n  at org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:102)\r\n  at org.apache.spark.sql.execution.datasources.CodecStreams$$anonfun$createOutputStream$1.apply(CodecStreams.scala:84)\r\n  at org.apache.spark.sql.execution.datasources.CodecStreams$$anonfun$createOutputStream$1.apply(CodecStreams.scala:84)\r\n  at scala.Option.map(Option.scala:146)\r\n  at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:84)\r\n  ...\r\n\r\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-10-25T14:46:03.399+0000","updated":"2017-10-25T14:46:03.399+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13111989/comment/16218819","id":"16218819","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"[~jojochuang] assigning to you to leave you to work out what to do here. if the previous patch hadn't shipped in 2.8 I'd just revert it; as it has it may be best to move forwards, rather than backwards.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-10-25T14:50:35.979+0000","updated":"2017-10-25T14:50:35.979+0000"}],"maxResults":2,"total":2,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-14981/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3lou7:"}}