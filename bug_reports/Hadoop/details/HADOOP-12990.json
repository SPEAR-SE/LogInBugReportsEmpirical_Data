{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12955333","self":"https://issues.apache.org/jira/rest/api/2/issue/12955333","key":"HADOOP-12990","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-04-01T16:45:24.682+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Apr 18 20:43:38 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-12990/watchers","watchCount":15,"isWatching":false},"created":"2016-04-01T15:42:27.787+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327179","id":"12327179","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-04-18T20:43:38.580+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12310687","id":"12310687","name":"io","description":""},{"self":"https://issues.apache.org/jira/rest/api/2/component/12312070","id":"12312070","name":"native","description":"The native code that is loaded into the jvm"}],"timeoriginalestimate":null,"description":"{{hdfs dfs -text}} hit exception when trying to view the compression file created by Linux lz4 tool.\n\nThe Hadoop version has HADOOP-11184 \"update lz4 to r123\", thus it is using LZ4 library in release r123.\n\nLinux lz4 version:\n{code}\n$ /tmp/lz4 -h 2>&1 | head -1\n*** LZ4 Compression CLI 64-bits r123, by Yann Collet (Apr  1 2016) ***\n{code}\n\nTest steps:\n{code}\n$ cat 10rows.txt\n001|c1|c2|c3|c4|c5|c6|c7|c8|c9\n002|c1|c2|c3|c4|c5|c6|c7|c8|c9\n003|c1|c2|c3|c4|c5|c6|c7|c8|c9\n004|c1|c2|c3|c4|c5|c6|c7|c8|c9\n005|c1|c2|c3|c4|c5|c6|c7|c8|c9\n006|c1|c2|c3|c4|c5|c6|c7|c8|c9\n007|c1|c2|c3|c4|c5|c6|c7|c8|c9\n008|c1|c2|c3|c4|c5|c6|c7|c8|c9\n009|c1|c2|c3|c4|c5|c6|c7|c8|c9\n010|c1|c2|c3|c4|c5|c6|c7|c8|c9\n$ /tmp/lz4 10rows.txt 10rows.txt.r123.lz4\nCompressed 310 bytes into 105 bytes ==> 33.87%\n$ hdfs dfs -put 10rows.txt.r123.lz4 /tmp\n$ hdfs dfs -text /tmp/10rows.txt.r123.lz4\n16/04/01 08:19:07 INFO compress.CodecPool: Got brand-new decompressor [.lz4]\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n    at org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:123)\n    at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:98)\n    at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n    at java.io.InputStream.read(InputStream.java:101)\n    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:85)\n    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:59)\n    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:119)\n    at org.apache.hadoop.fs.shell.Display$Cat.printToStdout(Display.java:106)\n    at org.apache.hadoop.fs.shell.Display$Cat.processPath(Display.java:101)\n    at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:317)\n    at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:289)\n    at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:271)\n    at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:255)\n    at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:118)\n    at org.apache.hadoop.fs.shell.Command.run(Command.java:165)\n    at org.apache.hadoop.fs.FsShell.run(FsShell.java:315)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n    at org.apache.hadoop.fs.FsShell.main(FsShell.java:372)\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"lz4 incompatibility between OS and Hadoop","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15221862","id":"15221862","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"body":"From the backtrace, {{getCompressedData}} got bogus len then used it to allocate memory.\n{code:java}\n  protected int getCompressedData() throws IOException {\n    checkStream();\n\n    // Get the size of the compressed chunk (always non-negative)\n    int len = rawReadInt();          <<<<<<<<<<<<<< got huge bogus value !\n\n    // Read len bytes from underlying stream \n    if (len > buffer.length) {\n      buffer = new byte[len];          <<<<<   OutOfMemory exception\n    }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-01T15:44:27.486+0000","updated":"2016-04-01T15:44:27.486+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15221946","id":"15221946","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"I don't think our Lz4Codec implementation actually uses the FRAME specification (http://cyan4973.github.io/lz4/lz4_Frame_format.html) when creating text based files. It seems it was added in as a codec for use inside block compression formats such as SequenceFiles/HFiles/etc., but wasn't oriented towards Text files from the looks of it, or was introduced at a time when there was no FRAME specification of LZ4.\n\nThe lz4 utility uses the frame specification:\n\n{code}\n# cat actual-file.txt\nhadoop,foo,hadoop,foo,hadoop,foo,hadoop,foo\n# lz4 actual-file.txt\n# lz4cat actual-file.txt.lz4\nhadoop,foo,hadoop,foo,hadoop,foo,hadoop,foo\n# cat actual-file.txt.lz4 | od -X\n0000000 184d2204 15a74064 bf000000 6f646168\n0000020 662c706f 0b2c6f6f 2c500900 0a6f6f66\n0000040 00000000 cf718d62\n0000050\n{code}\n\nNote the header magic bytes that match the FRAME specification: {{184d2204}}, as per http://cyan4973.github.io/lz4/lz4_Frame_format.html\n\nWhereas in Hadoop, we produce just the actual block compression form:\n\n{code}\n# cat StreamCompressor.java \nimport org.apache.hadoop.util.*;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.io.compress.*;\nimport org.apache.hadoop.conf.*;\n\npublic class StreamCompressor {\n\n  public static void main(String[] args) throws Exception {\n    String codecClassname = args[0];\n    Class<?> codecClass = Class.forName(codecClassname);\n    Configuration conf = new Configuration();\n    CompressionCodec codec = (CompressionCodec)\n      ReflectionUtils.newInstance(codecClass, conf);\n    \n    CompressionOutputStream out = codec.createOutputStream(System.out);\n    IOUtils.copyBytes(System.in, out, 4096, false);\n    out.finish();\n  }\n}\n# javac -cp $(hadoop classpath) StreamCompressor.java\n# java -cp $PWD StreamCompressor < actual-file.txt > hadoop-file.txt.lz4\n# # cat hadoop-file.lz4 | od -X\n0000000 2c000000 15000000 646168bf 2c706f6f\n0000020 2c6f6f66 5009000b 6f6f662c 0000000a\n0000035\n{code}\n\nNote that we are not writing any of the FRAME required elements (magic header, etc.), but are only writing the compressed block directly.\n\nTherefore, fundamentally, we are not interoperable with the {{lz4}} utility. The difference is very similar to the GPLExtras' {{LzoCodec}} vs. {{LzopCodec}}, the former is just the data compressing algorithm, but the latter is an actual framed format interoperable with {{lzop}} CLI utility.\n\nTo make ourselves interoperable, we'll need to introduce a new frame wrapping codec such as {{LZ4FrameCodec}}, and users could use that when they want to decompress or compress text data produced/readable by {{lz4/lz4cat}} CLI utilities.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-04-01T16:45:24.682+0000","updated":"2016-04-01T16:45:24.682+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15221965","id":"15221965","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"Is this actually expected to work?  Hadoop logic puts some framing around raw LZ4 so that the data is a sequence of length-prefixed compressed chunks.  I don't know the internals of the Linux lz4 tool, but unless it matches that framing strategy exactly (which I wouldn't expect it to), then I don't expect this would work.\n\nI know this is true of Snappy.  There are various Snappy CLIs out there, and they don't work well with our Snappy codec, because they don't implement the same framing.\n\nOf course, it would be nice if there was a more graceful failure mode than trying to do a huge allocation and hitting {{OutOfMemoryError}}.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-01T16:53:41.441+0000","updated":"2016-04-01T16:53:41.441+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15221968","id":"15221968","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~qwertymaniac].  Our comments crossed paths.  Yours has a lot of great additional details.  :-)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-01T16:54:40.460+0000","updated":"2016-04-01T16:54:40.460+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15222405","id":"15222405","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"body":"Should we support this use case?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-01T22:00:38.296+0000","updated":"2016-04-01T22:00:38.296+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15222434","id":"15222434","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"There does not also seem to be any benefit of using LZ4 Frame format with Hadoop based on the below point from its current documentation: http://cyan4973.github.io/lz4/lz4_Frame_format.html:\n\nbq. The data format defined by this specification does not attempt to allow random access to compressed data.\n\nIt does appear to however have an ability to concatenate frames, in sequential order, so one could technically index it to then make splits out of it (but only for concatenated files).\n\nDoesn't sound like a lot of good would come out for the amount of work needed to support the frame format; LZ4 seems best to use in existing container formats we already have (SequenceFiles, HFiles, any file format that accepts a codec or extensions of those), vs. direct data compression (such as text files).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-04-01T22:17:09.191+0000","updated":"2016-04-01T22:17:09.191+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15223148","id":"15223148","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"body":"Summary of the difference between Hadoop lz4 compression and OS lz4 tool:\n* lz4 tool uses [frame format|http://cyan4973.github.io/lz4/lz4_Frame_format.html]: header + len (little endian) + content + trailer\n* Hadoop uses this block-based format: orignalBlockSize + len (big endian) + content\n* Please note the different endianness for the len field","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-03T07:08:14.686+0000","updated":"2016-04-03T07:08:14.686+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15223149","id":"15223149","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"body":"Able to hack my way to make OS lz4 tool (r114) work with Hadoop (r123 lib):\n{code}\n$ lz4 -h 2>&1 | head -1\n*** LZ4 Compression CLI 64-bits r114, by Yann Collet (Apr 14 2014) ***\n\n$ lz4 10rows.txt 10rows.txt.r114.lz4\nCompressed 310 bytes into 105 bytes ==> 33.87%\n\n$ od -t x1 10rows.txt.r114.lz4\n0000000 04 22 4d 18 64 70 b9 56 00 00 00 ff 13 30 30 31\n0000020 7c 63 31 7c 63 32 7c 63 33 7c 63 34 7c 63 35 7c\n0000040 63 36 7c 63 37 7c 63 38 7c 63 39 0a 30 30 32 1f\n0000060 00 0b 1f 33 1f 00 0b 1f 34 1f 00 0b 1f 35 1f 00\n0000100 0b 1f 36 1f 00 0b 1f 37 1f 00 0b 1f 38 1f 00 0b\n0000120 1f 39 1f 00 0a 2f 31 30 1f 00 04 50 38 7c 63 39\n0000140 0a 00 00 00 00 eb 01 45 d5\n0000151\n\n### Skip 7-byte header and 4-byte len (\"56 00 00 00\" is 86)\n$ dd if=10rows.txt.r114.lz4 of=s11c86.r114.lz4 skip=11 bs=1 count=86\n86+0 records in\n86+0 records out\n86 bytes (86 B) copied, 0.000288006 s, 299 kB/s\n\n### Choose a block size > uncompressed content size\n$ echo -ne '\\x00\\x01\\x00\\x00' > originalBlockSize\n\n### Prepare the len in the endian Hadoop prefers\n$ echo -ne '\\x00\\x00\\x00\\x56' > len\n\n### orginalBlockSize + len + compressed_bytes\n$ cat originalBlockSize len s11c86.r114.lz4 >a2.r114.lz4\n\n$ hdfs dfs -put a2.r114.lz4 /tmp\n$ hdfs dfs -text /tmp/a2.r114.lz4\n16/04/02 23:34:52 INFO compress.CodecPool: Got brand-new decompressor [.lz4]\n001|c1|c2|c3|c4|c5|c6|c7|c8|c9\n002|c1|c2|c3|c4|c5|c6|c7|c8|c9\n003|c1|c2|c3|c4|c5|c6|c7|c8|c9\n004|c1|c2|c3|c4|c5|c6|c7|c8|c9\n005|c1|c2|c3|c4|c5|c6|c7|c8|c9\n006|c1|c2|c3|c4|c5|c6|c7|c8|c9\n007|c1|c2|c3|c4|c5|c6|c7|c8|c9\n008|c1|c2|c3|c4|c5|c6|c7|c8|c9\n009|c1|c2|c3|c4|c5|c6|c7|c8|c9\n010|c1|c2|c3|c4|c5|c6|c7|c8|c9\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-03T07:10:35.037+0000","updated":"2016-04-03T07:10:35.037+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15268231","id":"15268231","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cavanaug","name":"cavanaug","key":"cavanaug","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Cavanaugh","active":true,"timeZone":"America/Los_Angeles"},"body":"We have a lot of large json files we keep around as sort of a master archive.   Our internal analysis on compression shows that lz4 completely dominates gzip/bzip2/snappy/lzo in size and compression/decompression thruput.   In fact for a lot of our data (even other than json) lz4 shreds the competition to the point now I tell most folks not to even bother with gzip or bzip2 and just use lz4.\n\nHowever this causes problems if we ingest things into hdfs or s3 (Databricks) since the lz4 command line tool is incompatible with the hadoop-lz4 implementation.   In order to keep compatibility with existing files, would it be possible to update hadoop-lz4 to check if the signature is for the lz4 frame and then use the newer implementation, but if not then to use the existing legacy hadoop-lz4 format?\n\nOur really experienced java guy that had previously done some apache mode just left or I would have assigned him to produce a patch for this.   I think implementing this would be a big benefit to folks in mixed environments...\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cavanaug","name":"cavanaug","key":"cavanaug","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Cavanaugh","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-05-03T06:54:41.463+0000","updated":"2016-05-03T06:54:41.463+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15268366","id":"15268366","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"body":"[~cavanaug], lz4 command line and hadoop-lz4 use the same lz4 codec library. The difference is only the framing, see my comment and hack on 4/3.\n\nQuestions for your use case:\n* Do your JSON files contain a single JSON object or many JSON records?\n* After ingesting into HDFS, how do you plan to use the data?\n* Have considered these splittable container file formats with compression: SequenceFile, RCFile, ORC, Avro, Parquet? In the container, they can choose any Hadoop codec, including LZ4.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-05-03T08:31:09.770+0000","updated":"2016-05-03T08:31:09.770+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15289809","id":"15289809","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"I agree with [~qwertymaniac] that the right way to fix this is to add a separate codec intended to be compatible with the LZ4 CLI tools.  I don't think it would be too hard to accomplish.  We'd need to avoid using BlockCompressorStream/BlockDecompressorStream since those are doing the big-endian lengths that will totally confuse the CLI tools.  Instead we need to do the same little-endian block-length logic done by the CLI tool along with the 4-byte of zeros for the end marker.  Seems to simply be a 4-byte little-endian length per block, but sometimes the high bit can be set?  Haven't looked at the lz4 CLI code to see what the exact logic is.\n\nBonus points for including the xxhash code so Hadoop can generate and validate checksums.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2016-05-18T21:02:10.124+0000","updated":"2016-05-18T21:02:10.124+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15830536","id":"15830536","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ilganeli","name":"ilganeli","key":"ilganeli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ilya Ganelin","active":true,"timeZone":"Etc/UTC"},"body":"Hi, all - we've just run into this. Is there any chance that this has been resolved along the way? \n\nAny further advice on resolving this if I were to take it on?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ilganeli","name":"ilganeli","key":"ilganeli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ilya Ganelin","active":true,"timeZone":"Etc/UTC"},"created":"2017-01-19T20:18:09.444+0000","updated":"2017-01-19T20:18:09.444+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15830555","id":"15830555","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"body":"Not that I am aware of.\n\nGo for it!  Past comments are good source of info.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-19T20:28:49.233+0000","updated":"2017-01-19T20:28:49.233+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15830570","id":"15830570","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"After having worked on the ZStandard codec in HADOOP-13578, I have a fresher perspective on this.  One main problem with creating a separate codec for LZ4 compatibility is that the existing Hadoop LZ4 codec has claimed the standard '.lz4' extension.  That means when users upload files into HDFS that have been compressed with the standard LZ4 CLI tool, it will try to use the existing, broken LZ4 codec rather than any new one.  They'd have to rename the files to use some non-standard LZ4 extension to select the new codec.  That's not ideal.\n\nIn hindsight, the Hadoop LZ4 codec really should have used the streaming APIs for LZ4 rather than the one-shot or single-step APIs.  Then it wouldn't need the extra framing bytes that broke compatibility with the existing LZ4 CLI, and it wouldn't lead to weird failures where the decoder can't decode anything that was encoded with a larger buffer size.  The streaming API solves all those problems, being able to decode with an arbitrary user-supplied buffer size and without the extra block header hints that Hadoop added.\n\nThe cleanest solution from an end-user standpoint would be to have the existing LZ4 codec automatically detect the format when decoding so that we just have one codec and it works both with the old (IMHO broken) format and the standard LZ4 format.  I'm hoping there are some key signature bytes that LZ4 always places at the beginning of the compressed data stream so that we can automatically detect which one it is.  If that is possible then that would be my preference on how to tackle the issue.  If we can't then the end-user story is much less compelling -- two codecs with significant confusion on which one to use.\n\nHowever there is one gotcha even if we can pull off this approach.  Files generated on clusters with the updated LZ4 codec would not be able to be decoded on clusters that only have the old codec.  If that case has to be supported then we have no choice but to develop a new codec and make users live with the non-standard LZ4 file extensions used by the new codec.  .lz4 files uploaded to Hadoop would continue to fail as they do today until renamed to the non-standard extension.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2017-01-19T20:41:23.633+0000","updated":"2017-01-19T20:41:23.633+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15830582","id":"15830582","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ilganeli","name":"ilganeli","key":"ilganeli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ilya Ganelin","active":true,"timeZone":"Etc/UTC"},"body":"[~jzhuge] My proposed approach is to create a new file based loosely on hadoop/io/compress/Lz4Codec.java reproducing the byte structure analagous to your 4/3 hack. Does that seem reasonable?  \n\nIf my goal is ultimately to use this in something like Spark, if the version of Hadoop we're using is patched with the appropriate class, where would I add additional logic to switch between the two codecs (Lz4Codec vs. Lz4FrameCodec)?  \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ilganeli","name":"ilganeli","key":"ilganeli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ilya Ganelin","active":true,"timeZone":"Etc/UTC"},"created":"2017-01-19T20:48:31.639+0000","updated":"2017-01-19T20:48:31.639+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15830596","id":"15830596","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"bq. My proposed approach is to create a new file based loosely on hadoop/io/compress/Lz4Codec.java reproducing the byte structure analagous to your 4/3 hack. Does that seem reasonable? \n\nThe problem with that approach is the existing codec is using one-shot decode.  If the amount of data to decode exceeds the buffer size being used by the decoder it just blows up.  So it's not as simple as tacking on a header and passing it through to the existing code.  That will work with small amounts of data but not once it exceeds the decoder buffer size.\n\nTo handle arbitrary files generated by the LZ4 CLI tool it really needs to use the streaming API so the buffer sizes used by the encoder and decoder are decoupled, and the decoder can handle arbitrarily large amounts of input without needing to chunk it into one-shot blocks as the current one does.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2017-01-19T20:57:49.381+0000","updated":"2017-01-19T20:57:49.381+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15832709","id":"15832709","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ilganeli","name":"ilganeli","key":"ilganeli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ilya Ganelin","active":true,"timeZone":"Etc/UTC"},"body":"Is there a streaming version of the decoder for Lz4? \n\n[~jlowe] If I understood correctly, were you saying that I should be updating the Lz4Decompressor which is used within the Stream API?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ilganeli","name":"ilganeli","key":"ilganeli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ilya Ganelin","active":true,"timeZone":"Etc/UTC"},"created":"2017-01-21T01:22:14.431+0000","updated":"2017-01-21T04:46:49.535+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15834745","id":"15834745","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"bq. Is there a streaming version of the decoder for Lz4? \n\nThere is a streaming version of the lz4 compressor and decompressor.  See https://github.com/lz4/lz4/blob/dev/lib/lz4.h#L228 and https://github.com/lz4/lz4/blob/dev/lib/lz4.h#L273.  I'm not an expert on LZ4, but I'm assuming that these require the frame format rather than the block format.\n\nbq. If I understood correctly, were you saying that I should be updating the Lz4Decompressor which is used within the Stream API?\n\nIt would be updating both the compressor and decompressor to use the streaming API provided by lz4 as documented in lz4.h.\n\nAs I mentioned earlier, the big decision is whether we should try to fix the existing Lz4Codec (i.e.: the one that claims the standard '.lz4' file extension) to be compatible with the existing, Hadoop-proprietary format and also with files using the .lz4 extension or we should create a completely separate codec that of course cannot use the '.lz4' extension.  I'd prefer the former, but there will be some cross-cluster incompatibilities if a 'new' Lz4Codec generates a compressed file and someone tries to decode it with the 'old' Lz4Codec.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2017-01-23T15:28:55.029+0000","updated":"2017-01-23T15:28:55.029+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15834991","id":"15834991","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ilganeli","name":"ilganeli","key":"ilganeli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ilya Ganelin","active":true,"timeZone":"Etc/UTC"},"body":"[~jlowe] Who can weigh in on which path to go down? My personal vote is also for the former given that adding an additional codec with a different extension will complicate using Hadoop with any other tool. \n\nI'm currently delving into how the Decompressor is using the underlying C code via JNI since it seems that logic will need to be updated alongside the Java classes.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ilganeli","name":"ilganeli","key":"ilganeli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ilya Ganelin","active":true,"timeZone":"Etc/UTC"},"created":"2017-01-23T18:21:34.960+0000","updated":"2017-01-23T18:21:34.960+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12955333/comment/15973452","id":"15973452","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"My apologies for the delay, somehow I missed getting notified on your latest comment.\n\nIf this is really going to work as end-users expect then it needs to use the '.lz4' extension.  That means going with the approach of a unified Lz4Codec that can read both the legacy Lz4Codec and write the CLI-compatible one.  There would need to be a release note explaining the incompatibility for data written by the newer codec for clusters still running the older codec.  There are some other caveats, since this could cause issues for a rolling downgrade (i.e.: data written by the new codec before the downgrade can't be decoded after the downgrade).  We can mitigate this by making the output format of the new codec configurable and setting the default to be the legacy format, but then of course it doesn't work \"out of the box\" with the lz4 CLI tool which will be surprising to some.\n\nAnyway the first step is to see if the first proposal is even feasible -- can the codec reliably auto-detect which format is being used and properly decode both the legacy and CLI-compatible formats.  If that possibility exists then we can work through the logistics of whether the new codec emits the CLI-compatible format by default and how to handle the compatibility scenarios.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2017-04-18T20:43:38.580+0000","updated":"2017-04-18T20:43:38.580+0000"}],"maxResults":20,"total":20,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-12990/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2vitb:"}}