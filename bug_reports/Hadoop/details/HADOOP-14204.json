{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13057567","self":"https://issues.apache.org/jira/rest/api/2/issue/13057567","key":"HADOOP-14204","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334219","id":"12334219","description":"2.9.0 release","name":"2.9.0","archived":false,"released":true,"releaseDate":"2017-11-17"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12339180","id":"12339180","name":"3.0.0-alpha4","archived":false,"released":true,"releaseDate":"2017-07-07"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12340354","id":"12340354","name":"2.8.2","archived":false,"released":true,"releaseDate":"2017-10-24"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-03-20T19:54:07.870+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jun 08 00:13:36 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_13658634_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_64214279","customfield_12312321":null,"resolutiondate":"2017-03-21T13:24:44.422+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-14204/watchers","watchCount":7,"isWatching":false},"created":"2017-03-20T15:46:51.546+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329058","id":"12329058","description":"2.8.0 release","name":"2.8.0","archived":false,"released":true,"releaseDate":"2017-03-22"}],"issuelinks":[{"id":"12498459","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12498459","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"outwardIssue":{"id":"12780618","key":"HADOOP-11694","self":"https://issues.apache.org/jira/rest/api/2/issue/12780618","fields":{"summary":"Ãœber-jira: S3a phase II: robustness, scale and performance","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12498465","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12498465","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13057655","key":"SPARK-20038","self":"https://issues.apache.org/jira/rest/api/2/issue/13057655","fields":{"summary":"FileFormatWriter.ExecuteWriteTask.releaseResources() implementations to be re-entrant","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-06-08T00:13:36.508+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12311814","id":"12311814","name":"fs/s3","description":"S3A filesystem client and other S3 connectivity issues"}],"timeoriginalestimate":null,"description":"Stack trace seen trying to commit a multipart upload, as the EMR code (which takes a {{List<String> etags}} is trying to sort that list directly, which it can't do if the list doesn't want to be sorted.\n\nlater versions of the SDK clone the list before sorting.\n\nWe need to make sure that the list passed in can be sorted.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12339167","id":"12339167","name":"2.8.1","archived":false,"released":true,"releaseDate":"2017-06-08"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12859617","id":"12859617","filename":"HADOOP-14204-branch-2.8-001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-03-20T19:34:04.960+0000","size":983,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12859617/HADOOP-14204-branch-2.8-001.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"S3A multipart commit failing, \"UnsupportedOperationException at java.util.Collections$UnmodifiableList.sort\"","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/15932911","id":"15932911","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Stack. This is my github cloud examples running with Spark master built against hadoop-2.8.0 RC3\n\n{code}\n   org.apache.spark.SparkException: Job aborted.\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:196)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:161)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:161)\n      at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:161)\n      at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:137)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n      at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n      at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n      at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:93)\n      at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:93)\n      at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:442)\n      at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:478)\n      at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n      at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n      at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n      at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:93)\n      at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:93)\n      at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:606)\n      at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n      at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n      at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n      at com.hortonworks.spark.cloud.examples.S3DataFrameExample.action(S3DataFrameExample.scala:160)\n      at com.hortonworks.spark.cloud.ObjectStoreExample$class.action(ObjectStoreExample.scala:67)\n      at com.hortonworks.spark.cloud.examples.S3DataFrameExample.action(S3DataFrameExample.scala:56)\n      at com.hortonworks.spark.cloud.examples.S3DataFrameExampleSuite$$anonfun$2.apply$mcV$sp(S3DataFrameExampleSuite.scala:47)\n      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply$mcV$sp(CloudSuite.scala:133)\n      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply(CloudSuite.scala:131)\n      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply(CloudSuite.scala:131)\n      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)\n      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)\n      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n      at org.scalatest.Transformer.apply(Transformer.scala:22)\n      at org.scalatest.Transformer.apply(Transformer.scala:20)\n      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)\n      at org.scalatest.Suite$class.withFixture(Suite.scala:1122)\n      at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)\n      at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)\n      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\n      at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)\n      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(CloudSuite.scala:40)\n      at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)\n      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfter$$super$runTest(CloudSuite.scala:40)\n      at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)\n      at com.hortonworks.spark.cloud.CloudSuite.runTest(CloudSuite.scala:40)\n      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)\n      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)\n      at scala.collection.immutable.List.foreach(List.scala:381)\n      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\n      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)\n      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)\n      at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)\n      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)\n      at org.scalatest.Suite$class.run(Suite.scala:1424)\n      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)\n      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)\n      at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)\n      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfterAll$$super$run(CloudSuite.scala:40)\n      at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)\n      at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)\n      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfter$$super$run(CloudSuite.scala:40)\n      at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)\n      at com.hortonworks.spark.cloud.CloudSuite.run(CloudSuite.scala:40)\n      at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)\n      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)\n      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)\n      at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n      at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)\n      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)\n      at org.scalatest.Suite$class.run(Suite.scala:1421)\n      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)\n      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)\n      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)\n      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)\n      at scala.collection.immutable.List.foreach(List.scala:381)\n      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)\n      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)\n      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)\n      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)\n      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)\n      at org.scalatest.tools.Runner$.main(Runner.scala:860)\n      at org.scalatest.tools.Runner.main(Runner.scala)\n      Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:253)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:178)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:177)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.UnsupportedOperationException\n\tat java.util.Collections$UnmodifiableList.sort(Collections.java:1331)\n\tat java.util.Collections.sort(Collections.java:175)\n\tat com.amazonaws.services.s3.model.transform.RequestXmlFactory.convertToXmlByteArray(RequestXmlFactory.java:42)\n\tat com.amazonaws.services.s3.AmazonS3Client.completeMultipartUpload(AmazonS3Client.java:2692)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelper.completeMultipartUpload(S3AFileSystem.java:2298)\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:561)\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$200(S3ABlockOutputStream.java:454)\n\tat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:352)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:639)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:117)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:311)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:237)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1360)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:242)\n\t... 8 more\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:160)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:109)\n\t\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:317)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$1.apply$mcV$sp(FileFormatWriter.scala:245)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1369)\n\t\t... 9 more\n\nDriver stacktrace:\n      at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1495)\n      at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1483)\n      at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1482)\n      at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n      at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n      at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1482)\n      at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:810)\n      at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:810)\n      at scala.Option.foreach(Option.scala:257)\n      at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:810)\n      at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1710)\n      at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1665)\n      at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1654)\n      at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n      at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:635)\n      at org.apache.spark.SparkContext.runJob(SparkContext.scala:2019)\n      at org.apache.spark.SparkContext.runJob(SparkContext.scala:2040)\n      at org.apache.spark.SparkContext.runJob(SparkContext.scala:2072)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:176)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:161)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:161)\n      at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:161)\n      at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:137)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n      at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n      at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n      at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:93)\n      at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:93)\n      at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:442)\n      at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:478)\n      at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n      at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n      at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n      at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n      at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n      at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:93)\n      at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:93)\n      at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:606)\n      at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n      at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n      at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n      at com.hortonworks.spark.cloud.examples.S3DataFrameExample.action(S3DataFrameExample.scala:160)\n      at com.hortonworks.spark.cloud.ObjectStoreExample$class.action(ObjectStoreExample.scala:67)\n      at com.hortonworks.spark.cloud.examples.S3DataFrameExample.action(S3DataFrameExample.scala:56)\n      at com.hortonworks.spark.cloud.examples.S3DataFrameExampleSuite$$anonfun$2.apply$mcV$sp(S3DataFrameExampleSuite.scala:47)\n      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply$mcV$sp(CloudSuite.scala:133)\n      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply(CloudSuite.scala:131)\n      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply(CloudSuite.scala:131)\n      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)\n      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)\n      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n      at org.scalatest.Transformer.apply(Transformer.scala:22)\n      at org.scalatest.Transformer.apply(Transformer.scala:20)\n      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)\n      at org.scalatest.Suite$class.withFixture(Suite.scala:1122)\n      at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)\n      at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)\n      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\n      at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)\n      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(CloudSuite.scala:40)\n      at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)\n      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfter$$super$runTest(CloudSuite.scala:40)\n      at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)\n      at com.hortonworks.spark.cloud.CloudSuite.runTest(CloudSuite.scala:40)\n      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)\n      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)\n      at scala.collection.immutable.List.foreach(List.scala:381)\n      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\n      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)\n      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)\n      at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)\n      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)\n      at org.scalatest.Suite$class.run(Suite.scala:1424)\n      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)\n      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)\n      at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)\n      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfterAll$$super$run(CloudSuite.scala:40)\n      at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)\n      at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)\n      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfter$$super$run(CloudSuite.scala:40)\n      at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)\n      at com.hortonworks.spark.cloud.CloudSuite.run(CloudSuite.scala:40)\n      at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)\n      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)\n      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)\n      at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n      at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)\n      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)\n      at org.scalatest.Suite$class.run(Suite.scala:1421)\n      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)\n      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)\n      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)\n      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)\n      at scala.collection.immutable.List.foreach(List.scala:381)\n      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)\n      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)\n      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)\n      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)\n      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)\n      at org.scalatest.tools.Runner$.main(Runner.scala:860)\n      at org.scalatest.tools.Runner.main(Runner.scala)\n      Cause: org.apache.spark.SparkException: Task failed while writing rows\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:253)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:178)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:177)\n      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n      at org.apache.spark.scheduler.Task.run(Task.scala:108)\n      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)\n      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n      at java.lang.Thread.run(Thread.java:745)\n      Cause: java.lang.UnsupportedOperationException\n      at java.util.Collections$UnmodifiableList.sort(Collections.java:1331)\n      at java.util.Collections.sort(Collections.java:175)\n      at com.amazonaws.services.s3.model.transform.RequestXmlFactory.convertToXmlByteArray(RequestXmlFactory.java:42)\n      at com.amazonaws.services.s3.AmazonS3Client.completeMultipartUpload(AmazonS3Client.java:2692)\n      at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelper.completeMultipartUpload(S3AFileSystem.java:2298)\n      at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:561)\n      at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$200(S3ABlockOutputStream.java:454)\n      at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:352)\n      at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n      at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n      at org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:639)\n      at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:117)\n      at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)\n      at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:317)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:311)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:237)\n      at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1360)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:242)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:178)\n      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:177)\n      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n      at org.apache.spark.scheduler.Task.run(Task.scala:108)\n      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)\n      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n      at java.lang.Thread.run(Thread.java:745)\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-03-20T15:59:51.427+0000","updated":"2017-03-20T15:59:51.427+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/15933072","id":"15933072","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Issue is\n\n# AWS SDK assumes passed in {{List<PartETag>}} can be sorted.\n# We are generating it with {{Futures.allAsList(partETagsFutures).get();}}, which inside goes {{return new ListFuture<V>(ImmutableList.copyOf(futures), true,  MoreExecutors.sameThreadExecutor());}}. That is: returns an immutable list.\n\nFix is what the later SDKs do internally: copy the list elements into a new ArrayList.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-03-20T17:04:17.216+0000","updated":"2017-03-20T17:04:17.216+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/15933310","id":"15933310","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"This is one of those Schoedingbugs: it doesn't exist until it surfaces, but now you see it, it's obvious that the code never worked. Except it does, doesn't it?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-03-20T19:00:28.371+0000","updated":"2017-03-20T19:00:28.371+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/15933364","id":"15933364","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Patch 001; create a new, sortable list. This is what the later AWS sdk does internally, it is mostly harmless on those SDKs, and should prevent the problem on the version in Hadoop 2.7-2.8.\n\nTesting: s3a frankfurt, also rebuilt spark & ran the tests downstream, as that was where I saw it. No occurrences in repeated test runs.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-03-20T19:34:04.967+0000","updated":"2017-03-20T19:34:04.967+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/15933401","id":"15933401","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 55s{color} | {color:green} branch-2.8 passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 16s{color} | {color:green} branch-2.8 passed with JDK v1.8.0_121 {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 19s{color} | {color:green} branch-2.8 passed with JDK v1.7.0_121 {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 14s{color} | {color:green} branch-2.8 passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 24s{color} | {color:green} branch-2.8 passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} branch-2.8 passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 34s{color} | {color:green} branch-2.8 passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 13s{color} | {color:green} branch-2.8 passed with JDK v1.8.0_121 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 16s{color} | {color:green} branch-2.8 passed with JDK v1.7.0_121 {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 14s{color} | {color:green} the patch passed with JDK v1.8.0_121 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 17s{color} | {color:green} the patch passed with JDK v1.7.0_121 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 11s{color} | {color:orange} hadoop-tools/hadoop-aws: The patch generated 1 new + 5 unchanged - 1 fixed = 6 total (was 6) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 22s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 11s{color} | {color:green} the patch passed with JDK v1.8.0_121 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 15s{color} | {color:green} the patch passed with JDK v1.7.0_121 {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 23s{color} | {color:green} hadoop-aws in the patch passed with JDK v1.7.0_121. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 18s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 15m 19s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:5af2af1 |\n| JIRA Issue | HADOOP-14204 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12859617/HADOOP-14204-branch-2.8-001.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 3b61ea52345a 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | branch-2.8 / c7b6e0d |\n| Default Java | 1.7.0_121 |\n| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_121 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_121 |\n| findbugs | v3.0.0 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/artifact/patchprocess/diff-checkstyle-hadoop-tools_hadoop-aws.txt |\n| JDK v1.7.0_121  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/testReport/ |\n| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\n| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/console |\n| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-03-20T19:54:07.870+0000","updated":"2017-03-20T19:54:07.870+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/15933663","id":"15933663","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=liuml07","name":"liuml07","key":"liuml07","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=liuml07&avatarId=29203","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=liuml07&avatarId=29203","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=liuml07&avatarId=29203","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=liuml07&avatarId=29203"},"displayName":"Mingliang Liu","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 on this, Thanks Steve.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=liuml07","name":"liuml07","key":"liuml07","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=liuml07&avatarId=29203","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=liuml07&avatarId=29203","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=liuml07&avatarId=29203","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=liuml07&avatarId=29203"},"displayName":"Mingliang Liu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-03-20T22:06:21.209+0000","updated":"2017-03-20T22:06:21.209+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/15934616","id":"15934616","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Jenkins build Hadoop-trunk-Commit #11435 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11435/])\nHADOOP-14204 S3A multipart commit failing, (stevel: rev 2841666f1f2dec96761a0aa34a69cbb20297aa14)\n* (edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2017-03-21T13:21:07.619+0000","updated":"2017-03-21T13:21:07.619+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/15979460","id":"15979460","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"Please set the appropriate 3.x fix version when committing to trunk, thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-04-21T21:52:28.515+0000","updated":"2017-04-21T21:52:28.515+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13057567/comment/16041964","id":"16041964","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"body":"2.8.1 became a security release. Moving fix-version to 2.8.2 after the fact.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-08T00:13:36.483+0000","updated":"2017-06-08T00:13:36.483+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-14204/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3citz:"}}