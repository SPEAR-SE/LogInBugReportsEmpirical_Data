{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12554971","self":"https://issues.apache.org/jira/rest/api/2/issue/12554971","key":"HADOOP-8396","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2012-05-12T05:23:57.655+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sat May 12 10:31:48 UTC 2012","customfield_12310420":"239224","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_50300369_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2012-05-12T10:31:47.988+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-8396/watchers","watchCount":4,"isWatching":false},"created":"2012-05-11T20:33:28.008+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"labels":["DataStreamer","I/O","OutOfMemoryError","ResponseProcessor","hadoop,","leak","memory","rpc,"],"customfield_12312333":null,"customfield_12310230":"memory leak, hadoop","customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320152","id":"12320152","description":"maintenance release on branch-1.0","name":"1.0.2","archived":false,"released":true,"releaseDate":"2012-04-03"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2012-05-12T10:31:48.372+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12310687","id":"12310687","name":"io","description":""}],"timeoriginalestimate":null,"description":"We're trying to write about 1 few billion records, via \"Avro\". When we got this error, that's unrelated to our code:\n\n10725984 [Main] INFO net.gameloft.RnD.Hadoop.App - ## At: 2:58:43.290 # Written: 521000000 records\nException in thread \"DataStreamer for file /Streams/Cubed/Stuff/objGame/aRandomGame/objType/aRandomType/2012/05/11/20/29/Shard.avro block blk_3254486396346586049_75838\" java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:657)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:612)\n        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:184)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1202)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1046)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n        at $Proxy8.getProtocolVersion(Unknown Source)\n        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)\n        at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:160)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3117)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2586)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2790)\n10746169 [Main] INFO net.gameloft.RnD.Hadoop.App - ## At: 2:59:03.474 # Written: 522000000 records\nException in thread \"ResponseProcessor for block blk_4201760269657070412_73948\" java.lang.OutOfMemoryError\n        at sun.misc.Unsafe.allocateMemory(Native Method)\n        at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:117)\n        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:305)\n        at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:75)\n        at sun.nio.ch.IOUtil.read(IOUtil.java:223)\n        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)\n        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:55)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)\n        at java.io.DataInputStream.readFully(DataInputStream.java:195)\n        at java.io.DataInputStream.readLong(DataInputStream.java:416)\n        at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)\n        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2964)\n#\n# There is insufficient memory for the Java Runtime Environment to continue.\n# Native memory allocation (malloc) failed to allocate 32 bytes for intptr_t in /build/buildd/openjdk-6-6b23~pre11/build/openjdk/hotspot/src/share/vm/runtime/deoptimization.cpp\n[thread 1587264368 also had an error]\n[thread 1111309168 also had an error]\n[thread 1820371824 also had an error]\n[thread 1343454064 also had an error]\n[thread 1345444720 also had an error]\n# An error report file with more information is saved as:\n# [thread 1345444720 also had an error]\n[thread -1091290256 also had an error]\n[thread 678165360 also had an error]\n[thread 678497136 also had an error]\n[thread 675511152 also had an error]\n[thread 1385937776 also had an error]\n[thread 911969136 also had an error]\n[thread -1086207120 also had an error]\n[thread -1088251024 also had an error]\n[thread -1088914576 also had an error]\n[thread -1086870672 also had an error]\n[thread 441797488 also had an error][thread 445778800 also had an error]\n\n[thread 440400752 also had an error]\n[thread 444119920 also had an error][thread 1151298416 also had an error]\n\n[thread 443124592 also had an error]\n[thread 1152625520 also had an error]\n[thread 913628016 also had an error]\n[thread -1095345296 also had an error][thread 1390799728 also had an error]\n\n[thread 443788144 also had an error]\n[thread 676506480 also had an error]\n[thread 1630595952 also had an error]\npure virtual method called\nterminate called without an active exception\npure virtual method called\nAborted\n\nIt seems to be a memory leak. We were opening 5 - 10 buffers to different paths when writing and closing them. We've tested that those buffers do not overrun. And they don't. But watching the application continue writing, we saw that over a period of 5 to 6 hours, it kept constantly increasing in memory, not by the average of 8MB buffer that we've set, but my small values. I'm reading the code and it seems there's a memory leak somewhere, in the way Hadoop does buffer allocation. While we specifically close the buffers if the count of open buffers is above 5 (meaning 5 * 8MB per buffer) this bug still happens.\n\nCan it be fixed? As you can see from the strack trace, it writes a \"fan-out\" path of the type you see in the strack trace. We've let it execute till about 500M records, when this error blew. It's a blocker as these writers need to be production-grade ready, while they're not due to this native buffer allocation that when executing large amounts of writes, seems to generate a memory leak.\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"43597","customfield_12312823":null,"summary":"DataStreamer, OutOfMemoryError, unable to create new native thread","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Ubuntu 64bit, 4GB of RAM, Core Duo processors, commodity hardware.","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273597","id":"13273597","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"body":"Also, running \"htop\" on Linux, without anything else running, I've run the tool again. It started with 300 threads. Now it's at 524, 584. It doesn't seem to de-allocate threads, even if we're issuing a \"flush ()\" and \"close ()\" on the writers, which are instances of FSDataInput/OutputStreams. While I've written this it jumped to 644. So there's certainly a bug with how Hadoop does I/O threading.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"created":"2012-05-11T20:47:53.561+0000","updated":"2012-05-11T20:47:53.561+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273782","id":"13273782","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"body":"It ran for a while and died at 3881 threads. There's definitively a problem here in how Hadoop handles native threads. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"created":"2012-05-12T01:15:57.885+0000","updated":"2012-05-12T01:15:57.885+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273841","id":"13273841","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Catalin, \n \nDid you observe top report? Whether memory is growing for this process gradually?\nDid you check the Jmap report? which objects are leaking?\nIt seems to me that native memory is growing....Which GC algorithm are you using for the process? Here is some info about that: http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-user/201201.mbox/%3CCB3F0710.1D6DF%25knoguchi@yahoo-inc.com%3E\nWhat is the thread-max support in your machine? # cat /proc/sys/kernel/threads-max","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-05-12T05:23:57.655+0000","updated":"2012-05-12T05:23:57.655+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273873","id":"13273873","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"body":"I monitored the code with htop. Over time it grew from: \"Tasks: 35, 147 thr, 1 running\" to \"Tasks: 36, 3475 thr, 1 running\". Once I killed it via \"kill\" the number of \"thr\" kept dropping to the same 147. So it seems it's a direct relation between the fact that the application allocates a big number of, I guess, native threads but does not kill them when done.\n\nAlso, I'm calling Runtime.getRuntime ().gc () every time the count of open streams is bigger than 5. Also, I'm explicitly flushing and closing these streams before removing them and before running the getRuntime ().gc () method. I'm not using any specific \"GC\" strategy, but the default one defined by openjdk-6. There are no other GC parameters on the command line when the program is run.\n\nI'll be profiling the code today, using jmap and post the results here.\n\nThe output of cat /proc/sys/kernel/threads-max is: 48056\nStill, the code only gets to about 3/4/5.000 \"thr\" (threads) in htop, reaches about 500M written records (or less by a few millions) and dies.\n\nHere's a dump of jmap -histo:live pid:\n\n num     #instances         #bytes  class name\n----------------------------------------------\n   1:       1303640       96984920  [B\n   2:        976162       69580696  [C\n   3:        648949       31149552  java.nio.HeapByteBuffer\n   4:        647505       31080240  java.nio.HeapCharBuffer\n   5:        533222       12797328  java.util.HashMap$Entry\n   6:        481595       11558280  java.lang.String\n   7:          8086        4556064  [I\n   8:         29805        3901240  <constMethodKlass>\n   9:        177060        2832960  java.lang.Long\n  10:         29805        2388304  <methodKlass>\n  11:         58863        2354520  sun.misc.FloatingDecimal\n  12:             1        2097168  [Lorg.h2.util.CacheObject;\n  13:         50674        2041760  <symbolKlass>\n\n\nUsing \"watch jmap -histo:live\" to see reactions i get this on the CLI, every few million records:\nOpenJDK Server VM warning: GC locker is held; pre-dump GC was skipped\n\nAlso, I see the \"[B\" class name, alternating between 20MB and 90MB, with both of them growing constantly over time.\nAlso, after running the code for 15 minutes, \"Eden space\" and \"PS Old Generation\" started growing like crazy. \"Eden space\" started with an acceptable 25MB while \"PS Old Generation\" something small also (10/25MB, can't remember).\n\nHeap Configuration:\n   MinHeapFreeRatio = 40\n   MaxHeapFreeRatio = 70\n   MaxHeapSize      = 792723456 (756.0MB)\n   NewSize          = 1048576 (1.0MB)\n   MaxNewSize       = 4294901760 (4095.9375MB)\n   OldSize          = 4194304 (4.0MB)\n   NewRatio         = 2\n   SurvivorRatio    = 8\n   PermSize         = 16777216 (16.0MB)\n   MaxPermSize      = 134217728 (128.0MB)\n\nHeap Usage:\nPS Young Generation\nEden Space:\n   capacity = 260177920 (248.125MB)\n   used     = 104665056 (99.81637573242188MB)\n   free     = 155512864 (148.30862426757812MB)\n   40.22826225991814% used\nFrom Space:\n   capacity = 1441792 (1.375MB)\n   used     = 1409864 (1.3445510864257812MB)\n   free     = 31928 (0.03044891357421875MB)\n   97.78553355823864% used\nTo Space:\n   capacity = 1966080 (1.875MB)\n   used     = 0 (0.0MB)\n   free     = 1966080 (1.875MB)\n   0.0% used\nPS Old Generation\n   capacity = 528482304 (504.0MB)\n   used     = 31693784 (30.225547790527344MB)\n   free     = 496788520 (473.77445220947266MB)\n   5.9971324981205045% used\nPS Perm Generation\n   capacity = 16777216 (16.0MB)\n   used     = 13510752 (12.884857177734375MB)\n   free     = 3266464 (3.115142822265625MB)\n   80.53035736083984% used\n\nHope  this ton of information helps you.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"created":"2012-05-12T07:15:54.135+0000","updated":"2012-05-12T07:15:54.135+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273881","id":"13273881","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"body":"Reading this article: http://blog.egilh.com/2006/06/2811aspx.html given that the latest JVM allocates about 1M per thread, means that the 3.000/4.000 threads are consistent to the node I'm running this on which has about 3GB available for \"user-space\". In theory I could reduce the stack size for native threads via -Xss, but that would only increase the number of threads, without actually resolving the problem. I think the problem is that Hadoop should let go of native threads that have already written their data to the HDFS. And I've checked, after writing a few million records, executing a \"reader\" class on that data, returns the data, meaning Hadoop got to write these to the HDFS, but watching \"htop\" the number of threads and memory for this code kept increasing and only when writing started. We're writing from one single thread (main).\n\nHadoop should let go of native threads or instruct the JVM to loose these threads once it knows it's written the corresponding data.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"created":"2012-05-12T07:51:01.752+0000","updated":"2012-05-12T07:51:01.752+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273883","id":"13273883","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"{quote}\nnd I've checked, after writing a few million records, executing a \"reader\" class on that data, returns the data, meaning Hadoop got to write these to the HDFS, but watching \"htop\" the number of threads and memory for this code kept increasing and only when writing started. We're writing from one single thread (main).\n{quote}\nYou are not closing the file from application once your data write completed? When you open the stream to DFS, user only will start writing the data on that stream. Whatever data user writes, DataStreamer will writes to DNs. User only will will know, whether stream can be closed or not. Hadoop Client can not assume that user will not write any more data on that stream. Because, it is still possible that user can write some more data on that stream if stream still opens. Once that is closed automatically Streamer threads will exit. I am not sure you are about this lines or some other. Please correct if I understand ur point wrongly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-05-12T08:04:03.507+0000","updated":"2012-05-12T08:04:03.507+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273895","id":"13273895","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"It is also more likely that you're hitting a ulimit -n limit rather than an actual memory fill up. Hitting that limit will cause the JVM to exit with the same exception.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-05-12T09:07:28.432+0000","updated":"2012-05-12T09:07:28.432+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273905","id":"13273905","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"body":"Mark it as invalid please. We've found the culprit. The base writer was opening files but not closing them. It assumed that FileSystem.create (path, false) would append. A misinterpretation of the docs. We found \"FileSystem.append\" which does exactly what must be done in case a file exists. We were explicitly flushing and closing the streams. When we added a check to see if the file existed or not and opened a FSDataOutputStream via append, the number of threads and consumed memory kept well between 167 and 247, whenever our 'flushing & closing\" scheme went in.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=catalinalexandru.zamfir%40gameloft.com","name":"catalinalexandru.zamfir@gameloft.com","key":"catalinalexandru.zamfir@gameloft.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Catalin Alexandru Zamfir","active":true,"timeZone":"Etc/UTC"},"created":"2012-05-12T09:34:29.600+0000","updated":"2012-05-12T09:34:29.600+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12554971/comment/13273917","id":"13273917","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"Marking it as Invalid.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-05-12T10:31:48.278+0000","updated":"2012-05-12T10:31:48.278+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-8396/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i07ttb:"}}