{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12404208","self":"https://issues.apache.org/jira/rest/api/2/issue/12404208","key":"HADOOP-4162","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2008-09-11T21:10:38.179+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Aug 10 23:51:54 UTC 2010","customfield_12310420":"126346","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_537131842_*|*_1_*:*_2_*:*_27302481253_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2009-07-31T02:12:03.891+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-4162/watchers","watchCount":5,"isWatching":false},"created":"2008-09-11T20:58:30.796+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12312972","id":"12312972","description":"","name":"0.18.0","archived":false,"released":true,"releaseDate":"2008-08-22"}],"issuelinks":[{"id":"12336880","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12336880","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12495405","key":"MAPREDUCE-2258","self":"https://issues.apache.org/jira/rest/api/2/issue/12495405","fields":{"summary":"IFile reader closes stream and compressor in wrong order","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2011-01-13T06:39:01.956+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"CodecPool.getDecompressor(LzopCodec) always creates a brand-new decompressor. I investigated the code, the reason seems to be the following:\nLzopCodec inherits from LzoCodec. The getDecompressorType() method is supposed to return the concrete Decompressor class type the specific Codec class creates. In this case, LzopCodec creates LzopDecompressors and should return LzopDecompressor.class. But instead, it uses the getDecompressorType() method defined in the parent and returns LzoDecompressor.class.\n\nThis leads to CodecPool unable to properly recycle the decompressors.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12389973","id":"12389973","filename":"HADOOP-4162_0_20080911.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-11T21:10:38.140+0000","size":703,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12389973/HADOOP-4162_0_20080911.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"104525","customfield_12312823":null,"summary":"CodecPool.getDecompressor(LzopCodec) always creates a brand-new decompressor.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630374","id":"12630374","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"Hong, can you please try this patch and let me know? Thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-11T21:10:38.179+0000","updated":"2008-09-11T21:10:38.179+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630385","id":"12630385","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amirhyoussefi","name":"amirhyoussefi","key":"amirhyoussefi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Amir Youssefi","active":true,"timeZone":"America/Los_Angeles"},"body":"Before patch:\n2008-09-11 21:22:33,169 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:33,500 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:33,696 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:33,904 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:34,089 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:34,277 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:34,465 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:34,700 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:34,887 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n2008-09-11 21:22:35,074 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor\n\n\nAfter patch:\n\n2008-09-11 21:20:21,454 INFO  compress.CodecPool (CodecPool.java:getDecompressor(121)) - Got brand-new decompressor \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amirhyoussefi","name":"amirhyoussefi","key":"amirhyoussefi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Amir Youssefi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-11T21:25:42.134+0000","updated":"2008-09-11T21:25:42.134+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630387","id":"12630387","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"Green from me.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-11T21:30:29.569+0000","updated":"2008-09-11T21:30:29.569+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630388","id":"12630388","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for helping to test this Amir, marking this PA.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-11T21:32:30.038+0000","updated":"2008-09-11T21:32:30.038+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630400","id":"12630400","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"body":"Shouldn't this patch get applied to 0.18 if one wanted to use 0.18 to do experiments with compression of transient data at large scale?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-11T22:20:43.584+0000","updated":"2008-09-11T22:20:43.584+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630403","id":"12630403","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=chris.douglas","name":"chris.douglas","key":"chris.douglas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Douglas","active":true,"timeZone":"America/Los_Angeles"},"body":"This is probably more correct, but what is the use case? LzopDecompressor tracks and verifies block checksums for lzop streams, but without being initialized by enums to which only LzopCodec has access, it cannot be distinguished from LzoDecopressor. Using LzopCodec as anything but a stream doesn't make sense.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=chris.douglas","name":"chris.douglas","key":"chris.douglas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Douglas","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-11T22:22:28.073+0000","updated":"2008-09-11T22:22:28.073+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630404","id":"12630404","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=chris.douglas","name":"chris.douglas","key":"chris.douglas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Douglas","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Shouldn't this patch get applied to 0.18 if one wanted to use 0.18 to do experiments with compression of transient data at large scale?\n\nIntermediate data is compressed using LzoCodec, not LzopCodec. LzopCodec provides compatibility for users reading streams compressed by the [lzop|http://www.lzop.org] tool.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=chris.douglas","name":"chris.douglas","key":"chris.douglas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Douglas","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-11T22:28:25.566+0000","updated":"2008-09-11T22:28:25.566+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630413","id":"12630413","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=chris.douglas","name":"chris.douglas","key":"chris.douglas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Douglas","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Using LzopCodec as anything but a stream doesn't make sense.\n\nI should probably be clearer. Reusing the decompressor between streams makes sense, but using LzopDecompressor like LzoDecompressor or ZlibDecompressor to effect block compression for a structured file format is not going to work, or at least is unlikely to match the intent. I'm assuming this is related to HADOOP-3315, which- like SequenceFile- shouldn't use LzopCodec.\n\nAs written, an LzopDecompressor instance can't be reused between streams. The checksums aren't reset. LzopDecopressor should clear its checksum maps in initHeaderFlags before adding new ones.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=chris.douglas","name":"chris.douglas","key":"chris.douglas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Douglas","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-11T22:43:12.834+0000","updated":"2008-09-11T23:13:54.573+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630441","id":"12630441","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for the information. The problem was found in the development of TFIle (Hadoop-3315). The way we use Hadoop Compression in TFile is to take each compression block as a separate compression stream (each block writes conclude with compressor.finish()). It makes no assumption of any internals of compression algorithm. The tests show both LZOP and LZO work fine.\n\nAlso, based on the information you provided, it seems that existence of LzopDecompressor is to read lzop compressed data. So I changed to use LZO instead of LZOP internally for TFile now.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-12T00:39:03.022+0000","updated":"2008-09-12T00:39:03.022+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630451","id":"12630451","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amirhyoussefi","name":"amirhyoussefi","key":"amirhyoussefi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Amir Youssefi","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 for the patch. \n\nArun, thanks for the patch and rapid turn-around.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=amirhyoussefi","name":"amirhyoussefi","key":"amirhyoussefi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Amir Youssefi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-12T01:40:55.694+0000","updated":"2008-09-12T01:40:55.694+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12630469","id":"12630469","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=chris.douglas","name":"chris.douglas","key":"chris.douglas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Douglas","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. The way we use Hadoop Compression in TFile is to take each compression block as a separate compression stream (each block writes conclude with compressor.finish()). It makes no assumption of any internals of compression algorithm. The tests show both LZOP and LZO work fine.\nLZOP works because the streams are generated by LzopCodec, which disables all the block checksums (assuming its target will be HDFS, which keeps its own checksums). In that case, the LzopDecompresor is a passthrough to LzoDecompressor. If someone were to pick up a LzopDecompressor and use it on a stream with block checksums, it would fail if that decompressor were reused to open a TFile. Until LzopDecompressors can be reused without errors (i.e. initHeaderFlags clears the checksum flags before setting them for the next stream), I'm \\-1 on making them reusable through CodecPool.\n\nbq. it seems that existence of LzopDecompressor is to read lzop compressed data. So I changed to use LZO instead of LZOP internally for TFile now.\nThat sounds exactly right. Unless one wants to support a the C tool, LzoCodec should always be preferred.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=chris.douglas","name":"chris.douglas","key":"chris.douglas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Douglas","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-12T02:53:18.063+0000","updated":"2008-09-12T02:53:18.063+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12631142","id":"12631142","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12389973/HADOOP-4162_0_20080911.patch\n  against trunk revision 695569.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no tests are needed for this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    -1 contrib tests.  The patch failed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3261/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3261/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3261/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3261/console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-15T21:34:40.846+0000","updated":"2008-09-15T21:34:40.846+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12632064","id":"12632064","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"body":"Chris points out that we need to reset state in LzopDecompressor in the 'reset call...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=acmurthy","name":"acmurthy","key":"acmurthy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun C Murthy","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-09-18T02:44:41.880+0000","updated":"2008-09-18T02:44:41.880+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12648525","id":"12648525","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rafan","name":"rafan","key":"rafan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rong-En Fan","active":true,"timeZone":"Etc/UTC"},"body":"Is there any progress on this? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rafan","name":"rafan","key":"rafan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rong-En Fan","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-18T08:12:36.673+0000","updated":"2008-11-18T08:12:36.673+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12737396","id":"12737396","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"LZO is not part of hadoop any more... Will mark the issue invalid.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2009-07-31T02:11:52.925+0000","updated":"2009-07-31T02:11:52.925+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12897084","id":"12897084","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arov","name":"arov","key":"arov","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10438","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10438","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10438","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10438"},"displayName":"Alex Rovner","active":true,"timeZone":"America/New_York"},"body":"We are using 0.20 and LZO can be used for map compression. In which case the same issue appears:\n\n[2010-08-10 18:19:18,459] INFO  (JvmMetrics.java:71) - Initializing JVM Metrics with processName=SHUFFLE, sessionId=\n[2010-08-10 18:19:18,729] INFO  (GPLNativeCodeLoader.java:34) - Loaded native gpl library\n[2010-08-10 18:19:18,739] INFO  (LzoCodec.java:65) - Successfully loaded & initialized native-lzo library [hadoop-lzo rev fatal: Not a git repository]\n[2010-08-10 18:19:18,793] INFO  (ReduceTask.java:1014) - ShuffleRamManager: MemoryLimit=358881696, MaxSingleShuffleLimit=89720424\n[2010-08-10 18:19:18,836] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,839] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,841] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,842] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,844] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,845] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,847] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,849] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,851] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,852] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,854] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,855] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,857] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,859] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,863] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,865] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,867] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,881] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,885] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,886] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,888] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,889] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,891] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,892] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,894] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,895] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,897] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,899] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,901] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,902] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,904] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,905] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,907] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,908] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,910] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,912] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,914] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,915] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,917] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,918] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,920] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,921] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,923] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,924] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,926] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,928] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,930] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,931] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,933] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,934] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,936] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,938] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,939] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,941] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,942] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,943] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,945] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,946] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,947] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,948] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,949] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,950] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,951] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,953] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,954] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,955] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,956] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,957] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,958] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,960] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,961] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,962] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,963] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,964] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,964] INFO  (CodecPool.java:121) - Got brand-new decompressor\n[2010-08-10 18:19:18,970] INFO  (ReduceTask.java:2468) - attempt_201008101538_0280_r_000000_0 Thread started: Thread for merging on-disk files\n[2010-08-10 18:19:18,971] INFO  (ReduceTask.java:2473) - attempt_201008101538_0280_r_000000_0 Thread waiting: Thread for merging on-disk files\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arov","name":"arov","key":"arov","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10438","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10438","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10438","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10438"},"displayName":"Alex Rovner","active":true,"timeZone":"America/New_York"},"created":"2010-08-10T23:36:50.052+0000","updated":"2010-08-10T23:36:50.052+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12404208/comment/12897086","id":"12897086","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"@alex, your description looks like a very different problem from this jira (LzopCodec instead of LzoCodec). Additionally, LZO code is moved to github. You may want to create a new issue there. http://github.com/omalley/hadoop-gpl-compression/issues.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2010-08-10T23:51:54.279+0000","updated":"2010-08-10T23:51:54.279+0000"}],"maxResults":17,"total":17,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-4162/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0i8tz:"}}