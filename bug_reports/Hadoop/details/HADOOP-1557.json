{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12372796","self":"https://issues.apache.org/jira/rest/api/2/issue/12372796","key":"HADOOP-1557","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/2","id":"2","description":"The problem described is an issue which will never be fixed.","name":"Won't Fix"},"customfield_12312322":null,"customfield_12310220":"2007-07-02T18:23:57.102+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Aug 11 17:39:28 UTC 2008","customfield_12310420":"125312","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_35115886843_*|*_6_*:*_1_*:*_0_*|*_5_*:*_1_*:*_3109478802","customfield_12312321":null,"resolutiondate":"2008-08-11T17:39:28.025+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-1557/watchers","watchCount":0,"isWatching":false},"created":"2007-07-02T07:14:41.182+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2009-07-08T16:41:56.745+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Suppose a block has three replicas and two of the replicas are corrupted. If the replication factor of the file is reduced to 2. The filesystem should preferably delete the two corrupted replicas, otherwise it could lead to a corrupted file.\n\nOne option would be to make the datanode periodically validate all blocks with their corresponding CRCs. The other option would be to make the setReplication call validate existing replicas before deleting excess replicas.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"106172","customfield_12312823":null,"summary":"Deletion of excess replicas should prefer to delete corrupted replicas before deleting valid replicas","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12509636","id":"12509636","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sameerp","name":"sameerp","key":"sameerp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34061"},"displayName":"Sameer Paranjpye","active":true,"timeZone":"America/Los_Angeles"},"body":"Why not have clients and/or datanodes report corrupt replicas to the Namenode? Is this not already done?\n\nThe Namenode should remove and replace corrupt replicas when they are reported by clients/datanodes, probably on read to start off with. This can be subsequently enhanced to to periodic block scanning.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sameerp","name":"sameerp","key":"sameerp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34061","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34061","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34061","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34061"},"displayName":"Sameer Paranjpye","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-07-02T18:23:57.102+0000","updated":"2007-07-02T18:23:57.102+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12509697","id":"12509697","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> The Namenode should remove and replace corrupt replicas when they are reported by clients/datanodes, probably on read to start off with.\n\nThat's already done.  When a checksum error is encountered, and there's more than one replica, the offending replica is removed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-07-02T22:08:30.263+0000","updated":"2007-07-02T22:08:30.263+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12509698","id":"12509698","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"The bug is that when a setReplication() command is sent to the NameNode, no data blocks are being read by the client and/or datanode. A block that was corrupt on the datanode is not known to the namenode at that time.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2007-07-02T22:12:39.949+0000","updated":"2007-07-02T22:12:39.949+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12509949","id":"12509949","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> when a setReplication() command is sent to the NameNode, no data blocks are being read\n\nRight.  But a setReplication() triggers replications, and, when those replications happen, the data is read.  If, when writing the replica, the checksum of the received data does not match the checksum sent with that data, the receiving datanode should report to the namenode that the data was corrupt and abort the replication.  This would cause the source block to be removed (provided there are more replicas) and the namenode to initiate new replications from a different source.\n\nAfter HADOOP-1134, datanodes should always validate checksums as blocks are written.  Whenever there's a mismatch, the write should be aborted.  If the write is a replication (as opposed to an initial write) the mismatch should be reported to the namenode.  Does that sound like the right policy to you?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-07-03T17:31:25.319+0000","updated":"2007-07-03T17:31:25.319+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12509952","id":"12509952","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"That sounds like a good policy. However, the bug I was pointing to occurs when setReplication() is called to *decrease* the number of replicas. In this case, no data blocks are read.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2007-07-03T17:37:00.165+0000","updated":"2007-07-03T17:37:00.165+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12509957","id":"12509957","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> the bug I was pointing to occurs when setReplication() is called to decrease the number of replicas\n\nSorry it's taken me so long to understand this!  Yes, I see the issue now.  I'm not sure I yet have great sympathy for it.  In general, as one decreases the number of replicas, the chances that all of them may be corrupt increases.  After HADOOP-1134 we should primarily only see corruptions due to disk errors.  A disk can start failing at any time.  Validating some replicas before others are removed would somewhat reduce the chances that all replicas are corrupt, but not dramatically, so I'm not convinced it's worth the expense.\n\nDisk errors are not entirely random.  When we see a single error from a disk, we're likely to see more from that disk.  So keeping statistics of the number of corruptions identified per datanode would be very valuable.  And automatically taking datanodes offline when corruptions exceed some threshold might go farther towards addressing this issue than explicitly checking blocks as replication thresholds are reduced, since this would remove replicas from failing drives *before* they're read, replicated, de-replicated, etc.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-07-03T17:53:43.618+0000","updated":"2007-07-03T17:53:43.618+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12509963","id":"12509963","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"I agree. There is not much point in validating replicas just before a setReplication call is issued. Instead, a periodic disk block validation by the Datanode might be handy in detecting these types of problems.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2007-07-03T18:08:31.057+0000","updated":"2007-07-03T18:08:31.057+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12509974","id":"12509974","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> a periodic disk block validation by the Datanode might be handy in detecting these types of problems\n\nYes, it would, especially if the filesystem has been idle or offline for a time.  But for an actively used filesystem, normal use might identify failing drives as effectively.  Scanning the research on disk failures, it looks like they more frequently return a read error rather than corrupt data.  Currently, it looks like a datanode shuts down when it encounters a read error, which is probably sufficient.  The OS shouldn't return a read error unless it has retried several times, the drives ECC has failed, etc.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-07-03T18:43:35.448+0000","updated":"2007-07-03T18:43:35.448+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12372796/comment/12621515","id":"12621515","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"Periodic block verification and handling-corrupt replicas are now part of the Hadoop code base. No additional work is necessary for this one.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2008-08-11T17:39:28.015+0000","updated":"2008-08-11T17:39:28.015+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-1557/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0iizz:"}}