{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13041953","self":"https://issues.apache.org/jira/rest/api/2/issue/13041953","key":"HADOOP-14071","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2017-02-10T12:51:54.286+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Feb 16 09:45:29 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_391727573_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2017-02-14T17:58:32.102+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-14071/watchers","watchCount":4,"isWatching":false},"created":"2017-02-10T05:09:44.582+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12337975","id":"12337975","name":"3.0.0-alpha2","archived":false,"released":true,"releaseDate":"2017-01-25"}],"issuelinks":[{"id":"12494229","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12494229","type":{"id":"12310010","name":"Incorporates","inward":"is part of","outward":"incorporates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310010"},"inwardIssue":{"id":"13038288","key":"HADOOP-14028","self":"https://issues.apache.org/jira/rest/api/2/issue/13038288","fields":{"summary":"S3A BlockOutputStreams doesn't delete temporary files in multipart uploads or handle part upload failures","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-02-16T09:45:29.990+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12311814","id":"12311814","name":"fs/s3","description":"S3A filesystem client and other S3 connectivity issues"}],"timeoriginalestimate":null,"description":"When using the patch from HADOOP-14028, I fairly consistently get {{Failed to reset the request input stream}} exceptions. They're more likely to occur the larger the file that's being written (70GB in the extreme case, but it needs to be one file).\n\n{code}\n2017-02-10 04:21:43 WARN S3ABlockOutputStream:692 - Transfer failure of block FileBlock{index=416, destFile=/tmp/hadoop-root/s3a/s3ablock-0416-4228067786955989475.tmp, state=Upload, dataSize=11591473, limit=104857600}\n2017-02-10 04:21:43 WARN S3AInstrumentation:777 - Closing output stream statistics while data is still marked as pending upload in OutputStreamStatistics{blocksSubmitted=416, blocksInQueue=0, blocksActive=0, blockUploadsCompleted=416, blockUploadsFailed=3, bytesPendingUpload=209747761, bytesUploaded=43317747712, blocksAllocated=416, blocksReleased=416, blocksActivelyAllocated=0, exceptionsInMultipartFinalize=0, transferDuration=1389936 ms, queueDuration=519 ms, averageQueueTime=1 ms, totalUploadDuration=1390455 ms, effectiveBandwidth=3.1153649497466657E7 bytes/s}\nat org.apache.hadoop.fs.s3a.S3AUtils.extractException(S3AUtils.java:200)\nat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\nException in thread \"main\" org.apache.hadoop.fs.s3a.AWSClientIOException: Multi-part upload with id 'Xx.ezqT5hWrY1W92GrcodCip88i8rkJiOcom2nuUAqHtb6aQX__26FYh5uYWKlRNX5vY5ktdmQWlOovsbR8CLmxUVmwFkISXxDRHeor8iH9nPhI3OkNbWJJBLrvB3xLUuLX0zvGZWo7bUrAKB6IGxA--' to 2017/planet-170206.orc on 2017/planet-170206.orc: com.amazonaws.ResetException: Failed to reset the request input stream; If the request involves an input stream, the maximum stream buffer size can be configured via request.getRequestClientOptions().setReadLimit(int): Failed to reset the request input stream; If the request involves an input stream, the maximum stream buffer size can be configured via request.getRequestClientOptions().setReadLimit(int)\nat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.waitForAllPartUploads(S3ABlockOutputStream.java:539)\nat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$100(S3ABlockOutputStream.java:456)\nat org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:351)\nat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\nat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)\nat org.apache.orc.impl.PhysicalFsWriter.close(PhysicalFsWriter.java:221)\nat org.apache.orc.impl.WriterImpl.close(WriterImpl.java:2827)\nat net.mojodna.osm2orc.standalone.OsmPbf2Orc.convert(OsmPbf2Orc.java:296)\nat net.mojodna.osm2orc.Osm2Orc.main(Osm2Orc.java:47)\nCaused by: com.amazonaws.ResetException: Failed to reset the request input stream; If the request involves an input stream, the maximum stream buffer size can be configured via request.getRequestClientOptions().setReadLimit(int)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutor.resetRequestInputStream(AmazonHttpClient.java:1221)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1042)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:948)\nat org.apache.hadoop.fs.s3a.SemaphoredDelegatingExecutor$CallableWithPermitRelease.call(SemaphoredDelegatingExecutor.java:222)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:635)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:618)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:661)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:573)\nat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:445)\nat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4041)\nat com.amazonaws.services.s3.AmazonS3Client.doUploadPart(AmazonS3Client.java:3041)\nat com.amazonaws.services.s3.AmazonS3Client.uploadPart(AmazonS3Client.java:3026)\nat org.apache.hadoop.fs.s3a.S3AFileSystem.uploadPart(S3AFileSystem.java:1114)\nat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload$1.call(S3ABlockOutputStream.java:501)\nat org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload$1.call(S3ABlockOutputStream.java:492)\nat org.apache.hadoop.fs.s3a.SemaphoredDelegatingExecutor$CallableWithPermitRelease.call(SemaphoredDelegatingExecutor.java:222)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$300(AmazonHttpClient.java:586)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n... 20 more\nCaused by: java.io.IOException: Resetting to invalid mark\nat java.io.BufferedInputStream.reset(BufferedInputStream.java:448)\nat com.amazonaws.internal.SdkFilterInputStream.reset(SdkFilterInputStream.java:102)\nat com.amazonaws.event.ProgressInputStream.reset(ProgressInputStream.java:169)\nat com.amazonaws.internal.SdkFilterInputStream.reset(SdkFilterInputStream.java:102)\nat com.amazonaws.http.AmazonHttpClient$RequestExecutor.resetRequestInputStream(AmazonHttpClient.java:1219)\nat com.amazonaws.internal.SdkBufferedInputStream.reset(SdkBufferedInputStream.java:106)\n{code}\n\nPotentially relevant: https://github.com/aws/aws-sdk-java/issues/427","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"S3a: Failed to reset the request input stream","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mojodna","name":"mojodna","key":"mojodna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seth Fitzsimmons","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mojodna","name":"mojodna","key":"mojodna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seth Fitzsimmons","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13041953/comment/15861215","id":"15861215","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"\nWhat's happened is that the HTTP connection failed (is this long haul?), aws tried to reset the pointer (using mark/reset) and the buffer couldn't go back that far.\n\n\nWe saw this before, thought I'd eliminated it by not buffering the input stream, but instead sending the file input stream up direct. I'll review that code.\n\nIt may be we need to address this differently, simply by recognising the specific exception and retrying to send the block. That is: we implement the retry logic.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-02-10T12:51:54.286+0000","updated":"2017-02-10T12:51:54.286+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13041953/comment/15861404","id":"15861404","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Thomas+Demoor","name":"Thomas Demoor","key":"thomas demoor","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Thomas Demoor","active":true,"timeZone":"Etc/UTC"},"body":"For the ByteArrayInputStream & ByteBufferInputStream I don't think we currently set  {{request.getRequestClientOptions().setReadLimit}}. My understanding is that, based on the above, we should do this. Is that correct?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Thomas+Demoor","name":"Thomas Demoor","key":"thomas demoor","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Thomas Demoor","active":true,"timeZone":"Etc/UTC"},"created":"2017-02-10T15:20:14.728+0000","updated":"2017-02-10T15:20:14.728+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13041953/comment/15861412","id":"15861412","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"That AWS SDK issue does look relevant.\n\nFor the specific case of data source being a file, we could have the MPU request using that, rather than opening it ourselves. Will need some changes in the code, as currently the BlockOutputStream assumes the source is always some input stream...it'll have to support the option of a File, and if supplied, prefer that as the upload option.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-02-10T15:25:32.207+0000","updated":"2017-02-10T15:25:32.207+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13041953/comment/15861684","id":"15861684","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mojodna","name":"mojodna","key":"mojodna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seth Fitzsimmons","active":true,"timeZone":"America/Los_Angeles"},"body":"This is short-haul (EC2 in us-east-1 to S3 in us-standard).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mojodna","name":"mojodna","key":"mojodna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seth Fitzsimmons","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-10T18:58:12.868+0000","updated":"2017-02-10T18:58:12.868+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13041953/comment/15865735","id":"15865735","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"OK. I'm redoing the HADOOP-14028 patch with the File ref being passed down to AWS. Due to some technical issues (\"laptop is toast\") my dev time is somewhat crippled this week, so I'm not going to give a schedule for that being available. Hopefully in the next day or two â€”I just haven't got hadoop building locally right now","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-02-14T12:57:01.051+0000","updated":"2017-02-14T12:57:01.051+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13041953/comment/15866256","id":"15866256","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"can we move discussion to HADOOP-14208, and I resolve this as a dupe? Keep discussion in one place","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-02-14T17:58:00.956+0000","updated":"2017-02-14T17:58:00.956+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13041953/comment/15869635","id":"15869635","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mojodna","name":"mojodna","key":"mojodna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seth Fitzsimmons","active":true,"timeZone":"America/Los_Angeles"},"body":"For sure.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mojodna","name":"mojodna","key":"mojodna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seth Fitzsimmons","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-16T09:45:29.990+0000","updated":"2017-02-16T09:45:29.990+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-14071/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i39w1r:"}}