{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13189805","self":"https://issues.apache.org/jira/rest/api/2/issue/13189805","key":"HADOOP-15822","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12341706","id":"12341706","description":"2.10.0 Release","name":"2.10.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12342324","id":"12342324","description":"3.2 release","name":"3.2.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12343006","id":"12343006","description":"2.9.2 release","name":"2.9.2","archived":false,"released":true,"releaseDate":"2018-11-19"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12343400","id":"12343400","name":"3.0.4","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12343796","id":"12343796","name":"3.1.2","archived":false,"released":false}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2018-10-05T20:58:41.223+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Oct 24 05:12:19 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_802287_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_1587863279","customfield_12312321":null,"resolutiondate":"2018-10-24T05:03:54.837+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-15822/watchers","watchCount":6,"isWatching":false},"created":"2018-10-05T19:46:09.331+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334219","id":"12334219","description":"2.9.0 release","name":"2.9.0","archived":false,"released":true,"releaseDate":"2017-11-17"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12341431","id":"12341431","description":"3.0.0 GA release","name":"3.0.0","archived":false,"released":true,"releaseDate":"2017-12-13"}],"issuelinks":[{"id":"12544980","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12544980","type":{"id":"12310560","name":"Problem/Incident","inward":"is caused by","outward":"causes","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310560"},"inwardIssue":{"id":"13002509","key":"HADOOP-13578","self":"https://issues.apache.org/jira/rest/api/2/issue/13002509","fields":{"summary":"Add Codec for ZStandard Compression","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/2","id":"2","description":"A new feature of the product, which has yet to be developed.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype","name":"New Feature","subtask":false,"avatarId":21141}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-10-24T05:12:19.497+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"TestZStandardCompressorDecompressor fails a couple of tests on my machine with the latest zstd library (1.3.5).  Compression can fail to successfully finalize the stream when a small output buffer is used resulting in a failed to init error, and decompression with a direct buffer can fail with an invalid src size error.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12341706","id":"12341706","description":"2.10.0 Release","name":"2.10.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12342324","id":"12342324","description":"3.2 release","name":"3.2.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12343006","id":"12343006","description":"2.9.2 release","name":"2.9.2","archived":false,"released":true,"releaseDate":"2018-11-19"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12343400","id":"12343400","name":"3.0.4","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12343796","id":"12343796","name":"3.1.2","archived":false,"released":false}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12942610","id":"12942610","filename":"HADOOP-15822.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2018-10-05T19:52:13.979+0000","size":4881,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12942610/HADOOP-15822.001.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12942613","id":"12942613","filename":"HADOOP-15822.002.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2018-10-05T20:21:29.788+0000","size":4867,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12942613/HADOOP-15822.002.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"zstd compressor can fail with a small output buffer","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16640275","id":"16640275","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Sample test failures:\r\n{noformat}\r\n[INFO] Running org.apache.hadoop.io.compress.zstd.TestZStandardCompressorDecompressor\r\n[ERROR] Tests run: 19, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.758 s <<< FAILURE! - in org.apache.hadoop.io.compress.zstd.TestZStandardCompressorDecompressor\r\n[ERROR] testCompressingWithOneByteOutputBuffer(org.apache.hadoop.io.compress.zstd.TestZStandardCompressorDecompressor)  Time elapsed: 0.108 s  <<< ERROR!\r\njava.lang.InternalError: Context should be init first\r\n\tat org.apache.hadoop.io.compress.zstd.ZStandardCompressor.deflateBytesDirect(Native Method)\r\n\tat org.apache.hadoop.io.compress.zstd.ZStandardCompressor.compress(ZStandardCompressor.java:216)\r\n\tat org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)\r\n\tat org.apache.hadoop.io.compress.CompressorStream.finish(CompressorStream.java:92)\r\n\tat org.apache.hadoop.io.compress.zstd.TestZStandardCompressorDecompressor.testCompressingWithOneByteOutputBuffer(TestZStandardCompressorDecompressor.java:300)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)\r\n\r\n[ERROR] testZStandardDirectCompressDecompress(org.apache.hadoop.io.compress.zstd.TestZStandardCompressorDecompressor)  Time elapsed: 0.014 s  <<< ERROR!\r\njava.lang.InternalError: Src size is incorrect\r\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.inflateBytesDirect(Native Method)\r\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor.inflateDirect(ZStandardDecompressor.java:264)\r\n\tat org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor.decompress(ZStandardDecompressor.java:307)\r\n\tat org.apache.hadoop.io.compress.zstd.TestZStandardCompressorDecompressor.compressDecompressLoop(TestZStandardCompressorDecompressor.java:416)\r\n\tat org.apache.hadoop.io.compress.zstd.TestZStandardCompressorDecompressor.testZStandardDirectCompressDecompress(TestZStandardCompressorDecompressor.java:385)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)\r\n\r\n[INFO] \r\n[INFO] Results:\r\n[INFO] \r\n[ERROR] Errors: \r\n[ERROR]   TestZStandardCompressorDecompressor.testCompressingWithOneByteOutputBuffer:300 » Internal\r\n[ERROR]   TestZStandardCompressorDecompressor.testZStandardDirectCompressDecompress:385->compressDecompressLoop:416 » Internal\r\n[INFO] \r\n[ERROR] Tests run: 19, Failures: 0, Errors: 2, Skipped: 0\r\n{noformat}\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2018-10-05T19:48:51.588+0000","updated":"2018-10-05T19:48:51.588+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16640288","id":"16640288","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Compression flushing failure has to do with how the JNI wrapper code was invoking the zstd library.  When using a small output buffer sometimes flushStream or endStream needs to be called successively to finish flushing everything, but the JNI code would always invoke the compressStream method on a null input buffer before invoking the flush/end call.  Older versions of zstd apparently were OK with this, but the new ones are not.  This patch skips calling compressStream if there is nothing in the input buffer to compress, so the zstd library will see a contiguous sequence of end stream calls towards the end of compression when using small output buffers.\r\n\r\nThe decompress direct test failure is a bug in the interface between the Java layer and the JNI layer.  The function takes a buffer pointer, a buffer length, and a buffer offset, as arguments but the Java layer was using remaining() instead of limit() to send down the size of the buffer.  Occasionally during the test remaining() can be smaller than position() and the zstd library rightfully complains that we are asking it to use a buffer past the end of the reported length.  In addition the test would sometimes fail to flip the output buffer which would break the test when that occurs.\r\n\r\nThese tests also were not running during precommit because the zstandard libraries were missing from the build environment, so this patch adds the libzstd package to the build environment Dockerfile.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2018-10-05T19:59:31.606+0000","updated":"2018-10-05T19:59:31.606+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16640310","id":"16640310","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Minor fix to move the libzstd addition in the Dockerfile to its proper lexicographical place.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2018-10-05T20:22:40.289+0000","updated":"2018-10-05T20:22:40.289+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16640338","id":"16640338","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pbacsko","name":"pbacsko","key":"pbacsko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Bacsko","active":true,"timeZone":"Etc/UTC"},"body":"[~jlowe] what a strange coincidence. I was also testing zstandard today and set {{mapreduce.task.io.sort.mb}} to 2047, which is the max I guess. Now the mapper was running on a 10GiB zstd compressed text file and then failed. The {{equator}} became a negative number and {{collect()}} threw {{ArrayIndexOutOfBoundsException}}. Mapper output compression was also enabled, probably that's what really matters here. It failed after like 40 minutes.\r\n\r\nI'm not sure whether it's zstd or not, because I haven't had the time to try it with other codecs, but it's something worth keeping in mind.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pbacsko","name":"pbacsko","key":"pbacsko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Bacsko","active":true,"timeZone":"Etc/UTC"},"created":"2018-10-05T20:58:41.223+0000","updated":"2018-10-05T20:58:56.927+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16640410","id":"16640410","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"(!) A patch to the testing environment has been detected. \r\nRe-executing against the patched versions to perform further tests. \r\nThe console is at https://builds.apache.org/job/PreCommit-HADOOP-Build/15303/console in case of problems.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2018-10-05T22:13:29.588+0000","updated":"2018-10-05T22:13:29.588+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16640546","id":"16640546","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 18m 23s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  2m 11s{color} | {color:blue} Maven dependency ordering for branch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 23m  6s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 42s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  3m 25s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 13m 19s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 22s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: . {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 33s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  6m 34s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 24s{color} | {color:blue} Maven dependency ordering for patch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m 59s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 36s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} cc {color} | {color:green} 16m 36s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 16m 36s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  3m 47s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} hadolint {color} | {color:green}  0m  2s{color} | {color:green} There were no new hadolint issues. {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 12m 52s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} shellcheck {color} | {color:green}  0m  0s{color} | {color:green} There were no new shellcheck issues. {color} |\r\n| {color:green}+1{color} | {color:green} shelldocs {color} | {color:green}  0m 13s{color} | {color:green} There were no new shelldocs issues. {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 51s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: . {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 56s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  6m 17s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}179m 40s{color} | {color:red} root in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 56s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}353m  2s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency |\r\n|   | hadoop.hdfs.server.balancer.TestBalancer |\r\n|   | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |\r\n|   | hadoop.hdfs.server.namenode.ha.TestHAAppend |\r\n|   | hadoop.hdfs.server.datanode.TestDirectoryScanner |\r\n|   | hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:4b8c2b1 |\r\n| JIRA Issue | HADOOP-15822 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12942613/HADOOP-15822.002.patch |\r\n| Optional Tests |  dupname  asflicense  hadolint  shellcheck  shelldocs  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  cc  |\r\n| uname | Linux 7c0d146c9ac4 3.13.0-153-generic #203-Ubuntu SMP Thu Jun 14 08:52:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / cdf5d58 |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_181 |\r\n| shellcheck | v0.4.6 |\r\n| findbugs | v3.1.0-RC1 |\r\n| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/15303/artifact/out/patch-unit-root.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/15303/testReport/ |\r\n| Max. process+thread count | 2949 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-common-project/hadoop-common . U: . |\r\n| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/15303/console |\r\n| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2018-10-06T04:06:27.642+0000","updated":"2018-10-06T04:06:27.642+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16642000","id":"16642000","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pbacsko","name":"pbacsko","key":"pbacsko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Bacsko","active":true,"timeZone":"Etc/UTC"},"body":"I reproduced the problem. This is what happens if the sort buffer is 2047MiB.\r\n\r\n{noformat}\r\n...\r\n2018-10-08 08:15:04,126 INFO [main] org.apache.hadoop.mapred.MapTask: Spilling map output\r\n2018-10-08 08:15:04,126 INFO [main] org.apache.hadoop.mapred.MapTask: bufstart = 1267927860; bufend = 2082571562; bufvoid = 2146435072\r\n2018-10-08 08:15:04,126 INFO [main] org.apache.hadoop.mapred.MapTask: kvstart = 316981960(1267927840); kvend = 91355880(365423520); length = 225626081/134152192\r\n2018-10-08 08:15:04,126 INFO [main] org.apache.hadoop.mapred.MapTask: (EQUATOR) -1997752227 kvi 37170708(148682832)\r\n2018-10-08 08:16:24,712 INFO [SpillThread] org.apache.hadoop.mapred.MapTask: Finished spill 20\r\n2018-10-08 08:16:24,712 INFO [main] org.apache.hadoop.mapred.MapTask: (RESET) equator -1997752227 kv 37170708(148682832) kvi 37170708(148682832)\r\n2018-10-08 08:16:24,713 INFO [main] org.apache.hadoop.mapred.MapTask: Starting flush of map output\r\n2018-10-08 08:16:24,713 INFO [main] org.apache.hadoop.mapred.MapTask: (RESET) equator -1997752227 kv 37170708(148682832) kvi 37170708(148682832)\r\n2018-10-08 08:16:24,727 INFO [main] org.apache.hadoop.mapred.Merger: Merging 21 sorted segments\r\n2018-10-08 08:16:24,735 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,736 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,738 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,739 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,741 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,742 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,743 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,744 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,745 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,746 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,748 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,749 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,750 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,752 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,753 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,754 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,755 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,756 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,757 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,769 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.zst]\r\n2018-10-08 08:16:24,770 INFO [main] org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 21 segments left of total size: 35310116 bytes\r\n2018-10-08 08:16:30,104 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.ArrayIndexOutOfBoundsException\r\n\tat java.lang.System.arraycopy(Native Method)\r\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1469)\r\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1365)\r\n\tat java.io.DataOutputStream.writeByte(DataOutputStream.java:153)\r\n\tat org.apache.hadoop.io.WritableUtils.writeVLong(WritableUtils.java:273)\r\n\tat org.apache.hadoop.io.WritableUtils.writeVInt(WritableUtils.java:253)\r\n\tat org.apache.hadoop.io.Text.write(Text.java:330)\r\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)\r\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)\r\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1163)\r\n\tat org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:727)\r\n\tat org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)\r\n\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)\r\n\tat org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:47)\r\n\tat org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:36)\r\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\r\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)\r\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)\r\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1726)\r\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\r\n{noformat}\r\n\r\n[~jlowe] do you think it's related? Or is it something different, maybe MR-specific? See my comment above.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pbacsko","name":"pbacsko","key":"pbacsko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Bacsko","active":true,"timeZone":"Etc/UTC"},"created":"2018-10-08T15:20:18.747+0000","updated":"2018-10-08T15:28:26.049+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16642104","id":"16642104","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"bq. do you think it's related? Or is it something different, maybe MR-specific? \r\n\r\nI do not think it is related.  The MapOutput buffer code is miscalculating how much buffer space is remaining before it forces a spill.  In this failure case the buffer involved is not dealing with compressed data, so it should not matter what codec is being used.  Have you tried reproducing it with lz4 or no codec at all?\r\n\r\nI'll dig a bit into the Jenkins test failures to see if they are somehow related. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2018-10-08T16:32:55.689+0000","updated":"2018-10-08T16:32:55.689+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16642106","id":"16642106","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pbacsko","name":"pbacsko","key":"pbacsko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Bacsko","active":true,"timeZone":"Etc/UTC"},"body":"No, I still haven't had the time to check out with other codecs. But tomorrow I'll perform a test with no compression/snappy/lz4/etc.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pbacsko","name":"pbacsko","key":"pbacsko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Bacsko","active":true,"timeZone":"Etc/UTC"},"created":"2018-10-08T16:35:16.740+0000","updated":"2018-10-08T16:35:16.740+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16642522","id":"16642522","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Looked into the unit test failures.\r\n* TestNameNodeMetadataConsistency failure is an existing issue tracked by HDFS-11439\r\n* TestBalancer test has been failing in other precommit builds, filed HDFS-13975\r\n* TestStandbyCheckpoints does not look related and does not reproduce locally\r\n* TestHAAppend is an inode create timeout that does not look related and does not reproduce locally\r\n* TestDirectoryScanner is a timeout that does not look related and does not reproduce locally\r\n* TestTimelineReaderWebServicesHBaseStorage has been failing in nightly builds, filed YARN-8856\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2018-10-08T21:34:06.262+0000","updated":"2018-10-08T21:34:06.262+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16643416","id":"16643416","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pbacsko","name":"pbacsko","key":"pbacsko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Bacsko","active":true,"timeZone":"Etc/UTC"},"body":"[~jlowe] you were right, it's not related to zstandard. I reproduced this with other codecs + no compression. It's possibly an edge case. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=pbacsko","name":"pbacsko","key":"pbacsko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Peter Bacsko","active":true,"timeZone":"Etc/UTC"},"created":"2018-10-09T13:22:06.737+0000","updated":"2018-10-09T13:22:06.737+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16657047","id":"16657047","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~pbacsko] please file a new jira for the AIOOBE you found","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-10-19T16:39:52.024+0000","updated":"2018-10-19T16:39:52.024+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16660254","id":"16660254","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajisakaa","name":"ajisakaa","key":"ajisakaa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"},"displayName":"Akira Ajisaka","active":true,"timeZone":"Asia/Tokyo"},"body":"LGTM, +1. Thanks [~jlowe].","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajisakaa","name":"ajisakaa","key":"ajisakaa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"},"displayName":"Akira Ajisaka","active":true,"timeZone":"Asia/Tokyo"},"created":"2018-10-23T08:19:52.540+0000","updated":"2018-10-23T08:19:52.540+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16661732","id":"16661732","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajisakaa","name":"ajisakaa","key":"ajisakaa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"},"displayName":"Akira Ajisaka","active":true,"timeZone":"Asia/Tokyo"},"body":"Committed this to trunk, branch-3.2, branch-3.2.0, branch-3.1, branch-3.0, branch-2, and branch-2.9. Thanks [~jlowe] for the contribution!\r\n\r\nNote: In branch-2 and branch-2.9, the Ubuntu version in Dockerfile is Trusty and the version does not have libzstd1-dev package, so I didn't update the Dockerfile in branch-2 and branch-2.9.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajisakaa","name":"ajisakaa","key":"ajisakaa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ajisakaa&avatarId=17238","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ajisakaa&avatarId=17238","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ajisakaa&avatarId=17238","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ajisakaa&avatarId=17238"},"displayName":"Akira Ajisaka","active":true,"timeZone":"Asia/Tokyo"},"created":"2018-10-24T05:03:54.868+0000","updated":"2018-10-24T05:03:54.868+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13189805/comment/16661736","id":"16661736","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #15301 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/15301/])\nHADOOP-15822. zstd compressor can fail with a small output buffer. (aajisaka: rev 8f97d6f2cdfccefba5457ae3d561e9ce0109da3f)\n* (edit) dev-support/docker/Dockerfile\n* (edit) hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.c\n* (edit) hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.c\n* (edit) hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/zstd/TestZStandardCompressorDecompressor.java\n* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2018-10-24T05:12:19.497+0000","updated":"2018-10-24T05:12:19.497+0000"}],"maxResults":15,"total":15,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-15822/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3yw93:"}}