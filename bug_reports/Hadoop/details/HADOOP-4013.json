{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12402962","self":"https://issues.apache.org/jira/rest/api/2/issue/12402962","key":"HADOOP-4013","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2008-08-25T09:42:26.465+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Sep 25 16:08:47 UTC 2009","customfield_12310420":"19174","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_34274648069_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2009-09-25T16:08:47.573+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-4013/watchers","watchCount":3,"isWatching":false},"created":"2008-08-24T23:24:39.504+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12312972","id":"12312972","description":"","name":"0.18.0","archived":false,"released":true,"releaseDate":"2008-08-22"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2009-09-25T16:08:47.211+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12311814","id":"12311814","name":"fs/s3","description":"S3A filesystem client and other S3 connectivity issues"}],"timeoriginalestimate":null,"description":"I'm running Hadoop 0.18.0 with a Amazon S3 native filesystem input (s3n URL given for input on the commandline).  I'm having mapper tasks die, which is killing the job.  The error is \"java.net.SocketException: Connection reset\".\n\nI'm using streaming, but my code isn't using any S3 classes itself, this is being done for me by the input reader.  Traceback from the task details, and my invocation, are appended.\n\nSeveral mapper tasks complete before this happens, and I've had other jobs work with input from smaller Amazon S3 buckets for the same account.  So this looks like a connectivity issue, where the input reader should realize that it's calling a web service and try again.\n\nTraceback:\n\njava.net.SocketException: Connection reset\n\tat java.net.SocketInputStream.read(SocketInputStream.java:168)\n\tat com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)\n\tat com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputRecord.java:405)\n\tat com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:360)\n\tat com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:789)\n\tat com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:746)\n\tat com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:317)\n\tat org.apache.commons.httpclient.ContentLengthInputStream.read(ContentLengthInputStream.java:169)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:116)\n\tat org.apache.commons.httpclient.AutoCloseInputStream.read(AutoCloseInputStream.java:107)\n\tat org.jets3t.service.io.InterruptableInputStream.read(InterruptableInputStream.java:72)\n\tat org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.read(HttpMethodReleaseInputStream.java:123)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.read(NativeS3FileSystem.java:98)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:258)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:317)\n\tat java.io.DataInputStream.read(DataInputStream.java:132)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:237)\n\tat org.apache.hadoop.streaming.StreamXmlRecordReader.fastReadUntilMatch(StreamXmlRecordReader.java:248)\n\tat org.apache.hadoop.streaming.StreamXmlRecordReader.readUntilMatchEnd(StreamXmlRecordReader.java:123)\n\tat org.apache.hadoop.streaming.StreamXmlRecordReader.next(StreamXmlRecordReader.java:91)\n\tat org.apache.hadoop.streaming.StreamXmlRecordReader.next(StreamXmlRecordReader.java:46)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:165)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:45)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2209)\n\nPart of my Hadoop invocation (connection info censored, lots of -file includes removed)\n\nhadoop jar /usr/local/hadoop-0.18.0/contrib/streaming/hadoop-0.18.0-streaming.jar -mapper ./spinn3r_vector_mapper.py -input s3n://<key:key>@<bucket>/ -output vectors -jobconf mapred.output.compress=false -inputreader org.apache.hadoop.streaming.StreamXmlRecordReader,begin=<item>,end=</item> -jobconf mapred.map.tasks=128 -jobconf mapred.reduce.tasks=0 [...]\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"104619","customfield_12312823":null,"summary":"SocketException with S3 native file system causes job to fail","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=karl","name":"karl","key":"karl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karl Anderson","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=karl","name":"karl","key":"karl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karl Anderson","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12402962/comment/12625296","id":"12625296","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tomwhite","name":"tomwhite","key":"tomwhite","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tom White","active":true,"timeZone":"Europe/London"},"body":"I think the thing to do here is to have NativeS3FsInputStream retry a number of times if it gets an IOException from the underlying InputStream. It should call store.retrieve(key, pos) to get a new InputStream starting at the current position.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tomwhite","name":"tomwhite","key":"tomwhite","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tom White","active":true,"timeZone":"Europe/London"},"created":"2008-08-25T09:42:26.465+0000","updated":"2008-08-25T09:42:26.465+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12402962/comment/12642770","id":"12642770","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eliast","name":"eliast","key":"eliast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Elias Torres","active":true,"timeZone":"Etc/UTC"},"body":"Tom,\n\nDo you know if anybody is going to followup on this issue? Let me know if I should work on a patch. I think I see what you're saying.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eliast","name":"eliast","key":"eliast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Elias Torres","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-26T02:29:49.470+0000","updated":"2008-10-26T02:29:49.470+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12402962/comment/12642902","id":"12642902","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tomwhite","name":"tomwhite","key":"tomwhite","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tom White","active":true,"timeZone":"Europe/London"},"body":"Elias, I'm not aware of anyone working on this, so feel free to have a go.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tomwhite","name":"tomwhite","key":"tomwhite","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tom White","active":true,"timeZone":"Europe/London"},"created":"2008-10-27T11:55:47.116+0000","updated":"2008-10-27T11:55:47.116+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12402962/comment/12759601","id":"12759601","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tomwhite","name":"tomwhite","key":"tomwhite","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tom White","active":true,"timeZone":"Europe/London"},"body":"Duplicate of HADOOP-6254.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tomwhite","name":"tomwhite","key":"tomwhite","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tom White","active":true,"timeZone":"Europe/London"},"created":"2009-09-25T16:08:47.197+0000","updated":"2009-09-25T16:08:47.197+0000"}],"maxResults":4,"total":4,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-4013/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0i9ev:"}}