{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13115412","self":"https://issues.apache.org/jira/rest/api/2/issue/13115412","key":"HADOOP-15011","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2017-11-01T18:32:27.925+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Nov 02 20:59:34 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_100689768_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2017-11-02T21:00:01.256+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-15011/watchers","watchCount":3,"isWatching":false},"created":"2017-11-01T17:01:51.570+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/5","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/trivial.svg","name":"Trivial","id":"5"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12519155","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12519155","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12969687","key":"HADOOP-13145","self":"https://issues.apache.org/jira/rest/api/2/issue/12969687","fields":{"summary":"In DistCp, prevent unnecessary getFileStatus call when not preserving metadata.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-11-02T21:00:01.315+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12311814","id":"12311814","name":"fs/s3","description":"S3A filesystem client and other S3 connectivity issues"}],"timeoriginalestimate":null,"description":"I'm using the distcp option to copy the huge files from Hadoop to S3. Sometimes i'm getting the below error,\r\n\r\n*Command:* (Copying 378 GB data)\r\n\r\n_hadoop distcp -D HADOOP_OPTS=-Xmx12g -D HADOOP_CLIENT_OPTS='-Xmx12g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' -D 'mapreduce.map.memory.mb=12288' -D 'mapreduce.map.java.opts=-Xmx10g' -D 'mapreduce.reduce.memory.mb=12288' -D 'mapreduce.reduce.java.opts=-Xmx10g' '-Dfs.s3a.proxy.host=edhmgrn-prod.cloud.capitalone.com' '-Dfs.s3a.proxy.port=8088' '-Dfs.s3a.access.key=XXXXXXX' '-Dfs.s3a.secret.key=XXXXXXX' '-Dfs.s3a.connection.timeout=180000' '-Dfs.s3a.attempts.maximum=5' '-Dfs.s3a.fast.upload=true' '-Dfs.s3a.fast.upload.buffer=array' '-Dfs.s3a.fast.upload.active.blocks=50' '-Dfs.s3a.multipart.size=262144000' '-Dfs.s3a.threads.max=500' '-Dfs.s3a.threads.keepalivetime=600' '-Dfs.s3a.server-side-encryption-algorithm=AES256' -bandwidth 3072 -strategy dynamic -m 220 -numListstatusThreads 30 /src/ s3a://bucket/dest\r\n_\r\n17/11/01 12:23:27 INFO mapreduce.Job: Task Id : attempt_1497120915913_2792335_m_000165_0, Status : FAILED\r\nError: java.io.FileNotFoundException: No such file or directory: s3a://bucketname/filename\r\n\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1132)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:78)\r\n        at org.apache.hadoop.tools.util.DistCpUtils.preserve(DistCpUtils.java:197)\r\n        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:256)\r\n        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)\r\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\r\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\r\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\r\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1912)\r\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\r\n\r\n17/11/01 12:28:32 INFO mapreduce.Job: Task Id : attempt_1497120915913_2792335_m_000010_0, Status : FAILED\r\nError: java.io.IOException: File copy failed: hdfs://nameservice1/filena --> s3a://cof-prod-lake-card/src/seam/acct_scores/acctmdlscore_card_cobna_anon_vldtd/instnc_id=20161023000000/000004_0_copy_6\r\n        at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:284)\r\n        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:252)\r\n        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)\r\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\r\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\r\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\r\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1912)\r\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\r\nCaused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://nameservice1/filename to s3a://bucketname/filename\r\n        at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)\r\n        at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:280)\r\n        ... 10 more\r\nCaused by: com.cloudera.com.amazonaws.AmazonClientException: Failed to parse XML document with handler class com.cloudera.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler\r\n        at com.cloudera.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:164)\r\n        at com.cloudera.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseListBucketObjectsResponse(XmlResponsesSaxParser.java:299)\r\n        at com.cloudera.com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsUnmarshaller.unmarshall(Unmarshallers.java:77)\r\n        at com.cloudera.com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsUnmarshaller.unmarshall(Unmarshallers.java:74)\r\n        at com.cloudera.com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62)\r\n        at com.cloudera.com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31)\r\n        at com.cloudera.com.amazonaws.http.AmazonHttpClient.handleResponse(AmazonHttpClient.java:1072)\r\n        at com.cloudera.com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:746)\r\n        at com.cloudera.com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\r\n        at com.cloudera.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\r\n        at com.cloudera.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\r\n        at com.cloudera.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3738)\r\n        at com.cloudera.com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:653)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1096)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.deleteUnnecessaryFakeDirectories(S3AFileSystem.java:1279)\r\n        at org.apache.hadoop.fs.s3a.S3AFileSystem.finishedWrite(S3AFileSystem.java:1268)\r\n        at org.apache.hadoop.fs.s3a.S3AFastOutputStream.close(S3AFastOutputStream.java:257)\r\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\r\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n        at java.io.FilterOutputStream.close(FilterOutputStream.java:159)\r\n        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:261)\r\n        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToFile(RetriableFileCopyCommand.java:184)\r\n        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:124)\r\n        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)\r\n        at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\r\n        ... 11 more\r\nCaused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 2; XML document structures must start and end within the same entity.\r\n        at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)\r\n        at org.apache.xerces.util.ErrorHandlerWrapper.fatalError(Unknown Source)\r\n        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)\r\n        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)\r\n        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)\r\n        at org.apache.xerces.impl.XMLScanner.reportFatalError(Unknown Source)\r\n        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.endEntity(Unknown Source)\r\n        at org.apache.xerces.impl.XMLDocumentScannerImpl.endEntity(Unknown Source)\r\n        at org.apache.xerces.impl.XMLEntityManager.endEntity(Unknown Source)\r\n        at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)\r\n        at org.apache.xerces.impl.XMLEntityScanner.skipChar(Unknown Source)\r\n        at org.apache.xerces.impl.XMLDocumentScannerImpl$PrologDispatcher.dispatch(Unknown Source)\r\n        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\r\n        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\r\n        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\r\n        at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\r\n        at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\r\n        at com.cloudera.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:151)\r\n        ... 35 more\r\n\r\nAnd also please help me in choosing the number of mappers and what should I do to copy the data faster to S3.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Getting file not found exception while using distcp with s3a","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Logesh0589","name":"Logesh0589","key":"logesh0589","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10448","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10448","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10448","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10448"},"displayName":"Logesh Rangan","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Logesh0589","name":"Logesh0589","key":"logesh0589","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10448","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10448","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10448","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10448"},"displayName":"Logesh Rangan","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13115412/comment/16234533","id":"16234533","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"Looks like you are hit by S3's eventual consistency.\r\n\r\nCheck out S3Guard which should help with your problem:\r\nhttps://blog.cloudera.com/blog/2017/08/introducing-s3guard-s3-consistency-for-apache-hadoop/\r\nhttps://hortonworks.com/blog/s3guard-amazon-s3-consistency/","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-01T18:32:27.925+0000","updated":"2017-11-01T18:32:27.925+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13115412/comment/16234627","id":"16234627","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Logesh0589","name":"Logesh0589","key":"logesh0589","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10448","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10448","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10448","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10448"},"displayName":"Logesh Rangan","active":true,"timeZone":"Etc/UTC"},"body":"But our production environment doesn't offer a Dynamo DB instance for S3 Guard. Is there a way to tune the options for distcp to copy the huge files. I'm looking for below information,\r\n\r\n1) How to select the number of  map and it's size. I have a directory which has ~10000+ files with total size of ~250 GB. When I run with below option, it is taking ~1.30 hours.\r\n\r\nhadoop distcp -D HADOOP_OPTS=-Xmx12g -D HADOOP_CLIENT_OPTS='-Xmx12g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' -D 'mapreduce.map.memory.mb=12288' -D 'mapreduce.map.java.opts=-Xmx10g' -D 'mapreduce.reduce.memory.mb=12288' -D 'mapreduce.reduce.java.opts=-Xmx10g' '-Dfs.s3a.proxy.host=edhmgrn-prod.cloud.capitalone.com' '-Dfs.s3a.proxy.port=8088' '-Dfs.s3a.access.key=XXXXXXX' '-Dfs.s3a.secret.key=XXXXXXX' '-Dfs.s3a.connection.timeout=180000' '-Dfs.s3a.attempts.maximum=5' '-Dfs.s3a.fast.upload=true' '-Dfs.s3a.fast.upload.buffer=array' '-Dfs.s3a.fast.upload.active.blocks=50' '-Dfs.s3a.multipart.size=262144000' '-Dfs.s3a.threads.max=500' '-Dfs.s3a.threads.keepalivetime=600' '-Dfs.s3a.server-side-encryption-algorithm=AES256' -bandwidth 3072 -strategy dynamic -m 200 -numListstatusThreads 30 /src/ s3a://bucket/dest\r\n\r\n2) I'm not seeing the throughput of 3gbps even after configuring the -bandwidth as 3072. \r\n\r\n3) How to configure the Java heap and map size for the huge file, so that distcp will give better performance.\r\n\r\n4) WIth fast upload option, I'm writing the files to S3 using threads. Could you please help me in providing some tuning option for this.\r\n\r\nAppreciate Your Help.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Logesh0589","name":"Logesh0589","key":"logesh0589","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10448","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10448","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10448","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10448"},"displayName":"Logesh Rangan","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-01T19:37:31.406+0000","updated":"2017-11-01T19:37:31.406+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13115412/comment/16236593","id":"16236593","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"This is consistency, but not one you need s3guard. Looks more like HADOOP-13145; the stack is exactly the same as HADOOP-11487. Closing as a duplicate of those.\r\n\r\nThis was fixed a while back. What version of CDH are you using?\r\n\r\n* Hadoop 2.8 or the recent HDP and CDH releases have the higher performance upload\r\n* for config : [https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.2/bk_cloud-data-access/content/using-distcp.html]\r\n\r\nmake sure you aren't trying to use: --atomic  or any of the -poptions.\r\n\r\nbq. I'm not seeing the throughput of 3gbps \r\n\r\nI'd be surprised if s3 gave you that. Anyway, it's a \"maximum per node\", not any guarantee of actual B/W.\r\n\r\nAre you trying to write to S3 from a physical cluster, or inside EC2 itself. \r\n\r\n250 GB in 1h30 is 800 KB/s; 6-7 MBits. For a long-haul link, well, its conceivable that is the bandwidth. For in-EC2, its pretty bad.\r\n\r\nIt does a lot of throttling for writes to specific buckets and paths in it. You may find you get better performance by actually cranking back on how aggressive the bandwidth per node is, reducing the # of mappers. Try cutting it in half and seeing what happens. Then do it again.\r\n\r\nbq.  WIth fast upload option, I'm writing the files to S3 using threads. Could you please help me in providing some tuning option for this.\r\n\r\nhttps://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.2/bk_cloud-data-access/content/s3a-fast-upload.html\r\n\r\nIf want to benchmark your upload speed better, download and run https://github.com/steveloughran/cloudup ; for a bulk upload of local data. This isolates all network traffic for the upload, prioritises large files first, and shuffles the filenames to reduce throttling at the back end. Your bandwidth per node will not be > that","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-11-02T20:59:34.110+0000","updated":"2017-11-02T20:59:34.110+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-15011/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3m9xj:"}}