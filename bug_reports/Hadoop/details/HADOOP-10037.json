{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12673038","self":"https://issues.apache.org/jira/rest/api/2/issue/12673038","key":"HADOOP-10037","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327179","id":"12327179","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2015-01-16T13:55:08.587+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jun 30 07:22:32 UTC 2015","customfield_12310420":"352661","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_40081711965_*|*_5_*:*_2_*:*_5318693178_*|*_4_*:*_1_*:*_71000451","customfield_12312321":null,"resolutiondate":"2015-03-19T23:03:22.201+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-10037/watchers","watchCount":8,"isWatching":false},"created":"2013-10-09T16:06:36.658+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320352","id":"12320352","description":"hadoop-2.0.0-alpha release","name":"2.0.0-alpha","archived":false,"released":true,"releaseDate":"2012-05-23"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-06-30T07:22:32.713+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12311814","id":"12311814","name":"fs/s3","description":"S3A filesystem client and other S3 connectivity issues"}],"timeoriginalestimate":null,"description":"For months now we've been finding that we've been experiencing frequent data truncation issues when reading from S3 using the s3n:// protocol.  I finally was able to gather some debugging output on the issue in a job I ran last night, and so can finally file a bug report.\n\n\nThe job I ran last night was on a 16-node cluster (all of them AWS EC2 cc2.8xlarge machines, running Ubuntu 13.04 and Cloudera CDH4.3.0).  The job was a Hadoop streaming job, which reads through a large number (i.e., ~55,000) of files on S3, each of them approximately 300K bytes in size.\n\nAll of the files contain 46 columns of data in each record.  But I added in an extra check in my mapper code to count and verify the number of columns in every record - throwing an error and crashing the map task if the column count is wrong.\n\nIf you look in the attached task logs, you'll see 2 attempts on the same task.  The first one fails due to data truncated (i.e., my job intentionally fails the map task due to the current record failing the column count check).  The task then gets retried on a different machine and runs to a succesful completion.\n\nYou can see further evidence of the truncation further down in the task logs, where it displays the count of the records read:  the failed task says 32953 records read, while the successful task says 63133.\n\nAny idea what the problem might be here and/or how to work around it?  This issue is a very common occurrence on our clusters.  E.g., in the job I ran last night before I had gone to bed I had already encountered 8 such failuers, and the job was only 10% complete.  (~25,000 out of ~250,000 tasks.)\n\nI realize that it's common for I/O errors to occur - possibly even frequently - in a large Hadoop job.  But I would think that if an I/O failure (like a truncated read) did occur, that something in the underlying infrastructure code (i.e., either in NativeS3FileSystem or in jets3t) should detect the error and throw an IOException accordingly.  It shouldn't be up to the calling code to detect such failures, IMO.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12607587","id":"12607587","filename":"S3ReadFailedOnTruncation.html","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"created":"2013-10-09T16:07:10.761+0000","size":7693,"mimeType":"text/html","content":"https://issues.apache.org/jira/secure/attachment/12607587/S3ReadFailedOnTruncation.html"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12607588","id":"12607588","filename":"S3ReadSucceeded.html","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"created":"2013-10-09T16:07:10.765+0000","size":6231,"mimeType":"text/html","content":"https://issues.apache.org/jira/secure/attachment/12607588/S3ReadSucceeded.html"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"352948","customfield_12312823":null,"summary":"s3n read truncated, but doesn't throw exception ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Ubuntu Linux 13.04 running on Amazon EC2 (cc2.8xlarge)","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/13968425","id":"13968425","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"body":"FYI, I recently upgraded our clusters (from CDH 4.3.0 / Hadoop to ) and it looks like this issue might now be solved.  I'm seeing some of the tasks of our Hadoop jobs (failing) as they should with the following wrong-#-of-byes-read exception, which then forces a re-try of the task.\n\n{code}\norg.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 346403598; received: 15815108\n\tat org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184)\n\tat org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:204)\n\tat org.apache.http.impl.io.ContentLengthInputStream.close(ContentLengthInputStream.java:108)\n\tat org.apache.http.conn.BasicManagedEntity.streamClosed(BasicManagedEntity.java:164)\n\tat org.apache.http.conn.EofSensorInputStream.checkClose(EofSensorInputStream.java:237)\n\tat org.apache.http.conn.EofSensorInputStream.close(EofSensorInputStream.java:186)\n\tat org.apache.http.util.EntityUtils.consume(EntityUtils.java:87)\n\tat org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.releaseConnection(HttpMethodReleaseInputStream.java:102)\n\tat org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.close(HttpMethodReleaseInputStream.java:194)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.seek(NativeS3FileSystem.java:152)\n\tat org.apache.hadoop.fs.BufferedFSInputStream.seek(BufferedFSInputStream.java:89)\n\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:63)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:102)\n\tat com.macrosense.mapreduce.io.PingRecordReader.initialize(PingRecordReader.java:80)\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:478)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:671)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:330)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\n{code}\n\nLooks like this fix (in ContentLengthInputStream and/or EofSensorInputStream) was added to Apache HTTP Compoents and/or jets3t some time in the past few months","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"created":"2014-04-14T15:13:14.913+0000","updated":"2014-04-14T15:13:14.913+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/13968427","id":"13968427","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"body":"FYI, I recently upgraded our clusters (from CDH 4.3.0 to CDH5.0.0) and it looks like this issue might now be solved. I'm seeing some of the tasks of our Hadoop jobs (failing) as they should with the following wrong-#-of-byes-read exception, which then forces a re-try of the task.\n\n{code}\norg.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 346403598; received: 15815108\n\tat org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184)\n\tat org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:204)\n\tat org.apache.http.impl.io.ContentLengthInputStream.close(ContentLengthInputStream.java:108)\n\tat org.apache.http.conn.BasicManagedEntity.streamClosed(BasicManagedEntity.java:164)\n\tat org.apache.http.conn.EofSensorInputStream.checkClose(EofSensorInputStream.java:237)\n\tat org.apache.http.conn.EofSensorInputStream.close(EofSensorInputStream.java:186)\n\tat org.apache.http.util.EntityUtils.consume(EntityUtils.java:87)\n\tat org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.releaseConnection(HttpMethodReleaseInputStream.java:102)\n\tat org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.close(HttpMethodReleaseInputStream.java:194)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.seek(NativeS3FileSystem.java:152)\n\tat org.apache.hadoop.fs.BufferedFSInputStream.seek(BufferedFSInputStream.java:89)\n\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:63)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:102)\n\tat com.macrosense.mapreduce.io.PingRecordReader.initialize(PingRecordReader.java:80)\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:478)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:671)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:330)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\n{code}\n\nLooks like this fix (in ContentLengthInputStream and/or EofSensorInputStream) was added to Apache HTTP Compoents and/or jets3t some time in the past few months","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"created":"2014-04-14T15:17:22.629+0000","updated":"2014-04-14T15:17:22.629+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/14280242","id":"14280242","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"closing as Cannot Reproduce, as it appears to have gone away for you.\n\n# Hadoop 2.6 is using a much later version of jets3t\n# Hadoop 2.6 also offers a (compatible) s3a fiesystem which uses the AWS SDK instead. \n\nIf you do see this problem, try using s3a to see if it occurs there","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2015-01-16T13:55:08.587+0000","updated":"2015-01-16T13:55:08.587+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/14368435","id":"14368435","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsato","name":"tsato","key":"tsato","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Takenori Sato","active":true,"timeZone":"Asia/Tokyo"},"body":"I confirmed this happens on Hadoop 2.6.0, and found the reason.\n\nHere's the stacktrace.\n\n{quote}\n\n2015-03-13 20:17:24,866 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream threw an exception\norg.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 296587138; received: 155648\n\tat org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:184)\n\tat org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138)\n\tat org.jets3t.service.io.InterruptableInputStream.read(InterruptableInputStream.java:78)\n\tat org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.read(HttpMethodReleaseInputStream.java:146)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.read(NativeS3FileSystem.java:145)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:273)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n\tat java.io.DataInputStream.read(DataInputStream.java:100)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:185)\n\tat org.apache.pig.builtin.PigStorage.getNext(PigStorage.java:259)\n\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:204)\n\tat org.apache.tez.mapreduce.lib.MRReaderMapReduce.next(MRReaderMapReduce.java:116)\n\tat org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POSimpleTezLoad.getNextTuple(POSimpleTezLoad.java:106)\n\tat org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)\n\tat org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:246)\n\tat org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)\n\tat org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNextTuple(POFilter.java:91)\n\tat org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)\n\tat org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.getNextTuple(POStoreTez.java:117)\n\tat org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:313)\n\tat org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:192)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n\tat org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-03-13 20:17:24,867 [TezChild] INFO  org.apache.hadoop.fs.s3native.NativeS3FileSystem - Received IOException while reading 'user/hadoop/tsato/readlarge/input/cloudian-s3.log.20141119', attempting to reopen.\n2015-03-13 20:17:24,867 [TezChild] DEBUG org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream - Released HttpMethod as its response data stream is fully consumed\n2015-03-13 20:17:24,868 [TezChild] INFO  org.apache.tez.dag.app.TaskAttemptListenerImpTezDag - Commit go/no-go request from attempt_1426245338920_0001_1_00_000004_0\n2015-03-13 20:17:24,868 [TezChild] INFO  org.apache.tez.dag.app.dag.impl.TaskImpl - attempt_1426245338920_0001_1_00_000004_0 given a go for committing the task output.\n\n{quote}\n\nThe problem is that a job successfully finishes after the exception. Thus, its output gets truncated.\n\nThe cause is in the retry logic:\n\n{code:title=NativeS3FileSystem#read(byte[] b, int off, int len)|borderStyle=solid}\n      int result = -1;\n      try {\n        result = in.read(b, off, len);\n      } catch (EOFException eof) {\n        throw eof;\n      } catch (IOException e) {\n        LOG.info( \"Received IOException while reading '{}',\" +\n                  \" attempting to reopen.\", key);\n        seek(pos);\n        result = in.read(b, off, len);\n      }\n{code}\n\nAfter catching the IOException, it attempts to read again from the same inputstream. _NativeS3FileSystem#seek()_ does nothing because the position is not changed. Then, the successive _NativeS3FileSystem#read()_ returns -1 instead of throwing another exception to abort the job. It is done by *EofSensorInputStream* because _EofSensorInputStream#checkAbort()_ makes EofSensorInputStream return false from _EofSensorInputStream#isReadAllowed()_.. \n\n{code:title=EofSensorInputStream#read(byte[] b, int off, int len)|borderStyle=solid}\n    public int read() throws IOException {\n        int l = -1;\n\n        if (isReadAllowed()) {\n            try {\n                l = wrappedStream.read();\n                checkEOF(l);\n            } catch (IOException ex) {\n                checkAbort();\n                throw ex;\n            }\n        }\n\n        return l;\n    }\n{code}\n\nI didn't reach to the history why such a retry is attempted. Even If it does so, I think the inner inputstream should be carefully updated.\n\nIf that's not obvious today, I think making it simply fail without retry is reasonable.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsato","name":"tsato","key":"tsato","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Takenori Sato","active":true,"timeZone":"Asia/Tokyo"},"created":"2015-03-19T03:20:01.772+0000","updated":"2015-03-19T03:20:01.772+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/14368809","id":"14368809","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Searching for the string {{Premature end of Content-Length delimited message body}} brings up [a stack overflow post|\nhttp://stackoverflow.com/questions/9952815/s3-java-client-fails-a-lot-with-premature-end-of-content-length-delimited-mess] blaming the exception message on a GC of the s3 connection client.\n\nLooking at the handler code, it was meant to fix the operation by-reopening the connection. But an optimisation in Hadoop 2.4 (also needed to fix another problem), turned seek(getPos()) to a no-op. Some other way of explicitly re-opening the connection is going to be needed.\n\nFor now, try using s3a:// as the URL to the data. It has different issues in Hadoop 2.6, but by Hadoop 2.7 should be ready to replace s3n completely","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2015-03-19T10:17:07.571+0000","updated":"2015-03-19T10:17:07.571+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/14369350","id":"14369350","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"body":"I think what Takenori Sato is describing is a separate issue than the one I originally reported.  The issue I reported was solved long ago by the \"ConnectionClosedException: Premature end of Content-Length ...\" exception.  Prior to that fix no exception was thrown if the socket didn't successfully read all the data - it would just return the incomplete data.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=darose","name":"darose","key":"darose","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Rosenstrauch","active":true,"timeZone":"America/New_York"},"created":"2015-03-19T14:05:31.283+0000","updated":"2015-03-19T14:05:31.283+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/14370292","id":"14370292","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsato","name":"tsato","key":"tsato","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Takenori Sato","active":true,"timeZone":"Asia/Tokyo"},"body":"David, thanks for your clarification.\n\nI heard from Steve that my issue was introduced by some optimizations done for 2.4.\n\nSo let me close this as FIXED. I will create a new issue for mine.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsato","name":"tsato","key":"tsato","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Takenori Sato","active":true,"timeZone":"Asia/Tokyo"},"created":"2015-03-19T23:02:02.119+0000","updated":"2015-03-19T23:02:02.119+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/14370295","id":"14370295","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsato","name":"tsato","key":"tsato","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Takenori Sato","active":true,"timeZone":"Asia/Tokyo"},"body":"The issue that had reopened this turned out being a separate issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsato","name":"tsato","key":"tsato","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Takenori Sato","active":true,"timeZone":"Asia/Tokyo"},"created":"2015-03-19T23:03:22.243+0000","updated":"2015-03-19T23:03:22.243+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12673038/comment/14607762","id":"14607762","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"body":"Closing old tickets that are already part of a release.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-30T07:22:32.710+0000","updated":"2015-06-30T07:22:32.710+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-10037/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1oswf:"}}