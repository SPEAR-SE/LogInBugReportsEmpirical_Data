{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12393368","self":"https://issues.apache.org/jira/rest/api/2/issue/12393368","key":"HADOOP-3205","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12313563","id":"12313563","description":"","name":"0.21.0","archived":false,"released":true,"releaseDate":"2010-08-23"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2009-11-03T04:59:06.819+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jan 06 11:15:21 UTC 2010","customfield_12310420":"125834","customfield_12312320":null,"customfield_12310222":"10002_*:*_5_*:*_5348005077_*|*_1_*:*_5_*:*_49746431562_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2010-01-05T22:15:19.849+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-3205/watchers","watchCount":9,"isWatching":false},"created":"2008-04-08T06:14:43.210+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"5.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12314296","id":"12314296","description":"","name":"0.22.0","archived":false,"released":true,"releaseDate":"2011-12-10"}],"issuelinks":[{"id":"12327698","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12327698","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"outwardIssue":{"id":"12440081","key":"HDFS-755","self":"https://issues.apache.org/jira/rest/api/2/issue/12440081","fields":{"summary":"Read multiple checksum chunks at once in DFSInputStream","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12327316","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12327316","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12410156","key":"HDFS-347","self":"https://issues.apache.org/jira/rest/api/2/issue/12410156","fields":{"summary":"DFS read performance suboptimal when client co-located on nodes with data","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2010-08-24T20:34:03.718+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12310689","id":"12310689","name":"fs","description":"Generic FileSystem code"}],"timeoriginalestimate":null,"description":"Implementations of FSInputChecker and FSOutputSummer like DFS do not have access to full user buffer. At any time DFS can access only up to 512 bytes even though user usually reads with a much larger buffer (often controlled by io.file.buffer.size). This requires implementations to double buffer data if an implementation wants to read or write larger chunks of data from underlying storage.\n\nWe could separate changes for FSInputChecker and FSOutputSummer into two separate jiras.\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12426818","id":"12426818","filename":"hadoop-3205.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-12-03T21:46:11.743+0000","size":19100,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12426818/hadoop-3205.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12424246","id":"12424246","filename":"hadoop-3205.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-07T01:01:18.062+0000","size":11640,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12424246/hadoop-3205.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12424169","id":"12424169","filename":"hadoop-3205.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-06T00:12:43.473+0000","size":11441,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12424169/hadoop-3205.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12424165","id":"12424165","filename":"hadoop-3205.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-05T23:28:25.344+0000","size":8934,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12424165/hadoop-3205.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12424076","id":"12424076","filename":"hadoop-3205.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-04T23:55:01.272+0000","size":8116,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12424076/hadoop-3205.txt"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"105201","customfield_12312823":null,"summary":"Read multiple chunks directly from FSInputChecker subclass into user buffers","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12772878","id":"12772878","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Been looking at this ticket tonight. I'm not sure exactly what you're getting it. As I am understanding it, the wrapping looks something like:\n\nUser Reader -> FSInputChecker -> FSInputChecker subclass -> BufferedInputStream -> Underlying source\n\ne.g:\n{noformat}\n        java.io.FileInputStream.readBytes(FileInputStream.java:Unknown line)\n        java.io.FileInputStream.read(FileInputStream.java:199)\n        org.apache.hadoop.fs.RawLocalFileSystem$TrackingFileInputStream.read(RawLocalFileSystem.jav\na:90)\n        org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java\n:143)\n        java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        java.io.BufferedInputStream.read1(BufferedInputStream.java:258)\n        java.io.BufferedInputStream.read(BufferedInputStream.java:317)\n        java.io.DataInputStream.read(DataInputStream.java:132)\n        org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:385)\n        org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:224)\n        org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:238)\n        org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:190)\n        org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158)\n        java.io.DataInputStream.read(DataInputStream.java:83)\n        org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:72)\n{noformat}\n\nThe user's buffer size passed in to fs.open(...) controls the size of the BufferedInputStream that wraps the underlying input stream (ie raw file or socket). The FSInputChecker does indeed call read() on that BufferedInputStream once for every 512 bytes (directly into the user buffer), but in my profiling this doesn't seem to be a CPU hog, since it only results in one syscall to the underlying stream for every io.file.buffer.size.\n\nAs a test of the CPU overhead, I put an 800M file (checksummed) in /dev/shm and profiled hadoop fs -cat with io.file.buffer.size=64K. This obviously stresses the CPU hogs and syscall overhead without any actual disk involved. The top consumers are:\n\n{noformat}\n   1 61.17% 61.17%    4363 300617 org.apache.hadoop.fs.FSInputChecker.readChecksumChunk\n   2 13.11% 74.28%     935 300618 java.io.FileInputStream.readBytes\n   3  7.71% 82.00%     550 300632 java.io.DataInputStream.read\n   4  5.02% 87.02%     358 300600 java.io.FileOutputStream.writeBytes\n   5  3.76% 90.77%     268 300657 java.io.DataInputStream.readFully\n   6  1.67% 92.44%     119 300631 java.io.DataInputStream.readFully\n{noformat}\n\nThe particular line of readChecksumChunk that's consuming the time is line 241 (sum.update) - this indicates that the overhead here is just from checksumming and not from memory copies. The one possible gain I could see here would be to revert to a JNI implementation of CRC32 that can do multiple checksum chunks at once - we found that JNI was slow due to a constant overhead \"jumping the gap\" to C for small sizes, but we can probably get 50% checksum speedup for some buffers. This was originally rejected in HADOOP-6148 due to the complexity of maintaining two different CRC32 implementations.\n\nAre you suggesting here that we could do away with the internal buffer and assume that users are always going to do large reads? Doesn't that violate the contract of fs.open taking a buffer size?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-03T04:59:06.819+0000","updated":"2009-11-03T04:59:06.819+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12772914","id":"12772914","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"I think it would be greate if native checksumming code operates on direct buffers and verifies many 512B chunks at a time.\n\nSomething like what I suggested here: http://developer.yahoo.net/blogs/hadoop/2009/08/the_anatomy_of_hadoop_io_pipel.html","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2009-11-03T07:23:28.208+0000","updated":"2009-11-03T07:23:28.208+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773109","id":"12773109","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Hi Hong,\n\nThanks for the input. How do others feel about using a separate CRC32 path for the \"bulk checksum checking\" in the read path, probably through JNI when available? I had suggested this in HADOOP-6148 and people said it would be unmaintainable. Given that checksum algorithms rarely change and are easy to verify, I disagree, but would like to have some +1s for this direction before I spend the time writing the code.\n\nRegarding the other points in your blog post, it seems to imply that we'd have to change around a lot of the APIs to work with ByteBuffers rather than byte[], potentially all the way down to the user-facing layer. This would be a big API change. Where is a good place to start, and what kind of backwards compatibility layer will we need?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-03T18:42:55.634+0000","updated":"2009-11-03T18:42:55.634+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773132","id":"12773132","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. This was originally rejected in HADOOP-6148 due to the complexity of maintaining two different CRC32 \n\nThis jira is not about CRC32 cost, but I don't see why we can't use pure java CRC32 from HADOOP-6148. It is already used in DataNode. CRC32 implementation is transparent to FSInputChecker. If it is good for multiple other places in Hadoop, it is good for FileSystem as well.\n\nbq. Are you suggesting here that we could do away with the internal buffer and assume that users are always going to do large reads? Doesn't that violate the contract of fs.open taking a buffer size?\n\nessentially, yes. When the user gives large buffer, there is no need to copy to intermediate buffer. We would not require or assume the user gives a large buffer but the common case is that user does. DFSClient would read fixed length packet header from the underlying socket and then read the data directly to user buffer if the size is comparable or larger than the packet (64k).\n\nI don't see how any this would violate the contract. fs.open buffer size is only a hint.. underlying FS should know what is more optimal.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-11-03T19:20:35.388+0000","updated":"2009-11-03T19:20:35.388+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773168","id":"12773168","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"bq. I don't see why we can't use pure java CRC32 from HADOOP-6148\n\nWe already do use this - the microbenchmark above (reading checksummed files from /dev/shm) shows that CRC is the majority of the CPU overhead in FSInputChecker and that array copying makes up very little of the time.\n\nbq. When the user gives large buffer, there is no need to copy to intermediate buffer\n\nI see... so I guess what you're saying is that we should do away with the internal BufferedInputStream in DFSClient.BlockReader, and then occasionally insert a buffer only in the case when the user-provided buffer is small? This seems like a fair amount of confusing complexity due to the buffer management involved.\n\nDo we have some kind of benchmark that indicates that these copies make up any appreciable overhead compared to the fairly slow checksumming?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-03T20:20:43.099+0000","updated":"2009-11-03T20:20:43.099+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773191","id":"12773191","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"\nobviously, one buffer copy is not expected to consume more CPU than a CRC32 checksum (much less for checksum of small chunks like 512). I roughly esitmated each buffer copy to take around 1/3rd of what CRC32 takes (ratio might be larger with improved CRC32). Does it mean it is not worth fixing second or third large CPU hogs on client? Of course when there is compression and other higher processing is involved, even CRC32 wouldn't be the largest CPU hog.\n\nWe reduced CPU on DataNode while serving (HADOOP-2758, HADOOP-3164) mainly by avoiding buffer copies (there is CRC involved). All the benchmarks there measure CPU consumed based on actual CPU reported by the OS (not by a profiler).. it is also essentially a 'dfs -cat'.\n\nIn your tests is it reading a dfs file? I used 'dfs -cat' extensively in Datanode CPU benchmarks reported in the above Jiras.\n\nbq. This seems like a fair amount of confusing complexity due to the buffer management involved.\n\nI am not so sure. But just not buffering at all might be good enough (the smallest size would still be 512 bytes).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-11-03T21:03:10.643+0000","updated":"2009-11-03T21:03:10.643+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773193","id":"12773193","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Also, as Hong's blog post points out, there are copies at many places. Advantage of improving the interface is not just avoiding one copy, but could eventually support better handling of direct buffers. \n\nPlus, looking at how long this jira has been open, it is no blocker :-)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-11-03T21:05:37.113+0000","updated":"2009-11-03T21:05:37.113+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773199","id":"12773199","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Looking at the source of BufferedInputStream (at http://www.docjar.com/html/api/java/io/BufferedInputStream.java.html) it actually seems like BufferedInputStream is already handling pass-through to the underlying stream in the case that the read buffer is as large as its own buffer. That was the crucial bit I was missing that explains why performing the underlying reads in larger chunks would make a difference, even without removing the BIS.\n\nI'll give it a go and see if there is any discernible performance increase.\n\nbq. Plus, looking at how long this jira has been open, it is no blocker\n\nOf course :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-03T21:17:54.828+0000","updated":"2009-11-03T21:17:54.828+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773713","id":"12773713","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Here's a patch that implements the FSInputChecker side of this ticket.\n\nBenchmark results are promising. I put a 700MB file in /dev/shm with its associated checksum and then timed \"hadoop fs -cat /dev/shm/bigfile\" 100 times with the patch and without the patch. Here is R output from the analysis of these times:\n\n{noformat}\n> p.user <- read.table(file=\"/tmp/times.patch.user\")\n> p.sys <- read.table(file=\"/tmp/times.patch.sys\")\n> p.wall <- read.table(file=\"/tmp/times.patch.wall\")\n> t.user <- read.table(file=\"/tmp/times.trunk.user\")\n> t.sys <- read.table(file=\"/tmp/times.trunk.sys\")\n> t.wall <- read.table(file=\"/tmp/times.trunk.wall\")\n> t.test(t.user,p.user,alternative=\"greater\")\n\n        Welch Two Sample t-test\n\ndata:  t.user and p.user \nt = 21.0552, df = 134.54, p-value < 2.2e-16\nalternative hypothesis: true difference in means is greater than 0 \n95 percent confidence interval:\n 0.4654936       Inf \nsample estimates:\nmean of x mean of y \n 3.713000  3.207763 \n\n> 3.2077/3.713\n[1] 0.8639106\n> t.test(t.sys,p.sys,alternative=\"greater\")\n\n        Welch Two Sample t-test\n\ndata:  t.sys and p.sys \nt = 1.3567, df = 137.286, p-value = 0.08856\nalternative hypothesis: true difference in means is greater than 0 \n95 percent confidence interval:\n -0.003768599          Inf \nsample estimates:\nmean of x mean of y \n 0.980500  0.963421 \n\n> t.test(t.wall,p.wall,alternative=\"greater\")\n\n        Welch Two Sample t-test\n\ndata:  t.wall and p.wall \nt = 6.5711, df = 118.318, p-value = 7.034e-10\nalternative hypothesis: true difference in means is greater than 0 \n95 percent confidence interval:\n 0.3020628       Inf \nsample estimates:\nmean of x mean of y \n 7.667800  7.263816\n{noformat}\n\nTo interpret the results for those who don't know R:\n- The user time is reduced with 100% confidence. With 95% confidence it's reduced by at least 0.465s = 12.5%\n- The sys time is not significantly reduced - p > 0.05. This is consistent with our expectation that we're doing the same number of syscalls, just avoiding buffer copies in user space.\n- Wall clock time is reduced with 100% confidence. With 95% confidence it's reduced by at least 0.302s = 3.9%.\n\nI didn't include the R output, but analyis on the \"CPU%\" column of the \"time\" results gives 100% confidence of a reduction in CPU percent util, 95% confidence of at least 3.34%.\n\nThe patch itself can probably be improved - just wanted to get early comments. I did briefly test that HDFS still functions, but have not run through all the unit tests. I also want to rerun the above benchmarks with io.file.buffer.size tuned up to 64K or 128K as most people do in production.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-04T23:55:01.300+0000","updated":"2009-11-04T23:55:01.300+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773718","id":"12773718","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"body":"results looking good","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hong.tang","name":"hong.tang","key":"hong.tang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Tang","active":true,"timeZone":"Etc/UTC"},"created":"2009-11-05T00:01:13.013+0000","updated":"2009-11-05T00:01:13.013+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773721","id":"12773721","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Renaming ticket to focus on FSInputChecker. Let's do this one step at a time and open another JIRA for OutputSummer","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-05T00:10:33.425+0000","updated":"2009-11-05T00:10:33.425+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773731","id":"12773731","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Just reran benchmarks with 128K io.file.buffer.size. All of the improvements stayed practically identical, percentage-wise.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-05T00:46:23.437+0000","updated":"2009-11-05T00:46:23.437+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12773735","id":"12773735","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12424076/hadoop-3205.txt\n  against trunk revision 832590.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2009-11-05T01:27:24.947+0000","updated":"2009-11-05T01:27:24.947+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774127","id":"12774127","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Here's a patch which fixes the bugs that caused the unit test failures.\n\nThere's one TODO still in the code to figure out a good setting for MAX_CHUNKS (ie the max number of checksum chunks that should be read in one call to the underlying stream).\n\nThis is still TODO since I made an odd discovery about this - the logic we were going on here was that the performance improvement was due to an eliminated buffer copy when the size of the read where >= the size of the buffer in the underlying BufferedInputStream. This would mean that the correct size for MAX_CHUNKS is ceil(io.file.buffer.size / 512) (ie 256 for a 128KB buffer I was testing with). If MAX_CHUNKS is less than that, then reads to the BIS would be less than its buffer size and thus you'd incur a copy.\n\nHowever, my benchmarking shows that this *isn't* the performance gain. Even with MAX_CHUNKS set to 4, there's a significant performance gain over MAX_CHUNKS set to 1. There is no significant difference between MAX_CHUNKS=127 and MAX_CHUNKS=128 for a 64K buffer, whereas the understanding above would indicate that 128 would eliminate a copy whereas 127 would not.\n\nSo, I think this is actually improving performance because of some other effect like better cache locality by operating in larger chunks. Admittedly, cache locality is always the fallback excuse for a performance increase, but I don't have a better explanation yet. Anyone care to hazard a guess?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-05T23:28:25.377+0000","updated":"2009-11-05T23:28:25.377+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774137","id":"12774137","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12424165/hadoop-3205.txt\n  against trunk revision 832590.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2009-11-05T23:45:08.367+0000","updated":"2009-11-05T23:45:08.367+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774140","id":"12774140","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Previously I had been testing on a trunk from before HADOOP-6223. This new patch includes the same changes to ChecksumFs as were made to ChecksumFileSystem to fix TestLocalFSFileContextMainOperations.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-06T00:12:43.506+0000","updated":"2009-11-06T00:12:43.506+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774160","id":"12774160","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"\nI briefly went through the patch. looks fine. will review  soon. It updates readChunk()'s contract to include multiple chunks. \n\nAre the DFSClient changes going to be part of a different jira? Let me know if I should do that.\nI think hdfs client's read numbers would as good as slightly better.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-11-06T00:47:43.037+0000","updated":"2009-11-06T00:47:43.037+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774161","id":"12774161","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"> So, I think this is actually improving performance because of some other effect like better cache locality by operating in larger chunks. Admittedly, cache locality is always the fallback excuse for a performance increase, but I don't have a better explanation yet. Anyone care to hazard a guess?\n\nhmm... avoiding a copy should save measurable CPU. I will run some experiments as well.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-11-06T00:51:33.259+0000","updated":"2009-11-06T00:51:33.259+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774163","id":"12774163","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Yea, DFSClient will need a small edit as well, and I guess it will be a different JIRA since it's a different project. Annoyingly, the two jiras may have to be committed at the exact same time, since depending on the current implementation in DFSClient it may crash after this core change. I'll compile a core jar with this change and see whether HDFS still passes tests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-06T00:54:46.244+0000","updated":"2009-11-06T00:54:46.244+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774170","id":"12774170","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12424169/hadoop-3205.txt\n  against trunk revision 832590.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2009-11-06T01:22:58.873+0000","updated":"2009-11-06T01:22:58.873+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774520","id":"12774520","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"New patch adds back the static checksum2Long function, since it was public and in use by HDFS code. HDFS continues to compile and pass tests on my machine using a common jar built from this patch. I made checksum2Long be @Deprecated since it's no longer used by FSInputChecker and with this patch it makes more sense to use IntBuffer to store multiple checksums.\n\nI created HDFS-755 to track progress taking advantage of this feature for DFSClient.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-07T01:01:18.097+0000","updated":"2009-11-07T01:01:18.097+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12774559","id":"12774559","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12424246/hadoop-3205.txt\n  against trunk revision 833553.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2009-11-07T05:15:37.509+0000","updated":"2009-11-07T05:15:37.509+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12775179","id":"12775179","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"bq. -1 tests included. The patch doesn't appear to include any new or modified tests.\n\nI believe this code path to be well exercised by existing tests (almost all the tests that do file IO go through this path and exercise it pretty well - note that existing tests failed when there were bugs in prior versions of the patch)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-11-09T22:49:01.532+0000","updated":"2009-11-09T22:49:01.532+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12777805","id":"12777805","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Todd,\n\nNice change! \n\nWould be good to add a comment where the checksum array gets created indicating that the size of the  array determines the size of the underlying IO. \n\nIn readChunk an invalid size checksum file currently results in a ChecksumException, is now an EOFException, would be good to revert back to ChecksumException and add a unit test for this.\n\nFor sanity, do the existing tests cover both small, odd-size single block files?\n\nIn readChecksum I'd make expected and calculated ints (know they were longs before your change) since the code deals w 32-bit sums and just truncate sum.getValue() rather than cast and truncate the checksumInts.get() return value. Could assert sum.getValue() == 0xffffffffL & sum.getValue() if we're feeling paranoid.\n\nIt would be good to test the LocalFs version (appears LocalFs is untested) but since the LocalFs and LocalFileSystem diffs are the same let's leave that to a separate jira.\n\nNits:\n- Idealy the new illegal argument exception in FSInputChecker.set would be an assert since the given checkSumSize is not configurable or passed in at runtime, however since we don't enable asserts yet by default perhaps ok to leave as is.\n\n- The new code in readChecksum would be more readable if it were left pulled out into a separate function (how verifySum was).\n\n- Was this way before your change but the back-to-back if statements on line 252-253 could be combined triviallly.\n\nThanks,\nEli","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-11-14T02:13:42.916+0000","updated":"2009-11-14T02:13:42.916+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12784691","id":"12784691","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Circled around on this issue tonight and tried to look into the mysterious behavior with the value of MAX_CHUNKS (the constant that determines how many checksum chunks worth we'll read in a single go)\n\nI wrote a quick benchmark which read a 1GB file out of /dev/shm with checksums using different values of MAX_CHUNKS. For each value, I ran 50 separate trials and calculated the average, as well as doing t-tests to figure out which results were within noise of each other.\n\n{noformat}\n../hadoop-3205-bench/mc_64.user          3.0954\n../hadoop-3205-bench/mc_128.user         3.1036\n../hadoop-3205-bench/mc_8.user   3.1054\n../hadoop-3205-bench/mc_256.user         3.1104\n../hadoop-3205-bench/mc_32.user          3.1156 ** everything below here is within noise\n../hadoop-3205-bench/mc_16.user          3.1214\n../hadoop-3205-bench/mc_4.user   3.2896\n../hadoop-3205-bench/mc_2.user   3.427\n../hadoop-3205-bench/mc_1.user   3.6832\n\n../hadoop-3205-bench/mc_16.elapsed       3.423\n../hadoop-3205-bench/mc_64.elapsed       3.425\n../hadoop-3205-bench/mc_8.elapsed        3.4288\n../hadoop-3205-bench/mc_256.elapsed      3.4294\n../hadoop-3205-bench/mc_128.elapsed      3.434\n../hadoop-3205-bench/mc_32.elapsed       3.4392 ** everything below here is within noise\n../hadoop-3205-bench/mc_4.elapsed        3.6108\n../hadoop-3205-bench/mc_2.elapsed        3.7032\n../hadoop-3205-bench/mc_1.elapsed        3.9846\n{noformat}\n\nThese were all done with a 64KB io.file.buffer.size, which would make us expect an optimal value of 128, since it should eliminate a copy. The results show that there are no gains to be had after 16 or 32 chunks being read at a time (8-16KB). The L1 cache on this machine is 128K, so that's not the magic number either.\n\nSo basically, the performance improvement here remains a mystery to me, but it's clear there is one - about 13% for reading out of RAM on the machine above. Given these results, I'd propose hard coding MAX_CHUNKS to 32 rather than basing it on io.file.buffer.size as I earlier figured.\n\nOn a separate note, some review responses:\n\nbq. Was this way before your change but the back-to-back if statements on line 252-253 could be combined triviallly.\n\nI think you missed the \"chunkPos += read;\" outside the inner if? Java seems to occasionally return -1 for EOF for some reason so I was nervous about letting that happen outside the if. I'd be happy to add an assert read >= 0 though for this case and make it part of the contract of readChunks to never return negative.\n\n\nThe rest of the review makes sense, and I'll address those things and upload a new patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-12-02T07:52:41.577+0000","updated":"2009-12-02T07:52:41.577+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12785171","id":"12785171","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"I did some further investigation on this:\n\n- I ran the 1GB cat test using hprof=cpu=times to get accurate invocation counts for the various read calls. Increasing MAX_CHUNKS from 1 to 16 does exactly what's expected and reduces the number of calls to readChunks (and thus the input stream reads, etc) by exactly a factor of 16. The same is true of 128 - no noticeable differences. This is because System.arraycopy doesn't get accounted by hprof in this mode, for whatever reason.\n\n- I imported a copy of the BufferedInputStream source and made BufferedFSInputStream extend from it rather than from the java.io one. I added a System.err printout right before the System.arraycopy inside read1(). When I changed MAX_CHUNKS over from 127 to 128, I verified that it correctly avoided these copies and read directly into the buffer. So the goal of the JIRA to get rid of a copy was indeed accomplished.\n\nNow the confusing part: eliminating this copy does nothing in terms of performance. Comparing the MAX_CHUNKS=127 to MAX_CHUNKS=128 has no statistically significant effect on the speed of catting 1G from RAM.\n\nSo my best theory right now on why it's faster is that it's simply doing fewer function calls, each of which does more work with longer loops. This is better for loop unrolling, instruction cache locality, and avoiding function call overhead. Perhaps it inspires the JIT to work harder as well - who knows what black magic lurks there :)\n\nI think at this point I've sufficiently investigated this, unless anyone has questions. I'll make the changes that Eli suggested and upload a new patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-12-03T04:11:40.325+0000","updated":"2009-12-03T04:11:40.325+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12785551","id":"12785551","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"New version of the patch. This addresses Eli's review comments, and adds some extra tests (one for truncated checksum file throwing ChecksumException, another for odd sized read buffers in a file with a few chunks). I also tidied up some of the comments to make it clearer to implementors what's going on.\n\nJust to be doubly sure, I reran all the benchmarks overnight and confirmed that reading 32 chunks at once had all the performance improvement benefits of a larger value (and uses less memory). Also reran HDFS-755 tests against this build with assertions on and everything looked good (plenty of assertion failures, but none in the new code!)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-12-03T21:46:11.766+0000","updated":"2009-12-03T21:46:11.766+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12785561","id":"12785561","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12426818/hadoop-3205.txt\n  against trunk revision 886645.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 5 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/testReport/\nFindbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nCheckstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/artifact/trunk/build/test/checkstyle-errors.html\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2009-12-03T22:02:47.623+0000","updated":"2009-12-03T22:02:47.623+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12785709","id":"12785709","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. I think you missed the \"chunkPos += read;\" outside the inner if? Java seems to occasionally return -1 for EOF for some reason so I was nervous about letting that happen outside the if. I'd be happy to add an assert read >= 0 though for this case and make it part of the contract of readChunks to never return negative.\n\nYup, I think the way it is now is good. \n\nPatch looks great. Like the new comments and test modifications. Thanks for running the additional experiments. \n\n+1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-12-04T01:45:42.731+0000","updated":"2009-12-04T01:45:42.731+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12793123","id":"12793123","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"\n+1. The patch looks good.  \n\n> Like the new comments and test modifications. \nthanks for good comments and javadoc.\n\nI find it surprising as well, that avoiding a buffer copy does not show any improvement (so is 13% for function calls or other java voodoo). \n\nLimit of 32 chunks : it is true that your tests didn't show benefit beyond that for LocalFileSystem.. not sure if that justifies limiting fs implementation's access to user buffer. Is to reduce memory allocated for checksum buffer?\n\nI will run the tests on my laptop. The jira need not wait for my results.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-12-21T08:55:59.845+0000","updated":"2009-12-21T08:55:59.845+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12796871","id":"12796871","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tomwhite","name":"tomwhite","key":"tomwhite","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tom White","active":true,"timeZone":"Europe/London"},"body":"I've just committed this. Thanks Todd!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tomwhite","name":"tomwhite","key":"tomwhite","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tom White","active":true,"timeZone":"Europe/London"},"created":"2010-01-05T22:15:19.795+0000","updated":"2010-01-05T22:15:19.795+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12796886","id":"12796886","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Common-trunk-Commit #133 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/133/])\n    . Read multiple chunks directly from FSInputChecker subclass into user buffers. Contributed by Todd Lipcon.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2010-01-05T22:42:43.940+0000","updated":"2010-01-05T22:42:43.940+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12393368/comment/12797069","id":"12797069","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Common-trunk #210 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/210/])\n    . Read multiple chunks directly from FSInputChecker subclass into user buffers. Contributed by Todd Lipcon.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2010-01-06T11:15:21.345+0000","updated":"2010-01-06T11:15:21.345+0000"}],"maxResults":33,"total":33,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-3205/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0id07:"}}