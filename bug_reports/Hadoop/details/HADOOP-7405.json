{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12510902","self":"https://issues.apache.org/jira/rest/api/2/issue/12510902","key":"HADOOP-7405","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/2","id":"2","description":"The problem described is an issue which will never be fixed.","name":"Won't Fix"},"customfield_12312322":null,"customfield_12310220":"2011-06-20T04:42:38.886+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Nov 02 17:58:11 UTC 2011","customfield_12310420":"70834","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_11713128883_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2011-11-02T17:58:10.956+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-7405/watchers","watchCount":7,"isWatching":false},"created":"2011-06-20T04:19:22.144+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"labels":["regression"],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12316064","id":"12316064","description":"Security for hadoop-0.20","name":"0.20.203.0","archived":false,"released":true,"releaseDate":"2011-05-11"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12315569","id":"12315569","description":"","name":"0.23.0","archived":false,"released":true,"releaseDate":"2011-11-11"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2011-11-02T17:58:11.023+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312070","id":"12312070","name":"native","description":"The native code that is loaded into the jvm"}],"timeoriginalestimate":null,"description":"As a result of a ton of new code in libhadoop being added in 0.20.203/0.22, a lot of features that used to work no longer do reliably.  The most common problem is native compression, but other issues such as Mac OS X's group support broke as well.  The native code checks need to be refactored such that libhadoop.so should report what it supports rather than having the Java-side assume that if it loads, it is all supported.  This would allow us to stub routines until they've been vetted, removing the chances of such regressions appearing in the future.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"102548","customfield_12312823":null,"summary":"libhadoop is all or nothing","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Everything not Linux","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12510902/comment/13051796","id":"13051796","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"It seems the correct answer here is to actually use soversions properly, no?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2011-06-20T04:42:38.886+0000","updated":"2011-06-20T04:42:38.886+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12510902/comment/13051800","id":"13051800","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"That's an interesting idea. How would this work? Are you thinking that during the native load, we detect what soversion got loaded and then base available functionality on that?\n\nI was thinking (far) down the road that we could basically dlopen() modules from within libhadoop.so.  Then as functionality got ported, one would just drop in the appropriate shared lib. This also makes codec support very interesting:  instead of having different bootstrap code for every codec, one could generalize it more than it is today.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2011-06-20T05:01:08.303+0000","updated":"2011-06-20T05:01:08.303+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12510902/comment/13051808","id":"13051808","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"I'm thinking it would just expect a particular version, and if you have an old version on your system, it should refuse to load with an error logged.\n\nSupporting past versions of the .so on new versions of Hadoop is a large matrix to test -- we already do a bad job of testing native code.\n\nIf there are regressions with regard to OS support, we should catch those by having people with strange OSes testing trunk and voting on releases. Building a complicated framework into the code doesn't seem like the easiest solution to me.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2011-06-20T05:35:15.203+0000","updated":"2011-06-20T05:35:15.203+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12510902/comment/13051813","id":"13051813","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"Ah, ok.  The soversion essentially moves lockstep with a given release.  If we were to go to that method, we'd need to split libhadoop in half to fix the regression problem.\n\nGranted, it is simple, but it doesn't necessarily prevent the problem that we've found ourselves.  Unfortunately, I do not believe anyone on the PMC attempts to run non-Linux on a standard basis.  Using release votes to prevent these situations from happening is pretty much a non-starter.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2011-06-20T06:01:44.417+0000","updated":"2011-06-20T06:01:44.417+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12510902/comment/13052033","id":"13052033","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"body":"Since Hadoop Kerberos Mac OS X support was never fully there, it is not possible to compile libhadoop due to some compiler errors.\n\nHaving libhadoop working partially on different platforms does not seem a good idea as it will mislead users to believe that a particular platform is supported when is 'partially' supported. I'd rather go for the principle of least surprise here (all or nothing), this will make Hadoop easier to user.\n\nA while ago I've opened a HADOOP-7083 to enable running Hadoop with Kerberos ON without relying on some libhadoop functionality and the argument there was that doing that was a security risk. \n\nI'm not arguing for HADOOP-7083 to be considered again, what I'm trying to state is that if we go the partial libhadoop functionality we'll have to come up with a checklist that will have to identify if certain functionality provided by native libs is optional, can hadoop work without it, is a security risk, etc. This will complicate the not only the lives of developers but of users.\n\nBecause of this my take is that if we require native code to run Hadoop, we should provide the full set of native code for each platform we are building for. Only for functionality that there is Java fallback (at a cost of performance) native-code should be optional.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"created":"2011-06-20T16:13:34.879+0000","updated":"2011-06-20T16:13:34.879+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12510902/comment/13052118","id":"13052118","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"bq. Since Hadoop Kerberos Mac OS X support was never fully there, it is not possible to compile libhadoop due to some compiler errors.\n\nThe compiler errors are fairly simple to fix on Darwin.  I don't know why, but it seems like 9 times out of 10, we favor BSD functionality when we go with something non-portable. \n\nbg. Because of this my take is that if we require native code to run Hadoop, we should provide the full set of native code for each platform we are building for. \n\nRegardless of what happens in this jira, we need a testsuite for the C code anyway.  OS X actually proves out that even if the code compiles, it doesn't necessarily mean it works properly.  (See HADOOP-7367).\n\nbg. A while ago I've opened a HADOOP-7083 to enable running Hadoop with Kerberos ON without relying on some libhadoop functionality and the argument there was that doing that was a security risk. \n\nRight.  It wanted to create a third security mode where some stuff worked and some stuff didn't.  That's not quite what I'm asking for here and it wouldn't actually fix the problem we're hitting anyway. The security functionality is orthogonal to the compression functionality.  That's the base, surface issue.  Since it is in one big chunk, we broke *both*.\n\n(While I guess it wasn't obvious, I should probably state that I'm not looking for a \"partially working\" security mode.  The scope of what constitutes a working unit would still need to be defined.  It is more than reasonable to say that all of the functions that are directly security related would need to be ported and treated like one block.  Asking libhadoop.so if it \"supports security\" seems like a reasonable thing to ask it.)\n\nThe problem that we've got is that we have a lot of unrelated code sitting in libhadoop.so.  Every time we add something we run the risk of regressing features out of platforms other than Linux since those other platforms are an afterthought.  HADOOP-7206 may actually be a great example of this:  if we go with a pure native implementation, we won't be able to support Snappy on anything but Linux with the current state of things.  Lack of compression support has a *direct* impact on the client.  I'd be surprised if the majority of shops are only using Linux clients. \n\nWouldn't it be great to be able to ask the lib \"do you support gzip, do you support snappy, do you support lzo, do you support security, ...\"?  Then we could add code as needed, do ports as needed, etc.  An alternative would be that we start breaking libhadoop up into at least related functionality.\n\nI suppose the other outcome might be that we as a community just admit that we don't support Hadoop on anything but Linux and give up on any semblance of portability.  More and more code is being added or rewritten in C.  I would be surprised if this trend changes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2011-06-20T18:12:21.192+0000","updated":"2011-06-20T18:12:21.192+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12510902/comment/13052122","id":"13052122","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Allen, if you want to write a patch that adds a call like:\nboolean isNativeFeatureSupported(long featureCode);\n...with a set of enums for various features, I would happily review it.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2011-06-20T18:15:20.271+0000","updated":"2011-06-20T18:15:20.271+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12510902/comment/13142360","id":"13142360","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"Hadoop is not focused on portability.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2011-11-02T17:58:11.005+0000","updated":"2011-11-02T17:58:11.005+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-7405/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0hwmn:"}}