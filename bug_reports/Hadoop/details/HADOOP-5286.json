{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12415138","self":"https://issues.apache.org/jira/rest/api/2/issue/12415138","key":"HADOOP-5286","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2009-02-19T22:33:28.571+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Feb 27 21:49:30 UTC 2009","customfield_12310420":"126970","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_725160255_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2009-02-27T21:49:30.255+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-5286/watchers","watchCount":3,"isWatching":false},"created":"2009-02-19T12:23:30.155+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12313438","id":"12313438","description":"","name":"0.20.0","archived":false,"released":true,"releaseDate":"2009-04-22"}],"issuelinks":[{"id":"12323589","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12323589","type":{"id":"12310010","name":"Incorporates","inward":"is part of","outward":"incorporates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310010"},"inwardIssue":{"id":"12408554","key":"HADOOP-4664","self":"https://issues.apache.org/jira/rest/api/2/issue/12408554","fields":{"summary":"Parallelize job initialization","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2009-07-08T16:43:33.066+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"On a large cluster, we've observed that DFS client was blocked on reading a block of a file for almost 1 and half hours. The file was being read by the JobTracker of the cluster, and was a split file of a job. On the NameNode logs, we observed that the block had a message as follows:\n\nInconsistent size for block blk_2044238107768440002_840946 reported from <ip>:<port> current size is 195072 reported size is 1318567\n\nDetails follow.\n\n ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12401141","id":"12401141","filename":"dn-log.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-02-27T21:37:25.811+0000","size":3973079,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12401141/dn-log.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12400513","id":"12400513","filename":"jt-log-for-blocked-reads.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"created":"2009-02-19T12:33:13.746+0000","size":7510,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12400513/jt-log-for-blocked-reads.txt"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"103793","customfield_12312823":null,"summary":"DFS client blocked for a long time reading blocks of a file on the JobTracker","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12674982","id":"12674982","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"body":"The attached snippet from the JobTracker log indicates the exceptions thrown by the DFS client. Also, please note the timestamps between the messages. Ultimately the system recovered after almost 90 minutes and continued to process this job.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"created":"2009-02-19T12:33:15.339+0000","updated":"2009-02-19T12:33:15.339+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12674984","id":"12674984","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"body":"The NameNode log for the block blk_2044238107768440002_840946 shows the following:\n\n2009-02-19 09:52:50,952 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(blk_2044238107768440002_840946) successful\n2009-02-19 09:52:50,975 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Inconsistent size for block blk_2044238107768440002_840946 reported from <ip>:<port> current size is 195072 reported size is 1318567\n2009-02-19 09:52:50,975 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for blk_2044238107768440002_840946 on <ip>:<port> size 1318567\n2009-02-19 09:57:41,786 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask <ip>:<port> to replicate blk_2044238107768440002_840946 to datanode(s) <ip>:<port>\n2009-02-19 10:11:00,267 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask <ip>:<port> to replicate blk_2044238107768440002_840946 to datanode(s) <ip>:<port>\n2009-02-19 11:27:00,030 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask <ip>:<port> to replicate blk_2044238107768440002_840946 to datanode(s) <ip>:<port>\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"created":"2009-02-19T12:36:37.318+0000","updated":"2009-02-19T12:36:37.318+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12674986","id":"12674986","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"body":"I checked the data node mentioned in the exception traces of the attached file. It was slow to the point of being dead, though ping was responding. The node was being listed in the 'Live nodes' list on the DFS web UI.\n\nThis lockup caused HADOOP-5285, a complete lock-up of the Jobtracker cluster, which also exposed a bug in Map/Reduce.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"created":"2009-02-19T12:41:36.642+0000","updated":"2009-02-19T12:41:36.642+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12675162","id":"12675162","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"It looks that during the creation of the split file, a datanode in the pipeline was down and thus triggered pipeline recovery. The problem reported blow\n\n> 2009-02-19 09:52:50,975 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Inconsistent size for block blk_2044238107768440002_840946 reported from <ip>:<port> current size is 195072 reported size is 1318567\n\nwas also reported by HADOOP-5133 and fixed in HADOOP-5134. But I do not think this is the cause of the blocking read.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-19T22:33:28.571+0000","updated":"2009-02-19T22:33:28.571+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12675169","id":"12675169","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"> I checked the data node mentioned in the exception traces of the attached file. It was slow to the point of being dead, though ping was responding.\n\nI believe that the reading failure was caused by the slow datanode. I do not think that DFSClient could be blocked for 1 and half an hour. From the attached log, I do see that the same split file was read again and again. Does JobTrack reinsert a failed job back into the jobInitQueue?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-19T22:57:11.218+0000","updated":"2009-02-19T22:57:11.218+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12675240","id":"12675240","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"body":"From what I can see in the M/R code, the call to initTasks (done from the JobInitThread), is catching Throwable, logging it as an error, failing the job and going to the next job. I did not see the error message in the log file. There is no other exception handling in between. So if an exception was raised, the call would have been aborted. So, I believe the call to initTasks never returned.\n\nThe reason why there are multiple reads on the split file is because a single split file is typically across the same block (given the type of jobs we are running). Hence when M/R reads multiple fields in the split file, it trying to read the block multiple times (it looks like). The really important thing here is the time difference between the two logs here:\n\n2009-02-19 10:26:00,504 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain block blk_2044238107768440002_840946 from any node:  java.io.IOException: No live nodes contain current block\n2009-02-19 11:29:25,054 INFO org.apache.hadoop.mapred.JobInProgress: Input size for job job_200902190419_0419 = 635236206196\n\nThat's an hour - there are no logs in between on the M/R system. So, I believe the rest of the split file was being read slowly, but ultimately succeeding, because the job finally began to run.\n\nOne question. If the data node is so slow in reading, then is there a way to make the calls timeout sooner by some configuration, and thus cause the DFS to throw an exception, that would abort the job, and let other jobs proceed ? We could try out with such a configuration and see if that helps.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"created":"2009-02-20T04:43:45.764+0000","updated":"2009-02-20T04:43:45.764+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12675458","id":"12675458","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"I was wrong about JobTracker reading the split file multiple times. Retry is done by the DFSClient to recover from read error. DataNode does have write timeout introuduced by HADOOP-2346. The default timeout is 8 mins. In case of read failure, DFSClient retries 3 datanodes which are different if different ones are available. With HADOOP-3831, each datanode is read 2 times. So DFSClient retries 6 times before it declares a read failure. in this case, it seems that the 5th or 6th retry succeeded but reading took nearly 1 hour.\n\nHemanth, could you please provide related datanode logs to see what was really happened there? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-20T20:05:28.990+0000","updated":"2009-02-20T20:05:28.990+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12675480","id":"12675480","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Read timeout on client for reading from DataNode is 60 seconds (the write timeout of 8 minutes on DataNode does not matter).\n\nThis jira says 'DFSClient was blocked for 1.5 hours' : Is there is any log on the client during that time? Please attache all of that log.  \n\nOr is it more like 'DFSClient took 1.5 hours to read x bytes'.. this is always possible with extremely slow datanode (say it is serving 10 bytes every one minute). \n\nWhich one is more accurate?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-02-20T21:28:07.029+0000","updated":"2009-02-20T21:28:07.029+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12675517","id":"12675517","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"Thank Raghu for the clarification. I did not realize we have read timeout too. So either a slow reader or a slow writer might cause a read failure. But still multiple retries and slow writing by the datanode contributed to this 1.5 hours of reading. From the log, retries took at least 1/2 hour. The log did not show exactly how much time last successful read took because not every failed read was logged.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-21T00:30:58.934+0000","updated":"2009-02-21T00:30:58.934+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12676504","id":"12676504","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"body":"A few questions that we were asked offline. I'm posting the comments here:\n\nbq. What is start and end time of the 1.5 hour wait? \n\nbq. Did the client read() blocked for 1.5 hours or is that it failed to read  x bytes in 1.5 hours? I understand from clients point of view they might be same, but for HDFS, these two are different. \n\nI think your last point is relevant. Let me try and describe this a bit better. I don't think a single call was blocked. i.e. a single read did not block for 1.5 hours. From the attached log, and the relevant source code, I see reads are made at 3 different places:\n\nbq. [2009-02-19 10:03:29] at org.apache.hadoop.mapred.JobClient$RawSplit.readFields(JobClient.java:983)\nbq. [2009-02-19 10:19:46] at org.apache.hadoop.mapred.JobClient$RawSplit.readFields(JobClient.java:981)\nbq. [2009-02-19 10:26:00,504] at org.apache.hadoop.mapred.JobClient$RawSplit.readFields(JobClient.java:987)\n\nEach of these calls result in a org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1680) ultimately. And these logs come one after the other. So, the first call went from 10:03 to 10:19, and so on. Also, I don't think any call actually failed in the end. Because from the code (and line numbers), the code is progressing to make other read calls. Had there been an IOException, it would have bailed out right then. So, I believe the reads were happening very slowly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"created":"2009-02-25T05:06:53.242+0000","updated":"2009-02-25T05:06:53.242+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12677130","id":"12677130","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Hemanth. So it is the very slow reading that is causing the problem.\n\nThere are two issue here :\n\n* first the block has only one replica while writing. \n     ** Hairong mentioned earlier in the jira that root cause of this is fixed in HADOOP-5134\n\n* Unfortunately only replica is on a datanode that is extremely slow and hardly accessible. I will address this.\n   ** Some DataNodes are expected to be flaky and is a normal condition.\n   ** This is probably the reason why replication of the block also took very long time as well.\n\nAs I understand, the application (JobTracker in this case) is very sensitive to this delay. I think for a good design, this slow reading should be handled in the application. There is no QOS for hadoop file systems. Even if HDFS has some kind of option, app could face the same problem with LocalFS. If a datanode node slowly trickles data, and that is the only replica left, options for a filesystem are limited. In that sense I am not sure if that is a real bug.\n\nDo you think a critical service like JobTracker should be  sensitive to flaky datanode delays? It probably gets less likely as bugs like HADOOP-5134 are fixed, but delays could occur for various other reasons.\n\nWe reduce the timeout, number of retries etc in DFSClient while reading, but I don't think that addresses the basic issue.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-02-26T20:19:22.634+0000","updated":"2009-02-26T20:19:22.634+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12677150","id":"12677150","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"I would propose that we make a pool of JobInit threads, so that a single slow data node can't block all of the progress. If the pool had 5 threads, that would be more than enough to prevent slow blocks from completely blocking the JobTracker.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-02-26T21:38:47.779+0000","updated":"2009-02-26T21:38:47.779+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12677241","id":"12677241","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"body":"Reassigning to Devaraj for consideration in map/reduce code.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"created":"2009-02-27T04:00:38.602+0000","updated":"2009-02-27T04:00:38.602+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12677371","id":"12677371","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi Raghu, as has been suggested, we will change M/R to introduce multiple threads for initialization. There was effort on this front in HADOOP-4664, that we propose to take forward. \n\nHowever, I feel we should spend a little more time and look at the datanode log to make sure that it is indeed a hardware issue with the datanode in question and then keep it aside. Since this has only occurred once, I don't think it should be a blocker (I never did, in fact), and hence I've downgraded the severity and removed the fix in version. We will be providing the logs to you from the slow data node that we rebooted and it would be great if you can take a quick look to determine there's no hidden problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yhemanth","name":"yhemanth","key":"yhemanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hemanth Yamijala","active":true,"timeZone":"Asia/Kolkata"},"created":"2009-02-27T13:50:28.926+0000","updated":"2009-02-27T13:50:28.926+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12677544","id":"12677544","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Hemanth. I am attaching the datanode log.\n\nIt might not be just the hard disks that are slow, even network might be affected too, since there are write timeouts while writing to clients. The follow log shows how severely degraded this machine is :\n\n{quote} 2009-02-19 10:10:58,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockReport of 92 blocks got processed in 445591 msecs {quote}\n\nIt took 7.5 minutes to scan and report 92 blocks. \n\nTo see what is actually wrong with the machine, someone in charge of the hardware needs to take a look. I will close this issue for now. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-02-27T21:37:25.848+0000","updated":"2009-02-27T21:37:25.848+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12415138/comment/12677550","id":"12677550","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"This should be fixed by HADOOP-4664 (linked). Resolving as \"Duplicate\".","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-02-27T21:49:30.188+0000","updated":"2009-02-27T21:49:30.188+0000"}],"maxResults":16,"total":16,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-5286/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0i4bb:"}}