{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13206598","self":"https://issues.apache.org/jira/rest/api/2/issue/13206598","key":"HADOOP-16021","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2018-12-27T14:21:51.048+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jan 03 03:13:04 UTC 2019","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_554853350_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2019-01-02T22:15:55.759+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-16021/watchers","watchCount":3,"isWatching":false},"created":"2018-12-27T12:08:22.464+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":["bug"],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334005","id":"12334005","description":"2.7.3 release","name":"2.7.3","archived":false,"released":true,"releaseDate":"2016-08-25"}],"issuelinks":[{"id":"12551112","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12551112","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12968232","key":"HADOOP-13138","self":"https://issues.apache.org/jira/rest/api/2/issue/12968232","fields":{"summary":"Unable to append to a SequenceFile with Compression.NONE.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2019-01-03T03:13:04.892+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12330961","id":"12330961","name":"common"}],"timeoriginalestimate":null,"description":" \r\n I want append the data in a file , when i use SequenceFile.appendIfExists , it throw NullPointerException at at org.apache.hadoop.io.SequenceFile$Writer.(SequenceFile.java:1119)\r\n\r\nwhen i remove the 'appendIfExists', it works, but it will cover old file.\r\n\r\n \r\n\r\nwhen i try use CompressionType.RECORD or CompressionType.BLOCK throw \"not support\" exception\r\n\r\n \r\n{code:java}\r\n// my code\r\nSequenceFile.Writer writer = null; \r\n\r\nwriter = SequenceFile.createWriter(conf, \r\n    SequenceFile.Writer.file(path), \r\n    SequenceFile.Writer.keyClass(Text.class), \r\n    SequenceFile.Writer.valueClass(Text.class), \r\n    SequenceFile.Writer.appendIfExists(true) );\r\n{code}\r\n \r\n{code:java}\r\n// all my code\r\npublic class Writer1 implements VoidFunction<Iterator<Tuple2<String, String>>> {\r\n    private static Configuration conf = new Configuration();\r\n    private int MAX_LINE = 3; // little num,for test\r\n\r\n    @Override\r\n    public void call(Iterator<Tuple2<String, String>> iterator) throws Exception {\r\n        int partitionId = TaskContext.get().partitionId();\r\n        int count = 0;\r\n        SequenceFile.Writer writer = null;\r\n        while (iterator.hasNext()) {\r\n\r\n            Tuple2<String, String> tp = iterator.next();\r\n            Path path = new Path(\"D:/tmp-doc/logs/logs.txt\");\r\n\r\n            if (writer == null)\r\n                writer = SequenceFile.createWriter(conf, SequenceFile.Writer.file(path),\r\n                        SequenceFile.Writer.keyClass(Text.class),\r\n                        SequenceFile.Writer.valueClass(Text.class),\r\n                        SequenceFile.Writer.appendIfExists(true)\r\n                        );\r\n\r\n            writer.append(new Text(tp._1), new Text(tp._2));\r\n            count++;\r\n\r\n            if (count > MAX_LINE) {\r\n                IOUtils.closeStream(writer);\r\n                count = 0;\r\n                writer = SequenceFile.createWriter(... // same as above\r\n            }\r\n        }\r\n        if (count > 0) {\r\n            IOUtils.closeStream(writer);\r\n        }\r\n        IOUtils.closeStream(writer);\r\n    }\r\n}\r\n{code}\r\n // above code call by below\r\n{code:java}\r\nimport com.xxx.algo.hadoop.Writer1\r\nimport com.xxx.algo.utils.Utils\r\nimport kafka.serializer.StringDecoder\r\nimport org.apache.spark.sql.SparkSession\r\nimport org.apache.spark.streaming.kafka.KafkaUtils\r\nimport org.apache.spark.streaming.{Durations, StreamingContext}\r\nimport org.apache.spark.{SparkConf, SparkContext}\r\n\r\n\r\nobject KafkaSparkStreamingApp {\r\n  def main(args: Array[String]): Unit = {\r\n    val kafka = \"192.168.30.4:9092,192.168.30.5:9092,192.168.30.6:9092\"\r\n    val zk = \"192.168.30.4:2181,192.168.30.5:2181,192.168.30.6:2181\"\r\n    val topics = Set(\"test.aries.collection.appevent.biz\")\r\n    val tag = \"biz\"\r\n    val durationSeconds = 5000\r\n    val conf = new SparkConf()\r\n    conf.setAppName(\"user-log-consumer\")\r\n      .set(\"spark.serilizer\",\"org.apache.spark.serializer.KryoSerializer\")\r\n      .set(\"spark.kryo.registrationRequired\", \"true\")\r\n      .set(\"spark.defalut.parallelism\",\"2\")\r\n      .set(\"spark.rdd.compress\",\"true\")\r\n      .setMaster(\"local[2]\")\r\n    val sc = new SparkContext(conf)\r\n    val session = SparkSession.builder()\r\n      .config(conf)\r\n      .getOrCreate()\r\n    val ssc = new StreamingContext(sc, Durations.milliseconds(durationSeconds))\r\n    val kafkaParams = Map[String, String](\r\n      \"metadata.broker.list\" -> kafka,\r\n      \"bootstrap.servers\" -> kafka,\r\n      \"zookeeper.connect\" -> zk,\r\n      \"group.id\" -> \"recommend_stream_spark\",\r\n      \"key.serializer\" -> \"org.apache.kafka.common.serialization.StringSerializer\",\r\n      \"key.deserializer\" -> \"org.apache.kafka.common.serialization.StringDeserializer\",\r\n      \"value.deserializer\" -> \"org.apache.kafka.common.serialization.StringDeserializer\"\r\n    )\r\n    val stream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\r\n      ssc,\r\n      kafkaParams,\r\n      topics\r\n    )\r\n    val timeFieldName = \"log_time\"\r\n    stream.foreachRDD(rddMsg => {\r\n      rddMsg.map(msg => {\r\n        val value = msg._2\r\n        val time = Utils.getTime(value, timeFieldName)\r\n        new Tuple2(time + \",\" + tag, value)\r\n      })\r\n        .toJavaRDD().foreachPartition(new Writer1()) // here\r\n    })\r\n    ssc.start()\r\n    ssc.awaitTermination()\r\n  }\r\n}\r\n{code}\r\n{{more info see:[https://stackoverflow.com/questions/53943978/hadoop-sequencefile-createwriter-appendifexists-codec-cause-nullpointerexception]}}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12953192","id":"12953192","filename":"055.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"created":"2018-12-28T04:12:17.432+0000","size":310584,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12953192/055.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12953193","id":"12953193","filename":"62.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"created":"2018-12-28T04:12:29.122+0000","size":569648,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12953193/62.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12953182","id":"12953182","filename":"CompressionType.BLOCK-Not supported-error log.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"created":"2018-12-28T02:49:08.198+0000","size":19876,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12953182/CompressionType.BLOCK-Not+supported-error+log.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12953188","id":"12953188","filename":"CompressionType.NONE-NullPointerException-error log.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"created":"2018-12-28T03:45:51.644+0000","size":18789,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12953188/CompressionType.NONE-NullPointerException-error+log.txt"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"SequenceFile.createWriter appendIfExists codec cause NullPointerException","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"windows10 or Linux-centos , hadoop2.7.3, jdk8","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13206598/comment/16729639","id":"16729639","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Jack-Lee","name":"Jack-Lee","key":"jack-lee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"lqjacklee","active":true,"timeZone":"Asia/Hong_Kong"},"body":"[~xinkenny] could provide the whole code ? thanks . ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Jack-Lee","name":"Jack-Lee","key":"jack-lee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"lqjacklee","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2018-12-27T14:21:51.048+0000","updated":"2018-12-27T14:21:51.048+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13206598/comment/16729649","id":"16729649","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"body":"[~Jack-Lee] I have upload more code above again","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"created":"2018-12-27T14:39:47.190+0000","updated":"2018-12-27T15:59:48.466+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13206598/comment/16731579","id":"16731579","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Jack-Lee","name":"Jack-Lee","key":"jack-lee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"lqjacklee","active":true,"timeZone":"Asia/Hong_Kong"},"body":"[~xinkenny] Thanks ,I will try to reproduce it .","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Jack-Lee","name":"Jack-Lee","key":"jack-lee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34055","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34055","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34055","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34055"},"displayName":"lqjacklee","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2019-01-01T11:55:23.408+0000","updated":"2019-01-01T11:55:23.408+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13206598/comment/16732467","id":"16732467","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"inline stack of NPE\r\n{code}\r\n18/12/28 11:43:31 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 23)\r\njava.lang.NullPointerException\r\n\tat org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1119)\r\n\tat org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)\r\n\tat com.xxx.algo.hadoop.Writer1.call(Writer1.java:68)\r\n\tat com.xxx.algo.hadoop.Writer1.call(Writer1.java:34)\r\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:219)\r\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:219)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n{code}\r\n\r\nAnd\r\n{code}\r\n\r\n2018-11-01 20:55:20 ERROR Executor:91 - Exception in task 0.0 in stage 11.0 (TID 11)\r\njava.io.IOException: Not supported\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.append(ChecksumFileSystem.java:357)\r\n\tat org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1131)\r\n\tat org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1511)\r\n\tat org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:277)\r\n\tat com.xxx.algo.hadoop.Writer1.call(Writer1.java:68)\r\n\tat com.xxx.algo.hadoop.Writer1.call(Writer1.java:34)\r\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:219)\r\n\tat org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:219)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:935)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n{code}\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2019-01-02T22:11:19.301+0000","updated":"2019-01-02T22:11:19.301+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13206598/comment/16732471","id":"16732471","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"\r\nThe block UnsupportedOperationException is being raised because {{appendIfExists}} needs to append, and the FS in use doesn't support it. You aren't writing to HDFS, are you? file:??\r\n\r\nThe NPE is just HADOOP-13138; been fixed for a long time. Upgrading your Hadoop dependencies will make it go away. Closing as a duplicate. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2019-01-02T22:15:39.657+0000","updated":"2019-01-02T22:15:39.657+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13206598/comment/16732610","id":"16732610","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"body":"* my default fs maybe file。\r\n * I try to change my project maven dependency to hadoop2.8.0. but also cause NPE.\r\n * I will change my cluster environment to hadoop2.8.0 for test later.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xinkenny","name":"xinkenny","key":"xinkenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"asin","active":true,"timeZone":"Etc/UTC"},"created":"2019-01-03T03:13:04.892+0000","updated":"2019-01-03T03:13:04.892+0000"}],"maxResults":6,"total":6,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-16021/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|u00c1c:"}}