{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12832789","self":"https://issues.apache.org/jira/rest/api/2/issue/12832789","key":"HADOOP-12033","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2015-05-26T18:13:42.947+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Jun 26 16:43:48 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_2647963349_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-06-26T09:02:39.425+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-12033/watchers","watchCount":8,"isWatching":false},"created":"2015-05-26T17:29:56.110+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12428321","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12428321","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12545575","key":"HADOOP-8151","self":"https://issues.apache.org/jira/rest/api/2/issue/12545575","fields":{"summary":"Error handling in snappy decompressor throws invalid exceptions","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-06-26T16:43:48.673+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"We have noticed intermittent reducer task failures with the below exception:\n\n{code}\nError: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#9 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: java.lang.NoClassDefFoundError: Ljava/lang/InternalError at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect(Native Method) at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:239) at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:88) at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85) at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192) at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:534) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:329) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193) Caused by: java.lang.ClassNotFoundException: Ljava.lang.InternalError at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 9 more \n{code}\n\nUsually, the reduce task succeeds on retry. \n\nSome of the symptoms are similar to HADOOP-8423, but this fix is already included (this is on Hadoop 2.6).","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12740385","id":"12740385","filename":"0001-HADOOP-12033.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jhalfpenny","name":"jhalfpenny","key":"jhalfpenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jim Halfpenny","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-18T13:23:19.056+0000","size":1354,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12740385/0001-HADOOP-12033.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Reducer task failure with java.lang.NoClassDefFoundError: Ljava/lang/InternalError at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14559525","id":"14559525","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"body":"This looks likes hadoop native library was not loaded successfully.\nDid you see this warning message?\n      LOG.warn(\"Unable to load native-hadoop library for your platform... \" +\n               \"using builtin-java classes where applicable\");\nYou need configure LD_LIBRARY_PATH correctly in your environment.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-26T18:13:42.947+0000","updated":"2015-05-26T18:13:42.947+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14559543","id":"14559543","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"body":"Thanks for responding [~zxu]. The reducer task would succeed on retry, so I assumed it's not an environment problem. Below is the task syslog:\n{noformat}\n2015-05-21 18:33:10,773 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2015-05-21 18:33:10,976 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 60 second(s).\n2015-05-21 18:33:10,976 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started\n2015-05-21 18:33:10,991 INFO [main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:\n2015-05-21 18:33:10,991 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1432143397187_0004, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@5df3ade7)\n2015-05-21 18:33:11,132 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: RM_DELEGATION_TOKEN, Service: 100.76.156.98:9010, Ident: (owner=btbig2, renewer=mr token, realUser=hdp, issueDate=1432225097662, maxDate=1432829897662, sequenceNumber=2, masterKeyId=2)\n2015-05-21 18:33:11,351 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.\n2015-05-21 18:33:12,335 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 500ms before retrying again. Got null now.\n2015-05-21 18:33:13,804 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 1000ms before retrying again. Got null now.\n2015-05-21 18:33:16,308 INFO [main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: c:/apps/temp/hdfs/nm-local-dir/usercache/btbig2/appcache/application_1432143397187_0004\n2015-05-21 18:33:17,199 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n2015-05-21 18:33:17,402 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2-azure-file-system.properties\n2015-05-21 18:33:17,418 INFO [main] org.apache.hadoop.metrics2.sink.WindowsAzureETWSink: Init starting.\n2015-05-21 18:33:17,418 INFO [main] org.apache.hadoop.metrics2.sink.WindowsAzureETWSink: Successfully loaded native library. LibraryName = EtwLogger\n2015-05-21 18:33:17,418 INFO [main] org.apache.hadoop.metrics2.sink.WindowsAzureETWSink: Init completed. Native library loaded and ETW handle obtained.\n2015-05-21 18:33:17,418 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Sink azurefs2 started\n2015-05-21 18:33:17,433 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 60 second(s).\n2015-05-21 18:33:17,433 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: azure-file-system metrics system started\n2015-05-21 18:33:17,699 INFO [main] org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n2015-05-21 18:33:17,714 INFO [main] org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@36c76ec3\n2015-05-21 18:33:17,746 INFO [main] org.apache.hadoop.mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5c7b1796\n2015-05-21 18:33:17,793 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: MergerManager: memoryLimit=741710208, maxSingleShuffleLimit=185427552, mergeThreshold=489528768, ioSortFactor=100, memToMemMergeOutputsThreshold=100\n2015-05-21 18:33:17,793 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1432143397187_0004_r_001735_0 Thread started: EventFetcher for fetching Map Completion Events\n2015-05-21 18:33:19,187 INFO [fetcher#30] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning workernode165.btbig2.c2.internal.cloudapp.net:13562 with 1 to fetcher#30\n2015-05-21 18:33:19,187 INFO [fetcher#30] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to workernode165.btbig2.c2.internal.cloudapp.net:13562 to fetcher#30\n2015-05-21 18:33:19,187 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning workernode279.btbig2.c2.internal.cloudapp.net:13562 with 1 to fetcher#1\n2015-05-21 18:33:19,187 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 1 of 1 to workernode279.btbig2.c2.internal.cloudapp.net:13562 to fetcher#1\n(fetch logs removed)\n2015-05-21 19:25:08,983 INFO [fetcher#9] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: Assigning workernode133.btbig2.c2.internal.cloudapp.net:13562 with 88 to fetcher#9\n2015-05-21 19:25:08,983 INFO [fetcher#9] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: assigned 20 of 88 to workernode133.btbig2.c2.internal.cloudapp.net:13562 to fetcher#9\n2015-05-21 19:25:08,983 INFO [fetcher#9] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=13562/mapOutput?job=job_1432143397187_0004&reduce=1735&map=attempt_1432143397187_0004_m_006276_0,attempt_1432143397187_0004_m_006308_0,attempt_1432143397187_0004_m_006355_0,attempt_1432143397187_0004_m_006349_0,attempt_1432143397187_0004_m_006360_0,attempt_1432143397187_0004_m_006368_0,attempt_1432143397187_0004_m_006658_0,attempt_1432143397187_0004_m_008329_0,attempt_1432143397187_0004_m_008443_0,attempt_1432143397187_0004_m_008448_0,attempt_1432143397187_0004_m_008423_0,attempt_1432143397187_0004_m_008441_0,attempt_1432143397187_0004_m_008588_0,attempt_1432143397187_0004_m_008393_0,attempt_1432143397187_0004_m_010397_0,attempt_1432143397187_0004_m_010486_0,attempt_1432143397187_0004_m_010459_0,attempt_1432143397187_0004_m_010522_0,attempt_1432143397187_0004_m_010537_0,attempt_1432143397187_0004_m_010548_0 sent hash and received reply\n2015-05-21 19:25:08,999 INFO [fetcher#9] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#9 about to shuffle output of map attempt_1432143397187_0004_m_006276_0 decomp: 307061 len: 69824 to MEMORY\n2015-05-21 19:25:08,999 INFO [fetcher#9] org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: workernode133.btbig2.c2.internal.cloudapp.net:13562 freed by fetcher#9 in 23ms\n2015-05-21 19:25:08,999 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#9\n\tat org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.NoClassDefFoundError: Ljava/lang/InternalError\n\tat org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect(Native Method)\n\tat org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:239)\n\tat org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:88)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n\tat org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)\n\tat org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:534)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:329)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)\nCaused by: java.lang.ClassNotFoundException: Ljava.lang.InternalError\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\t... 9 more\n\n2015-05-21 19:25:08,999 INFO [main] org.apache.hadoop.mapred.Task: Runnning cleanup for the task\n\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"created":"2015-05-26T18:18:53.461+0000","updated":"2015-05-26T18:18:53.461+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14559596","id":"14559596","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Is it possible some early failure such as ClassNotFoundException or an ExceptionInInitializerError (indicating a failure in the static initialization block) or some incompatible version of the class found at runtime cause this exception?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-26T18:44:44.383+0000","updated":"2015-05-26T18:44:44.383+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14559608","id":"14559608","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"body":"If I had to guess (and I can only guess at this time:) ) I'd say this is something similar to the root cause from HADOOP-8423, where in case of a transient error (e.g. a networking error) someone's state gets out of sync, and results in a task failure.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"created":"2015-05-26T18:48:52.730+0000","updated":"2015-05-26T18:48:52.730+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14559791","id":"14559791","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"body":"If the problem turns around to be in MR, please move this to the MapReduce JIRA project.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-05-26T20:20:44.227+0000","updated":"2015-05-26T20:20:44.227+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14559801","id":"14559801","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"body":"bq. If the problem turns around to be in MR, please move this to the MapReduce JIRA project\nSounds good Vinod. I placed it under Hadoop based on my best guess. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"created":"2015-05-26T20:28:22.455+0000","updated":"2015-05-26T20:28:22.455+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14583680","id":"14583680","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~ivanmi], I looked at the hadoop-snappy library source code, it looks like the exception {{java.lang.NoClassDefFoundError: Ljava/lang/InternalError}} is from the following code at [SnappyDecompressor.c|https://github.com/electrum/hadoop-snappy/blob/master/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.c#L127]\n{code}\n  if (ret == SNAPPY_BUFFER_TOO_SMALL){\n    THROW(env, \"Ljava/lang/InternalError\", \"Could not decompress data. Buffer length is too small.\");\n  } else if (ret == SNAPPY_INVALID_INPUT){\n    THROW(env, \"Ljava/lang/InternalError\", \"Could not decompress data. Input is invalid.\");\n  } else if (ret != SNAPPY_OK){\n    THROW(env, \"Ljava/lang/InternalError\", \"Could not decompress data.\");\n  }\n{code}\nAnd also based on another HBASE issue HBASE-9644, this issue may be because corrupted map output data is fed to the SnappyDecompressor.\n\nI also found a bug at the above code in SnappyDecompressor.c. We should change the above code to:\n{code}\n  if (ret == SNAPPY_BUFFER_TOO_SMALL){\n    THROW(env, \"java/lang/InternalError\", \"Could not decompress data. Buffer length is too small.\");\n  } else if (ret == SNAPPY_INVALID_INPUT){\n    THROW(env, \"java/lang/InternalError\", \"Could not decompress data. Input is invalid.\");\n  } else if (ret != SNAPPY_OK){\n    THROW(env, \"java/lang/InternalError\", \"Could not decompress data.\");\n  }\n{code}\nI think SnappyDecompressor really want to throw java.lang.InternalError exception, but due to this bug, it throws {{java.lang.NoClassDefFoundError}}/{{ClassNotFoundException}}.\n\n{{THROW}} is defined at [org_apache_hadoop.h|https://github.com/electrum/hadoop-snappy/blob/master/src/main/native/src/org_apache_hadoop.h#L44]\n{code}\n#define THROW(env, exception_name, message) \\\n  { \\\n\tjclass ecls = (*env)->FindClass(env, exception_name); \\\n\tif (ecls) { \\\n\t  (*env)->ThrowNew(env, ecls, message); \\\n\t  (*env)->DeleteLocalRef(env, ecls); \\\n\t} \\\n  }\n{code}\nBased on the above code, you can see the correct parameter passed to {{FindClass}} should be \"java/lang/InternalError\" instead of \"Ljava/lang/InternalError\".\nAlso {{java.lang.InternalError}} exception will be handled correctly in Fetcher.java at the following code:\n{code}\n      // The codec for lz0,lz4,snappy,bz2,etc. throw java.lang.InternalError\n      // on decompression failures. Catching and re-throwing as IOException\n      // to allow fetch failure logic to be processed\n      try {\n        // Go!\n        LOG.info(\"fetcher#\" + id + \" about to shuffle output of map \"\n            + mapOutput.getMapId() + \" decomp: \" + decompressedLength\n            + \" len: \" + compressedLength + \" to \" + mapOutput.getDescription());\n        mapOutput.shuffle(host, is, compressedLength, decompressedLength,\n            metrics, reporter);\n      } catch (java.lang.InternalError e) {\n        LOG.warn(\"Failed to shuffle for fetcher#\"+id, e);\n        throw new IOException(e);\n      }\n{code}\nSo if SnappyDecompressor throws java.lang.InternalError exception, the reduce task won't fail and the map task may be rerun on another node after too many fetch failures.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-12T16:48:08.576+0000","updated":"2015-06-12T16:48:08.576+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14591774","id":"14591774","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jhalfpenny","name":"jhalfpenny","key":"jhalfpenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jim Halfpenny","active":true,"timeZone":"Etc/UTC"},"body":"Patch for HADOOP-12033. Modified SnappyDecompressor.c to contain the correct Java class names for thrown errors.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jhalfpenny","name":"jhalfpenny","key":"jhalfpenny","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jim Halfpenny","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-18T13:23:19.062+0000","updated":"2015-06-18T13:23:19.062+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14592381","id":"14592381","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"body":"thanks [~jhalfpenny] for the patch, It looks like the issue is fixed at HADOOP-8151 at trunk but HADOOP-8151 is not merged into branch-2:\nhttps://github.com/apache/hadoop/blob/branch-2/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.c#L121\nHi [~ivanmi], Did the branch you use have the fix for HADOOP-8151?\nHi [~vinodkv], It looks like HADOOP-8151 is also not merged into branch-2.7.0.\nCan we add HADOOP-8151 to 2.7.1 release?\nthanks\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-18T19:35:46.113+0000","updated":"2015-06-18T19:35:46.113+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14593631","id":"14593631","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"body":"Thanks [~zxu], [~jhalfpenny]! Very nice catch!\n\nbq. Hi Ivan Mitic, Did the branch you use have the fix for HADOOP-8151?\nThis fix is not included, user was on Hadoop 2.6.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"created":"2015-06-19T16:57:18.456+0000","updated":"2015-06-19T16:57:18.456+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14602462","id":"14602462","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=djp","name":"djp","key":"djp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=djp&avatarId=16954","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=djp&avatarId=16954","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=djp&avatarId=16954","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=djp&avatarId=16954"},"displayName":"Junping Du","active":true,"timeZone":"Asia/Shanghai"},"body":"It looks like the fix here is a sub set of HADOOP-8151. Shall we resolve this JIRA as a duplicated one, and reopen HADOOP-8151 for porting patch there to branch-2?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=djp","name":"djp","key":"djp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=djp&avatarId=16954","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=djp&avatarId=16954","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=djp&avatarId=16954","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=djp&avatarId=16954"},"displayName":"Junping Du","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-26T06:15:35.863+0000","updated":"2015-06-26T06:15:35.863+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14602480","id":"14602480","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"body":"[~djp], Yes, Your suggestion LGTM.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-26T06:43:09.139+0000","updated":"2015-06-26T06:43:09.139+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14602595","id":"14602595","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=djp","name":"djp","key":"djp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=djp&avatarId=16954","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=djp&avatarId=16954","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=djp&avatarId=16954","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=djp&avatarId=16954"},"displayName":"Junping Du","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks Zhihai Xu for confirmation on this. Already commit/merge HADOOP-8151 to branch-2, so resolve this JIRA as duplicated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=djp","name":"djp","key":"djp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=djp&avatarId=16954","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=djp&avatarId=16954","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=djp&avatarId=16954","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=djp&avatarId=16954"},"displayName":"Junping Du","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-26T09:02:39.449+0000","updated":"2015-06-26T09:02:39.449+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14603158","id":"14603158","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~djp] for committing HADOOP-8151 to branch-2!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zxu","name":"zxu","key":"zxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhihai xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-26T16:41:44.022+0000","updated":"2015-06-26T16:41:44.022+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12832789/comment/14603162","id":"14603162","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"body":"Thanks [~djp] and [~zxu]!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ivanmi","name":"ivanmi","key":"ivanmi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ivan Mitic","active":true,"timeZone":"Europe/Berlin"},"created":"2015-06-26T16:43:48.673+0000","updated":"2015-06-26T16:43:48.673+0000"}],"maxResults":15,"total":15,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-12033/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2f7mv:"}}