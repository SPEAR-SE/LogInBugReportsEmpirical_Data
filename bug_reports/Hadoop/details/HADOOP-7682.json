{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12524600","self":"https://issues.apache.org/jira/rest/api/2/issue/12524600","key":"HADOOP-7682","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/5","id":"5","description":"All attempts at reproducing this issue failed, or not enough information was available to reproduce the issue. Reading the code produces no clues as to why this behavior would occur. If more information appears later, please reopen the issue.","name":"Cannot Reproduce"},"customfield_12312322":null,"customfield_12310220":"2011-09-27T09:06:43.908+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Mar 11 19:57:50 UTC 2015","customfield_12310420":"3366","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_109085734695_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-03-11T19:57:49.973+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-7682/watchers","watchCount":28,"isWatching":false},"created":"2011-09-26T06:22:15.322+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":"ttprivate  0700 task tracker cygwin","customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12319501","id":"12319501","description":"maintenance release on branch-1.0","name":"1.0.1","archived":false,"released":true,"releaseDate":"2012-02-22"}],"issuelinks":[{"id":"12353203","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12353203","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12543126","key":"HADOOP-8089","self":"https://issues.apache.org/jira/rest/api/2/issue/12543126","fields":{"summary":"cannot submit job from Eclipse plugin running on Windows","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-03-11T19:57:50.013+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12310689","id":"12310689","name":"fs","description":"Generic FileSystem code"}],"timeoriginalestimate":null,"description":"ERROR org.apache.hadoop.mapred.TaskTracker:Can not start task tracker because java.io.IOException:Failed to set permissions of path:/tmp/hadoop-cyg_server/mapred/local/ttprivate to 0700\n    at org.apache.hadoop.fs.RawLocalFileSystem.checkReturnValue(RawLocalFileSystem.java:525)\n    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:499)\n    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:318)\n    at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:183)\n    at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:635)\n    at org.apache.hadoop.mapred.TaskTracker.(TaskTracker.java:1328)\n    at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3430)\n\nSince hadoop0.20.203 when the TaskTracker initialize, it checks the permission(TaskTracker Line 624) of (org.apache.hadoop.mapred.TaskTracker.TT_LOG_TMP_DIR,org.apache.hadoop.mapred.TaskTracker.TT_PRIVATE_DIR, org.apache.hadoop.mapred.TaskTracker.TT_PRIVATE_DIR).RawLocalFileSystem(http://svn.apache.org/viewvc/hadoop/common/tags/release-0.20.203.0/src/core/org/apache/hadoop/fs/RawLocalFileSystem.java?view=markup) call setPermission(Line 481) to deal with it, setPermission works fine on *nx, however,it dose not alway works on windows.\n\nsetPermission call setReadable of Java.io.File in the line 498, but according to the Table1 below provided by oracle,setReadable(false) will always return false on windows, the same as setExecutable(false).\n\nhttp://java.sun.com/developer/technicalArticles/J2SE/Desktop/javase6/enhancements/\n\nis it cause the task tracker \"Failed to set permissions\" to \"ttprivate to 0700\"?\nHadoop 0.20.202 works fine in the same environment. \n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12313563","id":"12313563","description":"","name":"0.21.0","archived":false,"released":true,"releaseDate":"2010-08-23"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"41862","customfield_12312823":null,"summary":"taskTracker could not start because \"Failed to set permissions\" to \"ttprivate to 0700\"","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=magic.xie","name":"magic.xie","key":"magic.xie","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Magic Xie","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=magic.xie","name":"magic.xie","key":"magic.xie","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Magic Xie","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"OS:WindowsXP SP3 , Filesystem :NTFS, cygwin 1.7.9-1, jdk1.6.0_05","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13115351","id":"13115351","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=malrin","name":"malrin","key":"malrin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matthieu Lecoutre","active":true,"timeZone":"Etc/UTC"},"body":"Thanks Magic Xie, \n  Same problem on 0.20.204 on windows","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=malrin","name":"malrin","key":"malrin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matthieu Lecoutre","active":true,"timeZone":"Etc/UTC"},"created":"2011-09-27T09:06:43.908+0000","updated":"2011-09-27T09:06:43.908+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13142849","id":"13142849","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=toddfast","name":"toddfast","key":"toddfast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Todd Fast","active":true,"timeZone":"America/Los_Angeles"},"body":"Also a problem on 0.20.205. Problem exists with or without running Hadoop with Cygwin. The only fix seems to be to downgrade to 0.20.2.\n\nAdditional comments around the Web on this bug:\nhttp://comments.gmane.org/gmane.comp.jakarta.lucene.hadoop.user/25837\nhttp://lucene.472066.n3.nabble.com/SimpleKMeansCLustering-quot-Failed-to-set-permissions-of-path-to-0700-quot-td3429867.html","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=toddfast","name":"toddfast","key":"toddfast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Todd Fast","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-11-03T05:35:21.841+0000","updated":"2011-11-03T05:35:21.841+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13176945","id":"13176945","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=verditer","name":"verditer","key":"verditer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vijaya Phanindra","active":true,"timeZone":"Etc/UTC"},"body":"The problem seems to exists for release 1.0.0 also. Works fine if downgraded to 0.20.2\n\njava.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-UserName\\mapred\\staging\\UserName-1687815415\\.staging to 0700\n        at org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:682)\n        at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:655)\n        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:509)\n        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:344)\n        at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:189)\n        at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:116)\n        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:856)\n        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1261)\n        at org.apache.hadoop.examples.Grep.run(Grep.java:69)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.examples.Grep.main(Grep.java:93)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:64)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=verditer","name":"verditer","key":"verditer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vijaya Phanindra","active":true,"timeZone":"Etc/UTC"},"created":"2011-12-29T02:06:18.910+0000","updated":"2011-12-29T02:06:18.910+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13180006","id":"13180006","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=davideagen","name":"davideagen","key":"davideagen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Eagen","active":true,"timeZone":"America/Chicago"},"body":"This worked on 0.22.0.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=davideagen","name":"davideagen","key":"davideagen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"David Eagen","active":true,"timeZone":"America/Chicago"},"created":"2012-01-05T00:07:22.925+0000","updated":"2012-01-05T00:07:22.925+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13229842","id":"13229842","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tigar_chen","name":"tigar_chen","key":"tigar_chen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"tigar","active":true,"timeZone":"Etc/UTC"},"body":"same problem with cygin~:('","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tigar_chen","name":"tigar_chen","key":"tigar_chen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"tigar","active":true,"timeZone":"Etc/UTC"},"created":"2012-03-15T02:33:15.325+0000","updated":"2012-03-15T02:33:15.325+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13229848","id":"13229848","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tigar_chen","name":"tigar_chen","key":"tigar_chen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"tigar","active":true,"timeZone":"Etc/UTC"},"body":"ps: my hadoop version is 1.0.1~","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tigar_chen","name":"tigar_chen","key":"tigar_chen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"tigar","active":true,"timeZone":"Etc/UTC"},"created":"2012-03-15T02:45:09.082+0000","updated":"2012-03-15T02:45:09.082+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13236645","id":"13236645","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"body":"\nThere's a bunch of issues at work.  I've patched this up locally\non my own 1.0.2-SNAPSHOT, but it takes a lot of yak-shaving to fix.\n\n\n\n--- \n\nFirst you need to set up hadoop-1.0.1 including source, ant, ivy,\nand cygwin with ssh/ssl and tcp_wrappers.\n\nThen use sshd_config to create a cyg_server priviledged user.\nFrom an admin cygwin shell, you then have to edit the /etc/passwd\nfile and give that user a valid shell and user home, change the\npassword for the user, and finally generate ssh keys for the user\nand copy the user's id_rsa.pub public key into ~/.ssh/authorized_keys.\n\nif done right you should be able to ssh cyg_server@localhost.\n\n\n--- \n\nNow the main problem is a confusion between the hadoop shell scripts\nthat expect unix paths like /tmp, and the haddop java binaries who\ninterpret this path as C:\\tmp.\n\nUnfortunately, neither Cygwin symlinks nor even Windows NT Junctions\nare supported by the java io filesystem.  Thus the only way to get\naround this is to enforce the cygwin paths to be identical to windows\npaths.\n\nI get around this by creating a circular symlink in \"/cygwin\" -> \"/\".\nTo avoid confusion with \"C:\" drive mappings, all my paths are relative.\nThis means that windows \"\\cygwin\\tmp\" equals cygwin's \"/cygwin/tmp\".\n\nFor pid files use /cygwin/tmp/\nFor tmp file  use /cygwin/tmp/haddop-${USER}/\nFor log files use /cygwin/tmp/haddop-${USER}/logs/\n\n\n--- \n\nFirst the ssh slaves invocation warpper is broken because it fails to\nprovide the user's ssh login, which isn't defaulted to in cygwin openssh.\n\n\nslaves.sh:\n\nfor slave in `cat \"$HOSTLIST\"|sed  \"s/#.*$//;/^$/d\"`; do\n ssh -l $USER $HADOOP_SSH_OPTS $slave $\"${@// /\\\\ }\" \\\n   2>&1 | sed \"s/^/$slave: /\" &\n if [ \"$HADOOP_SLAVE_SLEEP\" != \"\" ]; then\n   sleep $HADOOP_SLAVE_SLEEP\n fi\ndone\n\n\nNext the hadoop shell scripts are broken.  you need to fix the environments\nfor cygwin paths in hadoop-env.sh, and then make sure this file is invoked\nby both hadoop-config.sh, and finally the hadoop* sh wrapper script. For me\nits JRE java invocation was also broken, so I provide the whole srcript below.\n\n\nhadoop-env.sh:\n\n  HADOOP_PID_DIR=/cygwin/tmp/\n  HADOOP_TMP_DIR=/cygwin/tmp/hadoop-${USER}\n  HADOOP_LOG_DIR=/cygwin/tmp/hadoop-${USER}/logs\n\n\n\nhadoop (sh):\n\n\n#!/usr/bin/env bash\n\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# The Hadoop command script\n#\n# Environment Variables\n#\n#   JAVA_HOME        The java implementation to use.  Overrides JAVA_HOME.\n#\n#   HADOOP_CLASSPATH Extra Java CLASSPATH entries.\n#\n#   HADOOP_USER_CLASSPATH_FIRST      When defined, the HADOOP_CLASSPATH is \n#                                    added in the beginning of the global\n#                                    classpath. Can be defined, for example,\n#                                    by doing \n#                                    export HADOOP_USER_CLASSPATH_FIRST=true\n#\n#   HADOOP_HEAPSIZE  The maximum amount of heap to use, in MB. \n#                    Default is 1000.\n#\n#   HADOOP_OPTS      Extra Java runtime options.\n#   \n#   HADOOP_NAMENODE_OPTS       These options are added to HADOOP_OPTS \n#   HADOOP_CLIENT_OPTS         when the respective command is run.\n#   HADOOP_{COMMAND}_OPTS etc  HADOOP_JT_OPTS applies to JobTracker \n#                              for e.g.  HADOOP_CLIENT_OPTS applies to \n#                              more than one command (fs, dfs, fsck, \n#                              dfsadmin etc)  \n#\n#   HADOOP_CONF_DIR  Alternate conf dir. Default is ${HADOOP_HOME}/conf.\n#\n#   HADOOP_ROOT_LOGGER The root appender. Default is INFO,console\n#\n\nbin=`dirname \"$0\"`\nbin=`cd \"$bin\"; pwd`\n\ncygwin=false\ncase \"`uname`\" in\nCYGWIN*) cygwin=true;;\nesac\n\n\nif [ -e \"$bin\"/../libexec/hadoop-config.sh ]; then\n  . \"$bin\"/../libexec/hadoop-config.sh\nelse\n  . \"$bin\"/hadoop-config.sh\nfi\n\n\n# if no args specified, show usage\nif [ $# = 0 ]; then\n  echo \"Usage: hadoop [--config confdir] COMMAND\"\n  echo \"where COMMAND is one of:\"\n  echo \"  namenode -format     format the DFS filesystem\"\n  echo \"  secondarynamenode    run the DFS secondary namenode\"\n  echo \"  namenode             run the DFS namenode\"\n  echo \"  datanode             run a DFS datanode\"\n  echo \"  dfsadmin             run a DFS admin client\"\n  echo \"  mradmin              run a Map-Reduce admin client\"\n  echo \"  fsck                 run a DFS filesystem checking utility\"\n  echo \"  fs                   run a generic filesystem user client\"\n  echo \"  balancer             run a cluster balancing utility\"\n  echo \"  fetchdt              fetch a delegation token from the NameNode\"\n  echo \"  jobtracker           run the MapReduce job Tracker node\" \n  echo \"  pipes                run a Pipes job\"\n  echo \"  tasktracker          run a MapReduce task Tracker node\" \n  echo \"  historyserver        run job history servers as a standalone daemon\"\n  echo \"  job                  manipulate MapReduce jobs\"\n  echo \"  queue                get information regarding JobQueues\" \n  echo \"  version              print the version\"\n  echo \"  jar <jar>            run a jar file\"\n  echo \"  distcp <srcurl> <desturl> copy file or directories recursively\"\n  echo \"  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\"\n  echo \"  classpath            prints the class path needed to get the\"\n  echo \"                       Hadoop jar and the required libraries\"\n  echo \"  daemonlog            get/set the log level for each daemon\"\n  echo \" or\"\n  echo \"  CLASSNAME            run the class named CLASSNAME\"\n  echo \"Most commands print help when invoked w/o parameters.\"\n  exit 1\nfi\n\n# get arguments\nCOMMAND=$1\nshift\n\n# Determine if we're starting a secure datanode, and if so, redefine appropriate variables\nif [ \"$COMMAND\" == \"datanode\" ] && [ \"$EUID\" -eq 0 ] && [ -n \"$HADOOP_SECURE_DN_USER\" ]; then\n  HADOOP_PID_DIR=$HADOOP_SECURE_DN_PID_DIR\n  HADOOP_LOG_DIR=$HADOOP_SECURE_DN_LOG_DIR\n  HADOOP_IDENT_STRING=$HADOOP_SECURE_DN_USER\n  starting_secure_dn=\"true\"\nfi\n\nif [ \"$JAVA_HOME\" != \"\" ]; then\n  #echo \"JAVA_HOME: $JAVA_HOME\"\n  JAVA_HOME=\"$JAVA_HOME\"\nfi\n# some Java parameters\nif $cygwin; then\n  JAVA_HOME=`cygpath -w \"$JAVA_HOME\"`\n  #echo \"cygwin JAVA_HOME: $JAVA_HOME\"  \nfi\n  if [ \"$JAVA_HOME\" == \"\" ]; then\n  echo \"Error: JAVA_HOME is not set: $JAVA_HOME\"\n  exit 1\nfi\n\nJAVA=$JAVA_HOME/bin/java\nJAVA_HEAP_MAX=-Xmx1000m \n\n# check envvars which might override default args\nif [ \"$HADOOP_HEAPSIZE\" != \"\" ]; then\n  #echo \"run with heapsize $HADOOP_HEAPSIZE\"\n  JAVA_HEAP_MAX=\"-Xmx\"\"$HADOOP_HEAPSIZE\"\"m\"\n  #echo $JAVA_HEAP_MAX\nfi\n\n# CLASSPATH initially contains $HADOOP_CONF_DIR\nCLASSPATH=\"${HADOOP_CONF_DIR}\"\nif [ \"$HADOOP_USER_CLASSPATH_FIRST\" != \"\" ] && [ \"$HADOOP_CLASSPATH\" != \"\" ] ; then\n  CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}\nfi\nCLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar\n\n# for developers, add Hadoop classes to CLASSPATH\nif [ -d \"$HADOOP_HOME/build/classes\" ]; then\n  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/classes\nfi\nif [ -d \"$HADOOP_HOME/build/webapps\" ]; then\n  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build\nfi\nif [ -d \"$HADOOP_HOME/build/test/classes\" ]; then\n  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/test/classes\nfi\nif [ -d \"$HADOOP_HOME/build/tools\" ]; then\n  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/tools\nfi\n\n# so that filenames w/ spaces are handled correctly in loops below\nIFS=\n\n# for releases, add core hadoop jar & webapps to CLASSPATH\nif [ -e $HADOOP_PREFIX/share/hadoop/hadoop-core-* ]; then\n  # binary layout\n  if [ -d \"$HADOOP_PREFIX/share/hadoop/webapps\" ]; then\n    CLASSPATH=${CLASSPATH}:$HADOOP_PREFIX/share/hadoop\n  fi\n  for f in $HADOOP_PREFIX/share/hadoop/hadoop-core-*.jar; do\n    CLASSPATH=${CLASSPATH}:$f;\n  done\n\n  # add libs to CLASSPATH\n  for f in $HADOOP_PREFIX/share/hadoop/lib/*.jar; do\n    CLASSPATH=${CLASSPATH}:$f;\n  done\n\n  for f in $HADOOP_PREFIX/share/hadoop/lib/jsp-2.1/*.jar; do\n    CLASSPATH=${CLASSPATH}:$f;\n  done\n\n  for f in $HADOOP_PREFIX/share/hadoop/hadoop-tools-*.jar; do\n    TOOL_PATH=${TOOL_PATH}:$f;\n  done\nelse\n  # tarball layout\n  if [ -d \"$HADOOP_HOME/webapps\" ]; then\n    CLASSPATH=${CLASSPATH}:$HADOOP_HOME\n  fi\n  for f in $HADOOP_HOME/hadoop-core-*.jar; do\n    CLASSPATH=${CLASSPATH}:$f;\n  done\n\n  # add libs to CLASSPATH\n  for f in $HADOOP_HOME/lib/*.jar; do\n    CLASSPATH=${CLASSPATH}:$f;\n  done\n\n  if [ -d \"$HADOOP_HOME/build/ivy/lib/Hadoop/common\" ]; then\n    for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/*.jar; do\n      CLASSPATH=${CLASSPATH}:$f;\n    done\n  fi\n\n  for f in $HADOOP_HOME/lib/jsp-2.1/*.jar; do\n    CLASSPATH=${CLASSPATH}:$f;\n  done\n\n  for f in $HADOOP_HOME/hadoop-tools-*.jar; do\n    TOOL_PATH=${TOOL_PATH}:$f;\n  done\n  for f in $HADOOP_HOME/build/hadoop-tools-*.jar; do\n    TOOL_PATH=${TOOL_PATH}:$f;\n  done\nfi\n\n# add user-specified CLASSPATH last\nif [ \"$HADOOP_USER_CLASSPATH_FIRST\" = \"\" ] && [ \"$HADOOP_CLASSPATH\" != \"\" ]; then\n  CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}\nfi\n\n# default log directory & file\nif [ \"$HADOOP_LOG_DIR\" = \"\" ]; then\n  HADOOP_LOG_DIR=\"$HADOOP_HOME/logs\"\nfi\nif [ \"$HADOOP_LOGFILE\" = \"\" ]; then\n  HADOOP_LOGFILE='hadoop.log'\nfi\n\n# default policy file for service-level authorization\nif [ \"$HADOOP_POLICYFILE\" = \"\" ]; then\n  HADOOP_POLICYFILE=\"hadoop-policy.xml\"\nfi\n\n# restore ordinary behaviour\nunset IFS\n\n# figure out which class to run\nif [ \"$COMMAND\" = \"classpath\" ] ; then\n  if $cygwin; then\n    CLASSPATH=`cygpath -wp \"$CLASSPATH\"`\n  fi\n  echo $CLASSPATH\n  exit\nelif [ \"$COMMAND\" = \"namenode\" ] ; then\n  CLASS='org.apache.hadoop.hdfs.server.namenode.NameNode'\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_NAMENODE_OPTS\"\nelif [ \"$COMMAND\" = \"secondarynamenode\" ] ; then\n  CLASS='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_SECONDARYNAMENODE_OPTS\"\nelif [ \"$COMMAND\" = \"datanode\" ] ; then\n  CLASS='org.apache.hadoop.hdfs.server.datanode.DataNode'\n  if [ \"$starting_secure_dn\" = \"true\" ]; then\n    HADOOP_OPTS=\"$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS\"\n  else\n    HADOOP_OPTS=\"$HADOOP_OPTS -server $HADOOP_DATANODE_OPTS\"\n  fi\nelif [ \"$COMMAND\" = \"fs\" ] ; then\n  CLASS=org.apache.hadoop.fs.FsShell\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"dfs\" ] ; then\n  CLASS=org.apache.hadoop.fs.FsShell\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"dfsadmin\" ] ; then\n  CLASS=org.apache.hadoop.hdfs.tools.DFSAdmin\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"mradmin\" ] ; then\n  CLASS=org.apache.hadoop.mapred.tools.MRAdmin\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"fsck\" ] ; then\n  CLASS=org.apache.hadoop.hdfs.tools.DFSck\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"balancer\" ] ; then\n  CLASS=org.apache.hadoop.hdfs.server.balancer.Balancer\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_BALANCER_OPTS\"\nelif [ \"$COMMAND\" = \"fetchdt\" ] ; then\n  CLASS=org.apache.hadoop.hdfs.tools.DelegationTokenFetcher\nelif [ \"$COMMAND\" = \"jobtracker\" ] ; then\n  CLASS=org.apache.hadoop.mapred.JobTracker\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_JOBTRACKER_OPTS\"\nelif [ \"$COMMAND\" = \"historyserver\" ] ; then\n  CLASS=org.apache.hadoop.mapred.JobHistoryServer\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_JOB_HISTORYSERVER_OPTS\"\nelif [ \"$COMMAND\" = \"tasktracker\" ] ; then\n  CLASS=org.apache.hadoop.mapred.TaskTracker\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_TASKTRACKER_OPTS\"\nelif [ \"$COMMAND\" = \"job\" ] ; then\n  CLASS=org.apache.hadoop.mapred.JobClient\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"queue\" ] ; then\n  CLASS=org.apache.hadoop.mapred.JobQueueClient\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"pipes\" ] ; then\n  CLASS=org.apache.hadoop.mapred.pipes.Submitter\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"version\" ] ; then\n  CLASS=org.apache.hadoop.util.VersionInfo\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"jar\" ] ; then\n  CLASS=org.apache.hadoop.util.RunJar\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"distcp\" ] ; then\n  CLASS=org.apache.hadoop.tools.DistCp\n  CLASSPATH=${CLASSPATH}:${TOOL_PATH}\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"daemonlog\" ] ; then\n  CLASS=org.apache.hadoop.log.LogLevel\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"archive\" ] ; then\n  CLASS=org.apache.hadoop.tools.HadoopArchives\n  CLASSPATH=${CLASSPATH}:${TOOL_PATH}\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelif [ \"$COMMAND\" = \"sampler\" ] ; then\n  CLASS=org.apache.hadoop.mapred.lib.InputSampler\n  HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"\nelse\n  CLASS=$COMMAND\nfi\n\n\n# cygwin path translation\nif $cygwin; then\n  JAVA_HOME=`cygpath -w \"$JAVA_HOME\"`\n  CLASSPATH=`cygpath -wp \"$CLASSPATH\"`\n  HADOOP_HOME=`cygpath -w \"$HADOOP_HOME\"`\n  HADOOP_LOG_DIR=`cygpath -w \"$HADOOP_LOG_DIR\"`\n  TOOL_PATH=`cygpath -wp \"$TOOL_PATH\"`\nfi\n\n# setup 'java.library.path' for native-hadoop code if necessary\nJAVA_LIBRARY_PATH=''\n\n\nif [ -d \"${HADOOP_HOME}/build/native\" -o -d \"${HADOOP_HOME}/lib/native\" -o -e \"${HADOOP_PREFIX}/lib/libhadoop.a\" ]; then\n  JAVA_PLATFORM=`${JAVA} -classpath ${CLASSPATH} -Xmx32m ${HADOOP_JAVA_PLATFORM_OPTS} org.apache.hadoop.util.PlatformName | sed -e \"s/ /_/g\"`\n  #echo \"JAVA_PLATFORM: $JAVA_PLATFORM\"\n  \n  if [ \"$JAVA_PLATFORM\" = \"Windows_7-amd64-64\" ]; then\n    JSVC_ARCH=\"amd64\"\n  elif [ \"$JAVA_PLATFORM\" = \"Linux-amd64-64\" ]; then\n    JSVC_ARCH=\"amd64\"\n  else\n    JSVC_ARCH=\"i386\"\n  fi\n\n  if [ -d \"$HADOOP_HOME/build/native\" ]; then\n    JAVA_LIBRARY_PATH=${HADOOP_HOME}/build/native/${JAVA_PLATFORM}/lib\n  fi\n  \n  if [ -d \"${HADOOP_HOME}/lib/native\" ]; then\n    if [ \"x$JAVA_LIBRARY_PATH\" != \"x\" ]; then\n      JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}\n    else\n      JAVA_LIBRARY_PATH=${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}\n    fi\n  fi\n\n  if [ -e \"${HADOOP_PREFIX}/lib/libhadoop.a\" ]; then\n    JAVA_LIBRARY_PATH=${HADOOP_PREFIX}/lib\n  fi\nfi\n\n# cygwin path translation\nif $cygwin; then\n  JAVA_LIBRARY_PATH=`cygpath -wp \"$JAVA_LIBRARY_PATH\"`\n  PATH=\"/cygwin/bin:/cygwin/usr/bin:`cygpath -p ${PATH}`\"\nfi\n\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.tmp.dir=$HADOOP_TMP_DIR\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.log.file=$HADOOP_LOGFILE\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.home.dir=$HADOOP_HOME\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.id.str=$HADOOP_IDENT_STRING\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}\"\n\n#turn security logger on the namenode and jobtracker only\nif [ $COMMAND = \"namenode\" ] || [ $COMMAND = \"jobtracker\" ]; then\n  HADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,DRFAS}\"\nelse\n  HADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,NullAppender}\"\nfi\n\nif [ \"x$JAVA_LIBRARY_PATH\" != \"x\" ]; then\n  HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH\"\nfi  \nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE\"\n\n# Check to see if we should start a secure datanode\nif [ \"$starting_secure_dn\" = \"true\" ]; then\n  if [ \"$HADOOP_PID_DIR\" = \"\" ]; then\n    HADOOP_SECURE_DN_PID=\"/tmp/hadoop_secure_dn.pid\"\n  else\n    HADOOP_SECURE_DN_PID=\"$HADOOP_PID_DIR/hadoop_secure_dn.pid\"\n  fi\n\n  exec \"$HADOOP_HOME/libexec/jsvc.${JSVC_ARCH}\" -Dproc_$COMMAND -outfile \"$HADOOP_LOG_DIR/jsvc.out\" \\\n                                                -errfile \"$HADOOP_LOG_DIR/jsvc.err\" \\\n                                                -pidfile \"$HADOOP_SECURE_DN_PID\" \\\n                                                -nodetach \\\n                                                -user \"$HADOOP_SECURE_DN_USER\" \\\n                                                -cp \"$CLASSPATH\" \\\n                                                $JAVA_HEAP_MAX $HADOOP_OPTS \\\n                                                org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter \"$@\"\nelse\n  # run it\n  exec \"$JAVA\" -Dproc_$COMMAND $JAVA_HEAP_MAX $HADOOP_OPTS -classpath \"$CLASSPATH\" $CLASS \"$@\"\nfi\n\n\n\n\n----\n\n\nNext the hadoop fs and utilities are broken, as they expect shells\nwith POSIX /bin executables in their path (bash,chmod,chown,chgrp)\nFor various reasons it's a real bad idea to add \"/cygwin/bin\" to your\nwindows path, so we're going to have to fix the utility classes to\nbe cygwin aware and use the \"/cygwin/bin\" binaries instead.\n\nThis is why you need the source, because we're going to have to fix\nthe java source and recompile the hadoop core libraries (and why you\nneed ant ivy).\n\n----\n\nBefore we do this, the contrib Gridmix is broken as it uses a strange\ngeneric Enum code that just craps out in jdk/jre 1.7 and above.\nThe fix is to dumb it down and use untyped Enums.\n\n\nGridmix.java:\n\n/*  \n  private <T> String getEnumValues(Enum<? extends T>[] e) {\n    StringBuilder sb = new StringBuilder();\n    String sep = \"\";\n    for (Enum<? extends T> v : e) {\n      sb.append(sep);\n      sb.append(v.name());\n      sep = \"|\";\n    }\n    return sb.toString();\n  }\n*/\n  private String getEnumValues(Enum[] e) {\n    StringBuilder sb = new StringBuilder();\n    String sep = \"\";\n    for (Enum v : e) {\n      sb.append(sep);\n      sb.append(v.name());\n      sep = \"|\";\n    }\n    return sb.toString();\n  }\n\n\n---\n\nnext first the ivy build.xml and build-contrib scripts are broken,\nas they fail to set the correct compiler javac.target=1.7 everywhere.\n\nmodify all of these to include the following in all javac targets:\n\n\nbuild-contrib.xml:\n\n\n  <property name=\"javac.debug\" value=\"on\"/>\n  <property name=\"javac.version\" value=\"1.7\"/>\n\n  ...\n\n  <!-- ====================================================== -->\n  <!-- Compile a Hadoop contrib's files                       -->\n  <!-- ====================================================== -->\n  <target name=\"compile\" depends=\"init, ivy-retrieve-common\" unless=\"skip.contrib\">\n    <echo message=\"contrib: ${name}\"/>\n    <javac\n     encoding=\"${build.encoding}\"\n     srcdir=\"${src.dir}\"\n     includes=\"**/*.java\"\n     destdir=\"${build.classes}\"\n     target=\"${javac.version}\"\n     source=\"${javac.version}\"\n     optimize=\"${javac.optimize}\"\n     debug=\"${javac.debug}\"\n     deprecation=\"${javac.deprecation}\">\n     <classpath refid=\"contrib-classpath\"/>\n    </javac>\n  </target>\n\n---\n\nNext we fix the hadoop utilities Shell.java to use cygwin paths:\n\n\nShell.java:\n\n  /** Set to true on Windows platforms */\n  public static final boolean WINDOWS /* borrowed from Path.WINDOWS */\n                = System.getProperty(\"os.name\").startsWith(\"Windows\");\n  \n  /** a Unix command to get the current user's name */\n  public final static String USER_NAME_COMMAND = (WINDOWS ? \"/cygwin/bin/whoami\" : \"whoami\");\n  \n  /** a Unix command to get the current user's groups list */\n  public static String[] getGroupsCommand() {\n    return new String[]{ (WINDOWS ? \"/cygwin/bin/bash\" : \"bash\"), \"-c\", \"groups\"};\n  }\n  \n  /** a Unix command to get a given user's groups list */\n  public static String[] getGroupsForUserCommand(final String user) {\n    //'groups username' command return is non-consistent across different unixes\n    return new String [] {(WINDOWS ? \"/cygwin/bin/bash\" : \"bash\"), \"-c\", \"id -Gn \" + user};\n  }\n  \n  /** a Unix command to get a given netgroup's user list */\n  public static String[] getUsersForNetgroupCommand(final String netgroup) {\n    //'groups username' command return is non-consistent across different unixes\n    return new String [] {(WINDOWS ? \"/cygwin/bin/bash\" : \"bash\"), \"-c\", \"getent netgroup \" + netgroup};\n  }\n\n  \n  /** Return a Unix command to get permission information. */\n  public static String[] getGET_PERMISSION_COMMAND() {\n    //force /bin/ls, except on windows.\n    return new String[] {(WINDOWS ? \"/cygwin/bin/ls\" : \"/bin/ls\"), \"-ld\"};\n  }\n  \n  \n  /** a Unix command to set permission */\n  public static final String SET_PERMISSION_COMMAND = (WINDOWS ? \"/cygwin/bin/chmod\" : \"chmod\");\n  \n  /** a Unix command to set owner */\n  public static final String SET_OWNER_COMMAND = (WINDOWS ? \"/cygwin/bin/chown\" : \"chown\");\n\n  /** a Unix command to set group */\n  public static final String SET_GROUP_COMMAND = (WINDOWS ? \"/cygwin/bin/chgrp\" : \"chgrp\");\n\n  /** a Unix command to get ulimit of a process. */\n  public static final String ULIMIT_COMMAND = \"ulimit\";\n\n\n----\n\nLastly and despite this fix, hadoop filesystem's FileUtil complains\nabout RawLocalFileSystem, breaking during the directory creation and\nverification because the shell's return value is improperly parsed.\n\nYou can fix this in a number of ways.  I took the lazy approach and\njust made all mkdir functions catch all IOExceptions silently.\n\n\nRawLocalFileSystem.java:\n\n  /**\n   * Creates the specified directory hierarchy. Does not\n   * treat existence as an error.\n   */\n  public boolean mkdirs(Path f) throws IOException {\n\tboolean b = false;\n\ttry {\n      Path parent = f.getParent();\n      File p2f = pathToFile(f);\n      b = (parent == null || mkdirs(parent))\n       && (p2f.mkdir() || p2f.isDirectory());\n\t} catch (IOException e) {}\n\treturn b;\n  }\n\n  /** {@inheritDoc} */\n  @Override\n  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n\tboolean b = false;\n\ttry {\n      b = mkdirs(f);\n      setPermission(f, permission);\n\t} catch (IOException e) {}\n    return b;\n  }\n\n\n---\n\n\nFinally, rebuild hadoop with \"ant -f build.xml compile\".\ncopy the jars in the build directory oevrwriting the\nexisting jars in the hadoop home parent directory.\n\nreformat the namenode.\n\nand run start-all.sh.\n\n\nyou should see 4 java processes for the namenode, datanode,\njobtracker, and tasktracker.  that was a lot of yak shaving\njust to get this running.\n\n\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"created":"2012-03-23T14:59:01.197+0000","updated":"2012-03-23T14:59:01.197+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13257676","id":"13257676","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=davelatham","name":"davelatham","key":"davelatham","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dave Latham","active":true,"timeZone":"America/Los_Angeles"},"body":"Here's my understanding of this issue from digging around a bit in case it's helpful for others.  I could be mistaken on some of the details.\n\nSome time ago, RawLocalFileSystem.setPermission used to use a shell exec command to fork a process to alter permissions of a file.  \n\nSomewhere along the 0.20 branch (I believe in the 0.20.security branch) it was decided that this was too slow, and instead java.io.File.set{Readable|Writable|Executable} should be used (see HADOOP-6304 for one such issue.  It has a patch with the logic that wound up in FileUtil though perhaps committed from a different patch).  However the java.io.File methods don't allow one to directly set all the permissions so the code first has to clear permissions, then build them up again.  This resulted in two problems.  First, there is a race condition when the file briefly has no permissions even for the owner (see MAPREDUCE-2238 for more detail).  Second, Windows doesn't support clearing all permissions on the file (this JIRA).\n\nThe first problem was worked around in HADOOP-7110 (HADOOP-7432 backported it to this branch) by using JNI native code instead.  However, if the native code is not available, then it falls back to the java.io.File methods.  So, the second problem still remains, that FileUtil.setPermission (and thus the RawLocalFileSystem setPermission) does not work on Windows because Windows does not have the native code implementation and also fails the java.io.File fallback.\n\nThe issues FKorning ran into in a comment above appear to be wider than this particular JIRA, though I may have misunderstood what led to his shorn yak.\n\nWindows is listed as a supported platform for Hadoop, and some of our developers use Windows as a development environment, so it's important for us that hadoop at least functions on Windows, even if it's not as performant as our production clusters on Linux.  I noted that this is currently the highest voted open JIRA for hadoop.\n\nIn order for it to function on Windows, I added a final fallback in FileUtil.setPermission to revert to the older behavior of using a shell exec fork for setPermission when the other methods fail.  Perhaps it will be helpful for others:\n\n{code}\n  public static void setPermission(File f, FsPermission permission\n                                   ) throws IOException {\n    FsAction user = permission.getUserAction();\n    FsAction group = permission.getGroupAction();\n    FsAction other = permission.getOtherAction();\n\n    // use the native/fork if the group/other permissions are different\n    // or if the native is available    \n    if (group != other || NativeIO.isAvailable()) {\n      execSetPermission(f, permission);\n      return;\n    }\n    \n    try\n    {\n\t    boolean rv = true;\n\t    \n\t    // read perms\n\t    rv = f.setReadable(group.implies(FsAction.READ), false);\n\t    checkReturnValue(rv, f, permission);\n\t    if (group.implies(FsAction.READ) != user.implies(FsAction.READ)) {\n\t      f.setReadable(user.implies(FsAction.READ), true);\n\t      checkReturnValue(rv, f, permission);\n\t    }\n\t\n\t    // write perms\n\t    rv = f.setWritable(group.implies(FsAction.WRITE), false);\n\t    checkReturnValue(rv, f, permission);\n\t    if (group.implies(FsAction.WRITE) != user.implies(FsAction.WRITE)) {\n\t      f.setWritable(user.implies(FsAction.WRITE), true);\n\t      checkReturnValue(rv, f, permission);\n\t    }\n\t\n\t    // exec perms\n\t    rv = f.setExecutable(group.implies(FsAction.EXECUTE), false);\n\t    checkReturnValue(rv, f, permission);\n\t    if (group.implies(FsAction.EXECUTE) != user.implies(FsAction.EXECUTE)) {\n\t      f.setExecutable(user.implies(FsAction.EXECUTE), true);\n\t      checkReturnValue(rv, f, permission);\n\t    }\n    }\n    catch (IOException ioe)\n    {\n    \tLOG.warn(\"Java file permissions failed to set \" + f + \" to \" + permission + \" falling back to fork\");\n    \texecSetPermission(f, permission);\n    }\n    \n  }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=davelatham","name":"davelatham","key":"davelatham","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dave Latham","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-04-19T18:40:49.461+0000","updated":"2012-04-19T18:40:49.461+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13268979","id":"13268979","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"body":"\nI've managed to get this working to the point where jobs are dispatched, tasks executed, and results compiled.\n\n* [http://en.wikisource.org/wiki/User:Fkorning/Code/Haddoop-on-Cygwin]\n\nHowever we still need to get the servlets to understand cygwin symlinks.  I have no idea how to do this in Jetty.\n\nThese two links show how to allow Tomcat and jetty to follow symlinks, but I don't know if this works in cygwin.\n*  http://www.lamoree.com/machblog/index.cfm?event=showEntry&entryId=A2F0ED76-A500-41A6-A1DFDE0D1996F925\n*  http://stackoverflow.com/questions/315093/configure-symlinks-for-single-directory-in-tomcat\n\nOtherwise we'll have to open up the jetty code and replace java.io.File with org.apache.hadoop.fs.LinkedFile.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"created":"2012-05-05T15:02:38.846+0000","updated":"2012-05-05T15:02:38.846+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13268984","id":"13268984","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"body":"make that: http://en.wikisource.org/wiki/User:Fkorning/Code/Hadoop-on-Cygwin","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"created":"2012-05-05T15:16:42.309+0000","updated":"2012-05-05T15:16:42.309+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13396813","id":"13396813","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ismail_mca","name":"ismail_mca","key":"ismail_mca","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ismail Shaik","active":true,"timeZone":"Etc/UTC"},"body":"I have tested with v1.0.1 and same problen within eclipse/cygwin.Do you have quick workaround?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ismail_mca","name":"ismail_mca","key":"ismail_mca","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ismail Shaik","active":true,"timeZone":"Etc/UTC"},"created":"2012-06-19T14:34:34.177+0000","updated":"2012-06-19T14:34:34.177+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13397563","id":"13397563","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"body":"Ismail,\n\nYou misunderstand, I haven't patched the offical 1.0.1 codebase:\nI'm not an official hadoop contributor, I'm not really sure if the\ndevelopers will approve all my fixes such as adding a java.io.File\nwrapper etc.\n\n(to wit, no persons from the hadoop team, not even those who were\nassigned to the various cygwin bugs, have contacted me on this).\n  \nwhat I provide are instructions for you to patch 1.0.1 yourself,\nand hopefully provide guidance to the official hadoop developpers\non porting the software properly to cygwin.\n\nand no, if you read the doc, you'll see why there is no quick\nworkaround, short of falling back to 0.20.  But I rather think\nmost people would want a 1.x, especially compiled with 64-bit.\n\nit's not a simple code fix. you need to understand how cygwin works;\nyou need to customize and configure cygwin and the pre-requisite\nsofware such as java, ant, ivy, maven; you need to patch scripts;\nyou need to patch code; you need to add brand new classes to wrap\njava.io.File; you need to rebuild, reconfigure and redeploy it;\n\nfor a complete working fix, I want to port the latest version\nand release the code on sourceforge, but I'm a bit busy now.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"created":"2012-06-20T14:56:24.597+0000","updated":"2012-06-20T14:56:24.597+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13440120","id":"13440120","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=j_caplan","name":"j_caplan","key":"j_caplan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joshua Caplan","active":true,"timeZone":"America/Los_Angeles"},"body":"May I suggest a simple workaround for Windows users struggling with this new impediment.\n\ncreate a WinLocalFileSystem class (subclass of LocalFileSystem) which ignores IOExceptions on setPermissions() or, if you're feeling ambitious, does something more appropriate when trying to set them.  You will also need to override mkdirs( Path, FsPermission ) to delegate to mkdirs( Path ) and call this.setPermission.\n\nRegister this class as the implementation of the \"file\" schema, using the property definition\n\nfs.file.impl=com.package.name.WinLocalFileSystem\n\nThat got me past it for local testing.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=j_caplan","name":"j_caplan","key":"j_caplan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joshua Caplan","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-08-23T08:17:26.146+0000","updated":"2012-08-23T08:17:26.146+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13448072","id":"13448072","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=toddfast","name":"toddfast","key":"toddfast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Todd Fast","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks, Joshua! Your workaround got us running locally on Hadoop 1.0.3 without any issues. Here is a GitHub repository for the source (as well as a pre-built JAR):\n\nhttps://github.com/congainc/patch-hadoop_7682-1.0.x-win\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=toddfast","name":"toddfast","key":"toddfast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Todd Fast","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-09-04T21:18:21.476+0000","updated":"2012-09-04T21:18:21.476+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13453878","id":"13453878","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=496601562","name":"496601562","key":"496601562","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dongchao Ding","active":true,"timeZone":"Asia/Shanghai"},"body":"the same  on hadoop  1.0.1  with win7 and cygwin  。  \n because just test  ,so  i suggest a easy workaround :\n1、  alert  FileUtil\nmkdir $HADOOP_HOME/classes  \nand  then alter the  org.apache.hadoop.fs.FileUtil.checkReturnValue  remove the Exception .\nbuild  、copy FileUtil.class  to $HADOOP_HOME/classes   。\n\n\n2 、alter classpath \n\nadd shell in  $HADOOP_HOME/hadoop  file ：\n\nif [ -d \"$HADOOP_HOME/classes\" ]; then\n  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/classes\nfi\n\nmake sure this classpath is in front of  $HADOOP_HOME/hadoop-core-1.*.*.jar  。\n\n\nrestart    OK  \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=496601562","name":"496601562","key":"496601562","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dongchao Ding","active":true,"timeZone":"Asia/Shanghai"},"created":"2012-09-12T10:09:51.573+0000","updated":"2012-09-12T10:09:51.573+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13459067","id":"13459067","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=visioner.sadak","name":"visioner.sadak","key":"visioner.sadak","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"visionersadak","active":true,"timeZone":"Etc/UTC"},"body":"thanks a ton todd the path u mentioned got our task tracker running on windows but when we started creating a HAR job its throwing this error\n\njava.io.IOException: Failed to set permissions of path: \\tmp\\hadoop-cyg_server\\mapred\\local\\taskTracker\\sadak(myusername) to 0700\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=visioner.sadak","name":"visioner.sadak","key":"visioner.sadak","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"visionersadak","active":true,"timeZone":"Etc/UTC"},"created":"2012-09-19T20:36:29.004+0000","updated":"2012-09-19T20:36:29.004+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13461005","id":"13461005","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luiz.veronesi","name":"luiz.veronesi","key":"luiz.veronesi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luiz Veronesi","active":true,"timeZone":"Etc/UTC"},"body":"Thanks Todd Fast for this workaround.\n\nThat also solved running Nutch within Eclipse. \n\nExactly the same thing must be done to make it works.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luiz.veronesi","name":"luiz.veronesi","key":"luiz.veronesi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luiz Veronesi","active":true,"timeZone":"Etc/UTC"},"created":"2012-09-22T02:20:15.070+0000","updated":"2012-09-22T02:20:15.070+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13482671","id":"13482671","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fshao909","name":"fshao909","key":"fshao909","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"frank","active":true,"timeZone":"Etc/UTC"},"body":"Anyone find out what is causing visionersadak's error? Todd's fix worked great; now I'm getting the same \"failed to set permissions\" error.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fshao909","name":"fshao909","key":"fshao909","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"frank","active":true,"timeZone":"Etc/UTC"},"created":"2012-10-23T20:51:06.218+0000","updated":"2012-10-23T20:51:06.218+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13483537","id":"13483537","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=toddfast","name":"toddfast","key":"toddfast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Todd Fast","active":true,"timeZone":"America/Los_Angeles"},"body":"It sounded as if he was running hadoop in pseudo-distributed or fully distributed mode, and perhaps the way the child processes are spawned doesn't include the patch in the classpath. A quick test to verify this theory, though I wouldn't do this under normal running conditions, would be to put the patch JAR in the JDK's ${JAVA_HOME}/jre/lib/ext directory, which includes it in the classpath for all VMs. If that works, then you know that it's something to do with the local classpath when the hadoop VMs are started. At that point, you can further debug how the processes are spawned, or you can run in standalone mode instead.\n\nFor standalone mode, the patch is automatically added to the classpath from the ${HADOOP_HOME}/lib directory, and if the patch is installed correctly, you should see some logging that indicates it is working.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=toddfast","name":"toddfast","key":"toddfast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Todd Fast","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-10-24T20:10:27.037+0000","updated":"2012-10-24T20:10:27.037+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13541333","id":"13541333","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=huiwenhan","name":"huiwenhan","key":"huiwenhan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Han Hui Wen ","active":true,"timeZone":"Asia/Shanghai"},"body":"I find one approach : \n\nJust need change file ${HADOOP_HOME}/bin/hadoop-config.sh\n\nfrom :\n\nJAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} -Xmx32m ${HADOOP_JAVA_PLATFORM_OPTS} org.apache.hadoop.util.PlatformName | sed -e ”s/ /_/g”`\n\nto:\n\nJAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} -Xmx32m -classpath ${HADOOP_COMMON_HOME}/hadoop-common-0.21.0.jar org.apache.hadoop.util.PlatformName | sed -e ”s/ /_/g”`","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=huiwenhan","name":"huiwenhan","key":"huiwenhan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Han Hui Wen ","active":true,"timeZone":"Asia/Shanghai"},"created":"2012-12-31T11:13:15.228+0000","updated":"2012-12-31T11:13:15.228+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13553956","id":"13553956","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=o_nix","name":"o_nix","key":"o_nix","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kirill Vergun","active":true,"timeZone":"Europe/Helsinki"},"body":"visionersadak,\n\nare you running your installation under “single node” (pseudo-distributed) configuration?\nI am trying to do my own patching, it may work on such type of configuration.\n\nhttps://github.com/o-nix/hadoop-patches\n\nBut it slows down every hadoop shell command execution a lot.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=o_nix","name":"o_nix","key":"o_nix","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kirill Vergun","active":true,"timeZone":"Europe/Helsinki"},"created":"2013-01-15T16:26:51.017+0000","updated":"2013-01-15T16:26:51.017+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13554028","id":"13554028","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"body":"yes.  it is extremely slow, which sort of makes the whole windows thing a bit moot.  then again, if you have a farm of windows boxes sitting idle, you may as well use their cycles...\n\nSent from my iPhone\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fkorning","name":"fkorning","key":"fkorning","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"FKorning","active":true,"timeZone":"Etc/UTC"},"created":"2013-01-15T17:28:11.924+0000","updated":"2013-01-15T17:28:11.924+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13625368","id":"13625368","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dczajka","name":"dczajka","key":"dczajka","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daniel Czajka","active":true,"timeZone":"Etc/UTC"},"body":"Hey, I checked on latest stable 1.0.4/win7 and still the same error occurred. Are there any plans/timelines to fix it ?\n\nCheers,\nDaniel","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dczajka","name":"dczajka","key":"dczajka","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daniel Czajka","active":true,"timeZone":"Etc/UTC"},"created":"2013-04-08T13:44:28.279+0000","updated":"2013-04-08T13:44:28.279+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13648585","id":"13648585","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=giroux7103","name":"giroux7103","key":"giroux7103","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Giroux","active":true,"timeZone":"America/New_York"},"body":"I'm using 1.0.4...\n\nThis issue is coming from the org.apache.hadoop.fs.FileUtil class.  There is a static setPermission method that uses the standard java.io.File setPermission method which will always fail on Windows.  The quickest and least destructive way to resolve this is just create three new classes:\n\norg.apache.hadoop.fs.Win32LocalFileSystem\norg.apache.hadoop.fs.Win32RawLocalFileSystem\norg.apache.hadoop.fs.Win32FileUtil\n\nThey obviously extend the classes of the same name without the \"Win32\".  Here is the code for each:\n\n<begin Win32FileUtil>\n\npackage org.apache.hadoop.fs;\n\nimport java.io.File;\nimport java.io.IOException;\nimport org.apache.hadoop.fs.permission.FsPermission;\n\n/**\n * A collection of file-processing util methods\n */\npublic class Win32FileUtil extends FileUtil {\n\n\t\n\t/* override setPermission */\n\tpublic static void setPermission(File f, FsPermission permission\n                                   ) throws IOException {\n\t\t/* assume can't set permissions on win32 */\n\t\t/* do nothing */\n\t}\n}\n\n<end Win32FileUtil>\n\n<begin Win32RawLocalFileSystem>\n\npackage org.apache.hadoop.fs;\n\nimport java.io.IOException;\nimport org.apache.hadoop.fs.permission.FsPermission;\n\n/****************************************************************\n * Implement the FileSystem API for the raw local filesystem.\n *\n *****************************************************************/\npublic class Win32RawLocalFileSystem extends RawLocalFileSystem {\n  \n  public Win32RawLocalFileSystem() {\n    super();\n  }\n   \n  @Override\n  public void setPermission(Path p, FsPermission permission\n                            ) throws IOException {\n    Win32FileUtil.setPermission(pathToFile(p), permission);\n  }\n}\n\n<end Win32RawLocalFileSystem>\n\n\n<begin Win32LocalFileSystem>\n\npackage org.apache.hadoop.fs;\n\n\n/****************************************************************\n * Implement the FileSystem API for the checksumed local filesystem.\n *\n *****************************************************************/\npublic class Win32LocalFileSystem extends LocalFileSystem {\n\n\tpublic Win32LocalFileSystem() {\n\t\tthis(new Win32RawLocalFileSystem());\n\t}\n\t\n\tpublic Win32LocalFileSystem(FileSystem rawLocalFileSystem) {\n\t\tsuper(rawLocalFileSystem);\n\t\trfs = rawLocalFileSystem;\n\t}\n\n\t\n}\n\n<end Win32LocalFileSystem>\n\n\nFor a quickest compile, just manually compile and add these classes into the hadoop-core-1.0.4.jar (with this jar on the classpath) in the same order as above (backup your jar file first to be safe).  Then edit the core-site.xml configuration file with the following entry:\n\n<property>\n    <name>fs.file.impl</name>\n    <value>org.apache.hadoop.fs.Win32LocalFileSystem</value>\n</property>\n\nAll done - should work now.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=giroux7103","name":"giroux7103","key":"giroux7103","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Giroux","active":true,"timeZone":"America/New_York"},"created":"2013-05-03T17:19:19.337+0000","updated":"2013-05-03T17:19:19.337+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13648597","id":"13648597","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=giroux7103","name":"giroux7103","key":"giroux7103","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Giroux","active":true,"timeZone":"America/New_York"},"body":"Correction to my original post:  org.apache.hadoop.fs.FileUtil calls the standard java.io.File setReadable, setWritable, setExecutable methods which always fail on Windows.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=giroux7103","name":"giroux7103","key":"giroux7103","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Giroux","active":true,"timeZone":"America/New_York"},"created":"2013-05-03T17:27:30.946+0000","updated":"2013-05-03T17:27:30.946+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13649539","id":"13649539","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=toddfast","name":"toddfast","key":"toddfast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Todd Fast","active":true,"timeZone":"America/Los_Angeles"},"body":"Matt, the patch I created last year should fix the issue without needing to modify the Hadoop jars:\n\nhttps://github.com/congainc/patch-hadoop_7682-1.0.x-win","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=toddfast","name":"toddfast","key":"toddfast","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Todd Fast","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-05-06T06:26:06.193+0000","updated":"2013-05-06T06:26:06.193+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13676684","id":"13676684","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=j_caplan","name":"j_caplan","key":"j_caplan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joshua Caplan","active":true,"timeZone":"America/Los_Angeles"},"body":"While we're creating a WinLocalFileSystem object, I found another helpful override:\n\n\tpublic boolean rename( Path src, Path dst ) throws IOException {\n\t\tif ( exists( dst ) )\n\t\t\tdelete( dst, true );\n\t\treturn super.rename( src, dst );\n\t}\n\nThis solves the problem with DistributedCache on Windows for local testing wherein a cache file expected at\n\n /tmp/hadoop-user/mapred/local/archive/random1_random2_random3/path/lastPath/file\n\nends up at\n\n /tmp/hadoop-user/mapred/local/archive/random1_random2_random3/path/lastPath/lastPath-work--random4/file\n\nand is invisible to the tasks that need it.  Can't even express how much pain that alleviates.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=j_caplan","name":"j_caplan","key":"j_caplan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joshua Caplan","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-06T04:19:39.974+0000","updated":"2013-06-06T04:19:39.974+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/13739234","id":"13739234","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=magic.xie","name":"magic.xie","key":"magic.xie","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Magic Xie","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi all, this bug seems fixed.\nPlease see http://svn.apache.org/viewvc/hadoop/common/tags/release-2.1.0-beta-rc1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java?revision=1065901&view=markup\nand HADOOP-7126. \nFix file permission setting for RawLocalFileSystem on Windows. Contributed by Po Cheung.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=magic.xie","name":"magic.xie","key":"magic.xie","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Magic Xie","active":true,"timeZone":"Asia/Shanghai"},"created":"2013-08-14T05:11:26.606+0000","updated":"2013-08-14T05:11:26.606+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12524600/comment/14357478","id":"14357478","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"stale","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2015-03-11T19:57:50.007+0000","updated":"2015-03-11T19:57:50.007+0000"}],"maxResults":29,"total":29,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-7682/votes","votes":16,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i07j3r:"}}