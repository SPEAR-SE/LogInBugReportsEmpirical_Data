{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12385894","self":"https://issues.apache.org/jira/rest/api/2/issue/12385894","key":"HADOOP-2560","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2008-01-09T17:25:29.802+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jul 17 19:31:43 UTC 2014","customfield_12310420":"92421","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_205729051945_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-07-17T19:31:43.757+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-2560/watchers","watchCount":26,"isWatching":false},"created":"2008-01-09T16:34:11.879+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12321308","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12321308","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"inwardIssue":{"id":"12343443","key":"HADOOP-249","self":"https://issues.apache.org/jira/rest/api/2/issue/12343443","fields":{"summary":"Improving Map -> Reduce performance and Task JVM reuse","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12391716","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12391716","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"inwardIssue":{"id":"12407630","key":"HADOOP-4565","self":"https://issues.apache.org/jira/rest/api/2/issue/12407630","fields":{"summary":"MultiFileInputSplit can use data locality information to create splits","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12318801","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12318801","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12379954","key":"MAPREDUCE-93","self":"https://issues.apache.org/jira/rest/api/2/issue/12379954","fields":{"summary":"Job Tracker should prefer input-splits from overloaded racks","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12322182","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12322182","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12394484","key":"HADOOP-3293","self":"https://issues.apache.org/jira/rest/api/2/issue/12394484","fields":{"summary":"When an input split spans cross block boundary, the split location should be the host having most of bytes on it. ","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12321306","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12321306","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12343443","key":"HADOOP-249","self":"https://issues.apache.org/jira/rest/api/2/issue/12343443","fields":{"summary":"Improving Map -> Reduce performance and Task JVM reuse","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-07-17T19:31:43.798+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Currently, an input split contains a consecutive chunk of input file, which by default, corresponding to a DFS block.\nThis may lead to a large number of mapper tasks if the input data is large. This leads to the following problems:\n\n1. Shuffling cost: since the framework has to move M * R map output segments to the nodes running reducers, \nlarger M means larger shuffling cost.\n\n2. High JVM initialization overhead\n\n3. Disk fragmentation: larger number of map output files means lower read throughput for accessing them.\n\nIdeally, you want to keep the number of mappers to no more than 16 times the number of  nodes in the cluster.\nTo achive that, we can increase the input split size. However, if a split span over more than one dfs block,\nyou lose the data locality scheduling benefits.\n\nOne way to address this problem is to combine multiple input blocks with the same rack into one split.\nIf in average we combine B blocks into one split, then we will reduce the number of mappers by a factor of B.\nSince all the blocks for one mapper share a rack, thus we can benefit from rack-aware scheduling.\n\nThoughts?\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12393082","id":"12393082","filename":"multipleSplitsPerMapper.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2008-10-30T17:32:58.909+0000","size":12753,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12393082/multipleSplitsPerMapper.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"105653","customfield_12312823":null,"summary":"Processing multiple input splits per mapper task","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12557340","id":"12557340","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> combine multiple input blocks with the same rack into one split [ ... ]\n\nThat makes good sense to me.  The new Split class could look a lot like MultiFileSplit, but would additionally support a 'getStart(int)' method.  So perhaps MultiFileSplit could be extended for this purpose.  FileInputFormat could be modified to emit these when the number of splits would otherwise exceed some threshold.  But then all subclasses of FileInputFormat would need to be modified to be able to accept these.  That wouldn't be too hard.  FileInputFormat could implement getRecordReader(InputSplit) to break out the sub-splits, then call a new method, getRecordReader(FileSplit).  All existing subclasses could then just change the signature of their getRecordReader implementations in order to support the new feature.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-01-09T17:25:29.802+0000","updated":"2008-01-09T17:25:29.802+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12557396","id":"12557396","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"body":"A) this is important because it could lead to big throughput gains and seek reductions.\n\nB) It is not going to work to combine splits statically because block replicas are not co-resident.  This would lead to a huge performance hit due to loss of locality.  I think we will need to invest in more complexity to get the desired performance improvement here.\n\nMy gut is that we should do this dynamically in the task trackers.  This would let us do it when we are seeing good io throughput.  The map driver could always just request a new split after each input finishes.  The TT would keep a small number of candidate splits locally and decide after each map completes a split if it is going to hand it another one.  None of the public interfaces would need to change.\n\nWe would need to change the JT quite a bit to manage maps publishing split collections, but it seems fairly straightforward.  We could realize a huge performance gain on simple scanning jobs that process input quickly.  We could also see good shuffle improvements.\n\nThis would interact with speculative execution in undesirable ways...  Something to watch-out for.  There are a whole class of collation optimizations here.  The fact that we are sorting early may make a lot of them harder...  ugh.\n\nA related idea runping and I discussed is that if you have multiple spills in a map (combined or unchanged map), there is no point collating the spills if the reduce partitions are relatively large (say 1MB).  We could just make each spill an output to the reduces.  Even if they are small it would be more efficient to collate in larger units than within a single map, but that starts really broadening the design space...\n\nCould static splits combinations work at all?  Yes I think they might if we produced only a small number and executed them early, but this would reduce the possible gain we could get.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-01-09T19:42:28.307+0000","updated":"2008-01-09T19:42:28.307+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12557412","id":"12557412","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> It is not going to work to combine splits statically because block replicas are not co-resident.\n\nIf the number of blocks in the job input is hugely greater than the number of nodes, then it should be easy to find nodes that have a large number of blocks locally, and group the blocks thusly into tasks.  If a task fails, then the re-execution might not be local, but most tasks don't fail, and we can arrange things so that the first node a task is assigned to has all its blocks.  Or am i missing something?\n\nConsider the following algorithm:\n- build <node, block*> and <block, node*> maps for the job input files\n- N is the desired blocks/task\n- for (node : nodes) pop N blocks off each nodes list and add it to the list of tasks\n- as each block is popped, also remove it from all other node's lists, using the other map to accelerate this\n- repeat until nodes have fewer than N blocks, then emit tasks with fewer than N blocks as the tail of the job\n\nWouldn't that work?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-01-09T20:21:44.661+0000","updated":"2008-01-09T20:21:44.661+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12557421","id":"12557421","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=devaraj","name":"devaraj","key":"devaraj","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Devaraj Das","active":true,"timeZone":"Pacific/Pitcairn"},"body":"Good one!\nWe avoid a lot of diskIO/seeks in the final merge of spills on the map side. This will be benefecial for cases where the partitions can fit in the ramfs on the reduces. We get merge for almost free then. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=devaraj","name":"devaraj","key":"devaraj","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Devaraj Das","active":true,"timeZone":"Pacific/Pitcairn"},"created":"2008-01-09T20:35:49.287+0000","updated":"2008-01-09T20:35:49.287+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12557477","id":"12557477","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"body":"Nodes operate at different rates.  Failures happen.  In the face of several jobs running, some nodes may not even become available in a timely manner.  I think a static approach will not allow both the performance gains desired and preservation of reasonable throughput.\n\nThe current system takes full advantage of mapping jobs to nodes dynamically.  A static combination of splits will break all of this.  One could perhaps do something like what you suggest dynamically in the JT when a TT requests a new job.  This might be a good compromise implementation.  This would also let you observe some global statistics on speed of maps & size of outputs which would let you optimize cluster sizes.  Of course doing this all dynamically on the TTs might use fewer JT resources.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-01-09T23:27:50.183+0000","updated":"2008-01-09T23:27:50.183+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12557481","id":"12557481","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> The current system takes full advantage of mapping jobs to nodes dynamically.\n\nCurrently we compute and cache the mapping once per job, and then base all subsequent decisions on that cache.  We get ~99% job locality with that 'static' information.  Things should be about about the same if we group things, unless I'm missing something.\n\n> One could perhaps do something like what you suggest dynamically in the JT when a TT requests a new job.\n\nThat's a possible enhancement.  I'm not sure it's required for good localization, and it would add significant load to the namenode.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-01-09T23:42:41.071+0000","updated":"2008-01-09T23:42:41.071+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12557812","id":"12557812","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"body":"We queue each map for each candidate node (and now presumably rack) and pull them from consideration once they are scheduled on any node.\n\nThis gets much more complicated with map sets, since you will need to tag which maps in one set have been executed somewhere else and then replace them...  Much simpler to make the late binding decision to bundle them.\n\nI get a feeling this issue will be revisit more than once...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-01-10T21:44:57.911+0000","updated":"2008-01-10T21:44:57.911+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12557821","id":"12557821","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> Much simpler to make the late binding decision to bundle them.\n\nThe algorithm I outlined above could be done incrementally, rather than all up-front:\n- N is the desired splits/task\n- build <node, split*> map for the job inputs\n- when a node asks for a task, pop up to N splits off its list to form a task\n- if a node has no more splits, pop splits from other nodes\n- as each split is popped, remove it from other map entries\n\nThis is essentially the existing algorithm, except that we allocate more than one split per task.  In fact, the existing algorithm handles lots of other subtle cases like speculative execution, task failure, etc.  So the best way to implement this is probably to use the existing algorithm multiple times per task, etc.\n\nEarlier I'd spoke of implementing this up front, when constructing splits.  But if it's done this way, then we needn't actually change public APIs or InputFormats.  Tasks could simply internally be changed to execute a list of splits rather than a single split.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-01-10T22:22:21.994+0000","updated":"2008-01-10T22:22:21.994+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12558042","id":"12558042","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"I like that approach, Doug. We should also have a entry for splits that are local to no nodes in the map/reduce cluster and prefer to steal from them rather than other nodes. This would solve HADOOP-2014...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-01-11T17:59:15.905+0000","updated":"2008-01-11T17:59:15.905+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12622681","id":"12622681","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"body":"\nI like the algorithm Doug outlined above.\nA few thoughts for refinement:\n\nProcessing N input splits per mapper task may result in map output spills, depending on the value of N, the split sizes, the value of io.sort.mb, and the nature of the map function.\nThus, N should be  configured by the user on a per job basis. The default should be 1.\nN should be chosen in such a way that the mapper tasks processing N splits will not result in spills most time. \n\nThe actual number of splits a particular mapper task will take should vary, depending on the number of splits to be processed.\nWhen the number of splits to be processed is low, the number of splits to be processed by the next mapper task should be reduced, so that other tasks may \nprocess the splits in parallel.\n\nAll the splits processed by the same mapper task should share the same rackid so that rack locality should be maintained.\n\n \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"created":"2008-08-14T20:48:14.070+0000","updated":"2008-08-14T20:48:14.070+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12622685","id":"12622685","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"body":"\nIf we implement this Jira, I think the issue of reusing JVM outlined in HADOOP-249 becomes less urgent.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"created":"2008-08-14T20:52:05.244+0000","updated":"2008-08-14T20:52:05.244+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12622882","id":"12622882","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"I'd propose:\n  * when a mapper finishes, if the job still has >1 wave of maps left, the JT assigns a new map to join on the\nprevious one. You don't want to make the tail longer.\n  * Limit grouping to 5 or so. You want to bound the amount of grouping so that the costs of failure aren't too high.\n  * any task that has previously been killed or failed should not be grouped\n  * no task that is currently being grouped should be speculated on\n  * we can consider having the map task ask for a follow on map if it still has buffer capacity left. this is probably not aggressive enough because even with a merge step multiple mappers are faster if you include the shuffle time.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-08-15T14:59:50.092+0000","updated":"2008-08-15T14:59:50.092+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12623239","id":"12623239","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"body":"A few observations:\n\n1) You really, really don't want the user configuring the number of tasks to combine.  The system is hard enough for users to understand without asking them to do this!  Also, if the default is 1, this will effectively not be used.\n\n2) If you don't allow speculation on combined tasks, then you can really wedge the job, I don't think this restriction is a good idea.\n\n3) You don't want the node to stall going to the JT for a new task when a task is complete.  This will defeat a lot of the value of this optimization.  Perhaps the TT can request one more task than it has slots, in the hope that it can be assigned when the next task finishes?\n\n4) This is all about collation after map tasks.  I think we need to start thinking of that distinctly from maps themselves.  How can we make this work if we spill on average once per 2.5 maps?  Or 4 times for every 3 maps?  Right now the 4:3 case requires 8 spills, 4 collations and 4 fetches per reducer.  What if we could turn it into 3 spills, one collation and 1 fetch per reducer?  That would be a big saving. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-08-17T23:28:16.345+0000","updated":"2008-08-17T23:28:16.345+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12643381","id":"12643381","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"The resources needed in the Map phase typically depends on the amount of data it processes. Instead of specifying a grouping value of N, can we say that grouping will occur based on the size of the input data? The JT can then group as many mappers as needed to make the total input data be close to the specified value.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2008-10-29T00:08:18.731+0000","updated":"2008-10-29T00:08:18.731+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12643661","id":"12643661","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"body":"It would be great if we could just do that lazily...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-29T21:26:36.774+0000","updated":"2008-10-29T21:26:36.774+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12644041","id":"12644041","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"body":"\nSome more thought on this issue.\nI think it may be more effective and even simpler to think about combining splits based on their rack locality.\nIn this sense, this jira is related to H-3293\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-30T17:03:53.607+0000","updated":"2008-10-30T17:03:53.607+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12644048","id":"12644048","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"Here is a sample patch that combines N splits to be executed by the same mapper instance.  The logic is like this: The Normal logic is used to find the first split to be assigned to a mapper. Then if there exists other splits that are rack-local to the TT, then N-1 of those are selected and assigned to the same mapper. N is configurable per job.\n\nThis code still needs more testing, I am posting it early just as a reference.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2008-10-30T17:32:58.940+0000","updated":"2008-10-30T17:32:58.940+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12644050","id":"12644050","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"The problem that I am trying to solve is a job that processes a large number of input files. Each of these files are small. In the existing implementation, each mapper gets to process one split and each split is very small in size (limited by the size of the input file). I will investigate the implementation of MultipleInput.java to see if I can achieve  mappers to processes larger splits.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2008-10-30T17:40:14.457+0000","updated":"2008-10-30T17:40:14.457+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12644062","id":"12644062","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matei%40eecs.berkeley.edu","name":"matei@eecs.berkeley.edu","key":"matei@eecs.berkeley.edu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matei Zaharia","active":true,"timeZone":"Etc/UTC"},"body":"Instead of using a limit N on the number of splits, I'd have a limit on the number of MB of input data, call it L. Then a default value can be set for the cluster as a whole and it will do something reasonable in each job. If some jobs have really lightweight mappers they can also choose to increase it.\n\nI would first look to see if there are enough data-local input chunks to make L MB of total data. If not, put all of the data-local ones go on to rack-local ones. This is strictly better than picking just random rack-local splits.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matei%40eecs.berkeley.edu","name":"matei@eecs.berkeley.edu","key":"matei@eecs.berkeley.edu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matei Zaharia","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-30T18:15:54.977+0000","updated":"2008-10-30T18:15:54.977+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12644189","id":"12644189","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"body":"be careful of the starvation issues we've seen before!  You may dish out too much data to your first N nodes and distribute your input asymmetrically.  What about just keeping the JVM up and passing in more data as each \"task\" completes?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eric14","name":"eric14","key":"eric14","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"eric baldeschwieler","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-31T01:16:36.262+0000","updated":"2008-10-31T01:16:36.262+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/12644443","id":"12644443","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"Hi Eric, You are right, this could have unwelcome performance characteristics in certain cases. We will have to measure this first. \nI am actually more inclined to solve my problem using HADOOP-4565.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2008-10-31T22:12:05.217+0000","updated":"2008-10-31T22:12:05.217+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12385894/comment/14065422","id":"14065422","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"This appears to predate MFIF/CFIF, as introduced by HADOOP-4565 which appears to fix the issue.  I'm going to close this out as resolved as a result.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2014-07-17T19:31:43.792+0000","updated":"2014-07-17T19:31:43.792+0000"}],"maxResults":22,"total":22,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-2560/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0ifsn:"}}