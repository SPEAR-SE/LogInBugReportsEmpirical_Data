{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12690252","self":"https://issues.apache.org/jira/rest/api/2/issue/12690252","key":"HADOOP-10251","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326263","id":"12326263","description":"2.5.0 release","name":"2.5.0","archived":false,"released":true,"releaseDate":"2014-08-11"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2014-01-22T15:06:56.527+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Jun 12 08:04:01 UTC 2015","customfield_12310420":"369206","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_7879625095_*|*_1_*:*_1_*:*_8006181_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-04-23T19:05:54.315+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-10251/watchers","watchCount":11,"isWatching":false},"created":"2014-01-22T12:05:23.089+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"5.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12325048","id":"12325048","description":"2.2.0 release","name":"2.2.0","archived":false,"released":true,"releaseDate":"2013-10-15"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-06-12T08:04:01.164+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12316608","id":"12316608","name":"ha","description":"High Availability"}],"timeoriginalestimate":null,"description":"Following corner scenario happened in one of our cluster.\n\n1. NN1 was Active and NN2 was Standby\n2. NN2 machine's network was slow \n3. NN1 got shutdown.\n4. NN2 ZKFC got the notification and trying to check for old active for fencing. (This took little more time, again due to slow network)\n5. In between, NN1 got restarted by our automatic monitoring, and ZKFC made it Active.\n6. Now NN2 ZKFC got Old Active as NN1 and it did graceful fencing of NN1 to STANBY.\n7. Before writing ActiveBreadCrumb to ZK, NN2 ZKFC got session timeout and got shutdown before making NN2 Active.\n\n\n*Now cluster having both NameNodes as STANDBY.*\nNN1 ZKFC still thinks that its nameNode is in Active state. \nNN2 ZKFC waiting for election.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12640785","id":"12640785","filename":"HADOOP-10251.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-04-18T07:44:51.916+0000","size":8991,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12640785/HADOOP-10251.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12624583","id":"12624583","filename":"HADOOP-10251.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-23T06:12:50.149+0000","size":9091,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12624583/HADOOP-10251.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12624579","id":"12624579","filename":"HADOOP-10251.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-23T05:25:10.568+0000","size":8985,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12624579/HADOOP-10251.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12624575","id":"12624575","filename":"HADOOP-10251.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-23T05:12:56.326+0000","size":7176,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12624575/HADOOP-10251.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12624335","id":"12624335","filename":"HADOOP-10251.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-22T12:16:05.259+0000","size":6741,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12624335/HADOOP-10251.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"369511","customfield_12312823":null,"summary":"Both NameNodes could be in STANDBY State if SNN network is unstable","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13878576","id":"13878576","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"ZKFC health check, checks the state of the NameNode, but it doesnot validate it with expected state.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-22T12:07:15.701+0000","updated":"2014-01-22T12:07:15.701+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13878586","id":"13878586","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Attaching a patch for the above case. Please review","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-22T12:16:05.264+0000","updated":"2014-01-22T12:16:05.264+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13878739","id":"13878739","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12624335/HADOOP-10251.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:\n\n                  org.apache.hadoop.ha.TestZKFailoverController\n\n                                      The following test timeouts occurred in hadoop-common-project/hadoop-common:\n\norg.apache.hadoop.ha.TestZKFailoverControllerStress\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3457//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3457//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2014-01-22T15:06:56.527+0000","updated":"2014-01-22T15:06:56.527+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13879443","id":"13879443","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Updated the patch to fix test failures.\nThese are mainly time issues.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-23T05:12:56.330+0000","updated":"2014-01-23T05:12:56.330+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13879454","id":"13879454","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Updated the patch again.\nAlso enabled tests in Windows.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-23T05:25:10.572+0000","updated":"2014-01-23T05:25:10.572+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13879477","id":"13879477","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Attaching the updated patch, fixed some synchronization issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-23T06:12:50.153+0000","updated":"2014-01-23T06:12:50.153+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13890860","id":"13890860","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi, Can someone please review the patch.. Thanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-02-04T17:16:26.337+0000","updated":"2014-02-04T17:16:26.337+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13890924","id":"13890924","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12624583/HADOOP-10251.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 2 warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3528//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3528//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2014-02-04T17:55:06.527+0000","updated":"2014-02-04T17:55:06.527+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13913016","id":"13913016","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi.. \nCan please someone review this patch? \nThanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-02-26T15:27:16.001+0000","updated":"2014-02-26T15:27:16.001+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13969340","id":"13969340","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi.. \nCan please someone review this patch? \nThanks in advance","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-04-15T08:40:05.564+0000","updated":"2014-04-15T08:40:05.564+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13973090","id":"13973090","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"I think your idea will work and patch almost looks good to me.\n\nOne question:\n{code}\n /**\n+   * Callback interface for service state change events.\n+   * \n+   * This interface is called whenever there is a change in the service state.\n+   */\n{code}\nSeems like this interface will be called on every health monitor status check. But doc says its a service state changed event. It is exposing impl details as you do that state comparisions in implementation and do necessary actions on state change. So, at this interface level, we are not sure whether service state changed from last state or not right. Can you update this something like Service state notification?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-04-17T15:53:11.331+0000","updated":"2014-04-17T15:53:11.331+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13973859","id":"13973859","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Updated the java doc for the interface. \nRemoved the implementation level details in interface.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-04-18T07:44:51.931+0000","updated":"2014-04-18T07:44:51.931+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13973893","id":"13973893","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12640785/HADOOP-10251.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3809//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3809//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2014-04-18T08:38:24.097+0000","updated":"2014-04-18T08:38:24.097+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13977851","id":"13977851","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 on the latest patch. I will commit it shortly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-04-23T05:26:17.376+0000","updated":"2014-04-23T05:26:17.376+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13978716","id":"13978716","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"I have just committed this to trunk and branch-2 . Thanks a lot, Vinay for the patch!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-04-23T19:05:54.349+0000","updated":"2014-04-23T19:05:54.349+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13978796","id":"13978796","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Hadoop-trunk-Commit #5554 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5554/])\nHADOOP-10251. Both NameNodes could be in STANDBY State if SNN network is unstable. Contributed by Vinayakumar B. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589494)\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverController.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2014-04-23T19:55:58.014+0000","updated":"2014-04-23T19:55:58.014+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13979592","id":"13979592","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"ABORTED: Integrated in Hadoop-Yarn-trunk #550 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/550/])\nHADOOP-10251. Both NameNodes could be in STANDBY State if SNN network is unstable. Contributed by Vinayakumar B. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589494)\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverController.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2014-04-24T11:29:42.511+0000","updated":"2014-04-24T11:29:42.511+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13979710","id":"13979710","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Mapreduce-trunk #1767 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1767/])\nHADOOP-10251. Both NameNodes could be in STANDBY State if SNN network is unstable. Contributed by Vinayakumar B. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589494)\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverController.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2014-04-24T13:31:59.574+0000","updated":"2014-04-24T13:31:59.574+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/13981004","id":"13981004","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Hdfs-trunk #1742 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1742/])\nHADOOP-10251. Both NameNodes could be in STANDBY State if SNN network is unstable. Contributed by Vinayakumar B. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589494)\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java\n* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverController.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2014-04-25T13:40:13.597+0000","updated":"2014-04-25T13:40:13.597+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14578184","id":"14578184","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"hi，nn1 and nn2  are alternately transformed into active state, as long as running hdfs haadmin -failover nn1 nn2.\nAfter removing the code // healthMonitor.addServiceStateCallback(new ServiceStateCallBacks());   failover command  recovery normal\n\n\nthank you .","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-09T01:59:58.829+0000","updated":"2015-06-09T01:59:58.829+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14578728","id":"14578728","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. After removing the code // healthMonitor.addServiceStateCallback(new ServiceStateCallBacks()); failover command recovery normal\ni did not understand the problem, Can you elaborate more please?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-09T11:07:35.286+0000","updated":"2015-06-09T11:07:35.286+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14579884","id":"14579884","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"Attaching some logs：\n\n2015-06-10 02:57:53,727 DEBUG org.apache.hadoop.ipc.Server: Successfully authorized userInfo {\n  effectiveUser: \"hdfs\"\n}\nprotocol: \"org.apache.hadoop.ha.ZKFCProtocol\"\n\n2015-06-10 02:57:53,728 DEBUG org.apache.hadoop.ipc.Server:  got #0\n2015-06-10 02:57:53,728 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8019: org.apache.hadoop.ha.ZKFCProtocol.gracefulFailover from 10.43.156.196:49132 Call#0 Retry#0 for RpcKind RPC_PROTOCOL_BUFFER\n2015-06-10 02:57:53,730 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs (auth:SIMPLE) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n2015-06-10 02:57:53,755 DEBUG org.apache.hadoop.security.Groups: Returning fetched groups for 'hdfs'\n2015-06-10 02:57:53,755 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Allowed RPC access from hdfs (auth:SIMPLE) at 10.43.156.196\n2015-06-10 02:57:53,756 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:hdfs (auth:SIMPLE) from:org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:603)\n2015-06-10 02:57:53,760 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:null serverPath:null finished:false header:: 4,4  replyHeader:: 4,38654797504,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,F  response:: #aa636c75737465723139351236e6e321a67a646831393620ffffffa84628ffffffd33e,s{38654797500,38654797500,1433876199859,1433876199859,0,0,0,93891338261633392,31,0,38654797500} \n2015-06-10 02:57:53,768 DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to hdfs://zdh196:9000\n2015-06-10 02:57:53,770 INFO org.apache.hadoop.ha.ZKFailoverController: Asking NameNode at zdh196/10.43.156.196:9000 to cede its active state for 10000ms\n2015-06-10 02:57:53,772 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:57:53,779 DEBUG org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.\n2015-06-10 02:57:53,779 DEBUG org.apache.hadoop.ipc.Client: Connecting to zdh196/10.43.156.196:8019\n2015-06-10 02:57:53,781 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:8019 from hdfs: starting, having connections 2\n2015-06-10 02:57:53,781 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:8019 from hdfs sending #147\n2015-06-10 02:57:53,969 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x34dcf74b50a05d7\n2015-06-10 02:57:53,970 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x34dcf74b50a05d7\n2015-06-10 02:57:53,974 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: NodeDeleted with state:SyncConnected for path:/hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:53,979 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:8019 from hdfs got value #147\n2015-06-10 02:57:53,979 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 5,1  replyHeader:: 5,38654797507,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock \n2015-06-10 02:57:53,979 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: cedeActive took 200ms\n2015-06-10 02:57:53,982 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:53,982 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-10 02:57:53,985 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:null serverPath:null finished:false header:: 6,4  replyHeader:: 6,38654797507,-101  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,F  response::  \n2015-06-10 02:57:53,996 INFO org.apache.hadoop.ha.ActiveStandbyElector: No old node to fence\n2015-06-10 02:57:53,996 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n2015-06-10 02:57:54,002 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:null serverPath:null finished:false header:: 7,1  replyHeader:: 7,38654797508,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},0  response:: '/hadoop-ha/cluster195/ActiveBreadCrumb \n2015-06-10 02:57:54,006 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming active for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n\n2015-06-10 02:57:54,008 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...\n2015-06-10 02:57:54,010 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:57:54,012 DEBUG org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.\n2015-06-10 02:57:54,012 DEBUG org.apache.hadoop.ipc.Client: Connecting to zdh195/10.43.156.195:9000\n2015-06-10 02:57:54,014 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs: starting, having connections 3\n2015-06-10 02:57:54,014 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #148\n2015-06-10 02:57:54,084 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #149\n\n2015-06-10 02:57:56,071 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #148\n2015-06-10 02:57:56,071 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #149\n2015-06-10 02:57:56,071 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToActive took 2059ms\n2015-06-10 02:57:56,071 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 1988ms\n2015-06-10 02:57:56,072 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #150\n2015-06-10 02:57:56,073 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #150\n2015-06-10 02:57:56,073 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 2ms\n2015-06-10 02:57:56,073 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state\n\n2015-06-10 02:57:56,079 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 8,3  replyHeader:: 8,38654797508,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,T  response:: s{38654797507,38654797507,1433876273975,1433876273975,0,0,0,238074455480993239,31,0,38654797507} \n2015-06-10 02:57:56,081 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: StatNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n\n2015-06-10 02:57:56,092 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:8019 from hdfs got value #151\n2015-06-10 02:57:56,092 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: cedeActive took 18ms\n2015-06-10 02:57:56,092 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully became active. Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state\n2015-06-10 02:57:56,093 DEBUG org.apache.hadoop.ipc.Server: Served: gracefulFailover queueTime= 20 procesingTime= 2345\n2015-06-10 02:57:56,094 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8019: responding to org.apache.hadoop.ha.ZKFCProtocol.gracefulFailover from 10.43.156.196:49132 Call#0 Retry#0\n2015-06-10 02:57:56,095 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8019: responding to org.apache.hadoop.ha.ZKFCProtocol.gracefulFailover from 10.43.156.196:49132 Call#0 Retry#0 Wrote 32 bytes.\n2015-06-10 02:57:56,120 DEBUG org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8019: disconnecting client 10.43.156.196:49132. Number of active connections: 0\n2015-06-10 02:57:57,077 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #152\n2015-06-10 02:57:57,079 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #152\n2015-06-10 02:57:57,079 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 3ms\n2015-06-10 02:57:57,080 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #153\n2015-06-10 02:57:57,081 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #153\n2015-06-10 02:57:57,082 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 3ms\n2015-06-10 02:57:57,082 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.\n2015-06-10 02:57:57,082 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election\n2015-06-10 02:57:57,085 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Terminating ZK connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:57,085 DEBUG org.apache.zookeeper.ZooKeeper: Closing session: 0x34dcf74b50a05d7\n2015-06-10 02:57:57,085 DEBUG org.apache.zookeeper.ClientCnxn: Closing client for session: 0x34dcf74b50a05d7\n2015-06-10 02:57:57,088 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x34dcf74b50a05d7\n2015-06-10 02:57:57,089 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x34dcf74b50a05d7\n2015-06-10 02:57:57,090 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x34dcf74b50a05d7, packet:: clientPath:null serverPath:null finished:false header:: 9,-11  replyHeader:: 9,38654797511,0  request:: null response:: null\n2015-06-10 02:57:57,090 DEBUG org.apache.zookeeper.ClientCnxn: Disconnecting client for session: 0x34dcf74b50a05d7\n2015-06-10 02:57:57,090 INFO org.apache.zookeeper.ZooKeeper: Session: 0x34dcf74b50a05d7 closed\n2015-06-10 02:57:57,090 DEBUG org.apache.zookeeper.ClientCnxn: An exception was thrown while closing send thread for session 0x34dcf74b50a05d7 : Unable to read additional data from server sessionid 0x34dcf74b50a05d7, likely server has closed socket\n2015-06-10 02:57:57,091 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x34dcf74b50a05d7\n2015-06-10 02:57:57,091 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down\n2015-06-10 02:57:58,091 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #154\n2015-06-10 02:57:58,093 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #154\n2015-06-10 02:57:58,093 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 3ms\n2015-06-10 02:57:58,094 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #155\n2015-06-10 02:57:58,095 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #155\n2015-06-10 02:57:58,096 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 3ms\n2015-06-10 02:57:58,096 DEBUG org.apache.hadoop.ha.ZKFailoverController: rechecking for electability from bad state\n2015-06-10 02:57:58,099 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Attempting active election for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,102 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Establishing zookeeper connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,102 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@31b40327\n2015-06-10 02:57:58,105 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-10 02:57:58,106 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session\n2015-06-10 02:57:58,106 DEBUG org.apache.zookeeper.ClientCnxn: Session establishment request sent on zdh195/10.43.156.195:2181\n2015-06-10 02:57:58,109 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1967, negotiated timeout = 10000\n2015-06-10 02:57:58,112 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Created new connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,116 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: None with state:SyncConnected for path:null connectionState: TERMINATED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,116 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-10 02:57:58,117 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 1,1  replyHeader:: 1,38654797515,-110  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response::  \n2015-06-10 02:57:58,120 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: -110 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,123 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming standby for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,123 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at zdh195/10.43.156.195:9000 should become standby\n2015-06-10 02:57:58,125 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:57:58,125 DEBUG org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.\n2015-06-10 02:57:58,126 DEBUG org.apache.hadoop.ipc.Client: Connecting to zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,128 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #156\n2015-06-10 02:57:58,128 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs: starting, having connections 4\n2015-06-10 02:57:58,132 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #156\n2015-06-10 02:57:58,132 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToStandby took 7ms\n2015-06-10 02:57:58,132 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state\n2015-06-10 02:57:58,135 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Monitoring active leader for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,138 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 2,3  replyHeader:: 2,38654797515,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,T  response:: s{38654797512,38654797512,1433876277092,1433876277092,0,0,0,165948932252965222,31,0,38654797512} \n2015-06-10 02:57:58,141 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: StatNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,850 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x24d91acb5cb1967\n2015-06-10 02:57:58,850 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x24d91acb5cb1967\n2015-06-10 02:57:58,854 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: NodeDeleted with state:SyncConnected for path:/hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,858 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 3,1  replyHeader:: 3,38654797517,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock \n2015-06-10 02:57:58,861 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:57:58,861 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-10 02:57:58,863 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:null serverPath:null finished:false header:: 4,4  replyHeader:: 4,38654797517,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,F  response:: #aa636c75737465723139351236e6e321a67a646831393620ffffffa84628ffffffd33e,s{38654797508,38654797513,1433876273998,1433876277194,1,0,0,0,31,0,38654797508} \n2015-06-10 02:57:58,864 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e\n2015-06-10 02:57:58,867 DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to hdfs://zdh196:9000\n2015-06-10 02:57:58,868 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh196/10.43.156.196:9000\n2015-06-10 02:57:58,870 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:57:58,871 DEBUG org.apache.hadoop.ipc.Client: The ping interval is 60000 ms.\n2015-06-10 02:57:58,871 DEBUG org.apache.hadoop.ipc.Client: Connecting to zdh196/10.43.156.196:9000\n2015-06-10 02:57:58,872 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs sending #157\n2015-06-10 02:57:58,873 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs: starting, having connections 5\n2015-06-10 02:57:58,949 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs got value #157\n2015-06-10 02:57:58,949 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToStandby took 79ms\n2015-06-10 02:57:58,949 DEBUG org.apache.hadoop.ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:57:58,949 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to standby state without fencing\n2015-06-10 02:57:58,949 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n2015-06-10 02:57:58,954 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:null serverPath:null finished:false header:: 5,5  replyHeader:: 5,38654797518,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,1  response:: s{38654797508,38654797518,1433876273998,1433876278951,2,0,0,0,31,0,38654797508} \n2015-06-10 02:57:58,955 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming active for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n\n\n\n2015-06-10 02:57:58,955 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...\n2015-06-10 02:57:58,956 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:57:58,957 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #158\n\n2015-06-10 02:57:59,113 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #159\n2015-06-10 02:58:00,545 DEBUG org.apache.hadoop.ipc.Server: IPC Server idle connection scanner for port 8019: task running\n2015-06-10 02:58:00,573 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #159\n2015-06-10 02:58:00,573 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #158\n2015-06-10 02:58:00,574 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 1461ms\n2015-06-10 02:58:00,574 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToActive took 1617ms\n2015-06-10 02:58:00,574 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state\n2015-06-10 02:58:00,574 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #160\n\n2015-06-10 02:58:00,576 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #160\n2015-06-10 02:58:00,576 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 2ms\n2015-06-10 02:58:00,577 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Monitoring active leader for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:00,580 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 6,3  replyHeader:: 6,38654797520,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,T  response:: s{38654797517,38654797517,1433876278854,1433876278854,0,0,0,165948932252965223,31,0,38654797517} \n\n2015-06-10 02:58:01,578 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #161\n2015-06-10 02:58:01,580 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #161\n2015-06-10 02:58:01,580 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 2ms\n2015-06-10 02:58:01,581 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #162\n2015-06-10 02:58:01,582 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #162\n2015-06-10 02:58:01,582 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 2ms\n2015-06-10 02:58:01,583 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.\n2015-06-10 02:58:01,583 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election\n2015-06-10 02:58:01,585 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Terminating ZK connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:01,586 DEBUG org.apache.zookeeper.ZooKeeper: Closing session: 0x24d91acb5cb1967\n2015-06-10 02:58:01,586 DEBUG org.apache.zookeeper.ClientCnxn: Closing client for session: 0x24d91acb5cb1967\n2015-06-10 02:58:01,588 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x24d91acb5cb1967\n2015-06-10 02:58:01,588 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x24d91acb5cb1967\n2015-06-10 02:58:01,589 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1967, packet:: clientPath:null serverPath:null finished:false header:: 7,-11  replyHeader:: 7,38654797521,0  request:: null response:: null\n2015-06-10 02:58:01,589 DEBUG org.apache.zookeeper.ClientCnxn: Disconnecting client for session: 0x24d91acb5cb1967\n2015-06-10 02:58:01,589 DEBUG org.apache.zookeeper.ClientCnxn: An exception was thrown while closing send thread for session 0x24d91acb5cb1967 : Unable to read additional data from server sessionid 0x24d91acb5cb1967, likely server has closed socket\n2015-06-10 02:58:01,589 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1967 closed\n2015-06-10 02:58:01,590 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1967\n2015-06-10 02:58:01,590 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down\n2015-06-10 02:58:02,590 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #163\n2015-06-10 02:58:02,592 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #163\n2015-06-10 02:58:02,593 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getServiceStatus took 3ms\n2015-06-10 02:58:02,593 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #164\n2015-06-10 02:58:02,595 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #164\n2015-06-10 02:58:02,595 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: monitorHealth took 2ms\n2015-06-10 02:58:02,595 DEBUG org.apache.hadoop.ha.ZKFailoverController: rechecking for electability from bad state\n2015-06-10 02:58:02,598 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Attempting active election for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:02,600 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Establishing zookeeper connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:02,600 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@6d4f0202\n2015-06-10 02:58:02,603 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-10 02:58:02,603 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session\n2015-06-10 02:58:02,603 DEBUG org.apache.zookeeper.ClientCnxn: Session establishment request sent on zdh195/10.43.156.195:2181\n2015-06-10 02:58:02,606 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1969, negotiated timeout = 10000\n2015-06-10 02:58:02,610 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Created new connection for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:02,613 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: None with state:SyncConnected for path:null connectionState: TERMINATED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:02,614 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-10 02:58:02,614 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 1,1  replyHeader:: 1,38654797525,-110  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response::  \n2015-06-10 02:58:02,618 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: -110 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:02,621 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming standby for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:02,621 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at zdh195/10.43.156.195:9000 should become standby\n2015-06-10 02:58:02,622 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:58:02,622 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #165\n2015-06-10 02:58:02,624 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs got value #165\n2015-06-10 02:58:02,624 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToStandby took 2ms\n2015-06-10 02:58:02,624 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state\n2015-06-10 02:58:02,625 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Monitoring active leader for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:02,627 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 2,3  replyHeader:: 2,38654797525,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,T  response:: s{38654797522,38654797522,1433876281592,1433876281592,0,0,0,165948932252965224,31,0,38654797522} \n2015-06-10 02:58:02,628 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: StatNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:03,185 DEBUG org.apache.zookeeper.ClientCnxn: Got notification sessionid:0x24d91acb5cb1969\n2015-06-10 02:58:03,185 DEBUG org.apache.zookeeper.ClientCnxn: Got WatchedEvent state:SyncConnected type:NodeDeleted path:/hadoop-ha/cluster195/ActiveStandbyElectorLock for sessionid 0x24d91acb5cb1969\n2015-06-10 02:58:03,188 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Watcher event type: NodeDeleted with state:SyncConnected for path:/hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:03,191 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock serverPath:/hadoop-ha/cluster195/ActiveStandbyElectorLock finished:false header:: 3,1  replyHeader:: 3,38654797527,0  request:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,v{s{31,s{'world,'anyone}}},1  response:: '/hadoop-ha/cluster195/ActiveStandbyElectorLock \n2015-06-10 02:58:03,194 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: CreateNode result: 0 for path: /hadoop-ha/cluster195/ActiveStandbyElectorLock connectionState: CONNECTED  for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n2015-06-10 02:58:03,194 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-10 02:58:03,196 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:null serverPath:null finished:false header:: 4,4  replyHeader:: 4,38654797527,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,F  response:: #aa636c75737465723139351236e6e321a67a646831393620ffffffa84628ffffffd33e,s{38654797508,38654797523,1433876273998,1433876281689,3,0,0,0,31,0,38654797508} \n2015-06-10 02:58:03,198 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e\n2015-06-10 02:58:03,201 DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to hdfs://zdh196:9000\n2015-06-10 02:58:03,202 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh196/10.43.156.196:9000\n2015-06-10 02:58:03,204 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:58:03,205 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs sending #166\n2015-06-10 02:58:03,269 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh196/10.43.156.196:9000 from hdfs got value #166\n2015-06-10 02:58:03,270 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: transitionToStandby took 65ms\n2015-06-10 02:58:03,270 DEBUG org.apache.hadoop.ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:58:03,270 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to standby state without fencing\n2015-06-10 02:58:03,270 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n2015-06-10 02:58:03,274 DEBUG org.apache.zookeeper.ClientCnxn: Reading reply sessionid:0x24d91acb5cb1969, packet:: clientPath:null serverPath:null finished:false header:: 5,5  replyHeader:: 5,38654797528,0  request:: '/hadoop-ha/cluster195/ActiveBreadCrumb,#aa636c75737465723139351236e6e311a67a646831393520ffffffa84628ffffffd33e,3  response:: s{38654797508,38654797528,1433876273998,1433876283271,4,0,0,0,31,0,38654797508} \n2015-06-10 02:58:03,276 DEBUG org.apache.hadoop.ha.ActiveStandbyElector: Becoming active for elector id=1893177739 appData=0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e cb=Elector callbacks for NameNode at zdh195/10.43.156.195:9000\n\n2015-06-10 02:58:03,277 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...\n2015-06-10 02:58:03,278 DEBUG org.apache.hadoop.ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7007cf85\n2015-06-10 02:58:03,278 DEBUG org.apache.hadoop.ipc.Client: IPC Client (256152889) connection to zdh195/10.43.156.195:9000 from hdfs sending #167\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-10T02:36:37.038+0000","updated":"2015-06-10T02:36:37.038+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581560","id":"14581560","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"You can try the command hdfs haadmin -failover nn1 nn2 , and then see if the active node nn1 is normal.\nnn1 will always change state .active -> standby -> active -> standby .......\nsorry for my poor english ,hope you can understand.thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T07:02:07.007+0000","updated":"2015-06-11T07:02:07.007+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581590","id":"14581590","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Which version of Hadoop You are using?\nBecause I can see below logs (excluded DEBUG),\n{noformat}2015-06-10 02:57:56,073 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state\n2015-06-10 02:57:56,092 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully became active. Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state\n2015-06-10 02:57:57,082 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.{noformat}\n\nImmediately after {{becomeActive()}}, ERROR log is showing state expected is {{standby}}. {{serviceState}} is changed to {{active}} in {{becomeActive()}} immediately after above log.\n\nIMO, this is possible only if {{volatile}} is not present while declaring {{serviceState}}\n{code}private volatile HAServiceState serviceState = HAServiceState.INITIALIZING;{code}\n\ndo you have this  in your code?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-11T07:38:47.856+0000","updated":"2015-06-11T07:38:47.856+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581665","id":"14581665","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"version 2.5.0. \nvolatile is present ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T08:21:09.889+0000","updated":"2015-06-11T08:21:09.889+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581700","id":"14581700","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"There is no difference in the code for the normal auto failover and failover using haadmin.\n\nNormal auto failover is working for you?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-11T09:04:44.527+0000","updated":"2015-06-11T09:04:44.527+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581725","id":"14581725","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"auto failover is normal . when i kill the process of active namenode nn1 ,nn2 can transition to active .\nI find if the key ha.health-monitor.check-interval.ms setting is two different values. hdfs haadmin is no problem.\nnn1 setting\n        <property>\n        <name>ha.health-monitor.check-interval.ms</name>\n        <value>2000</value>\n        </property>\nnn2 setting\n        <property>\n        <name>ha.health-monitor.check-interval.ms</name>\n        <value>1000</value>\n        </property>\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T09:36:57.293+0000","updated":"2015-06-11T09:36:57.293+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581735","id":"14581735","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. when i kill the process of active namenode nn1 ,nn2 can transition to active .\nWhen you kill active nn2, whether nn1 is transitioning to active?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-11T09:44:21.042+0000","updated":"2015-06-11T09:44:21.042+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581763","id":"14581763","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:19.885+0000","updated":"2015-06-11T10:10:19.885+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581764","id":"14581764","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:26.341+0000","updated":"2015-06-11T10:10:26.341+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581765","id":"14581765","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:27.434+0000","updated":"2015-06-11T10:10:27.434+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581766","id":"14581766","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:30.774+0000","updated":"2015-06-11T10:10:30.774+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581767","id":"14581767","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:30.776+0000","updated":"2015-06-11T10:10:30.776+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581768","id":"14581768","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:30.959+0000","updated":"2015-06-11T10:10:30.959+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581769","id":"14581769","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:30.978+0000","updated":"2015-06-11T10:10:30.978+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581770","id":"14581770","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:30.981+0000","updated":"2015-06-11T10:10:30.981+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581771","id":"14581771","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:31.207+0000","updated":"2015-06-11T10:10:31.207+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581772","id":"14581772","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:32.222+0000","updated":"2015-06-11T10:10:32.222+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581773","id":"14581773","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:32.227+0000","updated":"2015-06-11T10:10:32.227+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581774","id":"14581774","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:32.296+0000","updated":"2015-06-11T10:10:32.296+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581775","id":"14581775","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:32.897+0000","updated":"2015-06-11T10:10:32.897+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581776","id":"14581776","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:40.772+0000","updated":"2015-06-11T10:10:40.772+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581777","id":"14581777","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:41.284+0000","updated":"2015-06-11T10:10:41.284+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581778","id":"14581778","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"yes .hadoop-deamon.sh start namenode nn1 then kill nn2 ，nn1 can  transition to active","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T10:10:41.324+0000","updated":"2015-06-11T10:10:41.324+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14581795","id":"14581795","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"I am not getting whats the problem in your cluster. That too only with haadmin failover where as auto failover works fine.\n\nCan you share the autofailover logs for zkfc.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-11T10:40:54.705+0000","updated":"2015-06-11T10:40:54.705+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14582751","id":"14582751","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Xinglong.Li","name":"Xinglong.Li","key":"xinglong.li","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Xinglong.Li","active":true,"timeZone":"Etc/UTC"},"body":"顶！","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Xinglong.Li","name":"Xinglong.Li","key":"xinglong.li","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Xinglong.Li","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-12T00:55:33.513+0000","updated":"2015-06-12T00:55:33.513+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14582800","id":"14582800","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"Use Cases：\n1.NN1 was Active and NN2 was Standby ,kill NN1 . NN2 transition to active.\n2.hadoop-daemon.sh start namenode NN2. NOW.NN1 was Standby and NN2 was Active .\n3.kill NN2 ,NN1 transition to active.\nAttaching hdfs-nn1-zkfc-host195.log file and hdfs-nn2-zkfc-host196.log file\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-12T01:36:39.464+0000","updated":"2015-06-12T01:36:39.464+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14582802","id":"14582802","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"############################hdfs-nn1-zkfc-host195.log################################\n\n2015-06-12 02:25:38,799 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Failed on local exception: java.io.EOFException; Host Details : local host is: \"zdh195/10.43.156.195\"; destination host is: \"zdh195\":9000; \n2015-06-12 02:25:38,799 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING\n2015-06-12 02:25:38,800 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_NOT_RESPONDING\n2015-06-12 02:25:38,800 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at zdh195/10.43.156.195:9000 and marking that fencing is necessary\n2015-06-12 02:25:38,800 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election\n2015-06-12 02:25:38,803 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1c33 closed\n2015-06-12 02:25:38,803 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1c33\n2015-06-12 02:25:38,803 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down\n2015-06-12 02:25:40,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 02:25:40,807 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Call From zdh195/10.43.156.195 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n...\n...\n...\n2015-06-12 02:27:22,976 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Call From zdh195/10.43.156.195 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n2015-06-12 02:27:24,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 02:27:24,979 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Call From zdh195/10.43.156.195 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n2015-06-12 02:27:26,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 02:27:27,810 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY\n2015-06-12 02:27:27,810 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_HEALTHY\n2015-06-12 02:27:27,811 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@651a6959\n2015-06-12 02:27:27,812 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-12 02:27:27,812 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session\n2015-06-12 02:27:27,814 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1c51, negotiated timeout = 10000\n2015-06-12 02:27:27,815 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-12 02:27:27,816 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at zdh195/10.43.156.195:9000 should become standby\n2015-06-12 02:27:27,824 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state\n2015-06-12 02:27:48,662 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-12 02:27:48,663 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e\n2015-06-12 02:27:48,665 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh196/10.43.156.196:9000\n2015-06-12 02:27:49,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 02:27:49,670 WARN org.apache.hadoop.ha.FailoverController: Unable to gracefully make NameNode at zdh196/10.43.156.196:9000 standby (unable to connect)\njava.net.ConnectException: Call From zdh195/10.43.156.195 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.GeneratedConstructorAccessor24.newInstance(Unknown Source)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1415)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1364)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy12.transitionToStandby(Unknown Source)\n\tat org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToStandby(HAServiceProtocolClientSideTranslatorPB.java:112)\n\tat org.apache.hadoop.ha.FailoverController.tryGracefulFence(FailoverController.java:172)\n\tat org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:516)\n\tat org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:507)\n\tat org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n\tat org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:894)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:901)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:800)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:415)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:605)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499)\nCaused by: java.net.ConnectException: 拒绝连接\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1382)\n\t... 14 more\n2015-06-12 02:27:49,671 INFO org.apache.hadoop.ha.NodeFencer: ====== Beginning Service Fencing Process... ======\n2015-06-12 02:27:49,671 INFO org.apache.hadoop.ha.NodeFencer: Trying method 1/2: org.apache.hadoop.ha.SshFenceByTcpPort(null)\n2015-06-12 02:27:49,674 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connecting to zdh196...\n2015-06-12 02:27:49,674 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connecting to zdh196 port 22\n2015-06-12 02:27:49,676 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connection established\n2015-06-12 02:27:49,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Remote version string: SSH-2.0-OpenSSH_5.3\n2015-06-12 02:27:49,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Local version string: SSH-2.0-JSCH-0.1.42\n2015-06-12 02:27:49,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: CheckCiphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-ctr,arcfour,arcfour128,arcfour256\n2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-ctr is not available.\n2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-ctr is not available.\n2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-cbc is not available.\n2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-cbc is not available.\n2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: arcfour256 is not available.\n2015-06-12 02:27:49,707 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT sent\n2015-06-12 02:27:49,708 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT received\n2015-06-12 02:27:49,708 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: server->client aes128-ctr hmac-md5 none\n2015-06-12 02:27:49,708 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: client->server aes128-ctr hmac-md5 none\n2015-06-12 02:27:49,712 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXDH_INIT sent\n2015-06-12 02:27:49,712 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: expecting SSH_MSG_KEXDH_REPLY\n2015-06-12 02:27:49,719 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: ssh_rsa_verify: signature true\n2015-06-12 02:27:49,720 WARN org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Permanently added 'zdh196' (RSA) to the list of known hosts.\n2015-06-12 02:27:49,720 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS sent\n2015-06-12 02:27:49,720 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS received\n2015-06-12 02:27:49,721 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_REQUEST sent\n2015-06-12 02:27:49,721 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_ACCEPT received\n2015-06-12 02:27:49,723 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password\n2015-06-12 02:27:49,723 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: gssapi-with-mic\n2015-06-12 02:27:49,727 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: publickey,keyboard-interactive,password\n2015-06-12 02:27:49,727 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: publickey\n2015-06-12 02:27:49,771 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentication succeeded (publickey).\n2015-06-12 02:27:49,772 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connected to zdh196\n2015-06-12 02:27:49,772 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Looking for process running on port 9000\n2015-06-12 02:27:51,758 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Indeterminate response from trying to kill service. Verifying whether it is running using nc...\n2015-06-12 02:27:53,454 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Verified that the service is down.\n2015-06-12 02:27:53,455 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Disconnecting from zdh196 port 22\n2015-06-12 02:27:53,455 INFO org.apache.hadoop.ha.NodeFencer: ====== Fencing successful by method org.apache.hadoop.ha.SshFenceByTcpPort(null) ======\n2015-06-12 02:27:53,455 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n2015-06-12 02:27:53,455 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Caught an exception, leaving main loop due to Socket closed\n\n2015-06-12 02:27:53,458 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...\n\n2015-06-12 02:27:54,871 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state\n\n2015-06-12 02:27:56,878 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.\n2015-06-12 02:27:56,878 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election\n2015-06-12 02:27:56,882 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1c51 closed\n2015-06-12 02:27:56,882 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1c51\n2015-06-12 02:27:56,882 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down\n2015-06-12 02:27:57,885 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@70f0acec\n2015-06-12 02:27:57,886 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-12 02:27:57,887 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session\n2015-06-12 02:27:57,889 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1c52, negotiated timeout = 10000\n2015-06-12 02:27:57,891 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-12 02:27:57,895 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-12 02:27:57,896 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e\n2015-06-12 02:27:57,896 INFO org.apache.hadoop.ha.ActiveStandbyElector: But old node has our own data, so don't need to fence it.\n2015-06-12 02:27:57,896 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n\n2015-06-12 02:27:57,899 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...\n2015-06-12 02:27:57,901 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-12T01:37:50.271+0000","updated":"2015-06-12T01:37:50.271+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14582806","id":"14582806","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"######################################hdfs-nn2-zkfc-host196.log file#######################################\n\n2015-06-12 02:25:54,146 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-12 02:25:54,147 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e\n2015-06-12 02:25:54,149 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh195/10.43.156.195:9000\n2015-06-12 02:25:55,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 02:25:55,152 WARN org.apache.hadoop.ha.FailoverController: Unable to gracefully make NameNode at zdh195/10.43.156.195:9000 standby (unable to connect)\njava.net.ConnectException: Call From zdh196/10.43.156.196 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.GeneratedConstructorAccessor24.newInstance(Unknown Source)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1415)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1364)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy12.transitionToStandby(Unknown Source)\n\tat org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToStandby(HAServiceProtocolClientSideTranslatorPB.java:112)\n\tat org.apache.hadoop.ha.FailoverController.tryGracefulFence(FailoverController.java:172)\n\tat org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:516)\n\tat org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:507)\n\tat org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n\tat org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:894)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:901)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:800)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:415)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:605)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499)\nCaused by: java.net.ConnectException: 拒绝连接\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1382)\n\t... 14 more\n2015-06-12 02:25:55,153 INFO org.apache.hadoop.ha.NodeFencer: ====== Beginning Service Fencing Process... ======\n2015-06-12 02:25:55,154 INFO org.apache.hadoop.ha.NodeFencer: Trying method 1/2: org.apache.hadoop.ha.SshFenceByTcpPort(null)\n2015-06-12 02:25:55,157 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connecting to zdh195...\n2015-06-12 02:25:55,157 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connecting to zdh195 port 22\n2015-06-12 02:25:55,159 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connection established\n2015-06-12 02:25:55,188 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Remote version string: SSH-2.0-OpenSSH_5.3\n2015-06-12 02:25:55,188 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Local version string: SSH-2.0-JSCH-0.1.42\n2015-06-12 02:25:55,189 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: CheckCiphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-ctr,arcfour,arcfour128,arcfour256\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-ctr is not available.\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-ctr is not available.\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-cbc is not available.\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-cbc is not available.\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: arcfour256 is not available.\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT sent\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT received\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: server->client aes128-ctr hmac-md5 none\n2015-06-12 02:25:55,194 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: client->server aes128-ctr hmac-md5 none\n2015-06-12 02:25:55,197 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXDH_INIT sent\n2015-06-12 02:25:55,197 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: expecting SSH_MSG_KEXDH_REPLY\n2015-06-12 02:25:55,205 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: ssh_rsa_verify: signature true\n2015-06-12 02:25:55,205 WARN org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Permanently added 'zdh195' (RSA) to the list of known hosts.\n2015-06-12 02:25:55,205 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS sent\n2015-06-12 02:25:55,205 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS received\n2015-06-12 02:25:55,206 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_REQUEST sent\n2015-06-12 02:25:55,207 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_ACCEPT received\n2015-06-12 02:25:55,209 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password\n2015-06-12 02:25:55,209 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: gssapi-with-mic\n2015-06-12 02:25:55,215 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: publickey,keyboard-interactive,password\n2015-06-12 02:25:55,215 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: publickey\n2015-06-12 02:25:55,263 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentication succeeded (publickey).\n2015-06-12 02:25:55,263 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connected to zdh195\n2015-06-12 02:25:55,263 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Looking for process running on port 9000\n2015-06-12 02:25:57,418 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Indeterminate response from trying to kill service. Verifying whether it is running using nc...\n2015-06-12 02:25:59,266 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Verified that the service is down.\n2015-06-12 02:25:59,266 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Disconnecting from zdh195 port 22\n2015-06-12 02:25:59,267 INFO org.apache.hadoop.ha.NodeFencer: ====== Fencing successful by method org.apache.hadoop.ha.SshFenceByTcpPort(null) ======\n2015-06-12 02:25:59,267 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n2015-06-12 02:25:59,267 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Caught an exception, leaving main loop due to Socket closed\n\n2015-06-12 02:25:59,271 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh196/10.43.156.196:9000 active...\n\n2015-06-12 02:26:00,596 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to active state\n\n2015-06-12 02:26:02,608 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh196/10.43.156.196:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.\n2015-06-12 02:26:02,608 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election\n2015-06-12 02:26:02,614 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1c4f closed\n2015-06-12 02:26:02,615 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1c4f\n2015-06-12 02:26:02,615 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down\n2015-06-12 02:26:03,619 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@33f89c4e\n2015-06-12 02:26:03,622 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-12 02:26:03,623 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session\n2015-06-12 02:26:03,626 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1c50, negotiated timeout = 10000\n2015-06-12 02:26:03,627 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-12 02:26:03,629 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-12 02:26:03,631 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e\n2015-06-12 02:26:03,631 INFO org.apache.hadoop.ha.ActiveStandbyElector: But old node has our own data, so don't need to fence it.\n2015-06-12 02:26:03,631 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n\n2015-06-12 02:26:03,635 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh196/10.43.156.196:9000 active...\n2015-06-12 02:26:03,638 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to active state\n\n2015-06-12 02:28:03,996 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh196/10.43.156.196:9000: Failed on local exception: java.io.EOFException; Host Details : local host is: \"zdh196/10.43.156.196\"; destination host is: \"zdh196\":9000; \n2015-06-12 02:28:03,997 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING\n2015-06-12 02:28:03,997 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh196/10.43.156.196:9000 entered state: SERVICE_NOT_RESPONDING\n2015-06-12 02:28:03,997 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at zdh196/10.43.156.196:9000 and marking that fencing is necessary\n2015-06-12 02:28:03,997 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election\n2015-06-12 02:28:04,000 INFO org.apache.zookeeper.ZooKeeper: Session: 0x24d91acb5cb1c50 closed\n2015-06-12 02:28:04,001 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x24d91acb5cb1c50\n2015-06-12 02:28:04,001 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down\n2015-06-12 02:28:06,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 02:28:06,004 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh196/10.43.156.196:9000: Call From zdh196/10.43.156.196 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n...\n...\n...\n2015-06-12 02:30:26,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 02:30:26,198 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh196/10.43.156.196:9000: Call From zdh196/10.43.156.196 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n2015-06-12 02:30:28,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 02:30:28,200 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh196/10.43.156.196:9000: Call From zdh196/10.43.156.196 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-12T01:39:08.594+0000","updated":"2015-06-12T01:39:08.594+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14582914","id":"14582914","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. 2015-06-12 02:26:02,608 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh196/10.43.156.196:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.\nbq. 2015-06-12 02:27:56,878 ERROR org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 has changed the serviceState to active. Expected was standby. Quitting election marking fencing necessary.\n\nThe use case mentioned is normal auto failover. Not the manual failover using haadmin commands.\nAnd I am seeing both NN1 and NN2 are not staying in Active mode if the transition happens from standby->active. This is strange.\n\nCan you check this ?\n\n1. Stop both ZKFCs. and NNs.\n2. Start only one ZKFC and NN. It should successfully convert to active and check whether it is staying in ACTIVE for long time.\n3. Attach the logs for ZKFC after restart, (logs from the restarted point, all lines)\n4. Also attach the ZKFailoverController.java source code you are using.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-12T04:30:09.975+0000","updated":"2015-06-12T04:30:09.975+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14583024","id":"14583024","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"Answer 2.when start only one ZKFC and NN ，the NN  can be staying in ACTIVE for long time.\n\n############################hdfs-nn1-zkfc-host195.log################################\n\n2015-06-12 07:18:11,471 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at zdh195/10.43.156.195:9000\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 05/15/2015 03:44 GMT\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=zdh195\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.7.0_55\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk/jre\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/home/hdfs/hdfs/etc/hadoop:/home/hdfs/hdfs/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hdfs/hdfs/share/hadoop/common/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-net-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cdh-commons-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/gson-2.2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zdh-restclient-1.04.01.jar:/home/hdfs/hdfs/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cooma-0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/common/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/security-checkclients-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-aws-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-databind-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-lzo-0.4.20.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-annotations-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/aws-java-sdk-1.7.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=/home/hdfs/hdfs/lib/native\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>\n2015-06-12 07:18:11,608 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux\n2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=amd64\n2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=2.6.32-431.el6.x86_64\n2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=hdfs\n2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/home/hdfs\n2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/home/hdfs/hdfs\n2015-06-12 07:18:11,609 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@3c1e1fd3\n2015-06-12 07:18:11,624 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh195/10.43.156.195:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-12 07:18:11,626 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh195/10.43.156.195:2181, initiating session\n2015-06-12 07:18:11,632 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh195/10.43.156.195:2181, sessionid = 0x24d91acb5cb1c57, negotiated timeout = 10000\n2015-06-12 07:18:11,636 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-12 07:18:11,662 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-06-12 07:18:11,684 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8019\n2015-06-12 07:18:11,712 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2015-06-12 07:18:11,713 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8019: starting\n2015-06-12 07:18:11,831 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY\n2015-06-12 07:18:11,831 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_HEALTHY\n2015-06-12 07:18:11,854 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at zdh195/10.43.156.195:9000 should become standby\n2015-06-12 07:18:11,863 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state\n2015-06-12 07:20:02,104 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at zdh195/10.43.156.195:9000\n2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 05/15/2015 03:44 GMT\n2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=zdh195\n2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.7.0_55\n2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation\n2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk/jre\n2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/home/hdfs/hdfs/etc/hadoop:/home/hdfs/hdfs/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hdfs/hdfs/share/hadoop/common/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-net-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cdh-commons-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/gson-2.2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zdh-restclient-1.04.01.jar:/home/hdfs/hdfs/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cooma-0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/common/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/security-checkclients-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-aws-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-databind-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-lzo-0.4.20.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-annotations-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/aws-java-sdk-1.7.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar\n2015-06-12 07:20:02,250 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=/home/hdfs/hdfs/lib/native\n2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\n2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>\n2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux\n2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=amd64\n2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=2.6.32-431.el6.x86_64\n2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=hdfs\n2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/home/hdfs\n2015-06-12 07:20:02,251 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/home/hdfs/hdfs\n2015-06-12 07:20:02,252 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@3ddc25a9\n2015-06-12 07:20:02,269 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh196/10.43.156.196:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-12 07:20:02,273 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh196/10.43.156.196:2181, initiating session\n2015-06-12 07:20:02,285 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh196/10.43.156.196:2181, sessionid = 0x14d91acb8921ca0, negotiated timeout = 10000\n2015-06-12 07:20:02,290 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-12 07:20:02,319 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-06-12 07:20:02,342 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8019\n2015-06-12 07:20:02,373 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2015-06-12 07:20:02,373 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8019: starting\n2015-06-12 07:20:03,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 07:20:03,446 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at zdh195/10.43.156.195:9000: Call From zdh195/10.43.156.195 to zdh195:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n2015-06-12 07:20:03,447 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING\n2015-06-12 07:20:03,447 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_NOT_RESPONDING\n2015-06-12 07:20:03,448 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at zdh195/10.43.156.195:9000 and marking that fencing is necessary\n2015-06-12 07:20:03,448 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election\n2015-06-12 07:20:03,451 INFO org.apache.zookeeper.ZooKeeper: Session: 0x14d91acb8921ca0 closed\n2015-06-12 07:20:03,451 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down\n2015-06-12 07:20:05,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh195/10.43.156.195:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 07:20:05,834 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY\n2015-06-12 07:20:05,834 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh195/10.43.156.195:9000 entered state: SERVICE_HEALTHY\n2015-06-12 07:20:05,844 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@14b867db\n2015-06-12 07:20:05,845 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh197/10.43.156.197:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-12 07:20:05,846 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh197/10.43.156.197:2181, initiating session\n2015-06-12 07:20:05,849 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh197/10.43.156.197:2181, sessionid = 0x34dcf74b50a0896, negotiated timeout = 10000\n2015-06-12 07:20:05,854 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-12 07:20:05,860 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-12 07:20:05,865 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e321a067a646831393620a84628d33e\n2015-06-12 07:20:05,868 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh196/10.43.156.196:9000\n2015-06-12 07:20:06,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: zdh196/10.43.156.196:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)\n2015-06-12 07:20:06,877 WARN org.apache.hadoop.ha.FailoverController: Unable to gracefully make NameNode at zdh196/10.43.156.196:9000 standby (unable to connect)\njava.net.ConnectException: Call From zdh195/10.43.156.195 to zdh196:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1415)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1364)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy12.transitionToStandby(Unknown Source)\n\tat org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToStandby(HAServiceProtocolClientSideTranslatorPB.java:112)\n\tat org.apache.hadoop.ha.FailoverController.tryGracefulFence(FailoverController.java:172)\n\tat org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:516)\n\tat org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:507)\n\tat org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n\tat org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:894)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:901)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:800)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:415)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:605)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:499)\nCaused by: java.net.ConnectException: 拒绝连接\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1382)\n\t... 14 more\n2015-06-12 07:20:06,881 INFO org.apache.hadoop.ha.NodeFencer: ====== Beginning Service Fencing Process... ======\n2015-06-12 07:20:06,881 INFO org.apache.hadoop.ha.NodeFencer: Trying method 1/2: org.apache.hadoop.ha.SshFenceByTcpPort(null)\n2015-06-12 07:20:06,899 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connecting to zdh196...\n2015-06-12 07:20:06,900 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connecting to zdh196 port 22\n2015-06-12 07:20:06,907 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connection established\n2015-06-12 07:20:06,931 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Remote version string: SSH-2.0-OpenSSH_5.3\n2015-06-12 07:20:06,931 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Local version string: SSH-2.0-JSCH-0.1.42\n2015-06-12 07:20:06,931 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: CheckCiphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-ctr,arcfour,arcfour128,arcfour256\n2015-06-12 07:20:07,105 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-ctr is not available.\n2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-ctr is not available.\n2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-cbc is not available.\n2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-cbc is not available.\n2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: arcfour256 is not available.\n2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT sent\n2015-06-12 07:20:07,106 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT received\n2015-06-12 07:20:07,107 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: server->client aes128-ctr hmac-md5 none\n2015-06-12 07:20:07,107 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: client->server aes128-ctr hmac-md5 none\n2015-06-12 07:20:07,115 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXDH_INIT sent\n2015-06-12 07:20:07,115 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: expecting SSH_MSG_KEXDH_REPLY\n2015-06-12 07:20:07,125 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: ssh_rsa_verify: signature true\n2015-06-12 07:20:07,128 WARN org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Permanently added 'zdh196' (RSA) to the list of known hosts.\n2015-06-12 07:20:07,128 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS sent\n2015-06-12 07:20:07,128 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS received\n2015-06-12 07:20:07,132 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_REQUEST sent\n2015-06-12 07:20:07,133 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_ACCEPT received\n2015-06-12 07:20:07,135 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password\n2015-06-12 07:20:07,135 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: gssapi-with-mic\n2015-06-12 07:20:07,142 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: publickey,keyboard-interactive,password\n2015-06-12 07:20:07,142 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: publickey\n2015-06-12 07:20:07,188 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentication succeeded (publickey).\n2015-06-12 07:20:07,189 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connected to zdh196\n2015-06-12 07:20:07,189 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Looking for process running on port 9000\n2015-06-12 07:20:09,289 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Indeterminate response from trying to kill service. Verifying whether it is running using nc...\n2015-06-12 07:20:11,049 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Verified that the service is down.\n2015-06-12 07:20:11,050 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Disconnecting from zdh196 port 22\n2015-06-12 07:20:11,052 INFO org.apache.hadoop.ha.NodeFencer: ====== Fencing successful by method org.apache.hadoop.ha.SshFenceByTcpPort(null) ======\n2015-06-12 07:20:11,052 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n2015-06-12 07:20:11,053 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Caught an exception, leaving main loop due to Socket closed\n2015-06-12 07:20:11,061 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh195/10.43.156.195:9000 active...\n2015-06-12 07:20:12,333 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to active state\n\n############################hdfs-nn2-zkfc-host196.log################################\n\n2015-06-12 07:18:26,905 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at zdh196/10.43.156.196:9000\n2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-cdh5.3.2--1, built on 05/15/2015 03:44 GMT\n2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=zdh196\n2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.7.0_55\n2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation\n2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk/jre\n2015-06-12 07:18:27,042 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/home/hdfs/hdfs/etc/hadoop:/home/hdfs/hdfs/share/hadoop/common/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cooma-0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/cdh-commons-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-net-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/common/lib/gson-2.2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-auth-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hdfs/hdfs/share/hadoop/common/lib/zdh-restclient-1.04.01.jar:/home/hdfs/hdfs/share/hadoop/common/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hdfs/hdfs/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/common/lib/security-checkclients-1.04.03.jar:/home/hdfs/hdfs/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hdfs/hdfs/share/hadoop/common/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/common/hadoop-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs:/home/hdfs/hdfs/share/hadoop/hdfs/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/aws-java-sdk-1.7.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-lzo-0.4.20.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-core-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpclient-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-annotations-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/httpcore-4.2.5.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jackson-databind-2.2.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/hadoop-aws-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-hdfs-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/share/hadoop/hdfs/hadoop-concurrent-copy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/activation-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/zookeeper-3.4.5-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-util-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jetty-6.1.26.cloudera.4.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hdfs/hdfs/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-client-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-api-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/avro-1.7.6-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.0-cdh5.3.2.jar:/home/hdfs/hdfs/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.0-cdh5.3.2-tests.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar:/home/hdfs/hdfs/contrib/capacity-scheduler/*.jar\n2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=/home/hdfs/hdfs/lib/native\n2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\n2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>\n2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux\n2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=amd64\n2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=2.6.32-431.el6.x86_64\n2015-06-12 07:18:27,043 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=hdfs\n2015-06-12 07:18:27,044 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/home/hdfs\n2015-06-12 07:18:27,044 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/home/hdfs/hdfs\n2015-06-12 07:18:27,044 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=zdh196:2181,zdh195:2181,zdh197:2181 sessionTimeout=10000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@1a722605\n2015-06-12 07:18:27,060 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server zdh196/10.43.156.196:2181. Will not attempt to authenticate using SASL (unknown error)\n2015-06-12 07:18:27,065 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to zdh196/10.43.156.196:2181, initiating session\n2015-06-12 07:18:27,073 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server zdh196/10.43.156.196:2181, sessionid = 0x14d91acb8921c9f, negotiated timeout = 10000\n2015-06-12 07:18:27,077 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.\n2015-06-12 07:18:27,104 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-06-12 07:18:27,125 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8019\n2015-06-12 07:18:27,155 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2015-06-12 07:18:27,155 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8019: starting\n2015-06-12 07:18:27,277 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_HEALTHY\n2015-06-12 07:18:27,277 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at zdh196/10.43.156.196:9000 entered state: SERVICE_HEALTHY\n2015-06-12 07:18:27,301 INFO org.apache.hadoop.ha.ActiveStandbyElector: Checking for any old active which needs to be fenced...\n2015-06-12 07:18:27,309 INFO org.apache.hadoop.ha.ActiveStandbyElector: Old node exists: 0a0a636c757374657231393512036e6e311a067a646831393520a84628d33e\n2015-06-12 07:18:27,312 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at zdh195/10.43.156.195:9000\n2015-06-12 07:18:27,338 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh195/10.43.156.195:9000 to standby state without fencing\n2015-06-12 07:18:27,338 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/cluster195/ActiveBreadCrumb to indicate that the local node is the most recent active...\n2015-06-12 07:18:27,348 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at zdh196/10.43.156.196:9000 active...\n2015-06-12 07:18:39,494 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at zdh196/10.43.156.196:9000 to active state\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-12T06:23:52.120+0000","updated":"2015-06-12T06:23:52.120+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14583029","id":"14583029","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.hadoop.ha;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.security.PrivilegedAction;\nimport java.security.PrivilegedExceptionAction;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\n\nimport org.apache.commons.logging.Log;\nimport org.apache.commons.logging.LogFactory;\nimport org.apache.hadoop.HadoopIllegalArgumentException;\nimport org.apache.hadoop.classification.InterfaceAudience;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.ha.ActiveStandbyElector.ActiveNotFoundException;\nimport org.apache.hadoop.ha.ActiveStandbyElector.ActiveStandbyElectorCallback;\nimport org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\nimport org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo;\nimport org.apache.hadoop.ha.HAServiceProtocol.RequestSource;\nimport org.apache.hadoop.util.ZKUtil;\nimport org.apache.hadoop.util.ZKUtil.ZKAuthInfo;\nimport org.apache.hadoop.ha.HealthMonitor.State;\nimport org.apache.hadoop.ipc.Server;\nimport org.apache.hadoop.security.AccessControlException;\nimport org.apache.hadoop.security.SecurityUtil;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.apache.hadoop.security.authorize.PolicyProvider;\nimport org.apache.hadoop.util.StringUtils;\nimport org.apache.zookeeper.KeeperException;\nimport org.apache.zookeeper.ZooDefs.Ids;\nimport org.apache.hadoop.util.ToolRunner;\nimport org.apache.zookeeper.data.ACL;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.base.Preconditions;\nimport com.google.common.base.Throwables;\nimport com.google.common.util.concurrent.ThreadFactoryBuilder;\n\n\n@InterfaceAudience.LimitedPrivate(\"HDFS\")\npublic abstract class ZKFailoverController {\n\n  static final Log LOG = LogFactory.getLog(ZKFailoverController.class);\n  \n  public static final String ZK_QUORUM_KEY = \"ha.zookeeper.quorum\";\n  private static final String ZK_SESSION_TIMEOUT_KEY = \"ha.zookeeper.session-timeout.ms\";\n  private static final int ZK_SESSION_TIMEOUT_DEFAULT = 5*1000;\n  private static final String ZK_PARENT_ZNODE_KEY = \"ha.zookeeper.parent-znode\";\n  public static final String ZK_ACL_KEY = \"ha.zookeeper.acl\";\n  private static final String ZK_ACL_DEFAULT = \"world:anyone:rwcda\";\n  public static final String ZK_AUTH_KEY = \"ha.zookeeper.auth\";\n  static final String ZK_PARENT_ZNODE_DEFAULT = \"/hadoop-ha\";\n\n  /**\n   * All of the conf keys used by the ZKFC. This is used in order to allow\n   * them to be overridden on a per-nameservice or per-namenode basis.\n   */\n  protected static final String[] ZKFC_CONF_KEYS = new String[] {\n    ZK_QUORUM_KEY,\n    ZK_SESSION_TIMEOUT_KEY,\n    ZK_PARENT_ZNODE_KEY,\n    ZK_ACL_KEY,\n    ZK_AUTH_KEY\n  };\n  \n  protected static final String USAGE = \n      \"Usage: java zkfc [ -formatZK [-force] [-nonInteractive] ]\";\n\n  /** Unable to format the parent znode in ZK */\n  static final int ERR_CODE_FORMAT_DENIED = 2;\n  /** The parent znode doesn't exist in ZK */\n  static final int ERR_CODE_NO_PARENT_ZNODE = 3;\n  /** Fencing is not properly configured */\n  static final int ERR_CODE_NO_FENCER = 4;\n  /** Automatic failover is not enabled */\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED = 5;\n  /** Cannot connect to ZooKeeper */\n  static final int ERR_CODE_NO_ZK = 6;\n  \n  protected Configuration conf;\n  private String zkQuorum;\n  protected final HAServiceTarget localTarget;\n\n  private HealthMonitor healthMonitor;\n  private ActiveStandbyElector elector;\n  protected ZKFCRpcServer rpcServer;\n\n  private State lastHealthState = State.INITIALIZING;\n\n  private volatile HAServiceState serviceState = HAServiceState.INITIALIZING;\n\n  /** Set if a fatal error occurs */\n  private String fatalError = null;\n\n  /**\n   * A future nanotime before which the ZKFC will not join the election.\n   * This is used during graceful failover.\n   */\n  private long delayJoiningUntilNanotime = 0;\n\n  /** Executor on which {@link #scheduleRecheck(long)} schedules events */\n  private ScheduledExecutorService delayExecutor =\n    Executors.newScheduledThreadPool(1,\n        new ThreadFactoryBuilder().setDaemon(true)\n            .setNameFormat(\"ZKFC Delay timer #%d\")\n            .build());\n\n  private ActiveAttemptRecord lastActiveAttemptRecord;\n  private Object activeAttemptRecordLock = new Object();\n\n\n  protected ZKFailoverController(Configuration conf, HAServiceTarget localTarget) {\n    this.localTarget = localTarget;\n    this.conf = conf;\n  }\n  \n\n  protected abstract byte[] targetToData(HAServiceTarget target);\n  protected abstract HAServiceTarget dataToTarget(byte[] data);\n  protected abstract void loginAsFCUser() throws IOException;\n  protected abstract void checkRpcAdminAccess()\n      throws AccessControlException, IOException;\n  protected abstract InetSocketAddress getRpcAddressToBindTo();\n  protected abstract PolicyProvider getPolicyProvider();\n\n  /**\n   * Return the name of a znode inside the configured parent znode in which\n   * the ZKFC will do all of its work. This is so that multiple federated\n   * nameservices can run on the same ZK quorum without having to manually\n   * configure them to separate subdirectories.\n   */\n  protected abstract String getScopeInsideParentNode();\n\n  public HAServiceTarget getLocalTarget() {\n    return localTarget;\n  }\n  \n  public int run(final String[] args) throws Exception {\n    if (!localTarget.isAutoFailoverEnabled()) {\n      LOG.fatal(\"Automatic failover is not enabled for \" + localTarget + \".\" +\n          \" Please ensure that automatic failover is enabled in the \" +\n          \"configuration before running the ZK failover controller.\");\n      return ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n    }\n    loginAsFCUser();\n    try {\n      return SecurityUtil.doAsLoginUserOrFatal(new PrivilegedAction<Integer>() {\n        @Override\n        public Integer run() {\n          try {\n            return doRun(args);\n          } catch (Exception t) {\n            throw new RuntimeException(t);\n          } finally {\n            if (elector != null) {\n              elector.terminateConnection();\n            }\n          }\n        }\n      });\n    } catch (RuntimeException rte) {\n      throw (Exception)rte.getCause();\n    }\n  }\n  \n\n  private int doRun(String[] args)\n      throws HadoopIllegalArgumentException, IOException, InterruptedException {\n    try {\n      initZK();\n    } catch (KeeperException ke) {\n      LOG.fatal(\"Unable to start failover controller. Unable to connect \"\n          + \"to ZooKeeper quorum at \" + zkQuorum + \". Please check the \"\n          + \"configured value for \" + ZK_QUORUM_KEY + \" and ensure that \"\n          + \"ZooKeeper is running.\");\n      return ERR_CODE_NO_ZK;\n    }\n    if (args.length > 0) {\n      if (\"-formatZK\".equals(args[0])) {\n        boolean force = false;\n        boolean interactive = true;\n        for (int i = 1; i < args.length; i++) {\n          if (\"-force\".equals(args[i])) {\n            force = true;\n          } else if (\"-nonInteractive\".equals(args[i])) {\n            interactive = false;\n          } else {\n            badArg(args[i]);\n          }\n        }\n        return formatZK(force, interactive);\n      } else {\n        badArg(args[0]);\n      }\n    }\n\n    if (!elector.parentZNodeExists()) {\n      LOG.fatal(\"Unable to start failover controller. \"\n          + \"Parent znode does not exist.\\n\"\n          + \"Run with -formatZK flag to initialize ZooKeeper.\");\n      return ERR_CODE_NO_PARENT_ZNODE;\n    }\n\n    try {\n      localTarget.checkFencingConfigured();\n    } catch (BadFencingConfigurationException e) {\n      LOG.fatal(\"Fencing is not configured for \" + localTarget + \".\\n\" +\n          \"You must configure a fencing method before using automatic \" +\n          \"failover.\", e);\n      return ERR_CODE_NO_FENCER;\n    }\n\n    initRPC();\n    initHM();\n    startRPC();\n    try {\n      mainLoop();\n    } finally {\n      rpcServer.stopAndJoin();\n      \n      elector.quitElection(true);\n      healthMonitor.shutdown();\n      healthMonitor.join();\n    }\n    return 0;\n  }\n\n  private void badArg(String arg) {\n    printUsage();\n    throw new HadoopIllegalArgumentException(\n        \"Bad argument: \" + arg);\n  }\n\n  private void printUsage() {\n    System.err.println(USAGE + \"\\n\");\n  }\n\n  private int formatZK(boolean force, boolean interactive)\n      throws IOException, InterruptedException {\n    if (elector.parentZNodeExists()) {\n      if (!force && (!interactive || !confirmFormat())) {\n        return ERR_CODE_FORMAT_DENIED;\n      }\n      \n      try {\n        elector.clearParentZNode();\n      } catch (IOException e) {\n        LOG.error(\"Unable to clear zk parent znode\", e);\n        return 1;\n      }\n    }\n    \n    elector.ensureParentZNode();\n    return 0;\n  }\n\n  private boolean confirmFormat() {\n    String parentZnode = getParentZnode();\n    System.err.println(\n        \"===============================================\\n\" +\n        \"The configured parent znode \" + parentZnode + \" already exists.\\n\" +\n        \"Are you sure you want to clear all failover information from\\n\" +\n        \"ZooKeeper?\\n\" +\n        \"WARNING: Before proceeding, ensure that all HDFS services and\\n\" +\n        \"failover controllers are stopped!\\n\" +\n        \"===============================================\");\n    try {\n      return ToolRunner.confirmPrompt(\"Proceed formatting \" + parentZnode + \"?\");\n    } catch (IOException e) {\n      LOG.debug(\"Failed to confirm\", e);\n      return false;\n    }\n  }\n\n  // ------------------------------------------\n  // Begin actual guts of failover controller\n  // ------------------------------------------\n  \n  private void initHM() {\n    healthMonitor = new HealthMonitor(conf, localTarget);\n    healthMonitor.addCallback(new HealthCallbacks());\n    healthMonitor.addServiceStateCallback(new ServiceStateCallBacks());\n    healthMonitor.start();\n  }\n  \n  protected void initRPC() throws IOException {\n    InetSocketAddress bindAddr = getRpcAddressToBindTo();\n    rpcServer = new ZKFCRpcServer(conf, bindAddr, this, getPolicyProvider());\n  }\n\n  protected void startRPC() throws IOException {\n    rpcServer.start();\n  }\n\n\n  private void initZK() throws HadoopIllegalArgumentException, IOException,\n      KeeperException {\n    zkQuorum = conf.get(ZK_QUORUM_KEY);\n    int zkTimeout = conf.getInt(ZK_SESSION_TIMEOUT_KEY,\n        ZK_SESSION_TIMEOUT_DEFAULT);\n    // Parse ACLs from configuration.\n    String zkAclConf = conf.get(ZK_ACL_KEY, ZK_ACL_DEFAULT);\n    zkAclConf = ZKUtil.resolveConfIndirection(zkAclConf);\n    List<ACL> zkAcls = ZKUtil.parseACLs(zkAclConf);\n    if (zkAcls.isEmpty()) {\n      zkAcls = Ids.CREATOR_ALL_ACL;\n    }\n    \n    // Parse authentication from configuration.\n    String zkAuthConf = conf.get(ZK_AUTH_KEY);\n    zkAuthConf = ZKUtil.resolveConfIndirection(zkAuthConf);\n    List<ZKAuthInfo> zkAuths;\n    if (zkAuthConf != null) {\n      zkAuths = ZKUtil.parseAuth(zkAuthConf);\n    } else {\n      zkAuths = Collections.emptyList();\n    }\n\n    // Sanity check configuration.\n    Preconditions.checkArgument(zkQuorum != null,\n        \"Missing required configuration '%s' for ZooKeeper quorum\",\n        ZK_QUORUM_KEY);\n    Preconditions.checkArgument(zkTimeout > 0,\n        \"Invalid ZK session timeout %s\", zkTimeout);\n    \n\n    elector = new ActiveStandbyElector(zkQuorum,\n        zkTimeout, getParentZnode(), zkAcls, zkAuths,\n        new ElectorCallbacks());\n  }\n  \n  private String getParentZnode() {\n    String znode = conf.get(ZK_PARENT_ZNODE_KEY,\n        ZK_PARENT_ZNODE_DEFAULT);\n    if (!znode.endsWith(\"/\")) {\n      znode += \"/\";\n    }\n    return znode + getScopeInsideParentNode();\n  }\n\n  private synchronized void mainLoop() throws InterruptedException {\n    while (fatalError == null) {\n      wait();\n    }\n    assert fatalError != null; // only get here on fatal\n    throw new RuntimeException(\n        \"ZK Failover Controller failed: \" + fatalError);\n  }\n  \n  private synchronized void fatalError(String err) {\n    LOG.fatal(\"Fatal error occurred:\" + err);\n    fatalError = err;\n    notifyAll();\n  }\n  \n  private synchronized void becomeActive() throws ServiceFailedException {\n    LOG.info(\"Trying to make \" + localTarget + \" active...\");\n    try {\n      HAServiceProtocolHelper.transitionToActive(localTarget.getProxy(\n          conf, FailoverController.getRpcTimeoutToNewActive(conf)),\n          createReqInfo());\n      String msg = \"Successfully transitioned \" + localTarget +\n          \" to active state\";\n      LOG.info(msg);\n      recordActiveAttempt(new ActiveAttemptRecord(true, msg));\n\n    } catch (Throwable t) {\n      String msg = \"Couldn't make \" + localTarget + \" active\";\n      LOG.fatal(msg, t);\n      \n      recordActiveAttempt(new ActiveAttemptRecord(false, msg + \"\\n\" +\n          StringUtils.stringifyException(t)));\n\n      if (t instanceof ServiceFailedException) {\n        throw (ServiceFailedException)t;\n      } else {\n        throw new ServiceFailedException(\"Couldn't transition to active\",\n            t);\n      }\n/*\n* TODO:\n* we need to make sure that if we get fenced and then quickly restarted,\n* none of these calls will retry across the restart boundary\n* perhaps the solution is that, whenever the nn starts, it gets a unique\n* ID, and when we start becoming active, we record it, and then any future\n* calls use the same ID\n*/\n      \n    }\n  }\n\n  /**\n   * Store the results of the last attempt to become active.\n   * This is used so that, during manually initiated failover,\n   * we can report back the results of the attempt to become active\n   * to the initiator of the failover.\n   */\n  private void recordActiveAttempt(\n      ActiveAttemptRecord record) {\n    synchronized (activeAttemptRecordLock) {\n      lastActiveAttemptRecord = record;\n      activeAttemptRecordLock.notifyAll();\n    }\n  }\n\n  /**\n   * Wait until one of the following events:\n   * <ul>\n   * <li>Another thread publishes the results of an attempt to become active\n   * using {@link #recordActiveAttempt(ActiveAttemptRecord)}</li>\n   * <li>The node enters bad health status</li>\n   * <li>The specified timeout elapses</li>\n   * </ul>\n   * \n   * @param timeoutMillis number of millis to wait\n   * @return the published record, or null if the timeout elapses or the\n   * service becomes unhealthy \n   * @throws InterruptedException if the thread is interrupted.\n   */\n  private ActiveAttemptRecord waitForActiveAttempt(int timeoutMillis)\n      throws InterruptedException {\n    long st = System.nanoTime();\n    long waitUntil = st + TimeUnit.NANOSECONDS.convert(\n        timeoutMillis, TimeUnit.MILLISECONDS);\n    \n    do {\n      // periodically check health state, because entering an\n      // unhealthy state could prevent us from ever attempting to\n      // become active. We can detect this and respond to the user\n      // immediately.\n      synchronized (this) {\n        if (lastHealthState != State.SERVICE_HEALTHY) {\n          // early out if service became unhealthy\n          return null;\n        }\n      }\n\n      synchronized (activeAttemptRecordLock) {\n        if ((lastActiveAttemptRecord != null &&\n            lastActiveAttemptRecord.nanoTime >= st)) {\n          return lastActiveAttemptRecord;\n        }\n        // Only wait 1sec so that we periodically recheck the health state\n        // above.\n        activeAttemptRecordLock.wait(1000);\n      }\n    } while (System.nanoTime() < waitUntil);\n    \n    // Timeout elapsed.\n    LOG.warn(timeoutMillis + \"ms timeout elapsed waiting for an attempt \" +\n        \"to become active\");\n    return null;\n  }\n\n  private StateChangeRequestInfo createReqInfo() {\n    return new StateChangeRequestInfo(RequestSource.REQUEST_BY_ZKFC);\n  }\n\n  private synchronized void becomeStandby() {\n    LOG.info(\"ZK Election indicated that \" + localTarget +\n        \" should become standby\");\n    try {\n      int timeout = FailoverController.getGracefulFenceTimeout(conf);\n      localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());\n      LOG.info(\"Successfully transitioned \" + localTarget +\n          \" to standby state\");\n    } catch (Exception e) {\n      LOG.error(\"Couldn't transition \" + localTarget + \" to standby state\",\n          e);\n      // TODO handle this. It's a likely case since we probably got fenced\n      // at the same time.\n    }\n    serviceState = HAServiceState.STANDBY;\n  }\n  \n\n  private synchronized void fenceOldActive(byte[] data) {\n    HAServiceTarget target = dataToTarget(data);\n    \n    try {\n      doFence(target);\n    } catch (Throwable t) {\n      recordActiveAttempt(new ActiveAttemptRecord(false, \"Unable to fence old active: \" + StringUtils.stringifyException(t)));\n      Throwables.propagate(t);\n    }\n  }\n  \n  private void doFence(HAServiceTarget target) {\n    LOG.info(\"Should fence: \" + target);\n    boolean gracefulWorked = new FailoverController(conf,\n        RequestSource.REQUEST_BY_ZKFC).tryGracefulFence(target);\n    if (gracefulWorked) {\n      // It's possible that it's in standby but just about to go into active,\n      // no? Is there some race here?\n      LOG.info(\"Successfully transitioned \" + target + \" to standby \" +\n          \"state without fencing\");\n      return;\n    }\n    \n    try {\n      target.checkFencingConfigured();\n    } catch (BadFencingConfigurationException e) {\n      LOG.error(\"Couldn't fence old active \" + target, e);\n      recordActiveAttempt(new ActiveAttemptRecord(false, \"Unable to fence old active\"));\n      throw new RuntimeException(e);\n    }\n    \n    if (!target.getFencer().fence(target)) {\n      throw new RuntimeException(\"Unable to fence \" + target);\n    }\n  }\n\n\n  /**\n   * Request from graceful failover to cede active role. Causes\n   * this ZKFC to transition its local node to standby, then quit\n   * the election for the specified period of time, after which it\n   * will rejoin iff it is healthy.\n   */\n  void cedeActive(final int millisToCede)\n      throws AccessControlException, ServiceFailedException, IOException {\n    try {\n      UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>() {\n        @Override\n        public Void run() throws Exception {\n          doCedeActive(millisToCede);\n          return null;\n        }\n      });\n    } catch (InterruptedException e) {\n      throw new IOException(e);\n    }\n  }\n  \n  private void doCedeActive(int millisToCede) \n      throws AccessControlException, ServiceFailedException, IOException {\n    int timeout = FailoverController.getGracefulFenceTimeout(conf);\n\n    // Lock elector to maintain lock ordering of elector -> ZKFC\n    synchronized (elector) {\n      synchronized (this) {\n        if (millisToCede <= 0) {\n          delayJoiningUntilNanotime = 0;\n          recheckElectability();\n          return;\n        }\n  \n        LOG.info(\"Requested by \" + UserGroupInformation.getCurrentUser() +\n            \" at \" + Server.getRemoteAddress() + \" to cede active role.\");\n        boolean needFence = false;\n        try {\n          localTarget.getProxy(conf, timeout).transitionToStandby(createReqInfo());\n          LOG.info(\"Successfully ensured local node is in standby mode\");\n        } catch (IOException ioe) {\n          LOG.warn(\"Unable to transition local node to standby: \" +\n              ioe.getLocalizedMessage());\n          LOG.warn(\"Quitting election but indicating that fencing is \" +\n              \"necessary\");\n          needFence = true;\n        }\n        delayJoiningUntilNanotime = System.nanoTime() +\n            TimeUnit.MILLISECONDS.toNanos(millisToCede);\n        elector.quitElection(needFence);\n        serviceState = HAServiceState.INITIALIZING;\n      }\n    }\n    recheckElectability();\n  }\n  \n  /**\n   * Coordinate a graceful failover to this node.\n   * @throws ServiceFailedException if the node fails to become active\n   * @throws IOException some other error occurs\n   */\n  void gracefulFailoverToYou() throws ServiceFailedException, IOException {\n    try {\n      UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>() {\n        @Override\n        public Void run() throws Exception {\n          doGracefulFailover();\n          return null;\n        }\n        \n      });\n    } catch (InterruptedException e) {\n      throw new IOException(e);\n    }\n  }\n\n  /**\n   * Coordinate a graceful failover. This proceeds in several phases:\n   * 1) Pre-flight checks: ensure that the local node is healthy, and\n   * thus a candidate for failover.\n   * 2) Determine the current active node. If it is the local node, no\n   * need to failover - return success.\n   * 3) Ask that node to yield from the election for a number of seconds.\n   * 4) Allow the normal election path to run in other threads. Wait until\n   * we either become unhealthy or we see an election attempt recorded by\n   * the normal code path.\n   * 5) Allow the old active to rejoin the election, so a future\n   * failback is possible.\n   */\n  private void doGracefulFailover()\n      throws ServiceFailedException, IOException, InterruptedException {\n    int timeout = FailoverController.getGracefulFenceTimeout(conf) * 2;\n    \n    // Phase 1: pre-flight checks\n    checkEligibleForFailover();\n    \n    // Phase 2: determine old/current active node. Check that we're not\n    // ourselves active, etc.\n    HAServiceTarget oldActive = getCurrentActive();\n    if (oldActive == null) {\n      // No node is currently active. So, if we aren't already\n      // active ourselves by means of a normal election, then there's\n      // probably something preventing us from becoming active.\n      throw new ServiceFailedException(\n          \"No other node is currently active.\");\n    }\n    \n    if (oldActive.getAddress().equals(localTarget.getAddress())) {\n      LOG.info(\"Local node \" + localTarget + \" is already active. \" +\n          \"No need to failover. Returning success.\");\n      return;\n    }\n    \n    // Phase 3: ask the old active to yield from the election.\n    LOG.info(\"Asking \" + oldActive + \" to cede its active state for \" +\n        timeout + \"ms\");\n    ZKFCProtocol oldZkfc = oldActive.getZKFCProxy(conf, timeout);\n    oldZkfc.cedeActive(timeout);\n\n    // Phase 4: wait for the normal election to make the local node\n    // active.\n    ActiveAttemptRecord attempt = waitForActiveAttempt(timeout + 60000);\n    \n    if (attempt == null) {\n      // We didn't even make an attempt to become active.\n      synchronized(this) {\n        if (lastHealthState != State.SERVICE_HEALTHY) {\n          throw new ServiceFailedException(\"Unable to become active. \" +\n            \"Service became unhealthy while trying to failover.\");          \n        }\n      }\n      \n      throw new ServiceFailedException(\"Unable to become active. \" +\n          \"Local node did not get an opportunity to do so from ZooKeeper, \" +\n          \"or the local node took too long to transition to active.\");\n    }\n\n    // Phase 5. At this point, we made some attempt to become active. So we\n    // can tell the old active to rejoin if it wants. This allows a quick\n    // fail-back if we immediately crash.\n    oldZkfc.cedeActive(-1);\n    \n    if (attempt.succeeded) {\n      LOG.info(\"Successfully became active. \" + attempt.status);\n    } else {\n      // Propagate failure\n      String msg = \"Failed to become active. \" + attempt.status;\n      throw new ServiceFailedException(msg);\n    }\n  }\n\n  /**\n   * Ensure that the local node is in a healthy state, and thus\n   * eligible for graceful failover.\n   * @throws ServiceFailedException if the node is unhealthy\n   */\n  private synchronized void checkEligibleForFailover()\n      throws ServiceFailedException {\n    // Check health\n    if (this.getLastHealthState() != State.SERVICE_HEALTHY) {\n      throw new ServiceFailedException(\n          localTarget + \" is not currently healthy. \" +\n          \"Cannot be failover target\");\n    }\n  }\n\n  /**\n   * @return an {@link HAServiceTarget} for the current active node\n   * in the cluster, or null if no node is active.\n   * @throws IOException if a ZK-related issue occurs\n   * @throws InterruptedException if thread is interrupted \n   */\n  private HAServiceTarget getCurrentActive()\n      throws IOException, InterruptedException {\n    synchronized (elector) {\n      synchronized (this) {\n        byte[] activeData;\n        try {\n          activeData = elector.getActiveData();\n        } catch (ActiveNotFoundException e) {\n          return null;\n        } catch (KeeperException ke) {\n          throw new IOException(\n              \"Unexpected ZooKeeper issue fetching active node info\", ke);\n        }\n        \n        HAServiceTarget oldActive = dataToTarget(activeData);\n        return oldActive;\n      }\n    }\n  }\n\n  /**\n   * Check the current state of the service, and join the election\n   * if it should be in the election.\n   */\n  private void recheckElectability() {\n    // Maintain lock ordering of elector -> ZKFC\n    synchronized (elector) {\n      synchronized (this) {\n        boolean healthy = lastHealthState == State.SERVICE_HEALTHY;\n    \n        long remainingDelay = delayJoiningUntilNanotime - System.nanoTime(); \n        if (remainingDelay > 0) {\n          if (healthy) {\n            LOG.info(\"Would have joined master election, but this node is \" +\n                \"prohibited from doing so for \" +\n                TimeUnit.NANOSECONDS.toMillis(remainingDelay) + \" more ms\");\n          }\n          scheduleRecheck(remainingDelay);\n          return;\n        }\n    \n        switch (lastHealthState) {\n        case SERVICE_HEALTHY:\n          elector.joinElection(targetToData(localTarget));\n          if (quitElectionOnBadState) {\n            quitElectionOnBadState = false;\n          }\n          break;\n          \n        case INITIALIZING:\n          LOG.info(\"Ensuring that \" + localTarget + \" does not \" +\n              \"participate in active master election\");\n          elector.quitElection(false);\n          serviceState = HAServiceState.INITIALIZING;\n          break;\n    \n        case SERVICE_UNHEALTHY:\n        case SERVICE_NOT_RESPONDING:\n          LOG.info(\"Quitting master election for \" + localTarget +\n              \" and marking that fencing is necessary\");\n          elector.quitElection(true);\n          serviceState = HAServiceState.INITIALIZING;\n          break;\n          \n        case HEALTH_MONITOR_FAILED:\n          fatalError(\"Health monitor failed!\");\n          break;\n          \n        default:\n          throw new IllegalArgumentException(\"Unhandled state:\" + lastHealthState);\n        }\n      }\n    }\n  }\n  \n  /**\n   * Schedule a call to {@link #recheckElectability()} in the future.\n   */\n  private void scheduleRecheck(long whenNanos) {\n    delayExecutor.schedule(\n        new Runnable() {\n          @Override\n          public void run() {\n            try {\n              recheckElectability();\n            } catch (Throwable t) {\n              fatalError(\"Failed to recheck electability: \" +\n                  StringUtils.stringifyException(t));\n            }\n          }\n        },\n        whenNanos, TimeUnit.NANOSECONDS);\n  }\n\n  int serviceStateMismatchCount = 0;\n  boolean quitElectionOnBadState = false;\n\n  void verifyChangedServiceState(HAServiceState changedState) {\n    synchronized (elector) {\n      synchronized (this) {\n        if (serviceState == HAServiceState.INITIALIZING) {\n          if (quitElectionOnBadState) {\n            LOG.debug(\"rechecking for electability from bad state\");\n            recheckElectability();\n          }\n          return;\n        }\n        if (changedState == serviceState) {\n          serviceStateMismatchCount = 0;\n          return;\n        }\n        if (serviceStateMismatchCount == 0) {\n          // recheck one more time. As this might be due to parallel transition.\n          serviceStateMismatchCount++;\n          return;\n        }\n        // quit the election as the expected state and reported state\n        // mismatches.\n        LOG.error(\"Local service \" + localTarget\n            + \" has changed the serviceState to \" + changedState\n            + \". Expected was \" + serviceState\n            + \". Quitting election marking fencing necessary.\");\n        delayJoiningUntilNanotime = System.nanoTime()\n            + TimeUnit.MILLISECONDS.toNanos(1000);\n        elector.quitElection(true);\n        quitElectionOnBadState = true;\n        serviceStateMismatchCount = 0;\n        serviceState = HAServiceState.INITIALIZING;\n      }\n    }\n  }\n\n  /**\n   * @return the last health state passed to the FC\n   * by the HealthMonitor.\n   */\n  @VisibleForTesting\n  synchronized State getLastHealthState() {\n    return lastHealthState;\n  }\n\n  private synchronized void setLastHealthState(HealthMonitor.State newState) {\n    LOG.info(\"Local service \" + localTarget +\n        \" entered state: \" + newState);\n    lastHealthState = newState;\n  }\n  \n  @VisibleForTesting\n  ActiveStandbyElector getElectorForTests() {\n    return elector;\n  }\n  \n  @VisibleForTesting\n  ZKFCRpcServer getRpcServerForTests() {\n    return rpcServer;\n  }\n\n  /**\n   * Callbacks from elector\n   */\n  class ElectorCallbacks implements ActiveStandbyElectorCallback {\n    @Override\n    public void becomeActive() throws ServiceFailedException {\n      ZKFailoverController.this.becomeActive();\n    }\n\n    @Override\n    public void becomeStandby() {\n      ZKFailoverController.this.becomeStandby();\n    }\n\n    @Override\n    public void enterNeutralMode() {\n    }\n\n    @Override\n    public void notifyFatalError(String errorMessage) {\n      fatalError(errorMessage);\n    }\n\n    @Override\n    public void fenceOldActive(byte[] data) {\n      ZKFailoverController.this.fenceOldActive(data);\n    }\n    \n    @Override\n    public String toString() {\n      synchronized (ZKFailoverController.this) {\n        return \"Elector callbacks for \" + localTarget;\n      }\n    }\n  }\n  \n  /**\n   * Callbacks from HealthMonitor\n   */\n  class HealthCallbacks implements HealthMonitor.Callback {\n    @Override\n    public void enteredState(HealthMonitor.State newState) {\n      setLastHealthState(newState);\n      recheckElectability();\n    }\n  }\n\n  /**\n   * Callbacks for HAServiceStatus\n   */\n  class ServiceStateCallBacks implements HealthMonitor.ServiceStateCallback {\n    @Override\n    public void reportServiceStatus(HAServiceStatus status) {\n      verifyChangedServiceState(status.getState());\n    }\n  }\n\n  private static class ActiveAttemptRecord {\n    private final boolean succeeded;\n    private final String status;\n    private final long nanoTime;\n    \n    public ActiveAttemptRecord(boolean succeeded, String status) {\n      this.succeeded = succeeded;\n      this.status = status;\n      this.nanoTime = System.nanoTime();\n    }\n  }\n\n}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-12T06:27:31.957+0000","updated":"2015-06-12T06:27:31.957+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14583083","id":"14583083","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. Answer 2.when start only one ZKFC and NN ，the NN can be staying in ACTIVE for long time.\nYes, fine.\n\nIts because, patch from this issue, is not merged properly to your code.\nIn your source code, {{becomeActive()}} doesn't have line {{serviceState = HAServiceState.ACTIVE;}}.\n\n{code}private synchronized void becomeActive() throws ServiceFailedException {\n LOG.info(\"Trying to make \" + localTarget + \" active...\");\n try\n {\n  HAServiceProtocolHelper.transitionToActive(localTarget.getProxy( conf, FailoverController.getRpcTimeoutToNewActive(conf)), createReqInfo());\n  String msg = \"Successfully transitioned \" + localTarget + \" to active state\";\n  LOG.info(msg);\n  recordActiveAttempt(new ActiveAttemptRecord(true, msg));\n } catch (Throwable t) {\n   String msg = \"Couldn't make \" + localTarget + \" active\";\n   LOG.fatal(msg, t);\n   recordActiveAttempt(new ActiveAttemptRecord(false, msg + \"\\n\" +\n     StringUtils.stringifyException(t)));\n   if (t instanceof ServiceFailedException) {\n      throw (ServiceFailedException)t; }\n   else {\n      throw new ServiceFailedException(\"Couldn't transition to active\", t);\n   }\n/*\nTODO:\nwe need to make sure that if we get fenced and then quickly restarted,\nnone of these calls will retry across the restart boundary\nperhaps the solution is that, whenever the nn starts, it gets a unique\nID, and when we start becoming active, we record it, and then any future\ncalls use the same ID\n*/\n }\n}{code}\n\n\nSo if the previous state of NameNode is not STANDBY, then it will stay for long time. But if its trasitioned from STANDBY, it will continously switch.\n\nAdd {{serviceState = HAServiceState.ACTIVE;}} in {{becomeActive()}}  after {{LOG.info(msg);}}, everything will be fine.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-12T07:19:04.659+0000","updated":"2015-06-12T07:19:04.659+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12690252/comment/14583120","id":"14583120","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"thank u very very much . i am so careless.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-12T08:04:01.164+0000","updated":"2015-06-12T08:04:01.164+0000"}],"maxResults":54,"total":54,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-10251/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1rmw7:"}}