{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13112651","self":"https://issues.apache.org/jira/rest/api/2/issue/13112651","key":"HADOOP-14989","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-10-29T07:42:10.902+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Dec 15 20:21:30 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-14989/watchers","watchCount":7,"isWatching":false},"created":"2017-10-27T16:41:17.396+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334689","id":"12334689","description":"2.6.5 release","name":"2.6.5","archived":false,"released":true,"releaseDate":"2016-10-08"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-06-02T17:38:31.651+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12310971","id":"12310971","name":"metrics","description":""}],"timeoriginalestimate":null,"description":"While doing some digging in the metrics2 system recently, we noticed that the way {{MutableStat}} values are collected (and thus {{MutableRate}}, since it is based off of {{MutableStat}}) mean that every time the value is snapshotted, all previous information is lost. So every time a JMX cache refresh occurs, it resets the {{MutableStat}}, meaning that all configured metrics sinks do not consider the previous statistics in their emitted values. The same behavior is true if you configured multiple sink periods.\r\n\r\n{{MutableStat}}, to compute its average value, maintains a total value since last snapshot, as well as operation count since last snapshot. Upon snapshotting, the average is calculated as (total / opCount) and placed into a gauge metric, and total / operation count are cleared. So the average value represents the average since the last snapshot. If we have only a single sink period ever snapshotting, this would result in the expected behavior that the value is the average over the reporting period. However, if multiple sink periods are configured, or if the JMX cache is refreshed, this is another snapshot operation. So, for example, if you have a FileSink configured at a 60 second interval and your JMX cache refreshes itself 1 second before the FileSink period fires, the values emitted to your FileSink only represent averages _over the last one second_.\r\n\r\nA few ways to solve this issue:\r\n* Make {{MutableRate}} manage its own average refresh, similar to {{MutableQuantiles}}, which has a refresh thread and saves a snapshot of the last quantile values that it will serve up until the next refresh. Given how many {{MutableRate}} metrics there are, a thread per metric is not really feasible, but could be done on e.g. a per-source basis. This has some downsides: if multiple sinks are configured with different periods, what is the right refresh period for the {{MutableRate}}? \r\n* Make {{MutableRate}} emit two counters, one for total and one for operation count, rather than an average gauge and an operation count counter. The average could then be calculated downstream from this information. This is cumbersome for operators and not backwards compatible. To improve on both of those downsides, we could have it keep the current behavior but _additionally_ emit the total as a counter. The snapshotted average is probably sufficient in the common case (we've been using it for years), and when more guaranteed accuracy is required, the average could be derived from the total and operation count.\r\n\r\nThe two above suggestions will fix this for both JMX and multiple sink periods, but may be overkill. Multiple sink periods are probably not necessary though we should at least document the behavior.\r\n\r\nOpen to suggestions & input here.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12343378","id":"12343378","description":"2.7.8 Release","name":"2.7.8","archived":false,"released":false}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12895276","id":"12895276","filename":"HADOOP-14989.test.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-01T21:09:05.746+0000","size":2979,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12895276/HADOOP-14989.test.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"metrics2 JMX cache refresh result in inconsistent Mutable(Stat|Rate) values","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16222652","id":"16222652","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"body":"Ping [~aw], [~eyang] based on involvement in initial metrics2 JIRAs (HADOOP-6919, HADOOP-6728)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-10-27T16:44:53.713+0000","updated":"2017-10-27T16:44:53.713+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16223883","id":"16223883","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"HI [~xkrogen],\r\n\r\nCan we keep both sinks using the same refresh rate, like 10 seconds?  I would not recommend to have different refresh rate, this is comparing data samples at different frequency.  The resulting graph will not look the same.  Total is a high watermark and it will eventually overflow.  This is the reason that Hadoop community favored gauge system to minimize compute and interested to monitor metrics at real time only during the development phase.\r\n\r\nIf we want to produce high fidelity data samples.  Time stamp, previous count, current count, and Time passed since last sample (or refresh rate) are the essential information to record for high fidelity data samples, but post processing is more expensive.  Gauge and average are only good for measuring velocity of the metrics for a point in time.  Most monitoring system can only handle time precision at second or minute scale.  Hence, MutableRate is heavily dependent on time precision that the down stream can consume.  One important limitation is JMX cache reset requires JMX sink to be the last one in the chain with slowest refresh rate to avoid accurate problem like you described.  JMX sink should not have a lower refresh rate than FileSink to avoid destroying samples before data is sent.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-10-29T07:42:10.902+0000","updated":"2017-10-29T07:42:10.902+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16234894","id":"16234894","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you for the comments [~eyang]! You actually made me realize I had a bit of a misunderstanding after digging into the code further. Let me try again:\r\n* The problem I described is definitely an issue if you specify multiple refresh rates. I agree there's not a great way around this issue but I think we should, at minimum, put something in the documentation indicating that it is not a good idea. Right now the behavior I describe when dealing with MutableRate values is not documented and would come as a surprise to an operator.\r\n* Specifying only a single refresh rate does not solve the JMX issue. The single-point collection of metrics for all sinks occurs in {{MetricsSystemImpl}}, specifically {{sampleMetrics()}}, which then passes off the single {{MetricsBuffer}} to all sinks. This is great. However, JMX avoids the {{MetricsSystemImpl}} code altogether, instead directly calling {{getMetrics()}} on each {{MetricsSourceAdapter}}. Thus JMX cache refills can destroy metrics values even if you correctly configure only one period. I have attached a patch, [^HADOOP-14989.test.patch], which demonstrates this issue - it's hacky but it should get the point across.\r\n\r\nIt seems to me the best way to fix this is to save the output values each time {{getMetrics()}} is called and use those for the cache. We can either\r\n* Call {{updateJmxCache()}} at the end of {{getMetrics()}} with the computed values\r\n* Store the return value of {{getMetrics()}} and use it as the input for {{updateJmxCache()}} next it is called, assuming that value is fresh enough.\r\n\r\nThe second is considerably more complex. It avoids some potential performance penalty of the {{updateAttrCache()}} and {{updateInfoCache()}} calls, which do create a bunch of objects. Not sure if it would be enough to be worth the extra complexity.\r\n\r\nWhile digging / testing I also noticed another bug which occurs if you have multiple sink periods set; see HADOOP-15008","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-01T22:44:22.892+0000","updated":"2017-11-01T22:44:22.892+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16234898","id":"16234898","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"body":"One more comment:\r\n{quote}\r\nTotal is a high watermark and it will eventually overflow.\r\n{quote}\r\nSure it is a high watermark, but so are all of the {{MutableCounter}} metrics in Hadoop. These all rise indefinitely; I fail to see how this situation is any different.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-01T22:50:19.447+0000","updated":"2017-11-01T22:50:44.553+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16235048","id":"16235048","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~xkrogen] Line 233, The test patch is comparing double to int, and val is value of 10.00.  If it did not reset, shouldn't it be 100000010?\r\nAs far as I know, JMX mbean calls resets internally, and there is no need to call it externally.  However, if multiple people are pulling from JMX, I don't know how the reset is managed.  Let me know if I misunderstood the test patch.\r\n\r\n{code}\r\nSure it is a high watermark, but so are all of the MutableCounter metrics in Hadoop. These all rise indefinitely; I fail to see how this situation is any different.\r\n{code}\r\n\r\nTrue, this is less of a concern on 64 bits system.  Overflow happens less frequently.  ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-02T01:18:30.782+0000","updated":"2017-11-02T01:18:30.782+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16235883","id":"16235883","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"body":"Assuming nothing else was submitting block reports, then {{val}} with the current code would be 10, but it should be 50000005 (it is an average so {{= 100000010/2}}). Since it is taking metrics from a minicluster there are also some real block reports that skew things; that's why I used a big value and a comparison rather than equal. Like I said, hacky. But the test will definitively pass if you omit the JMX call and definitely fail if you include it. I'll try to put together a real unit test for this.\r\n\r\nI am not sure what you mean about JMX mbean calling reset internally. Are you talking here about the metrics2 level reset ({{MetricsSourceAdapter#updateJmxCache()}}) or something at a JVM level? I explained how the cache reset is managed at the metrics2 level; let me know if there's something about my explanation that was not clear.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-02T15:13:33.663+0000","updated":"2017-11-02T15:14:01.258+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16236260","id":"16236260","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~xkrogen] Your observation is correct.  However, {{MetricsSourceAdapter}} can not call {{updateJmxCache}} at end of {{getMetrics}}.  It will just deadlock because {{updateJmxCache}} calls {{getMetrics}}.  {{MetricsSystemImpl}} was not used by JMX to avoid a dead lock in the timer thread where  {{MetricsSourceAdapter}} lock and is trying to grab the {{MetricsSystemImpl}} lock. The locking order isn't consistent in the \"push and pull\" part of {{MetricsSourceAdapter}} so it can deadlocked.\r\n\r\nIn your second suggestion, store the return value of {{getMetrics}} and use that to populate jmx cache, this is the correct logic, in a push vs pull system.  We need to be careful in the synchronization of cache value to MBean or it can cause mbean to fail with null value.  HADOOP-11361 has some of the background information of how the system arrived at the current state.  There is a new ReentrantLock utility in Java 7 which might help to reduce the deadlock in publishing metrics and retrieved cache by JMX.  This might be one way to solve the race condition and produce more accurate data for JMX.  HADOOP-12594 had an attempt in removing the deadlock, and it might be useful background information on how to solve this the proper way.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-02T17:54:58.742+0000","updated":"2017-11-02T17:54:58.742+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16236743","id":"16236743","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"body":"Perfect, thank you [~eyang]! That information & the pointers to previous JIRAs is very helpful. I will plan further and update...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xkrogen","name":"xkrogen","key":"xkrogen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xkrogen&avatarId=34526","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xkrogen&avatarId=34526","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xkrogen&avatarId=34526","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xkrogen&avatarId=34526"},"displayName":"Erik Krogen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-02T22:44:05.773+0000","updated":"2017-11-02T22:44:05.773+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13112651/comment/16293148","id":"16293148","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shv","name":"shv","key":"shv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Konstantin Shvachko","active":true,"timeZone":"America/Los_Angeles"},"body":"Moving target version to 2.7.6 due to 2.7.5 release.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shv","name":"shv","key":"shv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Konstantin Shvachko","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-12-15T20:21:30.569+0000","updated":"2017-12-15T20:21:30.569+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-14989/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3lswv:"}}