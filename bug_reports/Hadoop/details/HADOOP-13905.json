{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13028135","self":"https://issues.apache.org/jira/rest/api/2/issue/13028135","key":"HADOOP-13905","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-12-14T18:52:49.582+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Dec 14 18:52:49 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-13905/watchers","watchCount":2,"isWatching":false},"created":"2016-12-14T16:18:38.037+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329058","id":"12329058","description":"2.8.0 release","name":"2.8.0","archived":false,"released":true,"releaseDate":"2017-03-22"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-12-14T18:52:49.582+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12311814","id":"12311814","name":"fs/s3","description":"S3A filesystem client and other S3 connectivity issues"}],"timeoriginalestimate":null,"description":"Have 3 node setup: nn/slave/client. Client default fs is viewfs with the following mounttable:\n\n<configuration>\n    <property>\n        <name>fs.viewfs.mounttable.hadoopDemo.homedir</name>\n        <value>/home</value>\n    </property>\n    <property>\n        <name>fs.viewfs.mounttable.hadoopDemo.link./home</name>\n        <value>hdfs://namenode:9000</value>\n    </property>\n    <property>\n        <name>fs.viewfs.mounttable.hadoopDemo.link./tmp</name>\n        <value>hdfs://namenode:9000/tmp</value>\n    </property>\n    <property>\n        <name>fs.viewfs.mounttable.hadoopDemo.link./user</name>\n        <value>hdfs://namenode:9000/user</value>\n    </property>\n    <property>\n        <name>fs.viewfs.mounttable.hadoopDemo.link./s3a</name>\n        <value>s3a://cloudply-hadoop-demo/</value>\n    </property>\n</configuration>\n\ns3a credentials are configured in core-site.xml on the client node. Able to view/modify /s3a mount contents with hdfs commands. When I ran a wordcount example using this line (even without access to s3a share):\n\nhadoop jar $HADOOP_HOME/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.8.0-SNAPSHOT-sources.jar org.apache.hadoop.examples.WordCount /home/input /home/output\n\nit fails with the following exception: \n\n16/12/14 16:08:33 INFO client.RMProxy: Connecting to ResourceManager at namenode/172.18.0.2:8032\n16/12/14 16:08:33 INFO mapreduce.Cluster: Failed to use org.apache.hadoop.mapred.YarnClientProtocolProvider due to error:\njava.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n        at org.apache.hadoop.fs.AbstractFileSystem.newInstance(AbstractFileSystem.java:136)\n        at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:165)\n        at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:250)\n        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:342)\n        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:339)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711)\n        at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:339)\n        at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:456)\n        at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:482)\n        at org.apache.hadoop.mapred.YARNRunner.<init>(YARNRunner.java:148)\n        at org.apache.hadoop.mapred.YARNRunner.<init>(YARNRunner.java:132)\n        at org.apache.hadoop.mapred.YARNRunner.<init>(YARNRunner.java:122)\n        at org.apache.hadoop.mapred.YarnClientProtocolProvider.create(YarnClientProtocolProvider.java:34)\n        at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:111)\n        at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:98)\n        at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:91)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1311)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711)\n        at org.apache.hadoop.mapreduce.Job.connect(Job.java:1307)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1335)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1359)\n        at org.apache.hadoop.examples.WordCount.main(WordCount.java:87)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:234)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n        at org.apache.hadoop.fs.AbstractFileSystem.newInstance(AbstractFileSystem.java:134)\n        ... 32 more\nCaused by: org.apache.hadoop.HadoopIllegalArgumentException: FileSystem implementation error -  default port -1 is not valid\n        at org.apache.hadoop.fs.AbstractFileSystem.getUri(AbstractFileSystem.java:306)\n        at org.apache.hadoop.fs.AbstractFileSystem.<init>(AbstractFileSystem.java:266)\n        at org.apache.hadoop.fs.viewfs.ChRootedFs.<init>(ChRootedFs.java:102)\n        at org.apache.hadoop.fs.viewfs.ViewFs$1.getTargetFileSystem(ViewFs.java:220)\n        at org.apache.hadoop.fs.viewfs.ViewFs$1.getTargetFileSystem(ViewFs.java:209)\n        at org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:261)\n        at org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:333)\n        at org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:209)\n        at org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:209)\n        ... 37 more\nException in thread \"main\" java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.\n        at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:133)\n        at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:98)\n        at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:91)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1311)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711)\n        at org.apache.hadoop.mapreduce.Job.connect(Job.java:1307)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1335)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1359)\n        at org.apache.hadoop.examples.WordCount.main(WordCount.java:87)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:234)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)\n\nWhen I remove s3a mount from mounttable - I am able to run wordcount example.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Cannot run wordcount example when there's a mounttable configured with a link to s3a.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=oleg.v.khaschansky%40gmail.com","name":"oleg.v.khaschansky@gmail.com","key":"oleg.v.khaschansky@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Oleg Khaschansky","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=oleg.v.khaschansky%40gmail.com","name":"oleg.v.khaschansky@gmail.com","key":"oleg.v.khaschansky@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Oleg Khaschansky","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028135/comment/15749127","id":"15749127","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"looks like you've gone into a corner of the code nobody else has. \n\nFollowing the stack trace, its failing as a FS has to have a port if an authority is needed, and that is determined if uri.getAuthority()!=null, something that's not really the case in object stores. Now, for AbstractFS integration, HADOOP-11262, [~PieterReuse] had s3 declaring that it didn't need authority. Somehow view FS isn't picking it up, no doubt from this extra indirection\n\nPieter? Any thoughts","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2016-12-14T18:52:49.582+0000","updated":"2016-12-14T18:52:49.582+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-13905/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i37lkf:"}}