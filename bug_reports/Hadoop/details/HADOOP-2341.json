{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12383842","self":"https://issues.apache.org/jira/rest/api/2/issue/12383842","key":"HADOOP-2341","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/4","id":"4","description":"The problem is not completely described.","name":"Incomplete"},"customfield_12312322":null,"customfield_12310220":"2007-12-04T01:22:40.507+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Dec 20 01:14:00 UTC 2007","customfield_12310420":"81533","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_1385686151_*|*_6_*:*_1_*:*_0_*|*_5_*:*_1_*:*_1358398593","customfield_12312321":null,"resolutiondate":"2007-12-20T01:14:00.048+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-2341/watchers","watchCount":2,"isWatching":false},"created":"2007-12-04T00:19:13.897+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12312740","id":"12312740","description":"","name":"0.16.0","archived":false,"released":true,"releaseDate":"2008-02-07"}],"issuelinks":[{"id":"12318792","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12318792","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12383903","key":"HADOOP-2346","self":"https://issues.apache.org/jira/rest/api/2/issue/12383903","fields":{"summary":"DataNode should have timeout on socket writes.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2009-07-08T16:42:44.355+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"On trunk i continue to see the following in my data node logs:\n\n2007-12-03 15:46:47,696 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 42\n2007-12-03 15:46:48,135 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 41\n2007-12-03 15:46:48,439 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 40\n2007-12-03 15:46:48,479 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 39\n2007-12-03 15:46:48,611 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 38\n2007-12-03 15:46:48,898 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 37\n2007-12-03 15:46:48,989 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 36\n2007-12-03 15:46:51,010 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 35\n2007-12-03 15:46:51,758 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 34\n2007-12-03 15:46:52,148 DEBUG dfs.DataNode - XX.XX.XX.XXX:50010:Number of active connections is: 33\n\nThis number never returns to 0, even after many hours of no new data being manipulated or added into the DFS.\n\nLooking at netstat -tn i see significant amount of data in the send-q that never goes away:\n\ntcp        0  34240 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:55792   ESTABLISHED \ntcp        0  38968 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:38169   ESTABLISHED \ntcp        0  38456 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:35456   ESTABLISHED \ntcp        0  29640 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:59845   ESTABLISHED \ntcp        0  50168 ::ffff:XX.XX.XX.XXX:50010   ::ffff:YY.YY.YY.YY:44584   ESTABLISHED \n\nWhen sniffing the network I see that the remote side (YY.YY.YY.YY) is returning a window size of 0\n16:11:41.760474 IP XX.XX.XX.XXX.50010 > YY.YY.YY.YY.44584: . ack 3339984123 win 46 <nop,nop,timestamp 1786247180 885681789>\n16:11:41.761597 IP YY.YY.YY.YY.44584 > XX.XX.XX.XXX.50010: . ack 1 win 0 <nop,nop,timestamp 885801786 1775711351>\n\nThen we look at the stack traces on each datanode, I will have tons of threads that *never* go away in the following trace:\n{code}\nThread 6516 (org.apache.hadoop.dfs.DataNode$DataXceiver@166068b6):\n  State: RUNNABLE\n  Blocked count: 0\n  Waited count: 0\n  Stack:\n    java.net.SocketOutputStream.socketWrite0(Native Method)\n    java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)\n    java.net.SocketOutputStream.write(SocketOutputStream.java:136)\n    java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n    java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)\n    java.io.DataOutputStream.write(DataOutputStream.java:90)\n    org.apache.hadoop.dfs.DataNode$BlockSender.sendChunk(DataNode.java:1400)\n    org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1433)\n    org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:904)\n    org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:849)\n    java.lang.Thread.run(Thread.java:619)\n{code}\n\nUnfortunately there's very little in the logs with exceptions that could point to this.  I have some exceptions the following, but nothing that points to problems between XX and YY:\n{code}\n2007-12-02 11:19:47,889 WARN  dfs.DataNode - Unexpected error trying to delete block blk_4515246476002110310. Block not found in blockMap. \n2007-12-02 11:19:47,922 WARN  dfs.DataNode - java.io.IOException: Error in deleting blocks.\n        at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:750)\n        at org.apache.hadoop.dfs.DataNode.processCommand(DataNode.java:675)\n        at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:569)\n        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1720)\n        at java.lang.Thread.run(Thread.java:619)\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12371274","id":"12371274","filename":"dfsclient.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-08T01:01:51.830+0000","size":1943,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12371274/dfsclient.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12370955","id":"12370955","filename":"hregionserver-stack.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T17:42:53.448+0000","size":9898,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12370955/hregionserver-stack.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12370909","id":"12370909","filename":"stacks-XX.XX.XX.XXX.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T04:32:13.189+0000","size":30671,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12370909/stacks-XX.XX.XX.XXX.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12370910","id":"12370910","filename":"stacks-YY.YY.YY.YY.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T04:32:23.911+0000","size":42082,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12370910/stacks-YY.YY.YY.YY.txt"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"105747","customfield_12312823":null,"summary":"Datanode active connections never returns to 0","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548080","id":"12548080","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"It appears that there are HDFS clients running on machines marked as yy.yy.yy.yy. They are trying to read files from the datanodes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2007-12-04T01:22:40.507+0000","updated":"2007-12-04T01:22:40.507+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548083","id":"12548083","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"body":"but data is never read.  Both sides of the connection stack traces show each\nmachine in write.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T01:27:48.357+0000","updated":"2007-12-04T01:27:48.357+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548092","id":"12548092","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"body":"I am seeing this too.  If I look at datanode:50075/stacks, I see 70 threads stuck with: \n\nThread 3023977 (org.apache.hadoop.dfs.DataNode$DataXceiver@28075c96):\n  State: RUNNABLE\n  Blocked count: 0\n  Waited count: 0\n  Stack:\n    java.net.SocketOutputStream.socketWrite0(Native Method)\n    java.net.SocketOutputStream.socketWrite(Unknown Source)\n    java.net.SocketOutputStream.write(Unknown Source)\n    java.io.BufferedOutputStream.flushBuffer(Unknown Source)\n    java.io.BufferedOutputStream.write(Unknown Source)\n    java.io.DataOutputStream.write(Unknown Source)\n    org.apache.hadoop.dfs.DataNode$BlockSender.sendChunk(DataNode.java:1175)\n    org.apache.hadoop.dfs.DataNode$BlockSender.sendBlock(DataNode.java:1208)\n    org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:850)\n    org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:801)\n    java.lang.Thread.run(Unknown Source)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-04T02:17:41.087+0000","updated":"2007-12-04T02:17:41.087+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548101","id":"12548101","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"body":"In DataNode.BlockSender.sendBlock, we have\n{code}\n        while (endOffset > offset) {\n          // Write one data chunk per loop.\n          long len = sendChunk();\n          offset += len;\n          totalRead += len + checksumSize;\n        }\n{code}\n\nIn the BlockSender constructor, we have\n{code}\n        if (length >= 0) {\n          // Make sure endOffset points to end of a checksumed chunk.\n          long tmpLen = startOffset + length + (startOffset - offset);\n          if (tmpLen % bytesPerChecksum != 0) {\n            tmpLen += (bytesPerChecksum - tmpLen % bytesPerChecksum);\n          }\n          if (tmpLen < endOffset) {\n            endOffset = tmpLen;\n          }\n        }\n{code}\n\nSo in some cases, endOffset can include extra bytes for checksums in the constructor.  However, checksum bytes are never added to offset when it is compared to endOffset in the sendBlock method.  I believe this may be the problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-04T03:15:48.532+0000","updated":"2007-12-04T03:15:48.532+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548105","id":"12548105","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":" {quote}\ntcp 0 50168 ::ffff:XX.XX.XX.XXX:50010 ::ffff:YY.YY.YY.YY:44584 ESTABLISHED\n\nWhen sniffing the network I see that the remote side (YY.YY.YY.YY) is returning a window size of 0\n16:11:41.760474 IP XX.XX.XX.XXX.50010 > YY.YY.YY.YY.44584: . ack 3339984123 win 46 <nop,nop,timestamp 1786247180 885681789>\n16:11:41.761597 IP YY.YY.YY.YY.44584 > XX.XX.XX.XXX.50010: . ack 1 win 0 <nop,nop,timestamp 885801786 1775711351>\n{quote}\nThis is pretty odd. If I looked at just netstat, I would think client is reading a block. But if I look at the tcpdump, I would think client is writing data (note DataNode is acking 3Mb and client is acking 1 byte). Are you sure these two correspond to the same TCP connection?\n\nPlease include client side stack if you can. Also, if this is on Linux, with strace and lsof, you might be able to match stacktrace and the tcp connection involved on the Datanode.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-04T03:28:17.372+0000","updated":"2007-12-04T03:28:17.372+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548117","id":"12548117","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"body":"The connections are still sitting like this for the last few hours.. Here's the trace from both sides at the same time.\n\n20:17:40.669439 IP XX.XX.XX.XXX.50010 > YY.YY.YY.YY.44584: . ack 3339984123 win 46 <nop,nop,timestamp 1801007426 900441594>\n20:17:40.670752 IP YY.YY.YY.YY.44584 > XX.XX.XX.XXX.50010: . ack 1 win 0 <nop,nop,timestamp 900561591 1775711351>\n--\n20:17:40.683665 IP XX.XX.XX.XXX.50010 > YY.YY.YY.YY.44584: . ack 1 win 46 <nop,nop,timestamp 1801007426 900441594>\n20:17:40.683685 IP YY.YY.YY.YY.44584 > XX.XX.XX.XXX.50010: . ack 1 win 0 <nop,nop,timestamp 900561591 1775711351>\n\nlsof\njava    11091 user  363u  IPv6 1044460194       TCP YY.YY.YY.YY:44584->XX.XX.XX.XXX:50010 (ESTABLISHED)\n\njava    8853 user   53u  IPv6 335510414       TCP XX.XX.XX.XXX:50010->YY.YY.YY.YY:44584 (ESTABLISHED)\n\nstrace shows nothing even when tcp retransmits,\n\nstrace -p 11091\nProcess 11091 attached - interrupt to quit\nfutex(0x4022b9e0, FUTEX_WAIT, 11117, NULL^[[A^[[A <unfinished ...>\nProcess 11091 detached\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T04:27:17.454+0000","updated":"2007-12-04T04:27:17.454+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548118","id":"12548118","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"body":"stacktrace from XX.XX.XX.XXX host ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T04:28:04.317+0000","updated":"2007-12-04T04:28:04.317+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548119","id":"12548119","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"body":"jstack from YY.YY.YY.YY","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T04:28:41.823+0000","updated":"2007-12-04T04:28:41.823+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548137","id":"12548137","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Paul, will look at the traces. \nThe pid in lsof might not be the pid of the thread that is trying to write. For a given stack, jstack \nlists the pid as a hex, then you need to strace to see which fd it is trying to write to.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-04T06:01:40.827+0000","updated":"2007-12-04T06:01:40.827+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548156","id":"12548156","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"body":"This is related to hbase.  The question is, what's going on to keep all that data in the send and receive queues of each machine and sockets being blocked forever.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T07:15:32.090+0000","updated":"2007-12-04T07:15:32.090+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548310","id":"12548310","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"The stacks on both sides are DataNodes. So these data transfer (attempts) are between two datanodes?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-04T17:16:58.140+0000","updated":"2007-12-04T17:16:58.140+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548319","id":"12548319","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"body":"I thought it was, but after futher investigation it is between a datanode\nand a HRegionServer after I started digging a bit more and reran a job to\nverify.  I'll be attaching the trace from a HRegionServer.  I cannot see\nwhich thread would have that file open though through jstack or lsof.  I\nstraced every thread and none seemed to be stuck in read or reading any data\nfrom the socket between the two hosts.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ps0ps","name":"ps0ps","key":"ps0ps","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Paul Saab","active":true,"timeZone":"Etc/UTC"},"created":"2007-12-04T17:43:48.832+0000","updated":"2007-12-04T17:43:48.832+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548343","id":"12548343","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"body":"It seems like these socket reads/writes should time out eventually.  But the 70 threads on my datanode are still waiting on write.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-04T18:47:46.592+0000","updated":"2007-12-04T18:47:46.592+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548346","id":"12548346","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"> none seemed to be stuck in read or reading any data from the socket between the two hosts.\nThat could be the actual problem. If a client opens a HDFS file and stops reading in the middle, DataNode could be stuck forever. DataNode sets read timeout but not write timeout. I think it should (I will file a jira for it).\n\nI think we need to look for cases where HRegionServer might not be closing a DFS inputstream. You could /proc/HRegionServerPid/fds to see if it looks like there is an FD leak.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-04T18:49:45.694+0000","updated":"2007-12-04T18:49:45.694+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12548934","id":"12548934","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"I did a review of hbase.  Found a leak, HADOOP-2362, but it doesn't seem to fix the problem.\n\nBut on review, I'm now thinking that it could the way that Hbase is using hdfs that is making for all the occupied threads over in the datanode.  Below I explain why I think this.  Would appreciate any input or alternate suggestions for Hbase usage of hdfs.\n\nAfter doing an upload, I let the cluster alone till it hit a steady state.   Here's what netstat looks like if I only look for the ESTABLISHED connections:\n\n{code}\n[stack@aa0-000-13 hbase]$ netstat -tn|grep ESTABLISHED|sort > /tmp/ns.txt\n..\ntcp        0  95488 ::ffff:XX.XX.XX.140:50010  ::ffff:XX.XX.XX.140:32853  ESTABLISHED\ntcp        0  95488 ::ffff:XX.XX.XX.140:50010  ::ffff:XX.XX.XX.140:32860  ESTABLISHED\ntcp        0  95488 ::ffff:XX.XX.XX.140:50010  ::ffff:XX.XX.XX.140:32866  ESTABLISHED\ntcp        0  95488 ::ffff:XX.XX.XX.140:50010  ::ffff:XX.XX.XX.140:32872  ESTABLISHED\n...\ntcp    71976      0 ::ffff:XX.XX.XX.140:60511  ::ffff:XX.XX.XX.140:50010  ESTABLISHED \ntcp    71976      0 ::ffff:XX.XX.XX.140:60513  ::ffff:XX.XX.XX.140:50010  ESTABLISHED \ntcp    71976      0 ::ffff:XX.XX.XX.140:60711  ::ffff:XX.XX.XX.140:50010  ESTABLISHED \ntcp    71976      0 ::ffff:XX.XX.XX.140:60713  ::ffff:XX.XX.XX.140:50010  ESTABLISHED \ntcp    71976      0 ::ffff:XX.XX.XX.140:60999  ::ffff:XX.XX.XX.140:50010  ESTABLISHED\n{code}\n\nThe number remained constant.\n\nCounting the number of open connections, I was able to make rough correlation with count of open MapFiles in hbase.\n\nI wrote a little program:\n\n{code}\n   public static void main(String[] args) throws Exception {\n    HBaseConfiguration conf = new HBaseConfiguration();\n    MapFile.Reader reader =\n      new MapFile.Reader(FileSystem.get(conf), args[0], conf);\n    Thread.sleep(1000 * 1000);\n  }\n{code}\n\nEverytime I ran my program on XX.XX.XX.140, the ESTABLISHED count went up by one as did the number of occupied threads in the datanode. \n\nHbase keeps open MapFile.Readers.  The number of concurrent open Readers goes up as a server deploys more and more regions.\n\nIs there something wrong w/ this pattern -- should hbase not be doing this -- or is there something wrong w/ datanode such that a thread is completely occupied while a client has a MapFile.Reader open or is this just the way its supposed to work?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-06T06:36:18.177+0000","updated":"2007-12-06T06:36:18.177+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549092","id":"12549092","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"I stopped the cluster.  The connections went away.  I restarted the cluster.  They all came back.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-06T16:45:54.148+0000","updated":"2007-12-06T16:45:54.148+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549101","id":"12549101","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"> Hbase keeps open MapFile.Readers. The number of concurrent open Readers goes up as a server deploys more and more regions.\n\nAs mentioned earlier, if you open a DFS file you do take up one fd on client and one fd (and a thread) on datanode. What did you expect instead? Unlike normal file open, MapFile.Reader() seem to seek inside the constructor.. that pretty much assures client opens a socket to the datanode.  Also see HADOOP-2346.\n\n\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-06T17:29:52.311+0000","updated":"2007-12-06T17:29:52.311+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549110","id":"12549110","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"What is the desired bevaviour w.r.t Hbase with this pattern?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-06T17:48:28.229+0000","updated":"2007-12-06T17:48:28.229+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549128","id":"12549128","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"body":"I've noticed that the regionserver is using select for its io, which doesn't tie up threads, whereas the datanode seems to have one thread open/blocked for each write.  Is this intentional?  Shouldn't they be using the same mechanism?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-06T18:45:01.457+0000","updated":"2007-12-06T18:45:01.457+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549131","id":"12549131","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Thread is not the only cost. We pay for buffers, tcp connections and more. Why does the regionserver want to keep the connections open for so long?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-06T18:51:57.948+0000","updated":"2007-12-06T18:51:57.948+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549608","id":"12549608","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"Raghu:  Thanks for responding.\n\nNow we've done the tracing, I suppose it makes sense that an open hdfs file descriptor consumes a socket, thread, buffers, etc. in the data node.  That the cost is so high was a bit of a surprise going by the client side with its having only one connection to the datanode (Are there plans to do as Michael B asks above making server side do selects instead of a socket-per file?).\n\nI found some leaks after instrumenting SequenceFile and I upped the size of the files we keep over hdfs so we have less open files at any one time.   This helps some.  There still seems to be an issue w/ our not closing down everything as I see outstanding CLOSE_WAITS after I'm done with an upload.  I'm now trying to instrument DFSClient to see if I can figure whats going on.\n\nIn hbase, a table is made of regions; regions are made of stores and stores are made of mapfiles where the key is row/column/timestamp and the value is the cell data.  hbase keeps open all mapfiles so access to a 'row' is 'fast'.  Open mapfiles will have their index in memory, the socket connection up, etc.  The alternative of opening a file on every row access, I'd imagine, would be slower.\n\nDesire is consuming as few resources as possible in hdfs while keeping access as fast as possible.  Any suggestions on how we might improve our usage would be appreciated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-07T23:44:54.651+0000","updated":"2007-12-07T23:44:54.651+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549619","id":"12549619","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"> hbase keeps open all mapfiles so access to a 'row' is 'fast'.\n\nI guess what you are trying to avoid is talking to Namenode for file information each time. Does pread() (position read) suite you better? These files seem idle for most of the time. When you actually read, how much data do you read? I think there are going to be more users of pread().. for now it at least as fast as regular read.. there might some more optimizations in for this. Depending on amount of data you are reading, cost and latency of connection establishment might be negligible.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-08T00:44:11.585+0000","updated":"2007-12-08T00:44:11.585+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549628","id":"12549628","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"Ok.  So, now I think we're back around to Pauls' original complaint; that active connections never goes to zero.\n\nI instrumented DFSClient (See attached patch -- maybe I should go ahead and make an lsof-like tool for hdfs cients keeping a map of open files adding file on open and removing from the map on close?).  Every open during a test run has a corresponding close.  After the test run, I see this in datanode log:\n\n{code}\n...\n2007-12-08 00:56:06,429 INFO org.apache.hadoop.dfs.DataNode: XX.XX.XX.140:50010Served block blk_-107599750641758201 to /XX.XX.XX.140\n2007-12-08 00:56:06,429 DEBUG org.apache.hadoop.dfs.DataNode: XX.XX.XX.140:50010:Number of active connections is: 10\n2007-12-08 00:56:06,431 DEBUG org.apache.hadoop.dfs.DataNode: Number of active connections is: 10\n2007-12-08 00:56:06,431 INFO org.apache.hadoop.dfs.DataNode: XX.XX.XX.140:50010Served block blk_8573562922806881814 to /XX.XX.XX.140\n2007-12-08 00:56:06,431 DEBUG org.apache.hadoop.dfs.DataNode: XX.XX.XX.140:50010:Number of active connections is: 10\n..\n{code}\n\n... and this when I look w/ netstat:\n\n{code}\n[stack@aa0-000-13 ~]$ netstat -tn|grep -v ESTABLISHED|grep CLOSE_WAIT\ntcp       38      0 XX.XX.XX.140:51141         208.76.46.110:389           CLOSE_WAIT   \ntcp       38      0 XX.XX.XX.140:55612         208.76.46.110:389           CLOSE_WAIT   \ntcp       38      0 fdfb:c1e8:7640:2:2e0::53696 fdfb:c1e8:7640:6:2e0:81:389 CLOSE_WAIT  \ntcp        1      0 ::ffff:XX.XX.XX.140:56087  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp        1      0 ::ffff:XX.XX.XX.140:56088  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp        1      0 ::ffff:XX.XX.XX.140:53432  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp    37585      0 ::ffff:XX.XX.XX.140:53428  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp     1893      0 ::ffff:XX.XX.XX.140:53430  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp    28174      0 ::ffff:XX.XX.XX.140:53684  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp     1078      0 ::ffff:XX.XX.XX.140:53686  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp        1      0 ::ffff:XX.XX.XX.140:39643  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp    28230      0 ::ffff:XX.XX.XX.140:39777  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp    28230      0 ::ffff:XX.XX.XX.140:39782  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp        1      0 ::ffff:XX.XX.XX.140:39831  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp        1      0 ::ffff:XX.XX.XX.140:39832  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp        0      0 ::ffff:XX.XX.XX.140:40871  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp        1      0 ::ffff:XX.XX.XX.140:40872  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp        1      0 ::ffff:XX.XX.XX.140:41448  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp    34056      0 ::ffff:XX.XX.XX.140:41444  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT    \ntcp     1848      0 ::ffff:XX.XX.XX.140:41446  ::ffff:XX.XX.XX.140:50010  CLOSE_WAIT\n{code}\n\nAs far as I can tell, the application -- an hbase regionserver -- is calling close but the close is not being run properly inside in DFSClient.   Anyone have suggestions on how I might figure whats up w/ DFSClient or, if its hbase, how else I might go about looking for an improper close?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-08T01:01:55.717+0000","updated":"2007-12-08T01:01:55.717+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549636","id":"12549636","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"I briefly looked at DFSClient and could not find anything suspicious. Please let us know if you do or if there is a way to reproduce this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-08T02:00:38.871+0000","updated":"2007-12-08T02:00:38.871+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549747","id":"12549747","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"You can try printing file name  and local address:port each time a socket is opened and closed in DFSInputStream.\nThis will match filenames and the sockets from netstat.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-08T20:43:20.153+0000","updated":"2007-12-08T20:43:20.153+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12549751","id":"12549751","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the suggestion Raghu.  Let me try it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-08T20:57:17.571+0000","updated":"2007-12-08T20:57:17.571+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12550838","id":"12550838","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"On how to reproduce, first apply this to MapFile:\n\n{code}\nIndex: src/java/org/apache/hadoop/io/MapFile.java\n===================================================================\n--- src/java/org/apache/hadoop/io/MapFile.java  (revision 603416)\n+++ src/java/org/apache/hadoop/io/MapFile.java  (working copy)\n@@ -563,8 +563,12 @@\n     String out = args[1];\n \n     Configuration conf = new Configuration();\n-    FileSystem fs = FileSystem.getLocal(conf);\n+    // FileSystem fs = FileSystem.getLocal(conf);\n+    FileSystem fs = FileSystem.get(conf);\n     MapFile.Reader reader = new MapFile.Reader(fs, in, conf);\n+    LOG.info(\"READER opened -- go do a netstat -tn\");\n+    Thread.sleep(60*1000);\n+    LOG.info(\"READER moving on to invoke next\");\n     MapFile.Writer writer =\n       new MapFile.Writer(conf, fs, out, reader.getKeyClass(), reader.getValueClass());\n \n@@ -574,8 +578,14 @@\n \n     while (reader.next(key, value))               // copy all entries\n       writer.append(key, value);\n+    LOG.info(\"READER past append -- check netstat before close\");\n+    Thread.sleep(60*1000);\n \n     writer.close();\n+\n+    reader.close();\n+    LOG.info(\"READER closed -- waiting around 60seconds so can see netstat\");\n+    Thread.sleep(60*1000);\n   }\n \n }\n{code}\n\nIt adds a pause after opening a small map file in dfs and after running through append -- just before close -- as well as after close, just so you can do a netsat -tn to see the CLOSE_WAITs showing up.\n\nBuild and then do:\n\n{code}\n% netstat -tn|grep CLOSE_WAIT|wc\n{code}\n\nRemember the count.\n\nThen do the following where the file is any smallish MapFile up in hdfs:\n\n{code}\n./bin/hadoop org.apache.hadoop.io.MapFile /hbase123/hregion_-70236052/info/mapfiles/3026597445031962455 /x\n{code}\n\nPut it in background and do netstat every time it logs.\n\nOn open, you'll see two new CLOSE_WAITs, one for the data file and the other for the index.   They stick around through the append.  They go away on close of the map file.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-12T05:21:29.863+0000","updated":"2007-12-12T06:58:53.912+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551027","id":"12551027","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"> They go away on close of the map file.\n\nThis seems alright. I thought you meant you had connections in CLOSE_WAIT even after closing the file. Could you describe what the bug here is or what you expected to happen.\n\nHere is what I think happened and it is consistent with both your experiment and my understing of DFS:\nsay 'smallish' is 4k.\nMapFile.Reader() opens two DFS files and seek to some posision. \nSo DFSClient makes to connections for these files and asks the datanode(s) for data. \nDatanodes flush the data and all of it fits in receiver buffer, even if there is no read from the\napp. Datanode closes the connection resulting in CLOSE_WAIT on the DFSClient side.\n\nWhen MapFile reads, I guess it does not actually read past the EOF of either file so DFSClient does \nnot actually know that it reached EOF and so does not close the socket yet. \n\nThe socket is closed when the InputStream is closed.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-12T16:39:27.055+0000","updated":"2007-12-12T16:39:27.055+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551036","id":"12551036","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"I was just looking at it.  I agree w/ your description. \n\nHere's the problem I see in the current behavior.  See if you agree with me.\n\nStudying these CLOSE_WAITs over last few days, the client buffer shows 1 or 0 bytes in the queue.  At a minimum, I would expect that when client has read all of a block -- netstat shows queues of size 0 -- then the client should close its socket and free up datanode-side resources.  Queues with 1 byte only in them are suspicious.  It seems to be the most common socket stasis (You'll see this state if you run the above patch).   If I were to guess, the block math is either wrong or, since the state is common, could the client recognize this condition and pull in the lonely byte (closing socket afterward)?\n\nThese outstanding CLOSE_WAITs are an issue in hbase.  Our usage of hdfs is not like MRs where tasks march through MapFiles opening, next'ing and then closing moving on to the next file in the partition.  hbase holds open hundreds/thousands of MapFiles concurrently ready to field any random access the wiley client might throw at us.  Last night, Paul Saab did a billion plus upload into hbase and at one point on one data node, 300plus sockets were consumed showing the CLOSE_WAIT state.  HADOOP-2407 cuts that number in half but it seems reasonable that we can cut the number of consumed resources even further.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-12T17:16:00.963+0000","updated":"2007-12-12T17:16:00.963+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551059","id":"12551059","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":">Studying these CLOSE_WAITs over last few days, the client buffer shows 1 or 0 bytes in the queue. At a minimum, I would expect that when client has read all of a block - netstat shows queues of size 0 - then the client should close its socket and free up datanode-side resources.\n\nThis is ok. There are a few extra bytes in the stream at the end that indicate a proper end of stream to the client. A client would read that only when the client tries to read more. There is a BufferedInputStream between the socket and DFSClient to confuse the things more. \n\nOne could argue that DFSClient should try to read a few a bytes more that the user wants to read and close the socket... Thats a different issue. If we want to do this I think it should be done with a non-blocking read since we don't want user to wait more than what is required. Even if we did that I don't think it solves your problem, since this helps only if the random access is accessing at the end of a block. If you are accessing some other place, you would just be holding 64k (or 128k?) of precious kernel memory on both sides, only to throw away in next random access.\n\nI don't think streaming read is meant for random reads. I don't have much knowledge of access pattern or average size of reads in Hbase,  but did you look into using pread?\n\n> These outstanding CLOSE_WAITs are an issue in hbase. \n\nHow about idle connections? Is that ok?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-12T18:35:07.568+0000","updated":"2007-12-12T18:35:07.568+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551061","id":"12551061","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"body":"> How about idle connections? Is that ok?\n\nBecause datanode does not use select for io, each idle connection seems to consume a thread.  This gets expensive, not necessarily the connections themselves.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-12T18:40:12.633+0000","updated":"2007-12-12T18:40:12.633+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551066","id":"12551066","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"So an idle connection is more expensive than a connection in CLOSE_WAIT... I agree. Doesn't Hbase on DFS have both issues? Do you think at a given time HBase has a lot more connections in CLOSE_WAIT than idle? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-12T18:44:06.426+0000","updated":"2007-12-12T18:44:06.426+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551072","id":"12551072","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"body":"Well, hbase does use select, so it doesn't consume threads to have idle connections.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bien","name":"bien","key":"bien","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Bieniosek","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-12T18:53:47.418+0000","updated":"2007-12-12T18:53:47.418+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551076","id":"12551076","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Great. I would have thought tcp connections and their filled buffers would take more real memory than a thread.   \nCLOSE_WAIT connection don't consume any resources on Datanode.\n\nDo we still want to keep this jira open?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-12T19:01:50.246+0000","updated":"2007-12-12T19:01:50.246+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551214","id":"12551214","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"Pardon me Raghu, I don't follow what you mean by pread?  I presume you're not referring to the clib call and instead mean some java equiv.\n\nMapFile as-is works for hbase 'cos index eases random accesses.\n\nAt any given time, CLOSE_WAIT in client are minority; more usual is an idle open connection and yes, hbase has both issues.\n\nAbove I misspoke when I said an uploading last night was occupying 300+ connections all in CLOSE_WAIT.  It actually went over 900 open connections in the datanode.  I don't know how many were in CLOSE_WAIT (and yes, these CLOSE_WAITs are different from the client-side CLOSE_WAITS I've been going on about above). \n\nIs there an issue to move the datanode to use select that you know of?\n\nLets keep the issue open another while if you don't mind.  It would be good to have more confidence that we can account for all open sockets in the datanode before closing.  For example: the 900+ open connections in the datanode cited above seems excessive when I try counting how many open MapFiles there should have been present at the time.  Also, am having trouble matching blocked reads in client to blocked writes cited in thread dumps attached above.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-13T04:47:42.309+0000","updated":"2007-12-13T04:47:42.309+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551222","id":"12551222","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"\nSure. Please keep the Jira open if you think there is a bug.\n\npread I mentioned is FSInputStrea.read(long, buf, offset, len).\n\n> Is there an issue to move the datanode to use select that you know of?\n\nWhat does that fix? What is the problem it is causing now? It surely won't fix 900 (unaccounted) connections. IMHO reducing threads on Datanode just fixes just one of the symptoms and does not address the real issue of using streaming api to do sparse random reads.\n \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-13T05:45:07.497+0000","updated":"2007-12-13T05:45:07.497+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551240","id":"12551240","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":">>Is there an issue to move the datanode to use select that you know of?\n>What does that fix? \n\nI hope my comment not misunderstood. Of course using as few threads as required is better than using more. We should probably file a different jira if you want the feature any way. I am not sure if that is a fix for this jira since we don't know what the bug is yet. Also it needs async disk i/o along with async network i/o.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-13T07:07:24.681+0000","updated":"2007-12-13T07:07:24.681+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551580","id":"12551580","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> pread I mentioned is FSInputStream.read(long, buf, offset, len).\n\nMichael is calling MapFile#get().  That uses a buffered stream, since calls might sometimes be random, and they might sometimes be sequential, when enumerating a range of values.\n\nIt would certainly be nice if we could support keeping lots of HDFS files open without paying a large penalty on either the client or on the datanode.  Perhaps idle connections to datanodes should time out.  I think the existing client code would re-establish connections if the datanode were simply to close idle connections after a time.  Could that work?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-13T19:08:22.494+0000","updated":"2007-12-13T19:08:22.494+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12551597","id":"12551597","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"\n> Perhaps idle connections to datanodes should time out. \nI already filed HADOOP-2346.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-13T20:01:47.336+0000","updated":"2007-12-13T20:01:47.336+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12383842/comment/12553558","id":"12553558","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"body":"Resolving as incomplete.\n\nLast few uploads and internal use here have less open MapFiles because of changes made in hbase prompted by investigations around this issue.\n\nRegards datanode using select instead of a socket per, seems like folks are more leaning toward timing out idle connections as means of resource conservation. That'd work for hbase too.\n\nRegards the 900+ open mapfiles, let me find the illegitimate if any, and will open new issue if I find any.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"created":"2007-12-20T01:14:00.033+0000","updated":"2007-12-20T01:14:00.033+0000"}],"maxResults":40,"total":40,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-2341/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0igdj:"}}