{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12328376","self":"https://issues.apache.org/jira/rest/api/2/issue/12328376","key":"HADOOP-16","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12310812","id":"12310812","description":"","name":"0.1.0","archived":false,"released":true,"releaseDate":"2006-04-02"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2006-02-10T17:19:51.000+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Mar 08 08:49:48 UTC 2006","customfield_12310420":"80541","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_1194201000_*|*_6_*:*_1_*:*_0_*|*_5_*:*_2_*:*_3698619000_*|*_4_*:*_1_*:*_700969000","customfield_12312321":null,"resolutiondate":"2006-03-03T08:09:22.000+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-16/watchers","watchCount":1,"isWatching":false},"created":"2006-02-01T08:25:38.000+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12310812","id":"12310812","description":"","name":"0.1.0","archived":false,"released":true,"releaseDate":"2006-04-02"}],"issuelinks":[{"id":"12311791","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12311791","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12328268","key":"HADOOP-12","self":"https://issues.apache.org/jira/rest/api/2/issue/12328268","fields":{"summary":"InputFormat used in job must be in JobTracker classpath (not loaded from job JAR)","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2009-07-08T16:51:40.245+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"We've been using Nutch 0.8 (MapReduce) to perform some internet crawling. Things seemed to be going well until...\n\n060129 222409 Lost tracker 'tracker_56288'\n060129 222409 Task 'task_m_10gs5f' has been lost.\n060129 222409 Task 'task_m_10qhzr' has been lost.\n   ........\n   ........\n060129 222409 Task 'task_r_zggbwu' has been lost.\n060129 222409 Task 'task_r_zh8dao' has been lost.\n060129 222455 Server handler 8 on 8010 caught: java.net.SocketException: Socket closed\njava.net.SocketException: Socket closed\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)\n        at java.io.DataOutputStream.flush(DataOutputStream.java:106)\n        at org.apache.nutch.ipc.Server$Handler.run(Server.java:216)\n060129 222455 Adding task 'task_m_cia5po' to set for tracker 'tracker_56288'\n060129 223711 Adding task 'task_m_ffv59i' to set for tracker 'tracker_25647'\n\nI'm hoping that someone could explain why task_m_cia5po got added to tracker_56288 after this tracker was lost.\n\nThe Crawl .main process died with the following output:\n\n060129 221129 Indexer: adding segment: /user/crawler/crawl-20060129091444/segments/20060129200246\nException in thread \"main\" java.io.IOException: timed out waiting for response\n    at org.apache.nutch.ipc.Client.call(Client.java:296)\n    at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)\n    at $Proxy1.submitJob(Unknown Source)\n    at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)\n    at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)\n    at org.apache.nutch.indexer.Indexer.index(Indexer.java:263)\n    at org.apache.nutch.crawl.Crawl.main(Crawl.java:127)\n\nHowever, it definitely seems as if the JobTracker is still waiting for the job to finish (no failed jobs).\n\nDoug Cutting's response:\nThe bug here is that the RPC call times out while the map task is computing splits.  The fix is that the job tracker should not compute splits until after it has returned from the submitJob RPC.  Please submit a bug in Jira to help remind us to fix this.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12323473","id":"12323473","filename":"patch_h16.v0","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-28T08:46:46.000+0000","size":20272,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12323473/patch_h16.v0"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12322960","id":"12322960","filename":"patch.16","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-14T18:53:04.000+0000","size":2395,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12322960/patch.16"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"79273","customfield_12312823":null,"summary":"RPC call times out while indexing map task is computing splits","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=schmed","name":"schmed","key":"schmed","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Schneider","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=schmed","name":"schmed","key":"schmed","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Schneider","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"MapReduce multi-computer crawl environment: 11 machines (1 master with JobTracker/NameNode, 10 slaves with TaskTrackers/DataNodes)","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12365857","id":"12365857","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"body":"I'm seeing a similar problem with the latest code. I have a task that takes a lot of input files (currently ~1400, each several gigabytes each). The amount of time spent hitting hasTaskWithHit() for each task is exhorbitant, easily causing a timeout. This code does all of the work up front - if you had, say, 1400 tasks, and 2 taskrunners, you could easily dispatch the first 2xsimultaneous running tasks first, then go back and spend time calculating the rest of the matchups. Certainly, a lot of the steps in the job setup after listFiles() could be potentially slow for certain problem sizes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-10T17:19:51.000+0000","updated":"2006-02-10T17:19:51.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366022","id":"12366022","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=schmed","name":"schmed","key":"schmed","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Schneider","active":true,"timeZone":"America/Los_Angeles"},"body":"Everyone should probably be made aware of the strange behavior we see during indexing, at least for a relatively large number of large segments (topN=500K, depth=20) with a relatively large crawldb (50M URLs). Note that this was all performed with ipc.client.timeout set to 30 minutes.\n\nAfter launching the indexing job, the web UI shows all of the TaskTrackers, but the numbers in the \"Secs since heartbeat\" column just keep increasing. This goes on for about 10 minutes until the JobTracker finally loses all of them (and the tasks they were working on), as is shown in its log:\n\n060210 224115 parsing file:/home/crawler/nutch/conf/nutch-site.xml\n060210 225151 Lost tracker 'tracker_37064'\n060210 225151 Task 'task_m_4ftk58' has been lost.\n060210 225151 Task 'task_m_6ww2ri' has been lost.\n\n...(snip)...\n\n060210 225151 Task 'task_r_y6d190' has been lost.\n060210 225151 Lost tracker 'tracker_92921'\n060210 225151 Task 'task_m_9p24at' has been lost.\n\n...(etc)...\n\nAt this point, the web UI is still up, the job shows 0% complete, and the TaskTrackers table is empty. It goes on for an hour or so like this, during which any rational person would probably want to kill the job and start over.\n\nDon't do this! Keep the faith!!!\n\nAbout an hour later, the JobTracker magically reestablishes its connection to the TaskTrackers (which now have new names), as is shown in its log:\n\n060210 225151 Task 'task_r_yj3y3o' has been lost.\n060210 235403 Adding task 'task_m_k9u9a8' to set for tracker 'tracker_85874'\n060210 235404 Adding task 'task_m_pijt4q' to set for tracker 'tracker_61888'\n\n...(etc)...\n\nThe web UI also shows that the TaskTrackers are back (with their new names).\n\nThere's nothing in the TaskTracker logs during the initial 10 minutes, then a bunch of exiting and closing messages, until finally the TaskTrackers start \"Reinitializing local state\":\n\n060210 225403 Stopping server on 50050\n060210 230102 Server handler 4 on 50050: exiting\n\n...(snip)...\n\n060210 230105 Server handler 7 on 50050: exiting\n060210 232024 Server listener on port 50050: exiting\n060210 232403 Stopping server on 50040\n060210 234902 Server listener on port 50040: exiting\n060210 234925 Server connection on port 50040 from 192.168.1.5: exiting\n\n...(snip)...\n\n060210 235009 Server connection on port 50040 from 192.168.1.10: exiting\n060210 235013 Client connection to 192.168.1.4:50040: closing\n060210 235014 Client connection to 192.168.1.7:50040: closing\n060210 235015 Server connection on port 50040 from 192.168.1.7: exiting\n060210 235016 Server handler 0 on 50040: exiting\n\n...(snip)...\n\n060210 235024 Server handler 2 on 50040: exiting\n060210 235403 Reinitializing local state\n060210 235403 Server listener on port 50050: starting\n060210 235403 Server handler 0 on 50050: starting\n\n...(etc)...\n\nDuring the time that the TaskTrackers are lost, neither the master nor the slave machines seem to be using much of the CPU or RAM, and the DataNode logs are quiet. I suppose that it's probably I/O bound on the master machine, but even that seems mysterious to me. It would seem particularly inappropriate for the JobTracker to punt the TaskTrackers because the master was too busy to listen for their heartbeats.\n\nAt any rate, once the TaskTrackers go through the \"Reinitializing local state\" thing, the indexing job seems to proceed normally, and it eventually completes with no errors.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=schmed","name":"schmed","key":"schmed","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Schneider","active":true,"timeZone":"America/Los_Angeles"},"created":"2006-02-12T04:20:17.000+0000","updated":"2006-02-12T04:20:17.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366155","id":"12366155","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"body":"Looking further into my concern about hasTaskWithCacheHit overhead (called from obtainNewMapTask in JobInProgress).... it looks like obtainNewMapTask actually calls hasTaskWithCacheHit once for each map job, but always chooses the first choice - why even call the relatively expensive hasTaskWithCacheHit if you've already chosen a value for cacheTarget? Likewise, once you've filled in both cacheTarget and hasTarget, why not break out of that for loop entirely?\n\nAm I following the code flow incorrectly, or is obtainNewMapTask essentially making maps^2 RPC calls in response to a single incoming RPC call? Even if the jobtracker and namenode are running on the same host, that could easily explain the sensitivity to timeouts in the current code.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-13T16:22:46.000+0000","updated":"2006-02-13T16:22:46.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366235","id":"12366235","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"Bryan: I agree, this looks like a serious bug.  The TaskTracker should minimize the calls it makes to the NameNode.  Ideally it should only make a single call per job on each input split.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2006-02-14T04:17:00.000+0000","updated":"2006-02-14T04:17:00.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366316","id":"12366316","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"body":"\n  Here's a patch for the problem.  I changed two things.\n\n  1) The comment is right, there's no need to iterate through all choices in JobTracker\nafter we find a TaskInProgress to take the task.\n\n  2) Within a TaskInProgress (TIP), which tracks an individual split within a Job, we cache\nthe results of the getHints() call to the Distributed File System.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-14T18:53:04.000+0000","updated":"2006-02-14T18:53:04.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366317","id":"12366317","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"body":"\n  Sorry, my blurb above was a little unclear.\n\n  I should have said:\n\n  1)  Bryan's comment is right, we don't need to iterate through the whole\nlist in JobTracker's obtainNewMapTask call.  We now just do it until we\nfind a good cacheTarget or stdTarget value.\n\n  2) A TIP object tracks each individual split in the Job.  We cache\nthe data at each TIP.  This will be handy in case the TIP has to\nbe re-executed due to machine failure.  \n\n  I don't mind caching the hints aggressively, because it's \njust task-placement we're after.  If the hint is wrong (which only happens \nin case of machine failure), we might send the task to a suboptimal \nmachine.  No big deal.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-14T18:59:18.000+0000","updated":"2006-02-14T18:59:18.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366368","id":"12366368","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"body":"\n  Patch comitted.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-15T04:08:59.000+0000","updated":"2006-02-15T04:08:59.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366659","id":"12366659","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"body":"I'm not sure this patch does quite what's desired.\n\nIt looks like before it would try as hard as possible to find a task with a cache hit, then fail to running either just the first executable task. Now, it will search for the first task that's either a cache hit *or* executable..... In principle, wouldn't you only want to break out of the loop for finding a cache hit? This, of course, still causes timeouts - is there any way to actually precompute the list of cache hits, so that it's just a matter of picking a task from a priority queue?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-17T03:49:54.000+0000","updated":"2006-02-17T03:49:54.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366660","id":"12366660","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"body":"Of course, given the existing code, maybe it's worth trying a little harder - iterate through the list as before, but keep track of the time that's been taken, and give up if it gets to 1/2 of the RPC timeout, or something of the like.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-17T03:50:44.000+0000","updated":"2006-02-17T03:50:44.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12366680","id":"12366680","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"But, you're right, Bryan, I think this is still not optimal.  It should certainly check to see if more map tasks have local input data for the calling node before it gives up.  Ideally it should not be iterating through all map tasks either, but rather create a mapping of node -> mapTask* for tasks that have local data for a node.  We could keep a queue of tasks whose entries in this table have not yet been computed.  When we fail to find a task with local data for a node, then we can pop a few (10?) entries off the queue and enter them in the map, and if there are still no matches, just give the node a task with remote input data.  This way we'd avoid ever doing too much work in a single call, and ever iterating over all tasks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2006-02-17T05:59:32.000+0000","updated":"2006-02-17T05:59:32.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12367148","id":"12367148","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"body":"I have another idea.\n\nFirst, switch the delegation of responsiblity for job assignment. Right now, it happens in the JobTracker instance, in response to an obtainNewMapTask call. This scales very poorly. In particular, it causes the RPC-timeouts if you do any sort of serious work in obtainNewMapTask. There's another bug, just reported as HADOOP-43, which occurs if you spend too long in a RPC call, and also described in the second comment, above.\n\nSo, instead of having the JobTracker do this reactively, either:\n1) Precompute - probably most scalably done by starting a mini-job, which just computes the list of who has precached data from a given FileSplit.\n2) Compute on demand - as a TaskTracker job. This could work by some protocol of offering the TaskTracker a set of possible jobs it could do work on, letting it pick the ones it thinks are best, and return the remainders for assignment. This, of course, would only work well for instances where tasks >> tasktracker instances.\n\nI looked at implementing something like 1, but decided I think 2 is a much better option. 1 would put a lot more instantaneous demand on the namenode. Plus, once you've finished precomputing the best nodes, if nodes come or go you don't really have a solution. 2 seems to distribute both the work and some of the demand, and it makes it possible for the cluster to grow or shrink dramatically without failing to take advantage of the local storage available at each node. Unfortunately, without any pre-work, it's possible that, doing option 2, you'd pick bad subsets of work to distribute to each node, and get no local I/O improvement at all.\n\nI'd really like to see something done, perferably soon. With dozens of nodes, and hundreds of gbs of data in my current problem set, it's very nearly impossible to get the current code to make progress, without killing tasktrackers (some with lots of work units already completed). I can do some of the coding, if there's agreement for what direction to push.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-21T15:01:05.000+0000","updated":"2006-02-21T15:01:05.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12367400","id":"12367400","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"I'm re-opening this & assigning it to Mike.\n\nI think we can fix this by changing the TaskTracker to incrementally calculate things, as I alluded above.  The TaskTracker should avoid ever doing anything that might take very long.  RPC timeouts to the TaskTracker are very bad and must be avoided.\n\nWe can maintain a mapping of taskTracker->split, initially empty, and a queue of splits, initially filled with all of the splits.  (If basic split-generation is too expensive, since it calls file-length on each input file,, then we can eventually change the split API into a generator, which incrementally enumerates splits and use that in place of this queue.)  When a request for a task arrives from a tasktracker we can first examine the table.  If any splits are present for the calling tracker, then we return one and remove it from the table.  Otherwise we pop a constant number of splits (10?) off of the queue, enumerate the tasktrackers that host each split by calling the namenode, and add entries to the table.  Then we consult the table again.  In most cases (many more splits than tasktrackers) we should identify suitable splits.  If we fail then we assign a randomly selected, non-local split to the tasktracker.  Make sense?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2006-02-23T05:26:33.000+0000","updated":"2006-02-23T05:26:33.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12367417","id":"12367417","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"Eric Baldeschwieler wrote:\n> [...] why not just dedicate a thread  \n> to planning and then load a complete plan?  That can produce more  \n> optimal placement and a simpler to understand initialization sequence.\n\nA separate thread in the JobTracker?  That could be a good approach.  We'd have a queue of submitted but as-yet unplanned jobs.  The thread can then pop a job off the queue, compute its splits, then start populating a tasktracker->split table.  When tasktrackers poll for work they can consult this table, potentially while the thread is still populating it.\n\nI'm hesitant to move this out of the JobTracker into the TaskTracker, since that introduces complexity.  But a single thread in the JobTracker should be simple to add and should mostly solve this.  +1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2006-02-23T06:33:39.000+0000","updated":"2006-02-23T06:33:39.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12367421","id":"12367421","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"body":"My only concern is that this doesn't scale as well for really huge jobs, but, if implemented as you suggest (pre-filling, hand out non-matching jobs when you don't have matched jobs) seems like a reasonable trade off. \n\nI'd still think it'd be nice to better handle the case of newly-discovered tasktrackers, but that's probably much more minor than other scaling/responsiveness issues. Maybe a future enhancement for the JobTracker thread that does this work.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-23T07:03:31.000+0000","updated":"2006-02-23T07:03:31.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12367554","id":"12367554","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"body":"Having a separate thread in JobTracker can even pre-compute a data locality map that will enable optimal task assignment to task trackers.\nHere is the basic idea. As the thread computes the splits for a job, it can determine which nodes have local data for each split. The thread can populate \na map mapping a node to a  list of the splits whose data are on the node. When a tasktracker polls for a task, the JobTracker can look up the map by \nthe node name of the task tracker to see whether there are any un-assigned splits in the corresponding list. If yes,  assign one of them to the task tracker. \nOtherwise, randomlly choose a split for the task tracker.   ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-24T03:12:06.000+0000","updated":"2006-02-24T03:12:06.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12368047","id":"12368047","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"body":"\n  I created a separate thread on the JobTracker that handles file-splitting and\ngathering info about split-caches.  The job is placed into the PREP state\nuntil it is processed by this JobTracker thread.  After processing, it moves\ninto the RUNNING state.  \n\n  This should allow the hard split-work to continue but still allow the JobTracker\nto process heartbeats.  For now, the thread lives at the JobTracker but perhaps\nsomeday we'll move it to a TaskTracker thread.\n\n  I also fixed the alg for choosing a task-allocation given a tasktracker.\n(Whether cached, non-cached, or speculative.)\n\n  Let me know if this fixes some of the problems you've been seeing. \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=michael_cafarella","name":"michael_cafarella","key":"michael_cafarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mike Cafarella","active":true,"timeZone":"Etc/UTC"},"created":"2006-02-28T08:46:46.000+0000","updated":"2006-02-28T08:46:46.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12368622","id":"12368622","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"I applied Mike's patch, with a few small changes, fixing an NPE and ArrayOutOfBounds in dfs.\n\nThere's still a bug where, with mapred.tasktracker.tasks.maximum=2, only a single map and a single reduce are running on each node.  I believe the intent is that there should be up to two of each related to a single job, so that the reduce tasks can be copying data while the maps are still running.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2006-03-03T08:09:22.000+0000","updated":"2006-03-03T08:09:22.000+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12328376/comment/12369339","id":"12369339","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"body":"This is great, and finally fixes my issues with a large job that would never start.\n\nHowever, the way things are in this patch (and the current code), the job doesn't get started until the background thread finishes computing all of the cache hints. This takes far too long - it took 15 minutes on a recent run. During that time, of course, no other work was getting done. How about moving the cachedHints-filling-loop to the end of initTasks(), and go ahead and set the job to RUNNING and \"tasksInited=true\" in the meantime?\n\nDoing this locally lets work commence immediately, while the cache hints continue to get filled in for future task allocations.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bpendleton","name":"bpendleton","key":"bpendleton","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bryan Pendleton","active":true,"timeZone":"Etc/UTC"},"created":"2006-03-08T08:49:48.000+0000","updated":"2006-03-08T08:49:48.000+0000"}],"maxResults":18,"total":18,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-16/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0dwzb:"}}