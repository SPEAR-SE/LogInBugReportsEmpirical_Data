{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12391923","self":"https://issues.apache.org/jira/rest/api/2/issue/12391923","key":"HADOOP-3051","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2008-03-19T22:34:37.416+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Apr 08 05:45:33 UTC 2008","customfield_12310420":"125762","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_1724286177_*|*_6_*:*_1_*:*_0_*|*_5_*:*_1_*:*_720787859","customfield_12312321":null,"resolutiondate":"2008-04-08T21:13:31.993+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-3051/watchers","watchCount":1,"isWatching":false},"created":"2008-03-19T22:15:25.816+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12312913","id":"12312913","description":"","name":"0.17.0","archived":false,"released":true,"releaseDate":"2008-05-20"}],"issuelinks":[{"id":"12319860","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12319860","type":{"id":"12310010","name":"Incorporates","inward":"is part of","outward":"incorporates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310010"},"inwardIssue":{"id":"12392597","key":"HADOOP-3124","self":"https://issues.apache.org/jira/rest/api/2/issue/12392597","fields":{"summary":"DFS data node should not use hard coded 10 minutes as write timeout.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2009-07-08T16:42:58.578+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"I just ran an experiment with the latest nightly build hadoop-2008-03-15 available and after 2 minutes I'm getting a tons of \"java.io.IOException: Too many open files\" exceptions as shown here:\n\n{noformat} 2008-03-19 20:08:09,303 ERROR org.apache.hadoop.dfs.DataNode: \n141.30.xxx.xxx:50010:DataXceiver: java.io.IOException: Too many open files\n     at sun.nio.ch.IOUtil.initPipe(Native Method)\n     at sun.nio.ch.EPollSelectorImpl.<init>(Unknown Source)\n     at sun.nio.ch.EPollSelectorProvider.openSelector(Unknown Source)\n     at sun.nio.ch.Util.getTemporarySelector(Unknown Source)\n     at sun.nio.ch.SocketAdaptor.connect(Unknown Source)\n     at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1114)\n     at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:956)\n     at java.lang.Thread.run(Unknown Source){noformat}\n\nI ran the same experiment with same high workload (50 dfs clients with 40 streams each writing concurrently files on a 8 nodes DFS cluster) with the 0.16.1 release and no exception is thrown. So it looks like a bug to me...","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"105331","customfield_12312823":null,"summary":"DataXceiver: java.io.IOException: Too many open files","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andremartin","name":"andremartin","key":"andremartin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"André Martin","active":true,"timeZone":"Europe/Berlin"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andremartin","name":"andremartin","key":"andremartin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"André Martin","active":true,"timeZone":"Europe/Berlin"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12580590","id":"12580590","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"What is the fd limit you have for your JVM? 0.17 uses NIO sockets. Looks like JVM uses selector to wait for connect, and each selector in Java eats up 3 extra fds :(. DataNode will also use selectors if it needs to wait on sockets..\n\n> (50 dfs clients with 40 streams each writing concurrently files on a 8 nodes DFS cluster) \n2000 writes across 8 datanodes? \n\nyes, 0.17 eats more file descriptors, especially with loads that result in a lot of threads waiting on sockets (as opposed to disk i/o). If this is big problem we might default to not using extra fds and provide a config option to enable it. One would enable such an option if 'write timout' is required on sockets (HADOOP-2346).\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-03-19T22:34:37.416+0000","updated":"2008-03-19T22:35:22.305+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12580596","id":"12580596","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andremartin","name":"andremartin","key":"andremartin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"André Martin","active":true,"timeZone":"Europe/Berlin"},"body":"fd limit is 1024 according to \"ulimit -aH\"\n\nexactly 2000 writes across 8 datanodes...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andremartin","name":"andremartin","key":"andremartin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"André Martin","active":true,"timeZone":"Europe/Berlin"},"created":"2008-03-19T22:49:07.139+0000","updated":"2008-03-19T22:49:07.139+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12580603","id":"12580603","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"> fd limit is 1024 \n\nSeems quite low for a systems applications that are io intensive like this. 1024 probably made sense years ago. \n\nEven with 16, you are close to the limit. Assuming the replication is 3, at any time the datanodes are writing 3 * 2000 blocks => each datanode is writing 750 blocks. With uniform distribution, each block write takes 2.66 fds => each datanode needs 2000 fds.\n\nIrrespective the limit, looks like most users may not want 'write timeout'. May be by default HDFS should not make DataNode take more fds  than before (may be DFSClient too).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-03-19T23:03:18.376+0000","updated":"2008-03-19T23:03:18.376+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12580942","id":"12580942","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andremartin","name":"andremartin","key":"andremartin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"André Martin","active":true,"timeZone":"Europe/Berlin"},"body":"I increased the fd limit to 4096 :-) It performs way better now...\n\nAnyway I would recommend to set the default to \"not to use extra fds\" since I assume that a bunch of other users will have the same experience when upgrading from earlier releases...?!?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andremartin","name":"andremartin","key":"andremartin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"André Martin","active":true,"timeZone":"Europe/Berlin"},"created":"2008-03-20T22:02:18.629+0000","updated":"2008-03-20T22:02:18.629+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12580953","id":"12580953","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"\nEven 4k is not very large. I would certainly recommend higher number.\n\nI am marking this jira for 0.17.  I need to add a config option and a unit test so that non-default value works as expected.\n\nbtw, anyone has an idea how much it would take to implement a custom (bare bones) Selector, extending AbstractSelector?. If we could just use regular poll() (instead of epoll) and not support wakeUp(), then selector will not require any fds.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-03-20T22:36:59.573+0000","updated":"2008-03-20T22:36:59.573+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12581158","id":"12581158","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"I am seeing this exception on current trunk running TestCrcCorruption w/ 1024 descriptors (the default).  We should certainly be able to pass unit tests with 1024 descriptors, no?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-03-21T18:46:31.389+0000","updated":"2008-03-21T18:46:31.389+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12581163","id":"12581163","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"yes, since there is no reason to think TestCrcCorruption keeps lots of files open. I am trying to see why hundreds of datanode threads are waiting on socket reads.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-03-21T18:53:11.264+0000","updated":"2008-03-21T18:53:11.264+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12581183","id":"12581183","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"> I am seeing this exception on current trunk running TestCrcCorruption w/ 1024 descriptors (the default).\nThis will be fixed in HADOOP-3067.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-03-21T20:34:38.325+0000","updated":"2008-03-21T20:34:38.325+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12391923/comment/12586661","id":"12586661","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Looks like this is handled by HADOOP-3124, which adds a configuration variable that lets using standard sockets in DataNode. But default behavior uses NIO sockets.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-04-08T05:45:33.042+0000","updated":"2008-04-08T05:45:33.042+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-3051/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0idt3:"}}