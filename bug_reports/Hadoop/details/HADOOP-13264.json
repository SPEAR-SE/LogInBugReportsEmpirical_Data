{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12978456","self":"https://issues.apache.org/jira/rest/api/2/issue/12978456","key":"HADOOP-13264","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2016-06-13T22:35:54.104+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Aug 11 16:41:43 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_5077388427_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2016-08-11T16:41:43.238+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-13264/watchers","watchCount":6,"isWatching":false},"created":"2016-06-13T22:18:35.355+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332809","id":"12332809","description":"2.7.2 release","name":"2.7.2","archived":false,"released":true,"releaseDate":"2016-01-25"}],"issuelinks":[{"id":"12477422","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12477422","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12980693","key":"HDFS-10549","self":"https://issues.apache.org/jira/rest/api/2/issue/12980693","fields":{"summary":"Correctly revoke file leases when closing files","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12469597","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12469597","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12976961","key":"HDFS-10504","self":"https://issues.apache.org/jira/rest/api/2/issue/12976961","fields":{"summary":"DFSClient filesBeingWritten memory leak when client gets RemoteException - could only be replicated to 0 nodes instead of minReplication (=1)","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-08-11T16:42:08.190+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Using:\nhadoop-hdfs\\2.7.2\\hadoop-hdfs-2.7.2-sources.jar!\\org\\apache\\hadoop\\hdfs\\DFSOutputStream.java\n\nClose method fails when the client can't connect to any data nodes. When re-using the same DistributedFileSystem in the same JVM, if all the datanodes can't be accessed, then this causes a memory leak as the DFSClient#filesBeingWritten map is never cleared after that.\n\nSee test program provided by [~sebyonthenet] in comments below.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hadoop HDFS - DFSOutputStream close method fails to clean up resources in case no hdfs datanodes are accessible ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sebyonthenet","name":"sebyonthenet","key":"sebyonthenet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seb Mo","active":true,"timeZone":"America/Phoenix"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sebyonthenet","name":"sebyonthenet","key":"sebyonthenet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seb Mo","active":true,"timeZone":"America/Phoenix"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12978456/comment/15328425","id":"15328425","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sebyonthenet","name":"sebyonthenet","key":"sebyonthenet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seb Mo","active":true,"timeZone":"America/Phoenix"},"body":"Linked to the related ticket on which a sudden failure causes the JVM to die with OutOfMemory. In that test I was trying to migrate a couple of million files while the datanode ran out of diskspace. This caused the client to die, although I was calling close on the stream (using java8 close with resource), but since that failed, the mentioned map caused my JVM to die.\n\nCreated a simple test that attempts a write while the datanode is down. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sebyonthenet","name":"sebyonthenet","key":"sebyonthenet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seb Mo","active":true,"timeZone":"America/Phoenix"},"created":"2016-06-13T22:22:30.713+0000","updated":"2016-06-13T23:31:53.579+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12978456/comment/15328453","id":"15328453","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"Test code from [~sebyonthenet] (moving from description)\n\n{code}\n\nHere is a test program:\n\n\tpublic static void main(String args[]) throws Exception\n\t{\n\t\tfinal Configuration conf = new Configuration();\n\t\tconf.addResource(new FileInputStream(new File(\"core-site.xml\")));\n\t\tconf.addResource(new FileInputStream(new File(\"hdfs-site.xml\")));\n\n\t\tfinal DistributedFileSystem newFileSystem = (DistributedFileSystem)FileSystem.get(conf);\n\t\tOutputStream outputStream = null;\n\t\ttry\n\t\t{\n\t\t\toutputStream = newFileSystem.create(new Path(\"/user/ssmogos\", \"test1\"));\n\t\t\toutputStream.write(\"test\".getBytes());\n\t\t}\n\t\tcatch (IOException e)\n\t\t{\n\t\t\te.printStackTrace();//don't care about this\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\ttry\n\t\t\t{\n\t\t\t\tif (outputStream != null)\n\t\t\t\t\toutputStream.close();//now this one will fail to close the stream\n\t\t\t}\n\t\t\tcatch (IOException e)\n\t\t\t{\n\t\t\t\te.printStackTrace();//this will list the thrown exception from DFSOutputStream->flushInternal->checkClosed\n\t\t\t\t//TODO the DFSOutputStream#close->dfsClient.endFileLease(fileId) is never getting closed\n\t\t\t}\n\t\t}\n\n\t\tField field = DFSClient.class.getDeclaredField(\"filesBeingWritten\");\n\t\tfield.setAccessible(true);\n\t\tSystem.out.print(\"THIS SHOULD BE EMPTY: \" + field.get(newFileSystem.getClient()));\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-13T22:35:54.104+0000","updated":"2016-06-13T22:35:54.104+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12978456/comment/15329536","id":"15329536","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"body":"It looks like a dupe of HDFS-9812. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"created":"2016-06-14T13:54:41.050+0000","updated":"2016-06-14T13:54:41.050+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12978456/comment/15330278","id":"15330278","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sebyonthenet","name":"sebyonthenet","key":"sebyonthenet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seb Mo","active":true,"timeZone":"America/Phoenix"},"body":"Thanks [~kihwal], but that fix does not seem to address this problem.\n\nThe DFSOutpuStream#close()->closeImlp()->flushInternal() ->checkClosed() call still throws the lastException.get(), so going back on the stack to the DFSOutputStream#close, the dfsClient.endFileLease(fileId) still does not get called due to the thrown exception in the checkClosed method.\n\nJust to make sure, I've syned the 2.7 branch and built the latest 2.7.3 on my box and re-running my test still shows the problem being present, filesBeingWritten still keeps a reference to the stream that could not be closed. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sebyonthenet","name":"sebyonthenet","key":"sebyonthenet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Seb Mo","active":true,"timeZone":"America/Phoenix"},"created":"2016-06-14T19:50:32.613+0000","updated":"2016-06-14T20:26:45.758+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12978456/comment/15331932","id":"15331932","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"body":"Since it is closely related to HDFS-9812, [~linyiqun], can you take a look at this?  ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"created":"2016-06-15T15:39:27.167+0000","updated":"2016-06-15T15:39:27.167+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12978456/comment/15332987","id":"15332987","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"I think this problem is different with HDFS-9812. HDFS-9812 solved the problem that {{datastreamer}} thread not closed when failures happened in flushing data, and these logic was done in {{closeImpl}}. In this probloem, if the method {{closeImpl}} threw the IOException, the dfsClient.endFileLease(fileId) will not be called. If we want to fix this, I suggest that we would be better  to keep the synchronized block code, like this:\n\n{code}\n  public void close() throws IOException {\n    boolean threwException = false;\n    synchronized (this) {\n      try (TraceScope ignored =\n          dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src)) {\n        closeImpl();\n      } catch (IOException ioe) {\n        threwException = true;\n      }\n    }\n    dfsClient.endFileLease(fileId);\n    if (threwException) {\n      throw new IOException(\"Exception happened in closing the output stream.\");\n    }\n  }\n{code}\n\nCorrect me if I am wrong, thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-06-16T03:19:53.313+0000","updated":"2016-06-16T03:19:53.313+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12978456/comment/15333042","id":"15333042","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"I have looked the code, there were some other places will not release resources associated with stream. Like in {{DFSOutputStream#abort}}, {{DFSStripedOutputStream#abort}}. We could make all fixed in this jira. Who can assign this to me, I'd like to post a patch for this. In addition, this jira seems better to transform to HDFS ranther Hadoop Common.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-06-16T04:10:51.537+0000","updated":"2016-06-16T04:12:46.893+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12978456/comment/15417548","id":"15417548","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"I'm closing this as a dup of HDFS-10549, since [~linyiqun] is working on there and the change is in HDFS.\n\nThanks [~sebyonthenet] and all for the work here, let's follow up on HDFS-10549.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-11T16:41:43.347+0000","updated":"2016-08-11T16:41:43.347+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-13264/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2zep3:"}}