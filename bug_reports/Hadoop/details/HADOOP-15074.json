{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13121609","self":"https://issues.apache.org/jira/rest/api/2/issue/13121609","key":"HADOOP-15074","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-11-30T10:04:21.857+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jun 26 03:15:44 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-15074/watchers","watchCount":7,"isWatching":false},"created":"2017-11-29T10:35:45.650+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shashikant","name":"shashikant","key":"shashikant","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shashikant Banerjee","active":true,"timeZone":"Asia/Kolkata"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-06-26T03:15:44.540+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"SequenceFile#Writer flush does not update the length of the file. This happens because as part of the flush, {{UPDATE_LENGTH}} flag is not passed to the DFSOutputStream#hsync.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"SequenceFile#Writer flush does not update the length of the written file.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=msingh","name":"msingh","key":"msingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mukul Kumar Singh","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=msingh","name":"msingh","key":"msingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mukul Kumar Singh","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16272463","id":"16272463","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"you mean after a flush/sync the length of the file from a listing/getFileStatus hasn't changed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-11-30T10:04:21.857+0000","updated":"2017-11-30T10:04:21.857+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16278559","id":"16278559","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=msingh","name":"msingh","key":"msingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mukul Kumar Singh","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi [~stevel@apache.org], Yes, the length of the file wasn't updated after a hsync/hflush on the writer. This happens because the update length isn't passed as part of the flush/sync. Hence the length isn't updated as part of the sync request.\r\n\r\n{code}\r\n  @Override\r\n  public void hsync() throws IOException {\r\n    try (TraceScope ignored = dfsClient.newPathTraceScope(\"hsync\", src)) {\r\n      flushOrSync(true, EnumSet.noneOf(SyncFlag.class));\r\n    }\r\n  }\r\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=msingh","name":"msingh","key":"msingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mukul Kumar Singh","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-12-05T13:44:38.649+0000","updated":"2017-12-05T13:44:38.649+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16406096","id":"16406096","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shashikant","name":"shashikant","key":"shashikant","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shashikant Banerjee","active":true,"timeZone":"Asia/Kolkata"},"body":"I propose the following change in the DFSOutputStream#hsync api to ensure the file size gets updated at the namenode when hsync completes.\r\n{code:java}\r\n@Override\r\npublic void hsync() throws IOException {\r\n   try (TraceScope ignored = dfsClient.newPathTraceScope(\"hsync\", src)) {\r\n      flushOrSync(true, EnumSet.of(SyncFlag.UPDATE_LENGTH));\r\n   }\r\n}{code}\r\n[~arpitagarwal]/others, please let me know your opinion on this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shashikant","name":"shashikant","key":"shashikant","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shashikant Banerjee","active":true,"timeZone":"Asia/Kolkata"},"created":"2018-03-20T10:35:26.583+0000","updated":"2018-03-20T10:35:26.583+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16408715","id":"16408715","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Shashikant, I don't think we should change the default behavior of {{DFSOutputStream#hsync(void)}}. There is a {{DFSOutputStream#hsync(EnumSet<SyncFlag> syncFlags)}} overload that optionally takes sync flags. \r\n\r\nIf the desired behavior is for {{Writer#hsync}} to update the length on the NameNode, then it should pass the UPDATE_LENGTH flag. i.e. make the change in SequenceFileWriter.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-03-21T22:42:41.062+0000","updated":"2018-03-21T22:42:41.062+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16518779","id":"16518779","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harishjp","name":"harishjp","key":"harishjp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=harishjp&avatarId=27096","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=harishjp&avatarId=27096","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=harishjp&avatarId=27096","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=harishjp&avatarId=27096"},"displayName":"Harish Jaiprakash","active":true,"timeZone":"Asia/Kolkata"},"body":"[~arpitagarwal], SequenceFileWriter depends upon FSDataOutputStream object returned by FileSystem.create/append, which does not expose the hsync(EnumSet<SyncFlag> syncFlags). The DFSOutputStream gets wrapped into a FSDataOutputStream so it's not possible to fix in SequenceFile.\r\n\r\nThis bug makes it hard to implement producer / consumer using sequence files. We are a bit stuck on this. When would length get persisted, if hsync is never called with UPDATE_LENGTH? Is there like a periodic update of length or udpate when a block is full and written or only when close is called?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harishjp","name":"harishjp","key":"harishjp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=harishjp&avatarId=27096","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=harishjp&avatarId=27096","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=harishjp&avatarId=27096","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=harishjp&avatarId=27096"},"displayName":"Harish Jaiprakash","active":true,"timeZone":"Asia/Kolkata"},"created":"2018-06-21T01:21:25.825+0000","updated":"2018-06-21T01:21:25.825+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16519699","id":"16519699","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~harishjp], can you check whether the returned {{FSDataOutputStream}} is an instance of {{HdfsDataOutputStream}}. If so, then you can invoke _hsync(EnumSet<SyncFlag> syncFlags)_ on it. HdfsDataOutputStream is a public interface.\r\n\r\nI don't usually like suggesting reflection, but in this case it may be the cleanest solution.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-06-21T19:21:14.890+0000","updated":"2018-06-21T19:21:14.890+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16519709","id":"16519709","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"I don't like that, HdfsDataOutputStream is in hdfs-client JAR; you can't loop in hadoop-common. \r\n\r\nIn HADOOP-15229 we've discussed having an FS builder API to creating files; changing the hsync behaviour would be an option\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2018-06-21T19:33:11.104+0000","updated":"2018-06-21T19:33:11.104+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16521184","id":"16521184","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"My mistake: the builder API is for creating files, its for opening on the todo list.\r\n\r\nIf HDFS added an option like \"hdfs:update-length-on-hflush\", it could be picked up and used by sequence file.\r\n\r\nI'm a bit reluctant to go this way though, as there's a risk it gets used widely, and that may be bad for NN performance.\r\n\r\nI've seen code in the timeline service which does this by remembering the last length of a file, then opening it, attempting to seek past, and, if it can do that, read in the new stuff. Ugly, but works today","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2018-06-23T14:58:30.699+0000","updated":"2018-06-23T14:58:30.699+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16522778","id":"16522778","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. I don't like that, HdfsDataOutputStream is in hdfs-client JAR; you can't loop in hadoop-common.\r\n[~stevel@apache.org], sorry I didn't get what you mean. The change I suggested doesn't need any jar dependency changes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-06-25T20:33:32.685+0000","updated":"2018-06-25T20:33:32.685+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13121609/comment/16523129","id":"16523129","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"Never mind, I think I understand what you mean now. There is no way to make this change without exposing update_length flag via FileSystem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-06-26T03:15:44.540+0000","updated":"2018-06-26T03:15:44.540+0000"}],"maxResults":10,"total":10,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-15074/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3nc33:"}}