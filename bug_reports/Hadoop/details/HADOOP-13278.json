{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12979535","self":"https://issues.apache.org/jira/rest/api/2/issue/12979535","key":"HADOOP-13278","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310240","id":"12310240","key":"HADOOP","name":"Hadoop Common","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-06-15T21:33:21.965+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Aug 29 17:34:32 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-13278/watchers","watchCount":8,"isWatching":false},"created":"2016-06-15T21:31:16.597+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12479333","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12479333","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"inwardIssue":{"id":"13002138","key":"HADOOP-13572","self":"https://issues.apache.org/jira/rest/api/2/issue/13002138","fields":{"summary":"fs.s3native.mkdirs does not work if the user is only authorized to a subdirectory","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12527345","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12527345","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13131574","key":"HADOOP-15176","self":"https://issues.apache.org/jira/rest/api/2/issue/13131574","fields":{"summary":"Enhance IAM Assumed Role support in S3A client","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}},{"id":"12536486","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12536486","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13166248","key":"HADOOP-15542","self":"https://issues.apache.org/jira/rest/api/2/issue/13166248","fields":{"summary":"S3AFileSystem - FileAlreadyExistsException when prefix is a file and part of a directory tree","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12470519","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12470519","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12974195","key":"HADOOP-13222","self":"https://issues.apache.org/jira/rest/api/2/issue/12974195","fields":{"summary":"s3a.mkdirs() to delete empty fake parent directories","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}},{"id":"12542031","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12542031","type":{"id":"10001","name":"dependent","inward":"is depended upon by","outward":"depends upon","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10001"},"inwardIssue":{"id":"13173308","key":"HADOOP-15620","self":"https://issues.apache.org/jira/rest/api/2/issue/13173308","fields":{"summary":"Ãœber-jira: S3a phase VI: Hadoop 3.3 features","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-08-29T17:37:35.517+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12311814","id":"12311814","name":"fs/s3","description":"S3A filesystem client and other S3 connectivity issues"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12319643","id":"12319643","name":"tools","description":"Hadoop tools"}],"timeoriginalestimate":null,"description":"According to S3 semantics, there is no conflict if a bucket contains a key named {{a/b}} and also a directory named {{a/b/c}}. \"Directories\" in S3 are, after all, nothing but prefixes.\n\nHowever, the {{mkdirs}} call in {{S3AFileSystem}} does go out of its way to traverse every parent path component for the directory it's trying to create, making sure there's no file with that name. This is suboptimal for three main reasons:\n\n * Wasted API calls, since the client is getting metadata for each path component \n * This can cause *major* problems with buckets whose permissions are being managed by IAM, where access may not be granted to the root bucket, but only to some prefix. When you call {{mkdirs}}, even on a prefix that you have access to, the traversal up the path will cause you to eventually hit the root bucket, which will fail with a 403 - even though the directory creation call would have succeeded.\n * Some people might actually have a file that matches some other file's prefix... I can't see why they would want to do that, but it's not against S3's rules.\n\nI've opened a pull request with a simple patch that just removes this portion of the check. I have tested it with my team's instance of Spark + Luigi, and can confirm it works, and resolves the aforementioned permissions issue for a bucket on which we only had prefix access.\n\nThis is my first ticket/pull request against Hadoop, so let me know if I'm not following some convention properly :)","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"S3AFileSystem mkdirs does not need to validate parent path components","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=apetresc","name":"apetresc","key":"apetresc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=apetresc&avatarId=18532","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=apetresc&avatarId=18532","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=apetresc&avatarId=18532","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=apetresc&avatarId=18532"},"displayName":"Adrian Petrescu","active":true,"timeZone":"America/New_York"},"subtasks":[{"id":"13123671","key":"HADOOP-15102","self":"https://issues.apache.org/jira/rest/api/2/issue/13123671","fields":{"summary":"HADOOP-14831","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=apetresc","name":"apetresc","key":"apetresc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=apetresc&avatarId=18532","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=apetresc&avatarId=18532","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=apetresc&avatarId=18532","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=apetresc&avatarId=18532"},"displayName":"Adrian Petrescu","active":true,"timeZone":"America/New_York"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/15332628","id":"15332628","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"GitHub user apetresc opened a pull request:\n\n    https://github.com/apache/hadoop/pull/100\n\n    HADOOP-13278. S3AFileSystem mkdirs does not need to validate parent path components\n\n    According to S3 semantics, there is no conflict if a bucket contains a key named `a/b` and also a directory named `a/b/c`. \"Directories\" in S3 are, after all, nothing but prefixes.\n    \n    However, the `mkdirs` call in `S3AFileSystem` does go out of its way to traverse every parent path component for the directory it's trying to create, making sure there's no file with that name. This is suboptimal for three main reasons:\n    \n     * Wasted API calls, since the client is getting metadata for each path component \n     * This can cause *major* problems with buckets whose permissions are being managed by IAM, where access may not be granted to the root bucket, but only to some prefix. When you call `mkdirs`, even on a prefix that you have access to, the traversal up the path will cause you to eventually hit the root bucket, which will fail with a 403 - even though the directory creation call would have succeeded.\n     * Some people might actually have a file that matches some other file's prefix... I can't see why they would want to do that, but it's not against S3's rules.\n    \n    [I've opened a ticket](https://issues.apache.org/jira/browse/HADOOP-13278) on the Hadoop JIRA. This  pull request is a simple patch that just removes this portion of the check. I have tested it with my team's instance of Spark + Luigi, and can confirm it works, and resolves the aforementioned permissions issue for a bucket on which we only had prefix access.\n    \n    This is my first ticket/pull request against Hadoop, so let me know if I'm not following some convention properly :)\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/rubikloud/hadoop s3a-root-path-components\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/hadoop/pull/100.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #100\n    \n----\ncommit 8a28062d34e5f0c0b83a9577dc9d818bab58c269\nAuthor: Adrian Petrescu <apetresc@gmail.com>\nDate:   2016-06-15T14:15:21Z\n\n    No need to check parent path components when creating a directory.\n    \n    Given S3 semantics, there's actually no problem with having a/b/c be a prefix even if\n    a/b or a is already a file. So there's no need to check for it - it wastes API calls\n    and can lead to problems with access control if the caller only has permissions\n    starting at some prefix.\n\n----\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2016-06-15T21:33:21.965+0000","updated":"2016-06-15T21:33:21.965+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/15332668","id":"15332668","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. This is my first ticket/pull request against Hadoop, so let me know if I'm not following some convention properly \n\n[~apetresc], thank you very much for participating in Apache Hadoop!.  Please see our [HowToContribute|https://wiki.apache.org/hadoop/HowToContribute] wiki page for more details on how the contribution process works.  If you're interested in working on S3A, then also please pay particular attention to the section on [submitting patches against object store|https://wiki.apache.org/hadoop/HowToContribute#Submitting_patches_against_object_stores_such_as_Amazon_S3.2C_OpenStack_Swift_and_Microsoft_Azure].  That section discusses our requirements for integration testing of patches against the back-end services (S3, Azure Storage, etc.).\n\nI understand the motivation for the proposed change, but I have to vote -1, because it would violate the semantics required of a Hadoop-compatible file system.  The patch would allow a directory to be created as a descendant of a file, which works against expectations of applications in the Hadoop ecosystem.  More concretely, the patch causes a test failure in {{TestS3AContractMkdir#testMkdirOverParentFile}}, which tests for exactly this condition.  (See below.)\n\nHowever, you might be interested to know that there is a lot of other work in progress on hardening and optimizing S3A.  This is tracked in issues HADOOP-11694 and HADOOP-13204, and their sub-tasks.\n\n{code}\nRunning org.apache.hadoop.fs.contract.s3a.TestS3AContractMkdir\nTests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 11.502 sec <<< FAILURE! - in org.apache.hadoop.fs.contract.s3a.TestS3AContractMkdir\ntestMkdirOverParentFile(org.apache.hadoop.fs.contract.s3a.TestS3AContractMkdir)  Time elapsed: 1.924 sec  <<< FAILURE!\njava.lang.AssertionError: mkdirs did not fail over a file but returned true; ls s3a://cnauroth-test-aws-s3a/test/testMkdirOverParentFile[00] S3AFileStatus{path=s3a://cnauroth-test-aws-s3a/test/testMkdirOverParentFile; isDirectory=false; length=1024; replication=1; blocksize=33554432; modification_time=1466026655000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} isEmptyDirectory=false\n\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.apache.hadoop.fs.contract.AbstractContractMkdirTest.testMkdirOverParentFile(AbstractContractMkdirTest.java:95)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\n{code}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-15T21:50:01.881+0000","updated":"2016-06-15T21:50:01.881+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/15333584","id":"15333584","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Adrian: I recognise your disappointment here, and the cost of those checks.\n\n# it may be possible to do some aggregation of the checks, that is, if there was a way to do a single listObjects with a pattern of the entire path,then perhaps the parent path scan could be O(1), after which the rejection of operations where a parent is a file would be straightforward. \n\nActually, all we need to do is check for the pure file paths \"/a\", \"a/b\"; the fake directories \"/a/\", \"/a/b/\" are allowed, and after the creation an async deleteObjects call could clean them up, HADOOP-13222. That is something I could see working. It would still be O(path-elements), but the maximum number of operations would be reduced by a third.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2016-06-16T11:16:24.137+0000","updated":"2016-06-16T11:16:24.137+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/15333590","id":"15333590","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"BTW, can you make that root bucket problem a separate issue? That sounds an independent problem","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2016-06-16T11:20:27.361+0000","updated":"2016-06-16T11:20:27.361+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/15333823","id":"15333823","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=apetresc","name":"apetresc","key":"apetresc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=apetresc&avatarId=18532","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=apetresc&avatarId=18532","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=apetresc&avatarId=18532","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=apetresc&avatarId=18532"},"displayName":"Adrian Petrescu","active":true,"timeZone":"America/New_York"},"body":"Hey [~cnauroth] and [~stevel@apache.org]; thank you for the very detailed explanations. I can totally understand that Hadoop-compatible filesystems may have additional semantics beyond just those of the underlying object store, which this change would violate.\n\nI do want to note, though, that the main motivation behind my patch was the permissions-related issue it was solving, not the minor performance improvement. That issue still exists, and I'll file a new issue for it as [~stevel@apache.org] recommended, but first I just want to understand whether it is even possible/desirable for S3A to support buckets which have those kinds of IAM roles applied.\n\nConsider a bucket with a policy like the one below (which is a pretty common setup):\n{code}\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"prodS3EnvironmentData\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::mybucket/a/b/c/*\"\n            ]\n        }\n    ]\n}\n{code}\n\nGiven what I now understand about the S3A contract, it seems like you simply cannot layer an S3A filesystem on this bucket, even if it was \"rooted\" in directory {{a/b/c}}, since any directory, say {{a/b/c/d}} could never validate whether a file named {{a/b}} exists - it would throw a 403.\n\nIs this correct? If so, I'm not sure a separate issue is needed; the use case would simply be unsupported and I'll have to move my S3A filesystem to a bucket that grants Hadoop/Spark root access.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=apetresc","name":"apetresc","key":"apetresc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=apetresc&avatarId=18532","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=apetresc&avatarId=18532","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=apetresc&avatarId=18532","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=apetresc&avatarId=18532"},"displayName":"Adrian Petrescu","active":true,"timeZone":"America/New_York"},"created":"2016-06-16T13:50:10.581+0000","updated":"2016-06-16T13:50:10.581+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/15334120","id":"15334120","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"[~apetresc], I admit I hadn't considered interaction with IAM policies before, but I definitely see how this could be useful, and it's interesting to think about it.  Unfortunately, I don't see a viable way to satisfy the full range of possible authorization requirements that users have come to expect from a file system.\n\nFor the specific case that we started talking about here (walking up the ancestry to verify that there are no pre-existing files), it might work if that policy was changed slightly, so that the user was granted full access to /a/b/c/\\*, and also granted read-only access to /\\*.  I expect read access would be sufficient for the ancestry-checking logic.  Of course, if you also want to block read access to /, then this policy wouldn't satisfy the requirement.  It would only block write access on /.\n\nAnother consideration is handling of what we call a \"fake directory\", which is a pure metadata object used to indicate the presence of an empty directory.  For example, consider an administrator allocating a bucket, bootstrapping the initial /a/b/c directory structure by running mkdir, and then applying the policy I described above.  At this point, S3A has persisted /a/b/c to the bucket as what we call a \"fake directory\", which is a pure metadata object that indicates the presence of an empty directory.  After the first file put, say /a/b/c/d, S3A no longer needs that pure metadata object to indicate the presence of the directory.  Instead, the directory exists implicitly via the existence of the file /a/b/c/d.  At that point, S3A would clean up the fake directory by deleting /a/b/c.  That implies the user would need to be granted delete access to /a/b/c itself, not just /a/b/c/*.  Now if we further consider the user deleting /a/b/c/d after that, then S3A needs to recreate the fake directory at /a/b/c, so the user is going to need put access on /a/b/c.\n\nbq. Is this correct? If so, I'm not sure a separate issue is needed; the use case would simply be unsupported and I'll have to move my S3A filesystem to a bucket that grants Hadoop/Spark root access.\n\nDefinitely the typical usage is to dedicate the whole bucket to persistence of a single S3A file system, with the understanding of the authorization limitations that come with that.  Anyone who has credentials to access the bucket effectively has full access to that whole file system.  This is a known limitation, and it's common to other object store file systems like WASB too.  I'm not aware of anyone trying to use IAM policies to restrict access to a sub-tree.  Certainly it's not something we actively test within the project right now, so in that sense, it's unsupported and you'd be treading new ground.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-16T16:38:03.631+0000","updated":"2016-06-16T16:38:03.631+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/15334510","id":"15334510","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"..or \n-an FS could have some parent limit which it wouldn't try to recurse up beyond.\n-the special case of permission denied is somehow recognised as an IAM issue and skipped. Problem: how to do that?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2016-06-16T19:29:51.728+0000","updated":"2016-06-16T19:29:51.728+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/15555361","id":"15555361","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"I'm thinking we need to be able to recognise the problem of IAM permissions blocking read/write of a parent path, and simply stopping work there. One issue, now that we are doing all parent dir deletes as a single DELETE call, we'll need to recognise a partial failure of the operation due to security checks as a successful operation.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2016-10-07T15:19:37.514+0000","updated":"2016-10-07T15:19:37.514+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/16121451","id":"16121451","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"+ when you start using SSE-C & things, you need to handle decryption problems, otherwise SSE-C created mock dirs can't coexist with any other SSE-* encryption","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2017-08-10T10:49:51.934+0000","updated":"2017-08-10T10:49:51.934+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/16367595","id":"16367595","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"HADOOP-15176 handles the IAM write problem, where the caller doesn't have the permission to create/delete parent directory markers. That addresses the situation where a client has restricted write access.\r\n\r\nI don't think we can handle restricted read access, as it it at odds with the normal world view of a filesystem and so much could break.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2018-02-16T17:10:50.867+0000","updated":"2018-02-16T17:10:50.867+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/16513624","id":"16513624","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Linking to HADOOP-15220.\r\n\r\nFWIW, Hadoop 3.1 has more IAM role support and handling of failed writes up the tree. Failing reads isn't something that is coped with there, which is probably of interest to [~fabbri]. If we do want to handle that situation, it'll cover more than just mkdirs though; I can imagine delete() & the scan for mock directories after a PUT needing coverage","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2018-06-15T09:58:08.447+0000","updated":"2018-06-15T09:58:08.447+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12979535/comment/16596636","id":"16596636","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Moving to branch-3.3\r\n\r\nAs noted, we do want to check up the path, so the current PR isn't going to work. The one thing we can do is handle permissions \r\n\r\n# during that walk up the tree an {{AccessDeniedException}} is raised, that can be caught and used to indicate that  \"you can't do anything up there\", and the mkdirs simply assumes that all is good.\r\n# will need a test in org.apache.hadoop.fs.s3a.auth.ITestAssumeRole which creates a role with the restricted permissions (skipped if s3guard is enabled, BTW), and then verifies that the mkdirs(/a/b/c) fails even as getFileStatus(\"a\") fails because A is blocked. \r\n\r\nNot got time to work on this; postponing to 3.3+, contributions *with that test* welcome.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2018-08-29T17:34:32.255+0000","updated":"2018-08-29T17:34:32.255+0000"}],"maxResults":12,"total":12,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HADOOP-13278/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2zixz:"}}