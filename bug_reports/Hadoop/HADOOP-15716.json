{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [
            {
                "description": "Build scripts",
                "id": "12311543",
                "name": "build",
                "self": "https://issues.apache.org/jira/rest/api/2/component/12311543"
            },
            {
                "id": "12330961",
                "name": "common",
                "self": "https://issues.apache.org/jira/rest/api/2/component/12330961"
            }
        ],
        "created": "2018-09-03T15:55:09.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sreevaddi&avatarId=23370",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sreevaddi&avatarId=23370",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sreevaddi&avatarId=23370",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sreevaddi&avatarId=23370"
            },
            "displayName": "Sree Vaddi",
            "key": "sreevaddi",
            "name": "sreevaddi",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=sreevaddi",
            "timeZone": "America/Tijuana"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2018-09-03T16:18:15.215+0000",
        "customfield_12310222": null,
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i3xokf:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Sep 14 10:01:48 UTC 2018",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "When building hadoop (hdds exactly, but hadoop, too) for the very first time, Tests fails due to the dependency on the native lib (missing libhadoop.so).  As a work around, one can get past by skipping tests.  But it sounds chicken & egg situation, to have installed 'libhadoop.so' before building hadoop for the very first time.\r\n\r\n \r\n\r\nSuggestion to have a first time flag or some logic figure it, then skip the failing tests and/or compile/install libhadoop.so before running those failing tests.\r\n\r\n \r\n\r\n \r\n\r\nHW14169:hadoop svaddi$ mvn clean package install -Phdds -Pdist -Dtar\r\n\r\n\r\n[INFO] Running org.apache.hadoop.util.TestTime\r\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.06 s - in org.apache.hadoop.util.TestTime\r\n[INFO] Running org.apache.hadoop.util.TestNativeCodeLoader\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.117 s <<< FAILURE! - in org.apache.hadoop.util.TestNativeCodeLoader\r\n[ERROR] testNativeCodeLoaded(org.apache.hadoop.util.TestNativeCodeLoader)  Time elapsed: 0.027 s  <<< FAILURE!\r\njava.lang.AssertionError: TestNativeCodeLoader: libhadoop.so testing was required, but libhadoop.so was not loaded.\r\n    at org.junit.Assert.fail(Assert.java:88)\r\n    at org.apache.hadoop.util.TestNativeCodeLoader.testNativeCodeLoaded(TestNativeCodeLoader.java:48)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:498)\r\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)\r\n\r\n[INFO] Running org.apache.hadoop.util.TestLightWeightCache\r\n[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.516 s - in org.apache.hadoop.util.TestLightWeightCache\r\n\r\n[INFO] Running org.apache.hadoop.io.compress.lz4.TestLz4CompressorDecompressor\r\n[WARNING] Tests run: 13, Failures: 0, Errors: 0, Skipped: 13, Time elapsed: 0.128 s - in org.apache.hadoop.io.compress.lz4.TestLz4CompressorDecompressor\r\n[INFO] Running org.apache.hadoop.io.compress.TestCodec\r\n[ERROR] Tests run: 26, Failures: 1, Errors: 0, Skipped: 5, Time elapsed: 55.533 s <<< FAILURE! - in org.apache.hadoop.io.compress.TestCodec\r\n[ERROR] testCodecPoolCompressorReinit(org.apache.hadoop.io.compress.TestCodec)  Time elapsed: 0.031 s  <<< FAILURE!\r\njava.lang.AssertionError: Compressed bytes contrary to configuration\r\n    at org.junit.Assert.fail(Assert.java:88)\r\n    at org.junit.Assert.assertTrue(Assert.java:41)\r\n    at org.apache.hadoop.io.compress.TestCodec.gzipReinitTest(TestCodec.java:431)\r\n    at org.apache.hadoop.io.compress.TestCodec.testCodecPoolCompressorReinit(TestCodec.java:502)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:498)\r\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)\r\n    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)\r\n\r\n[INFO] Running org.apache.hadoop.io.compress.zlib.TestZlibCompressorDecompressor\r\n[WARNING] Tests run: 10, Failures: 0, Errors: 0, Skipped: 10, Time elapsed: 0.293 s - in org.apache.hadoop.io.compress.zlib.TestZlibCompressorDecompressor\r\n\r\n\r\n[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.371 s - in org.apache.hadoop.ipc.TestProtoBufRpc\r\n[INFO] Running org.apache.hadoop.ipc.TestIPC\r\n[ERROR] Tests run: 39, Failures: 0, Errors: 2, Skipped: 1, Time elapsed: 88.727 s <<< FAILURE! - in org.apache.hadoop.ipc.TestIPC\r\n[ERROR] testHttpGetResponse(org.apache.hadoop.ipc.TestIPC)  Time elapsed: 0.016 s  <<< ERROR!\r\njava.net.SocketException: Connection reset\r\n    at java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n    at java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n    at java.net.SocketInputStream.read(SocketInputStream.java:127)\r\n    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)\r\n    at org.apache.hadoop.ipc.TestIPC.doIpcVersionTest(TestIPC.java:1579)\r\n    at org.apache.hadoop.ipc.TestIPC.testHttpGetResponse(TestIPC.java:1074)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:498)\r\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n    at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n\r\n[ERROR] testIpcFromHadoop_0_18_13(org.apache.hadoop.ipc.TestIPC)  Time elapsed: 0.009 s  <<< ERROR!\r\njava.net.SocketException: Connection reset\r\n    at java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n    at java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n    at java.net.SocketInputStream.read(SocketInputStream.java:127)\r\n    at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:100)\r\n    at org.apache.hadoop.ipc.TestIPC.doIpcVersionTest(TestIPC.java:1579)\r\n    at org.apache.hadoop.ipc.TestIPC.testIpcFromHadoop_0_18_13(TestIPC.java:1056)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:498)\r\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n    at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n\r\n[INFO] Running org.apache.hadoop.ipc.TestRPCWaitForProxy\r\n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.31 s - in org.apache.hadoop.ipc.TestRPCWaitForProxy\r\n\r\n\r\n[INFO] Running org.apache.hadoop.fs.TestRawLocalFileSystemContract\r\n[ERROR] Tests run: 44, Failures: 0, Errors: 1, Skipped: 18, Time elapsed: 0.981 s <<< FAILURE! - in org.apache.hadoop.fs.TestRawLocalFileSystemContract\r\n[ERROR] testPermission(org.apache.hadoop.fs.TestRawLocalFileSystemContract)  Time elapsed: 0.296 s  <<< ERROR!\r\njava.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Ljava/lang/String;)Lorg/apache/hadoop/io/nativeio/NativeIO$POSIX$Stat;\r\n    at org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n    at org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:451)\r\n    at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:821)\r\n    at org.apache.hadoop.fs.TestRawLocalFileSystemContract.testPermission(TestRawLocalFileSystemContract.java:112)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:498)\r\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n    at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n\r\n[INFO] Running org.apache.hadoop.fs.TestFsShellTouch\r\n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.492 s - in org.apache.hadoop.fs.TestFsShellTouch\r\n\r\n\r\n[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.35 s - in org.apache.hadoop.conf.TestCommonConfigurationFields\r\n[INFO] Running org.apache.hadoop.conf.TestConfigRedactor\r\n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.182 s - in org.apache.hadoop.conf.TestConfigRedactor\r\n[INFO]\r\n[INFO] Results:\r\n[INFO]\r\n[ERROR] Failures:\r\n[ERROR]   TestCodec.testCodecPoolCompressorReinit:502->gzipReinitTest:431 Compressed bytes contrary to configuration\r\n[ERROR]   TestNativeCodeLoader.testNativeCodeLoaded:48 TestNativeCodeLoader: libhadoop.so testing was required, but libhadoop.so was not loaded.\r\n[ERROR] Errors:\r\n[ERROR]   TestRawLocalFileSystemContract.testPermission:112 » UnsatisfiedLink org.apache...\r\n[ERROR]   TestIPC.testHttpGetResponse:1074->doIpcVersionTest:1579 » Socket Connection re...\r\n[ERROR]   TestIPC.testIpcFromHadoop_0_18_13:1056->doIpcVersionTest:1579 » Socket Connect...\r\n[INFO]\r\n[ERROR] Tests run: 4130, Failures: 2, Errors: 3, Skipped: 358\r\n[INFO]\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary:\r\n[INFO]\r\n[INFO] Apache Hadoop Main 3.2.0-SNAPSHOT .................. SUCCESS [  1.428 s]\r\n[INFO] Apache Hadoop Build Tools .......................... SUCCESS [  2.570 s]\r\n[INFO] Apache Hadoop Project POM .......................... SUCCESS [  1.791 s]\r\n[INFO] Apache Hadoop Annotations .......................... SUCCESS [  4.855 s]\r\n[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  1.197 s]\r\n[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  2.706 s]\r\n[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  7.225 s]\r\n[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 12.220 s]\r\n[INFO] Apache Hadoop Auth ................................. SUCCESS [02:22 min]\r\n[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  4.535 s]\r\n[INFO] Apache Hadoop Common ............................... FAILURE [26:23 min]\r\n[INFO] Apache Hadoop NFS .................................. SKIPPED\r\n[INFO] Apache Hadoop KMS .................................. SKIPPED\r\n...\r\n...\r\n...\r\n[INFO] Apache Hadoop Cloud Storage Project ................ SKIPPED\r\n[INFO] Apache Hadoop Ozone Acceptance Tests 3.2.0-SNAPSHOT  SKIPPED\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 29:27 min\r\n[INFO] Finished at: 2018-09-02T06:00:21-07:00\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.21.0:test (default-test) on project hadoop-common: There are test failures.\r\n[ERROR]\r\n[ERROR] Please refer to /Users/svaddi/SreeVaddi/sources/github/sreev/hadoop/hadoop-common-project/hadoop-common/target/surefire-reports for the individual test results.\r\n[ERROR] Please refer to dump files (if any exist) [date]-jvmRun[N].dump, [date].dumpstream and [date]-jvmRun[N].dumpstream.\r\n[ERROR] -> [Help 1]\r\n[ERROR]",
        "duedate": null,
        "environment": "[INFO] Detecting the operating system and CPU architecture\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] os.detected.name: osx\r\n[INFO] os.detected.arch: x86_64\r\n[INFO] os.detected.version: 10.13\r\n[INFO] os.detected.version.major: 10\r\n[INFO] os.detected.version.minor: 13\r\n[INFO] os.detected.classifier: osx-x86_64",
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095"
            },
            "id": "12310240",
            "key": "HADOOP",
            "name": "Hadoop Common",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310240"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sreevaddi&avatarId=23370",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sreevaddi&avatarId=23370",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sreevaddi&avatarId=23370",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=sreevaddi&avatarId=23370"
            },
            "displayName": "Sree Vaddi",
            "key": "sreevaddi",
            "name": "sreevaddi",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=sreevaddi",
            "timeZone": "America/Tijuana"
        },
        "resolution": null,
        "resolutiondate": null,
        "status": {
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "id": "1",
            "name": "Open",
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "statusCategory": {
                "colorName": "blue-gray",
                "id": 2,
                "key": "new",
                "name": "To Do",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2"
            }
        },
        "subtasks": [],
        "summary": "native library dependency on the very first build",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2018-09-14T10:01:48.000+0000",
        "versions": [{
            "archived": false,
            "description": "3.2 release",
            "id": "12342324",
            "name": "3.2.0",
            "released": false,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12342324"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-15716/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-15716/watchers",
            "watchCount": 4
        },
        "workratio": -1
    },
    "id": "13182667",
    "key": "HADOOP-15716",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182667"
}