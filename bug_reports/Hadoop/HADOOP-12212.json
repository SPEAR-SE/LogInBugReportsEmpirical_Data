{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "description": "Hadoop configuration mechanism.",
            "id": "12310711",
            "name": "conf",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12310711"
        }],
        "created": "2015-07-10T03:55:25.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Joel",
            "key": "games2013",
            "name": "games2013",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=games2013",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2015-07-10T05:02:14.780+0000",
        "customfield_12310222": "1_*:*_1_*:*_4009019_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i2h2xz:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Jul 10 05:02:14 UTC 2015",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "Hi, I am trying to start the namenode but it keeps showing: Failed to start namenode. java.net.BindException: Address already in use;. netstat -a | grep 9000 returns \ntcp        0      0 *:9000                  *:*                     LISTEN     \ntcp6       0      0 [::]:9000               [::]:*                  LISTEN \n\nIs this normal or do I need to kill one of the processes?\n\nThe hdfs-site.xml is given below: <configuration> <property>    <name>dfs.replication<\/name>    <value>1<\/value>  <\/property>  <property>    <name>dfs.namenode.name.dir<\/name>    <value>file:///usr/local/hdfs/namenode<\/value>  <\/property>  <property>    <name>dfs.datanode.data.dir<\/name>    <value>file:///usr/local/hdfs/datanode<\/value>  <\/property> <\/configuration>\n\n\nnamenode logs are given below:\n------------------------------------------\n2015-07-10 00:27:02,513 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n2015-07-10 00:27:02,538 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []\n2015-07-10 00:27:07,549 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2015-07-10 00:27:09,284 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2015-07-10 00:27:09,285 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started\n2015-07-10 00:27:09,339 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000\n2015-07-10 00:27:09,340 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.\n2015-07-10 00:27:12,475 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-07-10 00:27:16,632 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\n2015-07-10 00:27:17,491 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n2015-07-10 00:27:17,702 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n2015-07-10 00:27:17,876 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined\n2015-07-10 00:27:17,941 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n2015-07-10 00:27:17,977 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\n2015-07-10 00:27:17,977 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n2015-07-10 00:27:17,977 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n2015-07-10 00:27:18,441 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\n2015-07-10 00:27:18,525 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\n2015-07-10 00:27:18,747 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070\n2015-07-10 00:27:18,760 INFO org.mortbay.log: jetty-6.1.26\n2015-07-10 00:27:20,832 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\n2015-07-10 00:27:23,404 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\n2015-07-10 00:27:23,416 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\n2015-07-10 00:27:24,034 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.\n2015-07-10 00:27:24,036 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true\n2015-07-10 00:27:24,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\n2015-07-10 00:27:24,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n2015-07-10 00:27:24,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n2015-07-10 00:27:24,854 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Jul 10 00:27:24\n2015-07-10 00:27:24,867 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap\n2015-07-10 00:27:24,883 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit\n2015-07-10 00:27:24,900 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB\n2015-07-10 00:27:24,901 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries\n2015-07-10 00:27:25,563 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false\n2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1\n2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512\n2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1\n2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2\n2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false\n2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000\n2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false\n2015-07-10 00:27:25,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n2015-07-10 00:27:25,638 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = joe (auth:SIMPLE)\n2015-07-10 00:27:25,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup\n2015-07-10 00:27:25,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true\n2015-07-10 00:27:25,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false\n2015-07-10 00:27:25,658 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true\n2015-07-10 00:27:26,354 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap\n2015-07-10 00:27:26,354 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit\n2015-07-10 00:27:26,355 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB\n2015-07-10 00:27:26,355 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries\n2015-07-10 00:27:26,993 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false\n2015-07-10 00:27:26,994 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true\n2015-07-10 00:27:26,994 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384\n2015-07-10 00:27:26,994 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times\n2015-07-10 00:27:27,064 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks\n2015-07-10 00:27:27,069 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit\n2015-07-10 00:27:27,070 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB\n2015-07-10 00:27:27,070 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries\n2015-07-10 00:27:27,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n2015-07-10 00:27:27,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\n2015-07-10 00:27:27,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\n2015-07-10 00:27:27,105 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n2015-07-10 00:27:27,105 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n2015-07-10 00:27:27,105 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n2015-07-10 00:27:27,113 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled\n2015-07-10 00:27:27,113 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n2015-07-10 00:27:27,197 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache\n2015-07-10 00:27:27,197 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit\n2015-07-10 00:27:27,197 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB\n2015-07-10 00:27:27,197 INFO org.apache.hadoop.util.GSet: capacity      = 2^16 = 65536 entries\n2015-07-10 00:27:27,403 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /usr/local/hdfs/namenode/in_use.lock acquired by nodename 11822@joe-virtual-machine\n2015-07-10 00:27:27,882 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /usr/local/hdfs/namenode/current\n2015-07-10 00:27:28,446 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.\n2015-07-10 00:27:28,758 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\n2015-07-10 00:27:28,784 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /usr/local/hdfs/namenode/current/fsimage_0000000000000000000\n2015-07-10 00:27:28,826 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@fd6cd8 expecting start txid #1\n2015-07-10 00:27:28,840 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /usr/local/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002\n2015-07-10 00:27:28,912 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/usr/local/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002' to transaction ID 1\n2015-07-10 00:27:29,079 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /usr/local/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds\n2015-07-10 00:27:29,164 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\n2015-07-10 00:27:29,174 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3\n2015-07-10 00:27:29,854 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups\n2015-07-10 00:27:29,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 2611 msecs\n2015-07-10 00:27:33,403 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000\n2015-07-10 00:27:33,490 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-07-10 00:27:33,625 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state\n2015-07-10 00:27:33,628 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3\n2015-07-10 00:27:33,639 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 57 \n2015-07-10 00:27:33,642 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /usr/local/hdfs/namenode/current/edits_inprogress_0000000000000000003 -> /usr/local/hdfs/namenode/current/edits_0000000000000000003-0000000000000000004\n2015-07-10 00:27:33,781 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state\n2015-07-10 00:27:33,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state\n2015-07-10 00:27:33,885 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\n2015-07-10 00:27:33,905 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...\n2015-07-10 00:27:33,907 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.\n2015-07-10 00:27:33,907 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.\n2015-07-10 00:27:33,970 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.\njava.net.BindException: Problem binding to [localhost:9000] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:721)\n\tat org.apache.hadoop.ipc.Server.bind(Server.java:425)\n\tat org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:574)\n\tat org.apache.hadoop.ipc.Server.<init>(Server.java:2215)\n\tat org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:938)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:534)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:509)\n\tat org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:783)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:343)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:672)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:645)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:810)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:794)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1487)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1553)\nCaused by: java.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:444)\n\tat sun.nio.ch.Net.bind(Net.java:436)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n\tat org.apache.hadoop.ipc.Server.bind(Server.java:408)\n\t... 13 more\n2015-07-10 00:27:34,004 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1\n2015-07-10 00:27:34,007 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at joe-virtual-machine/192.168.197.146\n************************************************************/",
        "duedate": "2015-07-24",
        "environment": "Ubuntu 14.04 trusty",
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [
            "hadoop",
            "hdfs",
            "namenode"
        ],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310240&avatarId=10095",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310240&avatarId=10095",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310240&avatarId=10095",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310240&avatarId=10095"
            },
            "id": "12310240",
            "key": "HADOOP",
            "name": "Hadoop Common",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310240"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Joel",
            "key": "games2013",
            "name": "games2013",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=games2013",
            "timeZone": "Etc/UTC"
        },
        "resolution": {
            "description": "This issue was automatically closed",
            "id": "10000",
            "name": "Auto Closed",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/10000"
        },
        "resolutiondate": "2015-07-10T05:02:14.000+0000",
        "status": {
            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
            "id": "6",
            "name": "Closed",
            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "Hi, I am trying to start the namenode but it keeps showing: Failed to start namenode. java.net.BindException: Address already in use",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2015-07-10T05:02:39.000+0000",
        "versions": [{
            "archived": false,
            "description": "2.7.0 release",
            "id": "12327583",
            "name": "2.7.0",
            "releaseDate": "2015-04-20",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12327583"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-12212/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HADOOP-12212/watchers",
            "watchCount": 2
        },
        "workratio": -1
    },
    "id": "12844041",
    "key": "HADOOP-12212",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12844041"
}